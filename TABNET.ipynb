{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c70e55f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d7d4133",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:\\\\Personal projects\\\\House prices advanced Regression\\\\train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed7d0d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7579d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Target\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "# Features\n",
    "X = df.drop(columns=[\"SalePrice\", \"Id\"])\n",
    "y_log = np.log1p(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3247201",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y_log, \n",
    "                                                  test_size=0.2, \n",
    "\n",
    "                                                  random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0baa60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 79) (292, 79)\n",
      "(1168,) (292,)\n"
     ]
    }
   ],
   "source": [
    "# performing a sanity check\n",
    "print(X_train.shape, X_val.shape)\n",
    "print(y_train.shape, y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d3b6085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1168.000000\n",
       "mean       12.030658\n",
       "std         0.390606\n",
       "min        10.460271\n",
       "25%        11.775297\n",
       "50%        12.013707\n",
       "75%        12.278049\n",
       "max        13.521141\n",
       "Name: SalePrice, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63f988d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ce09b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns: 36 Categorical:   43\n"
     ]
    }
   ],
   "source": [
    "num_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns\n",
    "print(\"Numerical columns:\", len(num_cols) , \"Categorical:  \" , len(cat_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f84c19cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-tabnet in c:\\python310\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\python310\\lib\\site-packages (from pytorch-tabnet) (1.26.4)\n",
      "Requirement already satisfied: scikit_learn>0.21 in c:\\python310\\lib\\site-packages (from pytorch-tabnet) (1.7.2)\n",
      "Requirement already satisfied: scipy>1.4 in c:\\python310\\lib\\site-packages (from pytorch-tabnet) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.3 in c:\\python310\\lib\\site-packages (from pytorch-tabnet) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm>=4.36 in c:\\python310\\lib\\site-packages (from pytorch-tabnet) (4.67.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\python310\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\python310\\lib\\site-packages (from scikit_learn>0.21->pytorch-tabnet) (3.6.0)\n",
      "Requirement already satisfied: filelock in c:\\python310\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\python310\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (4.15.0)\n",
      "Requirement already satisfied: networkx in c:\\python310\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\python310\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (2025.7.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\python310\\lib\\site-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\python310\\lib\\site-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from tqdm>=4.36->pytorch-tabnet) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -p (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-tabnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23722df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1168, 285) (292, 285) (1168, 1) (292, 1)\n"
     ]
    }
   ],
   "source": [
    "numerric_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_pipe = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "pre_processor = ColumnTransformer(transformers=[\n",
    "    ('num', numerric_pipe, num_cols),\n",
    "    ('cat', categorical_pipe, cat_cols)\n",
    "\n",
    "], remainder=\"drop\")\n",
    "# Fit on train only (prevents leakage)\n",
    "X_train_pp = pre_processor.fit_transform(X_train)\n",
    "X_val_pp   = pre_processor.transform(X_val)\n",
    "\n",
    "\n",
    "# TabNet needs dense float32 arrays\n",
    "X_train_pp = X_train_pp.toarray() if hasattr(X_train_pp, \"toarray\") else X_train_pp\n",
    "X_val_pp   = X_val_pp.toarray() if hasattr(X_val_pp, \"toarray\") else X_val_pp\n",
    "\n",
    "X_train_pp = X_train_pp.astype(np.float32)\n",
    "X_val_pp   = X_val_pp.astype(np.float32)\n",
    "\n",
    "y_train_np = y_train.values.reshape(-1, 1).astype(np.float32)  # log target\n",
    "y_val_np   = y_val.values.reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "print(X_train_pp.shape, X_val_pp.shape, y_train_np.shape, y_val_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "926ee344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 147.7285| val_0_rmse: 11.6184 |  0:00:00s\n",
      "epoch 1  | loss: 132.49654| val_0_rmse: 11.34497|  0:00:00s\n",
      "epoch 2  | loss: 121.20547| val_0_rmse: 11.14136|  0:00:00s\n",
      "epoch 3  | loss: 109.39629| val_0_rmse: 10.91993|  0:00:00s\n",
      "epoch 4  | loss: 98.82285| val_0_rmse: 10.66445|  0:00:00s\n",
      "epoch 5  | loss: 86.93841| val_0_rmse: 10.39225|  0:00:01s\n",
      "epoch 6  | loss: 79.15044| val_0_rmse: 10.13826|  0:00:01s\n",
      "epoch 7  | loss: 74.23303| val_0_rmse: 9.84275 |  0:00:01s\n",
      "epoch 8  | loss: 65.72555| val_0_rmse: 9.51688 |  0:00:01s\n",
      "epoch 9  | loss: 58.35172| val_0_rmse: 9.17883 |  0:00:01s\n",
      "epoch 10 | loss: 53.68911| val_0_rmse: 8.79881 |  0:00:01s\n",
      "epoch 11 | loss: 46.80884| val_0_rmse: 8.42198 |  0:00:02s\n",
      "epoch 12 | loss: 44.1147 | val_0_rmse: 8.04785 |  0:00:02s\n",
      "epoch 13 | loss: 38.03884| val_0_rmse: 7.68657 |  0:00:02s\n",
      "epoch 14 | loss: 36.63087| val_0_rmse: 7.32319 |  0:00:02s\n",
      "epoch 15 | loss: 30.59234| val_0_rmse: 6.93193 |  0:00:02s\n",
      "epoch 16 | loss: 29.72782| val_0_rmse: 6.50919 |  0:00:02s\n",
      "epoch 17 | loss: 27.28934| val_0_rmse: 6.07639 |  0:00:02s\n",
      "epoch 18 | loss: 23.66768| val_0_rmse: 5.66919 |  0:00:02s\n",
      "epoch 19 | loss: 20.52987| val_0_rmse: 5.27988 |  0:00:03s\n",
      "epoch 20 | loss: 17.44999| val_0_rmse: 4.90969 |  0:00:03s\n",
      "epoch 21 | loss: 17.64619| val_0_rmse: 4.53868 |  0:00:03s\n",
      "epoch 22 | loss: 15.29894| val_0_rmse: 4.1836  |  0:00:03s\n",
      "epoch 23 | loss: 13.26551| val_0_rmse: 3.86786 |  0:00:03s\n",
      "epoch 24 | loss: 14.10392| val_0_rmse: 3.57306 |  0:00:03s\n",
      "epoch 25 | loss: 12.24624| val_0_rmse: 3.35706 |  0:00:03s\n",
      "epoch 26 | loss: 9.60391 | val_0_rmse: 3.18517 |  0:00:04s\n",
      "epoch 27 | loss: 7.95207 | val_0_rmse: 3.05803 |  0:00:04s\n",
      "epoch 28 | loss: 6.47098 | val_0_rmse: 2.96601 |  0:00:04s\n",
      "epoch 29 | loss: 6.74653 | val_0_rmse: 2.89646 |  0:00:04s\n",
      "epoch 30 | loss: 5.87022 | val_0_rmse: 2.80817 |  0:00:04s\n",
      "epoch 31 | loss: 4.82972 | val_0_rmse: 2.68051 |  0:00:04s\n",
      "epoch 32 | loss: 5.4514  | val_0_rmse: 2.56919 |  0:00:04s\n",
      "epoch 33 | loss: 3.87737 | val_0_rmse: 2.41269 |  0:00:04s\n",
      "epoch 34 | loss: 3.55209 | val_0_rmse: 2.18612 |  0:00:05s\n",
      "epoch 35 | loss: 2.78384 | val_0_rmse: 1.95472 |  0:00:05s\n",
      "epoch 36 | loss: 2.63203 | val_0_rmse: 1.74483 |  0:00:05s\n",
      "epoch 37 | loss: 3.11121 | val_0_rmse: 1.63417 |  0:00:05s\n",
      "epoch 38 | loss: 2.46758 | val_0_rmse: 1.61605 |  0:00:05s\n",
      "epoch 39 | loss: 1.8934  | val_0_rmse: 1.67591 |  0:00:05s\n",
      "epoch 40 | loss: 1.78415 | val_0_rmse: 1.70772 |  0:00:05s\n",
      "epoch 41 | loss: 1.78177 | val_0_rmse: 1.61014 |  0:00:06s\n",
      "epoch 42 | loss: 1.64318 | val_0_rmse: 1.46695 |  0:00:06s\n",
      "epoch 43 | loss: 1.21221 | val_0_rmse: 1.31598 |  0:00:06s\n",
      "epoch 44 | loss: 1.19398 | val_0_rmse: 1.25956 |  0:00:06s\n",
      "epoch 45 | loss: 1.11376 | val_0_rmse: 1.32554 |  0:00:06s\n",
      "epoch 46 | loss: 0.91624 | val_0_rmse: 1.39638 |  0:00:06s\n",
      "epoch 47 | loss: 1.08233 | val_0_rmse: 1.40934 |  0:00:06s\n",
      "epoch 48 | loss: 0.94622 | val_0_rmse: 1.15878 |  0:00:06s\n",
      "epoch 49 | loss: 1.02384 | val_0_rmse: 0.98765 |  0:00:07s\n",
      "epoch 50 | loss: 0.80925 | val_0_rmse: 1.04761 |  0:00:07s\n",
      "epoch 51 | loss: 0.79695 | val_0_rmse: 1.10709 |  0:00:07s\n",
      "epoch 52 | loss: 0.84372 | val_0_rmse: 1.11796 |  0:00:07s\n",
      "epoch 53 | loss: 0.68042 | val_0_rmse: 1.04107 |  0:00:07s\n",
      "epoch 54 | loss: 0.68328 | val_0_rmse: 0.92609 |  0:00:07s\n",
      "epoch 55 | loss: 0.91379 | val_0_rmse: 0.86692 |  0:00:07s\n",
      "epoch 56 | loss: 0.69062 | val_0_rmse: 0.85979 |  0:00:07s\n",
      "epoch 57 | loss: 0.58449 | val_0_rmse: 0.88544 |  0:00:08s\n",
      "epoch 58 | loss: 0.54848 | val_0_rmse: 0.85683 |  0:00:08s\n",
      "epoch 59 | loss: 0.6385  | val_0_rmse: 0.82193 |  0:00:08s\n",
      "epoch 60 | loss: 0.45124 | val_0_rmse: 0.72971 |  0:00:08s\n",
      "epoch 61 | loss: 0.63078 | val_0_rmse: 0.76002 |  0:00:08s\n",
      "epoch 62 | loss: 0.47047 | val_0_rmse: 0.81755 |  0:00:08s\n",
      "epoch 63 | loss: 0.43027 | val_0_rmse: 0.57385 |  0:00:08s\n",
      "epoch 64 | loss: 0.60444 | val_0_rmse: 0.47174 |  0:00:08s\n",
      "epoch 65 | loss: 0.70636 | val_0_rmse: 0.50749 |  0:00:08s\n",
      "epoch 66 | loss: 0.54304 | val_0_rmse: 0.68473 |  0:00:09s\n",
      "epoch 67 | loss: 0.52088 | val_0_rmse: 0.80094 |  0:00:09s\n",
      "epoch 68 | loss: 0.67453 | val_0_rmse: 0.73935 |  0:00:09s\n",
      "epoch 69 | loss: 0.69602 | val_0_rmse: 0.57041 |  0:00:09s\n",
      "epoch 70 | loss: 0.54617 | val_0_rmse: 0.43872 |  0:00:09s\n",
      "epoch 71 | loss: 0.55222 | val_0_rmse: 0.42984 |  0:00:09s\n",
      "epoch 72 | loss: 0.47253 | val_0_rmse: 0.47292 |  0:00:09s\n",
      "epoch 73 | loss: 0.45588 | val_0_rmse: 0.57092 |  0:00:09s\n",
      "epoch 74 | loss: 0.38311 | val_0_rmse: 0.57754 |  0:00:10s\n",
      "epoch 75 | loss: 0.45568 | val_0_rmse: 0.57135 |  0:00:10s\n",
      "epoch 76 | loss: 0.45264 | val_0_rmse: 0.51113 |  0:00:10s\n",
      "epoch 77 | loss: 0.44113 | val_0_rmse: 0.46001 |  0:00:10s\n",
      "epoch 78 | loss: 0.33862 | val_0_rmse: 0.44959 |  0:00:10s\n",
      "epoch 79 | loss: 0.41038 | val_0_rmse: 0.47489 |  0:00:10s\n",
      "epoch 80 | loss: 0.29966 | val_0_rmse: 0.49674 |  0:00:10s\n",
      "epoch 81 | loss: 0.2793  | val_0_rmse: 0.46904 |  0:00:10s\n",
      "epoch 82 | loss: 0.34417 | val_0_rmse: 0.45173 |  0:00:11s\n",
      "epoch 83 | loss: 0.25316 | val_0_rmse: 0.47066 |  0:00:11s\n",
      "epoch 84 | loss: 0.26325 | val_0_rmse: 0.49175 |  0:00:11s\n",
      "epoch 85 | loss: 0.24555 | val_0_rmse: 0.50601 |  0:00:11s\n",
      "epoch 86 | loss: 0.29151 | val_0_rmse: 0.48221 |  0:00:11s\n",
      "epoch 87 | loss: 0.32059 | val_0_rmse: 0.40761 |  0:00:11s\n",
      "epoch 88 | loss: 0.24713 | val_0_rmse: 0.4507  |  0:00:11s\n",
      "epoch 89 | loss: 0.21507 | val_0_rmse: 0.47582 |  0:00:11s\n",
      "epoch 90 | loss: 0.27825 | val_0_rmse: 0.42987 |  0:00:11s\n",
      "epoch 91 | loss: 0.2375  | val_0_rmse: 0.45938 |  0:00:12s\n",
      "epoch 92 | loss: 0.22461 | val_0_rmse: 0.54724 |  0:00:12s\n",
      "epoch 93 | loss: 0.25082 | val_0_rmse: 0.52128 |  0:00:12s\n",
      "epoch 94 | loss: 0.21589 | val_0_rmse: 0.39699 |  0:00:12s\n",
      "epoch 95 | loss: 0.27212 | val_0_rmse: 0.4182  |  0:00:12s\n",
      "epoch 96 | loss: 0.22831 | val_0_rmse: 0.59062 |  0:00:12s\n",
      "epoch 97 | loss: 0.26007 | val_0_rmse: 0.59439 |  0:00:12s\n",
      "epoch 98 | loss: 0.23886 | val_0_rmse: 0.52743 |  0:00:12s\n",
      "epoch 99 | loss: 0.17728 | val_0_rmse: 0.35502 |  0:00:13s\n",
      "epoch 100| loss: 0.24389 | val_0_rmse: 0.35436 |  0:00:13s\n",
      "epoch 101| loss: 0.26873 | val_0_rmse: 0.47159 |  0:00:13s\n",
      "epoch 102| loss: 0.18936 | val_0_rmse: 0.5439  |  0:00:13s\n",
      "epoch 103| loss: 0.20952 | val_0_rmse: 0.53725 |  0:00:13s\n",
      "epoch 104| loss: 0.17318 | val_0_rmse: 0.37879 |  0:00:13s\n",
      "epoch 105| loss: 0.15491 | val_0_rmse: 0.33533 |  0:00:13s\n",
      "epoch 106| loss: 0.15866 | val_0_rmse: 0.41092 |  0:00:13s\n",
      "epoch 107| loss: 0.19416 | val_0_rmse: 0.41033 |  0:00:13s\n",
      "epoch 108| loss: 0.16972 | val_0_rmse: 0.37373 |  0:00:14s\n",
      "epoch 109| loss: 0.15094 | val_0_rmse: 0.36268 |  0:00:14s\n",
      "epoch 110| loss: 0.16951 | val_0_rmse: 0.38541 |  0:00:14s\n",
      "epoch 111| loss: 0.14327 | val_0_rmse: 0.42998 |  0:00:14s\n",
      "epoch 112| loss: 0.19302 | val_0_rmse: 0.4223  |  0:00:14s\n",
      "epoch 113| loss: 0.19385 | val_0_rmse: 0.32604 |  0:00:14s\n",
      "epoch 114| loss: 0.32606 | val_0_rmse: 0.32374 |  0:00:14s\n",
      "epoch 115| loss: 0.25409 | val_0_rmse: 0.32393 |  0:00:15s\n",
      "epoch 116| loss: 0.20665 | val_0_rmse: 0.36133 |  0:00:15s\n",
      "epoch 117| loss: 0.14031 | val_0_rmse: 0.37117 |  0:00:15s\n",
      "epoch 118| loss: 0.15142 | val_0_rmse: 0.31705 |  0:00:15s\n",
      "epoch 119| loss: 0.118   | val_0_rmse: 0.32968 |  0:00:15s\n",
      "epoch 120| loss: 0.10821 | val_0_rmse: 0.40603 |  0:00:15s\n",
      "epoch 121| loss: 0.17922 | val_0_rmse: 0.50645 |  0:00:15s\n",
      "epoch 122| loss: 0.25078 | val_0_rmse: 0.47915 |  0:00:15s\n",
      "epoch 123| loss: 0.23096 | val_0_rmse: 0.3628  |  0:00:16s\n",
      "epoch 124| loss: 0.18429 | val_0_rmse: 0.32397 |  0:00:16s\n",
      "epoch 125| loss: 0.17772 | val_0_rmse: 0.3114  |  0:00:16s\n",
      "epoch 126| loss: 0.16417 | val_0_rmse: 0.35645 |  0:00:16s\n",
      "epoch 127| loss: 0.17142 | val_0_rmse: 0.37369 |  0:00:16s\n",
      "epoch 128| loss: 0.14023 | val_0_rmse: 0.33468 |  0:00:16s\n",
      "epoch 129| loss: 0.1185  | val_0_rmse: 0.3174  |  0:00:16s\n",
      "epoch 130| loss: 0.13116 | val_0_rmse: 0.31576 |  0:00:17s\n",
      "epoch 131| loss: 0.11739 | val_0_rmse: 0.38478 |  0:00:17s\n",
      "epoch 132| loss: 0.13547 | val_0_rmse: 0.39906 |  0:00:17s\n",
      "epoch 133| loss: 0.16278 | val_0_rmse: 0.3328  |  0:00:17s\n",
      "epoch 134| loss: 0.10851 | val_0_rmse: 0.30806 |  0:00:17s\n",
      "epoch 135| loss: 0.13357 | val_0_rmse: 0.32399 |  0:00:17s\n",
      "epoch 136| loss: 0.22303 | val_0_rmse: 0.35418 |  0:00:17s\n",
      "epoch 137| loss: 0.1236  | val_0_rmse: 0.42666 |  0:00:17s\n",
      "epoch 138| loss: 0.15648 | val_0_rmse: 0.44537 |  0:00:17s\n",
      "epoch 139| loss: 0.18393 | val_0_rmse: 0.40575 |  0:00:18s\n",
      "epoch 140| loss: 0.22493 | val_0_rmse: 0.34393 |  0:00:18s\n",
      "epoch 141| loss: 0.18896 | val_0_rmse: 0.34477 |  0:00:18s\n",
      "epoch 142| loss: 0.20856 | val_0_rmse: 0.34114 |  0:00:18s\n",
      "epoch 143| loss: 0.20197 | val_0_rmse: 0.34205 |  0:00:18s\n",
      "epoch 144| loss: 0.13272 | val_0_rmse: 0.40419 |  0:00:18s\n",
      "epoch 145| loss: 0.15782 | val_0_rmse: 0.39127 |  0:00:18s\n",
      "epoch 146| loss: 0.18389 | val_0_rmse: 0.33199 |  0:00:18s\n",
      "epoch 147| loss: 0.12157 | val_0_rmse: 0.32557 |  0:00:18s\n",
      "epoch 148| loss: 0.14737 | val_0_rmse: 0.32867 |  0:00:19s\n",
      "epoch 149| loss: 0.11761 | val_0_rmse: 0.35392 |  0:00:19s\n",
      "epoch 150| loss: 0.10885 | val_0_rmse: 0.3634  |  0:00:19s\n",
      "epoch 151| loss: 0.11919 | val_0_rmse: 0.33254 |  0:00:19s\n",
      "epoch 152| loss: 0.10172 | val_0_rmse: 0.33875 |  0:00:19s\n",
      "epoch 153| loss: 0.1019  | val_0_rmse: 0.36417 |  0:00:19s\n",
      "epoch 154| loss: 0.12503 | val_0_rmse: 0.33897 |  0:00:19s\n",
      "epoch 155| loss: 0.11616 | val_0_rmse: 0.32301 |  0:00:19s\n",
      "epoch 156| loss: 0.11033 | val_0_rmse: 0.31654 |  0:00:20s\n",
      "epoch 157| loss: 0.10458 | val_0_rmse: 0.34749 |  0:00:20s\n",
      "epoch 158| loss: 0.14691 | val_0_rmse: 0.37612 |  0:00:20s\n",
      "epoch 159| loss: 0.12409 | val_0_rmse: 0.35797 |  0:00:20s\n",
      "epoch 160| loss: 0.10835 | val_0_rmse: 0.32646 |  0:00:20s\n",
      "epoch 161| loss: 0.11415 | val_0_rmse: 0.32668 |  0:00:20s\n",
      "epoch 162| loss: 0.15    | val_0_rmse: 0.34937 |  0:00:20s\n",
      "epoch 163| loss: 0.11164 | val_0_rmse: 0.3923  |  0:00:20s\n",
      "epoch 164| loss: 0.11055 | val_0_rmse: 0.38416 |  0:00:21s\n",
      "\n",
      "Early stopping occurred at epoch 164 with best_epoch = 134 and best_val_0_rmse = 0.30806\n",
      "RMSE (USD): $70,580\n",
      "MAE  (USD): $41,506\n",
      "MAPE: 22.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# --- TabNet: model definition + training + evaluation (fixed) ---\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "tabnet_gpu = TabNetRegressor(\n",
    "    n_d=32,\n",
    "    n_a=32,\n",
    "    n_steps=5,\n",
    "    gamma=1.5,\n",
    "    n_independent=2,\n",
    "    n_shared=2,\n",
    "    lambda_sparse=1e-4,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),\n",
    "    mask_type=\"entmax\",\n",
    "    device_name=DEVICE\n",
    ")\n",
    "\n",
    "tabnet_gpu.fit(\n",
    "    X_train_pp, y_train_np,\n",
    "    eval_set=[(X_val_pp, y_val_np)],\n",
    "    eval_metric=[\"rmse\"],\n",
    "    max_epochs=200,\n",
    "    patience=30,\n",
    "    batch_size=2048,\n",
    "    virtual_batch_size=256,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "# ---- Evaluation (use the trained model: tabnet_gpu) ----\n",
    "val_pred_log = tabnet_gpu.predict(X_val_pp).reshape(-1)\n",
    "\n",
    "y_val_usd = np.expm1(y_val)\n",
    "val_pred_usd = np.expm1(val_pred_log)\n",
    "\n",
    "rmse_usd = np.sqrt(mean_squared_error(y_val_usd, val_pred_usd))\n",
    "mae_usd  = mean_absolute_error(y_val_usd, val_pred_usd)\n",
    "mape     = np.mean(np.abs((y_val_usd - val_pred_usd) / y_val_usd)) * 100\n",
    "\n",
    "print(f\"RMSE (USD): ${rmse_usd:,.0f}\")\n",
    "print(f\"MAE  (USD): ${mae_usd:,.0f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050585a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tabnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, mean_absolute_error\n\u001b[1;32m----> 4\u001b[0m val_pred_log \u001b[38;5;241m=\u001b[39m \u001b[43mtabnet\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_val_pp)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m y_val_usd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpm1(y_val)\n\u001b[0;32m      7\u001b[0m val_pred_usd \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpm1(val_pred_log)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tabnet' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc64c20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-28 13:03:09,985] A new study created in memory with name: no-name-aea485b3-defc-48aa-b5cb-6e82bd95346e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 236.94077| val_0_rmse: 12.36984|  0:00:00s\n",
      "epoch 1  | loss: 215.0728| val_0_rmse: 12.07688|  0:00:00s\n",
      "epoch 2  | loss: 201.21378| val_0_rmse: 11.87047|  0:00:00s\n",
      "epoch 3  | loss: 182.64177| val_0_rmse: 11.70593|  0:00:01s\n",
      "epoch 4  | loss: 168.3806| val_0_rmse: 11.56545|  0:00:01s\n",
      "epoch 5  | loss: 155.49223| val_0_rmse: 11.40629|  0:00:01s\n",
      "epoch 6  | loss: 140.72847| val_0_rmse: 11.24254|  0:00:01s\n",
      "epoch 7  | loss: 130.78879| val_0_rmse: 11.08058|  0:00:02s\n",
      "epoch 8  | loss: 118.72376| val_0_rmse: 10.88294|  0:00:02s\n",
      "epoch 9  | loss: 108.43079| val_0_rmse: 10.67912|  0:00:02s\n",
      "epoch 10 | loss: 94.71946| val_0_rmse: 10.42032|  0:00:03s\n",
      "epoch 11 | loss: 86.02881| val_0_rmse: 10.10105|  0:00:03s\n",
      "epoch 12 | loss: 78.40481| val_0_rmse: 9.80875 |  0:00:03s\n",
      "epoch 13 | loss: 68.4954 | val_0_rmse: 9.46372 |  0:00:03s\n",
      "epoch 14 | loss: 63.39425| val_0_rmse: 9.2287  |  0:00:04s\n",
      "epoch 15 | loss: 55.98323| val_0_rmse: 8.94937 |  0:00:04s\n",
      "epoch 16 | loss: 49.75786| val_0_rmse: 8.63064 |  0:00:04s\n",
      "epoch 17 | loss: 43.04226| val_0_rmse: 8.21418 |  0:00:04s\n",
      "epoch 18 | loss: 39.58885| val_0_rmse: 7.9825  |  0:00:04s\n",
      "epoch 19 | loss: 36.16748| val_0_rmse: 7.6866  |  0:00:05s\n",
      "epoch 20 | loss: 33.0517 | val_0_rmse: 7.49    |  0:00:05s\n",
      "epoch 21 | loss: 32.17414| val_0_rmse: 7.1606  |  0:00:05s\n",
      "epoch 22 | loss: 27.97306| val_0_rmse: 6.829   |  0:00:06s\n",
      "epoch 23 | loss: 24.4101 | val_0_rmse: 6.55547 |  0:00:06s\n",
      "epoch 24 | loss: 24.04206| val_0_rmse: 6.21062 |  0:00:06s\n",
      "epoch 25 | loss: 21.87959| val_0_rmse: 5.8929  |  0:00:06s\n",
      "epoch 26 | loss: 19.97903| val_0_rmse: 5.45938 |  0:00:07s\n",
      "epoch 27 | loss: 23.91709| val_0_rmse: 5.19772 |  0:00:07s\n",
      "epoch 28 | loss: 20.96206| val_0_rmse: 5.1525  |  0:00:07s\n",
      "epoch 29 | loss: 20.32326| val_0_rmse: 5.00694 |  0:00:08s\n",
      "epoch 30 | loss: 18.74432| val_0_rmse: 4.59144 |  0:00:08s\n",
      "epoch 31 | loss: 17.37183| val_0_rmse: 4.58105 |  0:00:08s\n",
      "epoch 32 | loss: 19.8375 | val_0_rmse: 4.3833  |  0:00:08s\n",
      "epoch 33 | loss: 16.88534| val_0_rmse: 4.27655 |  0:00:09s\n",
      "epoch 34 | loss: 15.73662| val_0_rmse: 4.16669 |  0:00:09s\n",
      "epoch 35 | loss: 15.03131| val_0_rmse: 4.11859 |  0:00:09s\n",
      "epoch 36 | loss: 12.63884| val_0_rmse: 3.93582 |  0:00:09s\n",
      "epoch 37 | loss: 13.09727| val_0_rmse: 3.966   |  0:00:10s\n",
      "epoch 38 | loss: 9.92628 | val_0_rmse: 3.93935 |  0:00:10s\n",
      "epoch 39 | loss: 10.37629| val_0_rmse: 3.9066  |  0:00:10s\n",
      "epoch 40 | loss: 9.4911  | val_0_rmse: 3.85714 |  0:00:10s\n",
      "epoch 41 | loss: 8.88949 | val_0_rmse: 3.75286 |  0:00:11s\n",
      "epoch 42 | loss: 8.41635 | val_0_rmse: 3.6659  |  0:00:11s\n",
      "epoch 43 | loss: 8.04817 | val_0_rmse: 3.58685 |  0:00:11s\n",
      "epoch 44 | loss: 7.5198  | val_0_rmse: 3.41524 |  0:00:12s\n",
      "epoch 45 | loss: 6.65166 | val_0_rmse: 3.26275 |  0:00:12s\n",
      "epoch 46 | loss: 6.40754 | val_0_rmse: 3.09178 |  0:00:12s\n",
      "epoch 47 | loss: 5.90551 | val_0_rmse: 2.81049 |  0:00:12s\n",
      "epoch 48 | loss: 5.68004 | val_0_rmse: 2.7429  |  0:00:13s\n",
      "epoch 49 | loss: 4.66636 | val_0_rmse: 2.49126 |  0:00:13s\n",
      "epoch 50 | loss: 4.14492 | val_0_rmse: 2.17046 |  0:00:13s\n",
      "epoch 51 | loss: 3.88776 | val_0_rmse: 1.90203 |  0:00:14s\n",
      "epoch 52 | loss: 3.63909 | val_0_rmse: 1.69045 |  0:00:14s\n",
      "epoch 53 | loss: 3.33429 | val_0_rmse: 1.66165 |  0:00:14s\n",
      "epoch 54 | loss: 4.09288 | val_0_rmse: 1.4244  |  0:00:14s\n",
      "epoch 55 | loss: 3.61336 | val_0_rmse: 1.25491 |  0:00:15s\n",
      "epoch 56 | loss: 3.60389 | val_0_rmse: 1.20976 |  0:00:15s\n",
      "epoch 57 | loss: 3.07984 | val_0_rmse: 1.13455 |  0:00:15s\n",
      "epoch 58 | loss: 2.5367  | val_0_rmse: 1.06522 |  0:00:16s\n",
      "epoch 59 | loss: 2.63407 | val_0_rmse: 1.18977 |  0:00:16s\n",
      "epoch 60 | loss: 2.3794  | val_0_rmse: 1.07939 |  0:00:16s\n",
      "epoch 61 | loss: 1.97709 | val_0_rmse: 1.30985 |  0:00:16s\n",
      "epoch 62 | loss: 2.09611 | val_0_rmse: 0.81945 |  0:00:16s\n",
      "epoch 63 | loss: 2.12299 | val_0_rmse: 0.75306 |  0:00:17s\n",
      "epoch 64 | loss: 1.77676 | val_0_rmse: 0.73706 |  0:00:17s\n",
      "epoch 65 | loss: 1.71754 | val_0_rmse: 0.66134 |  0:00:17s\n",
      "epoch 66 | loss: 1.82732 | val_0_rmse: 0.64172 |  0:00:17s\n",
      "epoch 67 | loss: 1.61664 | val_0_rmse: 0.65223 |  0:00:18s\n",
      "epoch 68 | loss: 1.40184 | val_0_rmse: 0.68913 |  0:00:18s\n",
      "epoch 69 | loss: 1.70299 | val_0_rmse: 0.6678  |  0:00:18s\n",
      "epoch 70 | loss: 1.61548 | val_0_rmse: 0.60995 |  0:00:18s\n",
      "epoch 71 | loss: 1.57039 | val_0_rmse: 0.56422 |  0:00:19s\n",
      "epoch 72 | loss: 1.43485 | val_0_rmse: 0.55255 |  0:00:19s\n",
      "epoch 73 | loss: 1.3543  | val_0_rmse: 0.56838 |  0:00:19s\n",
      "epoch 74 | loss: 1.38809 | val_0_rmse: 0.58473 |  0:00:19s\n",
      "epoch 75 | loss: 1.57401 | val_0_rmse: 0.58296 |  0:00:20s\n",
      "epoch 76 | loss: 1.22588 | val_0_rmse: 0.5874  |  0:00:20s\n",
      "epoch 77 | loss: 1.20728 | val_0_rmse: 0.625   |  0:00:20s\n",
      "epoch 78 | loss: 1.2476  | val_0_rmse: 0.6637  |  0:00:20s\n",
      "epoch 79 | loss: 1.19897 | val_0_rmse: 0.63414 |  0:00:21s\n",
      "epoch 80 | loss: 1.13264 | val_0_rmse: 0.58257 |  0:00:21s\n",
      "epoch 81 | loss: 1.13826 | val_0_rmse: 0.55771 |  0:00:21s\n",
      "epoch 82 | loss: 0.99883 | val_0_rmse: 0.56229 |  0:00:21s\n",
      "epoch 83 | loss: 1.11113 | val_0_rmse: 0.57211 |  0:00:21s\n",
      "epoch 84 | loss: 1.10424 | val_0_rmse: 0.59012 |  0:00:22s\n",
      "epoch 85 | loss: 1.08645 | val_0_rmse: 0.5819  |  0:00:22s\n",
      "epoch 86 | loss: 0.94626 | val_0_rmse: 0.56212 |  0:00:22s\n",
      "epoch 87 | loss: 1.05914 | val_0_rmse: 0.52813 |  0:00:22s\n",
      "epoch 88 | loss: 1.10904 | val_0_rmse: 0.51632 |  0:00:23s\n",
      "epoch 89 | loss: 1.00599 | val_0_rmse: 0.52357 |  0:00:23s\n",
      "epoch 90 | loss: 0.89862 | val_0_rmse: 0.55863 |  0:00:23s\n",
      "epoch 91 | loss: 0.86069 | val_0_rmse: 0.56706 |  0:00:23s\n",
      "epoch 92 | loss: 0.79973 | val_0_rmse: 0.54798 |  0:00:24s\n",
      "epoch 93 | loss: 0.83222 | val_0_rmse: 0.54329 |  0:00:24s\n",
      "epoch 94 | loss: 0.95496 | val_0_rmse: 0.56727 |  0:00:24s\n",
      "epoch 95 | loss: 0.73518 | val_0_rmse: 0.52271 |  0:00:24s\n",
      "epoch 96 | loss: 0.84082 | val_0_rmse: 0.49378 |  0:00:25s\n",
      "epoch 97 | loss: 0.81572 | val_0_rmse: 0.53675 |  0:00:25s\n",
      "epoch 98 | loss: 0.83739 | val_0_rmse: 0.53657 |  0:00:25s\n",
      "epoch 99 | loss: 0.8475  | val_0_rmse: 0.50729 |  0:00:25s\n",
      "epoch 100| loss: 0.80772 | val_0_rmse: 0.50006 |  0:00:26s\n",
      "epoch 101| loss: 0.73845 | val_0_rmse: 0.52057 |  0:00:26s\n",
      "epoch 102| loss: 0.6887  | val_0_rmse: 0.51336 |  0:00:26s\n",
      "epoch 103| loss: 0.79217 | val_0_rmse: 0.51704 |  0:00:27s\n",
      "epoch 104| loss: 0.72487 | val_0_rmse: 0.51149 |  0:00:27s\n",
      "epoch 105| loss: 0.7527  | val_0_rmse: 0.50955 |  0:00:27s\n",
      "epoch 106| loss: 0.675   | val_0_rmse: 0.50904 |  0:00:27s\n",
      "epoch 107| loss: 0.67875 | val_0_rmse: 0.54084 |  0:00:28s\n",
      "epoch 108| loss: 0.8046  | val_0_rmse: 0.56376 |  0:00:28s\n",
      "epoch 109| loss: 0.87636 | val_0_rmse: 0.50994 |  0:00:28s\n",
      "epoch 110| loss: 0.69829 | val_0_rmse: 0.52763 |  0:00:28s\n",
      "epoch 111| loss: 0.71288 | val_0_rmse: 0.56402 |  0:00:28s\n",
      "epoch 112| loss: 0.71844 | val_0_rmse: 0.53506 |  0:00:29s\n",
      "epoch 113| loss: 0.6518  | val_0_rmse: 0.48256 |  0:00:29s\n",
      "epoch 114| loss: 0.6676  | val_0_rmse: 0.47349 |  0:00:29s\n",
      "epoch 115| loss: 0.55066 | val_0_rmse: 0.47136 |  0:00:29s\n",
      "epoch 116| loss: 0.63565 | val_0_rmse: 0.46648 |  0:00:30s\n",
      "epoch 117| loss: 0.58304 | val_0_rmse: 0.47847 |  0:00:30s\n",
      "epoch 118| loss: 0.61845 | val_0_rmse: 0.4906  |  0:00:30s\n",
      "epoch 119| loss: 0.6324  | val_0_rmse: 0.48407 |  0:00:30s\n",
      "epoch 120| loss: 0.51268 | val_0_rmse: 0.45303 |  0:00:31s\n",
      "epoch 121| loss: 0.52553 | val_0_rmse: 0.44837 |  0:00:31s\n",
      "epoch 122| loss: 0.58298 | val_0_rmse: 0.45197 |  0:00:31s\n",
      "epoch 123| loss: 0.54067 | val_0_rmse: 0.46558 |  0:00:31s\n",
      "epoch 124| loss: 0.51912 | val_0_rmse: 0.46205 |  0:00:32s\n",
      "epoch 125| loss: 0.5293  | val_0_rmse: 0.4487  |  0:00:32s\n",
      "epoch 126| loss: 0.57628 | val_0_rmse: 0.44525 |  0:00:32s\n",
      "epoch 127| loss: 0.47737 | val_0_rmse: 0.46405 |  0:00:32s\n",
      "epoch 128| loss: 0.55909 | val_0_rmse: 0.50479 |  0:00:33s\n",
      "epoch 129| loss: 0.49003 | val_0_rmse: 0.52571 |  0:00:33s\n",
      "epoch 130| loss: 0.54223 | val_0_rmse: 0.45855 |  0:00:33s\n",
      "epoch 131| loss: 0.49243 | val_0_rmse: 0.44507 |  0:00:33s\n",
      "epoch 132| loss: 0.56462 | val_0_rmse: 0.43763 |  0:00:33s\n",
      "epoch 133| loss: 0.60732 | val_0_rmse: 0.44775 |  0:00:34s\n",
      "epoch 134| loss: 0.57522 | val_0_rmse: 0.47769 |  0:00:34s\n",
      "epoch 135| loss: 0.57281 | val_0_rmse: 0.4293  |  0:00:34s\n",
      "epoch 136| loss: 0.51771 | val_0_rmse: 0.4656  |  0:00:34s\n",
      "epoch 137| loss: 0.50971 | val_0_rmse: 0.49052 |  0:00:35s\n",
      "epoch 138| loss: 0.44919 | val_0_rmse: 0.49495 |  0:00:35s\n",
      "epoch 139| loss: 0.56423 | val_0_rmse: 0.47592 |  0:00:35s\n",
      "epoch 140| loss: 0.46043 | val_0_rmse: 0.47057 |  0:00:35s\n",
      "epoch 141| loss: 0.42323 | val_0_rmse: 0.43521 |  0:00:36s\n",
      "epoch 142| loss: 0.42838 | val_0_rmse: 0.42418 |  0:00:36s\n",
      "epoch 143| loss: 0.41132 | val_0_rmse: 0.41343 |  0:00:36s\n",
      "epoch 144| loss: 0.43475 | val_0_rmse: 0.41212 |  0:00:37s\n",
      "epoch 145| loss: 0.39554 | val_0_rmse: 0.54255 |  0:00:37s\n",
      "epoch 146| loss: 0.4218  | val_0_rmse: 0.49324 |  0:00:37s\n",
      "epoch 147| loss: 0.33358 | val_0_rmse: 0.46772 |  0:00:37s\n",
      "epoch 148| loss: 0.37933 | val_0_rmse: 0.45894 |  0:00:37s\n",
      "epoch 149| loss: 0.36109 | val_0_rmse: 0.42819 |  0:00:38s\n",
      "epoch 150| loss: 0.39593 | val_0_rmse: 0.42828 |  0:00:38s\n",
      "epoch 151| loss: 0.36708 | val_0_rmse: 0.4285  |  0:00:38s\n",
      "epoch 152| loss: 0.38358 | val_0_rmse: 0.42028 |  0:00:38s\n",
      "epoch 153| loss: 0.37237 | val_0_rmse: 0.42147 |  0:00:39s\n",
      "epoch 154| loss: 0.35041 | val_0_rmse: 0.42009 |  0:00:39s\n",
      "epoch 155| loss: 0.34027 | val_0_rmse: 0.41112 |  0:00:39s\n",
      "epoch 156| loss: 0.36947 | val_0_rmse: 0.40343 |  0:00:39s\n",
      "epoch 157| loss: 0.37068 | val_0_rmse: 0.40815 |  0:00:40s\n",
      "epoch 158| loss: 0.3594  | val_0_rmse: 0.39575 |  0:00:40s\n",
      "epoch 159| loss: 0.3567  | val_0_rmse: 0.42389 |  0:00:40s\n",
      "epoch 160| loss: 0.32791 | val_0_rmse: 0.42331 |  0:00:40s\n",
      "epoch 161| loss: 0.32106 | val_0_rmse: 0.42708 |  0:00:41s\n",
      "epoch 162| loss: 0.37733 | val_0_rmse: 0.39503 |  0:00:41s\n",
      "epoch 163| loss: 0.30692 | val_0_rmse: 0.46098 |  0:00:41s\n",
      "epoch 164| loss: 0.36509 | val_0_rmse: 0.55052 |  0:00:41s\n",
      "epoch 165| loss: 0.29657 | val_0_rmse: 0.47169 |  0:00:42s\n",
      "epoch 166| loss: 0.36087 | val_0_rmse: 0.44396 |  0:00:42s\n",
      "epoch 167| loss: 0.40834 | val_0_rmse: 0.4004  |  0:00:42s\n",
      "epoch 168| loss: 0.35115 | val_0_rmse: 0.41592 |  0:00:42s\n",
      "epoch 169| loss: 0.32078 | val_0_rmse: 0.43962 |  0:00:43s\n",
      "epoch 170| loss: 0.37683 | val_0_rmse: 0.4311  |  0:00:43s\n",
      "epoch 171| loss: 0.37846 | val_0_rmse: 0.36803 |  0:00:43s\n",
      "epoch 172| loss: 0.3318  | val_0_rmse: 0.39616 |  0:00:43s\n",
      "epoch 173| loss: 0.37494 | val_0_rmse: 0.44923 |  0:00:44s\n",
      "epoch 174| loss: 0.46764 | val_0_rmse: 0.36827 |  0:00:44s\n",
      "epoch 175| loss: 0.31871 | val_0_rmse: 0.48821 |  0:00:44s\n",
      "epoch 176| loss: 0.50279 | val_0_rmse: 0.58906 |  0:00:44s\n",
      "epoch 177| loss: 0.59113 | val_0_rmse: 0.47143 |  0:00:45s\n",
      "epoch 178| loss: 0.39356 | val_0_rmse: 0.36129 |  0:00:45s\n",
      "epoch 179| loss: 0.32357 | val_0_rmse: 0.50783 |  0:00:45s\n",
      "epoch 180| loss: 0.54779 | val_0_rmse: 0.49324 |  0:00:45s\n",
      "epoch 181| loss: 0.54067 | val_0_rmse: 0.45491 |  0:00:46s\n",
      "epoch 182| loss: 0.35503 | val_0_rmse: 0.38948 |  0:00:46s\n",
      "epoch 183| loss: 0.29418 | val_0_rmse: 0.37731 |  0:00:46s\n",
      "epoch 184| loss: 0.3302  | val_0_rmse: 0.39325 |  0:00:46s\n",
      "epoch 185| loss: 0.27631 | val_0_rmse: 0.37179 |  0:00:46s\n",
      "epoch 186| loss: 0.28426 | val_0_rmse: 0.39897 |  0:00:47s\n",
      "epoch 187| loss: 0.26098 | val_0_rmse: 0.39301 |  0:00:47s\n",
      "epoch 188| loss: 0.30525 | val_0_rmse: 0.37266 |  0:00:47s\n",
      "epoch 189| loss: 0.3052  | val_0_rmse: 0.37445 |  0:00:47s\n",
      "epoch 190| loss: 0.29136 | val_0_rmse: 0.38678 |  0:00:48s\n",
      "epoch 191| loss: 0.28823 | val_0_rmse: 0.38877 |  0:00:48s\n",
      "epoch 192| loss: 0.26828 | val_0_rmse: 0.41098 |  0:00:48s\n",
      "epoch 193| loss: 0.2741  | val_0_rmse: 0.40892 |  0:00:48s\n",
      "epoch 194| loss: 0.26836 | val_0_rmse: 0.41153 |  0:00:49s\n",
      "epoch 195| loss: 0.24641 | val_0_rmse: 0.4115  |  0:00:49s\n",
      "epoch 196| loss: 0.25058 | val_0_rmse: 0.3913  |  0:00:49s\n",
      "epoch 197| loss: 0.2533  | val_0_rmse: 0.38452 |  0:00:49s\n",
      "epoch 198| loss: 0.2532  | val_0_rmse: 0.39031 |  0:00:50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:04:00,507] Trial 0 finished with value: 0.3612857290465453 and parameters: {'n_d': 64, 'n_a': 48, 'n_steps': 5, 'gamma': 1.997060055576506, 'n_independent': 3, 'n_shared': 2, 'lambda_sparse': 1e-06, 'mask_type': 'sparsemax', 'lr': 0.005993026166684755, 'batch_size': 2048, 'virtual_batch_size': 64}. Best is trial 0 with value: 0.3612857290465453.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199| loss: 0.25446 | val_0_rmse: 0.38802 |  0:00:50s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 178 and best_val_0_rmse = 0.36129\n",
      "Trial 000 | rmse_log=0.36129 | RMSE$=72,398 | MAE$=49,806 | MAPE=30.66% | n_d/n_a=64/48 steps=5 lr=0.00599 batch=2048 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 130.28296| val_0_rmse: 11.78817|  0:00:00s\n",
      "epoch 1  | loss: 124.02073| val_0_rmse: 11.69137|  0:00:00s\n",
      "epoch 2  | loss: 117.20892| val_0_rmse: 11.54193|  0:00:01s\n",
      "epoch 3  | loss: 109.40998| val_0_rmse: 11.30943|  0:00:01s\n",
      "epoch 4  | loss: 103.6444| val_0_rmse: 11.13621|  0:00:01s\n",
      "epoch 5  | loss: 95.47769| val_0_rmse: 10.90759|  0:00:02s\n",
      "epoch 6  | loss: 89.62303| val_0_rmse: 10.68467|  0:00:02s\n",
      "epoch 7  | loss: 84.15169| val_0_rmse: 10.46933|  0:00:02s\n",
      "epoch 8  | loss: 81.01638| val_0_rmse: 10.23221|  0:00:03s\n",
      "epoch 9  | loss: 73.07444| val_0_rmse: 9.99403 |  0:00:03s\n",
      "epoch 10 | loss: 69.90287| val_0_rmse: 9.75402 |  0:00:03s\n",
      "epoch 11 | loss: 62.90485| val_0_rmse: 9.4429  |  0:00:04s\n",
      "epoch 12 | loss: 60.34529| val_0_rmse: 9.13869 |  0:00:04s\n",
      "epoch 13 | loss: 54.80289| val_0_rmse: 8.77303 |  0:00:04s\n",
      "epoch 14 | loss: 49.33863| val_0_rmse: 8.46781 |  0:00:05s\n",
      "epoch 15 | loss: 44.91549| val_0_rmse: 8.11499 |  0:00:05s\n",
      "epoch 16 | loss: 42.74916| val_0_rmse: 7.95494 |  0:00:06s\n",
      "epoch 17 | loss: 39.5403 | val_0_rmse: 7.85588 |  0:00:06s\n",
      "epoch 18 | loss: 36.3115 | val_0_rmse: 7.48217 |  0:00:06s\n",
      "epoch 19 | loss: 34.52273| val_0_rmse: 7.18376 |  0:00:07s\n",
      "epoch 20 | loss: 29.54094| val_0_rmse: 6.97625 |  0:00:07s\n",
      "epoch 21 | loss: 27.82137| val_0_rmse: 6.66281 |  0:00:07s\n",
      "epoch 22 | loss: 27.11805| val_0_rmse: 6.50999 |  0:00:08s\n",
      "epoch 23 | loss: 24.93066| val_0_rmse: 6.35996 |  0:00:08s\n",
      "epoch 24 | loss: 22.23157| val_0_rmse: 6.0192  |  0:00:08s\n",
      "epoch 25 | loss: 21.02187| val_0_rmse: 5.89062 |  0:00:09s\n",
      "epoch 26 | loss: 20.82531| val_0_rmse: 5.63719 |  0:00:09s\n",
      "epoch 27 | loss: 18.33339| val_0_rmse: 5.53629 |  0:00:09s\n",
      "epoch 28 | loss: 18.0115 | val_0_rmse: 5.17365 |  0:00:10s\n",
      "epoch 29 | loss: 16.943  | val_0_rmse: 5.04902 |  0:00:10s\n",
      "epoch 30 | loss: 15.6503 | val_0_rmse: 4.79346 |  0:00:10s\n",
      "epoch 31 | loss: 15.11573| val_0_rmse: 4.50913 |  0:00:11s\n",
      "epoch 32 | loss: 13.35464| val_0_rmse: 4.33806 |  0:00:11s\n",
      "epoch 33 | loss: 13.47363| val_0_rmse: 4.05529 |  0:00:11s\n",
      "epoch 34 | loss: 11.65149| val_0_rmse: 3.97297 |  0:00:12s\n",
      "epoch 35 | loss: 11.51023| val_0_rmse: 3.78079 |  0:00:12s\n",
      "epoch 36 | loss: 12.19569| val_0_rmse: 3.63748 |  0:00:12s\n",
      "epoch 37 | loss: 10.49154| val_0_rmse: 3.49852 |  0:00:13s\n",
      "epoch 38 | loss: 9.80318 | val_0_rmse: 3.47339 |  0:00:13s\n",
      "epoch 39 | loss: 9.69496 | val_0_rmse: 3.37555 |  0:00:14s\n",
      "epoch 40 | loss: 9.49153 | val_0_rmse: 3.19422 |  0:00:14s\n",
      "epoch 41 | loss: 9.41399 | val_0_rmse: 3.11243 |  0:00:14s\n",
      "epoch 42 | loss: 7.55933 | val_0_rmse: 3.01418 |  0:00:15s\n",
      "epoch 43 | loss: 7.80685 | val_0_rmse: 2.94582 |  0:00:15s\n",
      "epoch 44 | loss: 7.63594 | val_0_rmse: 2.90766 |  0:00:15s\n",
      "epoch 45 | loss: 7.54489 | val_0_rmse: 2.8237  |  0:00:16s\n",
      "epoch 46 | loss: 7.38477 | val_0_rmse: 2.79498 |  0:00:16s\n",
      "epoch 47 | loss: 6.57577 | val_0_rmse: 2.6453  |  0:00:16s\n",
      "epoch 48 | loss: 6.29788 | val_0_rmse: 2.48084 |  0:00:17s\n",
      "epoch 49 | loss: 6.05259 | val_0_rmse: 2.33048 |  0:00:17s\n",
      "epoch 50 | loss: 5.00054 | val_0_rmse: 2.24513 |  0:00:17s\n",
      "epoch 51 | loss: 4.68607 | val_0_rmse: 2.0985  |  0:00:18s\n",
      "epoch 52 | loss: 4.48073 | val_0_rmse: 1.94996 |  0:00:18s\n",
      "epoch 53 | loss: 4.59362 | val_0_rmse: 1.87686 |  0:00:18s\n",
      "epoch 54 | loss: 4.48145 | val_0_rmse: 1.89949 |  0:00:19s\n",
      "epoch 55 | loss: 3.98857 | val_0_rmse: 1.90748 |  0:00:19s\n",
      "epoch 56 | loss: 4.67231 | val_0_rmse: 1.90453 |  0:00:20s\n",
      "epoch 57 | loss: 3.63214 | val_0_rmse: 1.80166 |  0:00:20s\n",
      "epoch 58 | loss: 3.64006 | val_0_rmse: 1.75357 |  0:00:20s\n",
      "epoch 59 | loss: 3.64251 | val_0_rmse: 1.67123 |  0:00:21s\n",
      "epoch 60 | loss: 3.09071 | val_0_rmse: 1.57527 |  0:00:21s\n",
      "epoch 61 | loss: 3.79547 | val_0_rmse: 1.49838 |  0:00:21s\n",
      "epoch 62 | loss: 3.43212 | val_0_rmse: 1.38368 |  0:00:22s\n",
      "epoch 63 | loss: 3.34306 | val_0_rmse: 1.39839 |  0:00:22s\n",
      "epoch 64 | loss: 3.16227 | val_0_rmse: 1.32269 |  0:00:22s\n",
      "epoch 65 | loss: 2.82569 | val_0_rmse: 1.21445 |  0:00:23s\n",
      "epoch 66 | loss: 2.41233 | val_0_rmse: 1.18932 |  0:00:23s\n",
      "epoch 67 | loss: 2.39236 | val_0_rmse: 1.17903 |  0:00:23s\n",
      "epoch 68 | loss: 2.35694 | val_0_rmse: 1.1021  |  0:00:24s\n",
      "epoch 69 | loss: 2.50136 | val_0_rmse: 0.94191 |  0:00:24s\n",
      "epoch 70 | loss: 2.45687 | val_0_rmse: 0.92918 |  0:00:24s\n",
      "epoch 71 | loss: 1.93205 | val_0_rmse: 0.90318 |  0:00:25s\n",
      "epoch 72 | loss: 2.56825 | val_0_rmse: 0.93733 |  0:00:25s\n",
      "epoch 73 | loss: 2.23689 | val_0_rmse: 0.91429 |  0:00:25s\n",
      "epoch 74 | loss: 1.84578 | val_0_rmse: 0.93459 |  0:00:26s\n",
      "epoch 75 | loss: 1.88237 | val_0_rmse: 0.9129  |  0:00:26s\n",
      "epoch 76 | loss: 1.55125 | val_0_rmse: 0.84896 |  0:00:26s\n",
      "epoch 77 | loss: 1.74249 | val_0_rmse: 0.79737 |  0:00:27s\n",
      "epoch 78 | loss: 1.75334 | val_0_rmse: 0.84719 |  0:00:27s\n",
      "epoch 79 | loss: 1.52244 | val_0_rmse: 0.86433 |  0:00:28s\n",
      "epoch 80 | loss: 1.42393 | val_0_rmse: 0.82872 |  0:00:28s\n",
      "epoch 81 | loss: 1.53877 | val_0_rmse: 0.76792 |  0:00:28s\n",
      "epoch 82 | loss: 1.30795 | val_0_rmse: 0.73681 |  0:00:29s\n",
      "epoch 83 | loss: 1.22234 | val_0_rmse: 0.70798 |  0:00:29s\n",
      "epoch 84 | loss: 1.22014 | val_0_rmse: 0.67216 |  0:00:29s\n",
      "epoch 85 | loss: 1.27777 | val_0_rmse: 0.64069 |  0:00:30s\n",
      "epoch 86 | loss: 1.21166 | val_0_rmse: 0.69729 |  0:00:30s\n",
      "epoch 87 | loss: 1.21198 | val_0_rmse: 0.66351 |  0:00:30s\n",
      "epoch 88 | loss: 1.12035 | val_0_rmse: 0.63889 |  0:00:31s\n",
      "epoch 89 | loss: 1.27061 | val_0_rmse: 0.59188 |  0:00:31s\n",
      "epoch 90 | loss: 1.09397 | val_0_rmse: 0.68884 |  0:00:31s\n",
      "epoch 91 | loss: 1.09521 | val_0_rmse: 0.634   |  0:00:32s\n",
      "epoch 92 | loss: 0.98139 | val_0_rmse: 0.62768 |  0:00:32s\n",
      "epoch 93 | loss: 0.96439 | val_0_rmse: 0.58249 |  0:00:32s\n",
      "epoch 94 | loss: 0.98188 | val_0_rmse: 0.60774 |  0:00:33s\n",
      "epoch 95 | loss: 0.90397 | val_0_rmse: 0.65377 |  0:00:33s\n",
      "epoch 96 | loss: 1.23217 | val_0_rmse: 0.63846 |  0:00:33s\n",
      "epoch 97 | loss: 0.96815 | val_0_rmse: 0.64464 |  0:00:34s\n",
      "epoch 98 | loss: 0.9452  | val_0_rmse: 0.60049 |  0:00:34s\n",
      "epoch 99 | loss: 0.90028 | val_0_rmse: 0.61988 |  0:00:34s\n",
      "epoch 100| loss: 0.88902 | val_0_rmse: 0.60193 |  0:00:35s\n",
      "epoch 101| loss: 0.86451 | val_0_rmse: 0.61702 |  0:00:35s\n",
      "epoch 102| loss: 0.80387 | val_0_rmse: 0.52748 |  0:00:35s\n",
      "epoch 103| loss: 0.80497 | val_0_rmse: 0.5042  |  0:00:36s\n",
      "epoch 104| loss: 0.79765 | val_0_rmse: 0.55524 |  0:00:36s\n",
      "epoch 105| loss: 0.71013 | val_0_rmse: 0.56957 |  0:00:36s\n",
      "epoch 106| loss: 0.72292 | val_0_rmse: 0.68331 |  0:00:37s\n",
      "epoch 107| loss: 0.7217  | val_0_rmse: 0.60318 |  0:00:37s\n",
      "epoch 108| loss: 0.68738 | val_0_rmse: 0.61112 |  0:00:37s\n",
      "epoch 109| loss: 0.70164 | val_0_rmse: 0.61719 |  0:00:38s\n",
      "epoch 110| loss: 0.74259 | val_0_rmse: 0.50844 |  0:00:38s\n",
      "epoch 111| loss: 0.71452 | val_0_rmse: 0.46966 |  0:00:38s\n",
      "epoch 112| loss: 0.69464 | val_0_rmse: 0.58761 |  0:00:39s\n",
      "epoch 113| loss: 0.61878 | val_0_rmse: 0.5908  |  0:00:39s\n",
      "epoch 114| loss: 0.66767 | val_0_rmse: 0.48709 |  0:00:39s\n",
      "epoch 115| loss: 0.68494 | val_0_rmse: 0.50632 |  0:00:40s\n",
      "epoch 116| loss: 0.61804 | val_0_rmse: 0.60568 |  0:00:40s\n",
      "epoch 117| loss: 0.58416 | val_0_rmse: 0.50898 |  0:00:40s\n",
      "epoch 118| loss: 0.60647 | val_0_rmse: 0.52297 |  0:00:41s\n",
      "epoch 119| loss: 0.57961 | val_0_rmse: 0.50399 |  0:00:41s\n",
      "epoch 120| loss: 0.5795  | val_0_rmse: 0.47344 |  0:00:41s\n",
      "epoch 121| loss: 0.51689 | val_0_rmse: 0.47835 |  0:00:42s\n",
      "epoch 122| loss: 0.60117 | val_0_rmse: 0.50726 |  0:00:42s\n",
      "epoch 123| loss: 0.54377 | val_0_rmse: 0.47202 |  0:00:42s\n",
      "epoch 124| loss: 0.53154 | val_0_rmse: 0.43675 |  0:00:43s\n",
      "epoch 125| loss: 0.46994 | val_0_rmse: 0.44594 |  0:00:43s\n",
      "epoch 126| loss: 0.56092 | val_0_rmse: 0.44933 |  0:00:44s\n",
      "epoch 127| loss: 0.55616 | val_0_rmse: 0.4718  |  0:00:44s\n",
      "epoch 128| loss: 0.43915 | val_0_rmse: 0.47818 |  0:00:44s\n",
      "epoch 129| loss: 0.44421 | val_0_rmse: 0.53265 |  0:00:45s\n",
      "epoch 130| loss: 0.49723 | val_0_rmse: 0.62337 |  0:00:45s\n",
      "epoch 131| loss: 0.39765 | val_0_rmse: 0.45909 |  0:00:45s\n",
      "epoch 132| loss: 0.43887 | val_0_rmse: 0.47596 |  0:00:46s\n",
      "epoch 133| loss: 0.41629 | val_0_rmse: 0.49869 |  0:00:46s\n",
      "epoch 134| loss: 0.3837  | val_0_rmse: 0.52378 |  0:00:47s\n",
      "epoch 135| loss: 0.40291 | val_0_rmse: 0.55462 |  0:00:47s\n",
      "epoch 136| loss: 0.41138 | val_0_rmse: 0.5306  |  0:00:47s\n",
      "epoch 137| loss: 0.39878 | val_0_rmse: 0.49981 |  0:00:48s\n",
      "epoch 138| loss: 0.4195  | val_0_rmse: 0.50059 |  0:00:48s\n",
      "epoch 139| loss: 0.37597 | val_0_rmse: 0.49286 |  0:00:48s\n",
      "epoch 140| loss: 0.44763 | val_0_rmse: 0.61987 |  0:00:49s\n",
      "epoch 141| loss: 0.47053 | val_0_rmse: 0.53037 |  0:00:49s\n",
      "epoch 142| loss: 0.45151 | val_0_rmse: 0.50677 |  0:00:49s\n",
      "epoch 143| loss: 0.37532 | val_0_rmse: 0.47592 |  0:00:50s\n",
      "epoch 144| loss: 0.41084 | val_0_rmse: 0.46101 |  0:00:50s\n",
      "epoch 145| loss: 0.35719 | val_0_rmse: 0.46804 |  0:00:50s\n",
      "epoch 146| loss: 0.46029 | val_0_rmse: 0.47018 |  0:00:51s\n",
      "epoch 147| loss: 0.38091 | val_0_rmse: 0.486   |  0:00:51s\n",
      "epoch 148| loss: 0.33006 | val_0_rmse: 0.57638 |  0:00:51s\n",
      "epoch 149| loss: 0.39175 | val_0_rmse: 0.44874 |  0:00:52s\n",
      "\n",
      "Early stopping occurred at epoch 149 with best_epoch = 124 and best_val_0_rmse = 0.43675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:04:53,191] Trial 1 finished with value: 0.4367521109214149 and parameters: {'n_d': 16, 'n_a': 32, 'n_steps': 6, 'gamma': 1.8473207934175908, 'n_independent': 3, 'n_shared': 1, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'lr': 0.003370731375124169, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 0 with value: 0.3612857290465453.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 001 | rmse_log=0.43675 | RMSE$=90,712 | MAE$=58,077 | MAPE=34.91% | n_d/n_a=16/32 steps=6 lr=0.00337 batch=512 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 233.65228| val_0_rmse: 12.09262|  0:00:00s\n",
      "epoch 1  | loss: 209.96654| val_0_rmse: 11.91351|  0:00:00s\n",
      "epoch 2  | loss: 190.95381| val_0_rmse: 11.7221 |  0:00:00s\n",
      "epoch 3  | loss: 174.0119| val_0_rmse: 11.5512 |  0:00:00s\n",
      "epoch 4  | loss: 157.57309| val_0_rmse: 11.38877|  0:00:01s\n",
      "epoch 5  | loss: 145.97742| val_0_rmse: 11.18589|  0:00:01s\n",
      "epoch 6  | loss: 133.92834| val_0_rmse: 11.01944|  0:00:01s\n",
      "epoch 7  | loss: 124.11076| val_0_rmse: 10.84418|  0:00:01s\n",
      "epoch 8  | loss: 113.25388| val_0_rmse: 10.54903|  0:00:02s\n",
      "epoch 9  | loss: 102.91029| val_0_rmse: 10.24145|  0:00:02s\n",
      "epoch 10 | loss: 94.29987| val_0_rmse: 9.94754 |  0:00:02s\n",
      "epoch 11 | loss: 86.67839| val_0_rmse: 9.83106 |  0:00:02s\n",
      "epoch 12 | loss: 76.55566| val_0_rmse: 9.49537 |  0:00:02s\n",
      "epoch 13 | loss: 69.98643| val_0_rmse: 9.30096 |  0:00:03s\n",
      "epoch 14 | loss: 63.27626| val_0_rmse: 9.02773 |  0:00:03s\n",
      "epoch 15 | loss: 55.70904| val_0_rmse: 8.76092 |  0:00:03s\n",
      "epoch 16 | loss: 50.94567| val_0_rmse: 8.4745  |  0:00:03s\n",
      "epoch 17 | loss: 44.59332| val_0_rmse: 8.13784 |  0:00:03s\n",
      "epoch 18 | loss: 39.52348| val_0_rmse: 7.72566 |  0:00:04s\n",
      "epoch 19 | loss: 34.75576| val_0_rmse: 7.44561 |  0:00:04s\n",
      "epoch 20 | loss: 32.146  | val_0_rmse: 7.00194 |  0:00:04s\n",
      "epoch 21 | loss: 30.26457| val_0_rmse: 6.78014 |  0:00:04s\n",
      "epoch 22 | loss: 28.46593| val_0_rmse: 6.36944 |  0:00:04s\n",
      "epoch 23 | loss: 26.50302| val_0_rmse: 6.16736 |  0:00:04s\n",
      "epoch 24 | loss: 23.8439 | val_0_rmse: 5.8531  |  0:00:05s\n",
      "epoch 25 | loss: 22.68106| val_0_rmse: 5.62208 |  0:00:05s\n",
      "epoch 26 | loss: 22.19259| val_0_rmse: 5.36893 |  0:00:05s\n",
      "epoch 27 | loss: 19.29551| val_0_rmse: 5.11335 |  0:00:05s\n",
      "epoch 28 | loss: 21.13309| val_0_rmse: 4.9815  |  0:00:05s\n",
      "epoch 29 | loss: 20.53727| val_0_rmse: 4.87807 |  0:00:06s\n",
      "epoch 30 | loss: 22.03717| val_0_rmse: 4.5733  |  0:00:06s\n",
      "epoch 31 | loss: 20.96627| val_0_rmse: 4.46673 |  0:00:06s\n",
      "epoch 32 | loss: 19.23058| val_0_rmse: 4.30423 |  0:00:06s\n",
      "epoch 33 | loss: 19.31011| val_0_rmse: 4.16006 |  0:00:06s\n",
      "epoch 34 | loss: 18.19339| val_0_rmse: 3.98765 |  0:00:07s\n",
      "epoch 35 | loss: 16.8546 | val_0_rmse: 3.95395 |  0:00:07s\n",
      "epoch 36 | loss: 16.70722| val_0_rmse: 3.76441 |  0:00:07s\n",
      "epoch 37 | loss: 17.28429| val_0_rmse: 3.70696 |  0:00:07s\n",
      "epoch 38 | loss: 13.41735| val_0_rmse: 3.59472 |  0:00:07s\n",
      "epoch 39 | loss: 16.64751| val_0_rmse: 3.83612 |  0:00:08s\n",
      "epoch 40 | loss: 15.196  | val_0_rmse: 3.86287 |  0:00:08s\n",
      "epoch 41 | loss: 12.98673| val_0_rmse: 3.84149 |  0:00:08s\n",
      "epoch 42 | loss: 12.03785| val_0_rmse: 3.82773 |  0:00:08s\n",
      "epoch 43 | loss: 10.8454 | val_0_rmse: 3.74443 |  0:00:08s\n",
      "epoch 44 | loss: 9.95922 | val_0_rmse: 3.7357  |  0:00:08s\n",
      "epoch 45 | loss: 9.57824 | val_0_rmse: 3.65112 |  0:00:09s\n",
      "epoch 46 | loss: 9.49011 | val_0_rmse: 3.80332 |  0:00:09s\n",
      "epoch 47 | loss: 8.75612 | val_0_rmse: 3.85374 |  0:00:09s\n",
      "epoch 48 | loss: 9.81778 | val_0_rmse: 3.80186 |  0:00:09s\n",
      "epoch 49 | loss: 8.40422 | val_0_rmse: 3.67341 |  0:00:09s\n",
      "epoch 50 | loss: 8.14681 | val_0_rmse: 3.57587 |  0:00:10s\n",
      "epoch 51 | loss: 7.43559 | val_0_rmse: 3.47107 |  0:00:10s\n",
      "epoch 52 | loss: 6.99686 | val_0_rmse: 3.25532 |  0:00:10s\n",
      "epoch 53 | loss: 5.62622 | val_0_rmse: 3.03691 |  0:00:10s\n",
      "epoch 54 | loss: 5.51522 | val_0_rmse: 2.60768 |  0:00:10s\n",
      "epoch 55 | loss: 5.01833 | val_0_rmse: 2.39922 |  0:00:11s\n",
      "epoch 56 | loss: 4.12742 | val_0_rmse: 2.12232 |  0:00:11s\n",
      "epoch 57 | loss: 4.97091 | val_0_rmse: 1.93874 |  0:00:11s\n",
      "epoch 58 | loss: 4.72554 | val_0_rmse: 1.71232 |  0:00:11s\n",
      "epoch 59 | loss: 4.32535 | val_0_rmse: 1.67292 |  0:00:11s\n",
      "epoch 60 | loss: 4.15736 | val_0_rmse: 1.53995 |  0:00:12s\n",
      "epoch 61 | loss: 3.64524 | val_0_rmse: 1.48862 |  0:00:12s\n",
      "epoch 62 | loss: 4.00511 | val_0_rmse: 1.43228 |  0:00:12s\n",
      "epoch 63 | loss: 3.82069 | val_0_rmse: 1.36415 |  0:00:12s\n",
      "epoch 64 | loss: 3.3347  | val_0_rmse: 1.32093 |  0:00:12s\n",
      "epoch 65 | loss: 3.25015 | val_0_rmse: 1.17696 |  0:00:13s\n",
      "epoch 66 | loss: 2.62923 | val_0_rmse: 1.25008 |  0:00:13s\n",
      "epoch 67 | loss: 2.63358 | val_0_rmse: 1.09305 |  0:00:13s\n",
      "epoch 68 | loss: 2.56131 | val_0_rmse: 1.06807 |  0:00:13s\n",
      "epoch 69 | loss: 2.36822 | val_0_rmse: 0.85983 |  0:00:13s\n",
      "epoch 70 | loss: 1.76377 | val_0_rmse: 0.76048 |  0:00:14s\n",
      "epoch 71 | loss: 1.81991 | val_0_rmse: 0.69287 |  0:00:14s\n",
      "epoch 72 | loss: 1.97953 | val_0_rmse: 0.60058 |  0:00:14s\n",
      "epoch 73 | loss: 1.62493 | val_0_rmse: 0.60835 |  0:00:14s\n",
      "epoch 74 | loss: 1.66366 | val_0_rmse: 0.63377 |  0:00:14s\n",
      "epoch 75 | loss: 1.38345 | val_0_rmse: 0.69874 |  0:00:14s\n",
      "epoch 76 | loss: 1.42832 | val_0_rmse: 0.71539 |  0:00:15s\n",
      "epoch 77 | loss: 1.50299 | val_0_rmse: 0.68308 |  0:00:15s\n",
      "epoch 78 | loss: 1.50029 | val_0_rmse: 0.58606 |  0:00:15s\n",
      "epoch 79 | loss: 1.43771 | val_0_rmse: 0.45957 |  0:00:15s\n",
      "epoch 80 | loss: 1.35665 | val_0_rmse: 0.46986 |  0:00:15s\n",
      "epoch 81 | loss: 1.29738 | val_0_rmse: 0.45554 |  0:00:16s\n",
      "epoch 82 | loss: 1.18424 | val_0_rmse: 0.44336 |  0:00:16s\n",
      "epoch 83 | loss: 1.24211 | val_0_rmse: 0.47331 |  0:00:16s\n",
      "epoch 84 | loss: 1.10095 | val_0_rmse: 0.53896 |  0:00:16s\n",
      "epoch 85 | loss: 1.11876 | val_0_rmse: 0.51752 |  0:00:16s\n",
      "epoch 86 | loss: 0.97754 | val_0_rmse: 0.48523 |  0:00:17s\n",
      "epoch 87 | loss: 0.88445 | val_0_rmse: 0.44621 |  0:00:17s\n",
      "epoch 88 | loss: 0.90893 | val_0_rmse: 0.43159 |  0:00:17s\n",
      "epoch 89 | loss: 0.89742 | val_0_rmse: 0.39971 |  0:00:17s\n",
      "epoch 90 | loss: 0.90706 | val_0_rmse: 0.42096 |  0:00:17s\n",
      "epoch 91 | loss: 1.01845 | val_0_rmse: 0.40311 |  0:00:18s\n",
      "epoch 92 | loss: 0.90472 | val_0_rmse: 0.44808 |  0:00:18s\n",
      "epoch 93 | loss: 0.90744 | val_0_rmse: 0.41289 |  0:00:18s\n",
      "epoch 94 | loss: 0.77434 | val_0_rmse: 0.42659 |  0:00:18s\n",
      "epoch 95 | loss: 0.64937 | val_0_rmse: 0.45927 |  0:00:18s\n",
      "epoch 96 | loss: 0.80359 | val_0_rmse: 0.39981 |  0:00:18s\n",
      "epoch 97 | loss: 0.69862 | val_0_rmse: 0.40306 |  0:00:19s\n",
      "epoch 98 | loss: 0.6134  | val_0_rmse: 0.39529 |  0:00:19s\n",
      "epoch 99 | loss: 0.64982 | val_0_rmse: 0.39413 |  0:00:19s\n",
      "epoch 100| loss: 0.70894 | val_0_rmse: 0.38465 |  0:00:19s\n",
      "epoch 101| loss: 0.60068 | val_0_rmse: 0.35995 |  0:00:19s\n",
      "epoch 102| loss: 0.60174 | val_0_rmse: 0.32993 |  0:00:20s\n",
      "epoch 103| loss: 0.52679 | val_0_rmse: 0.36714 |  0:00:20s\n",
      "epoch 104| loss: 0.50797 | val_0_rmse: 0.35655 |  0:00:20s\n",
      "epoch 105| loss: 0.58447 | val_0_rmse: 0.32637 |  0:00:20s\n",
      "epoch 106| loss: 0.5949  | val_0_rmse: 0.33424 |  0:00:20s\n",
      "epoch 107| loss: 0.57262 | val_0_rmse: 0.34403 |  0:00:21s\n",
      "epoch 108| loss: 0.57142 | val_0_rmse: 0.3333  |  0:00:21s\n",
      "epoch 109| loss: 0.55887 | val_0_rmse: 0.34954 |  0:00:21s\n",
      "epoch 110| loss: 0.55055 | val_0_rmse: 0.3473  |  0:00:21s\n",
      "epoch 111| loss: 0.40716 | val_0_rmse: 0.4031  |  0:00:21s\n",
      "epoch 112| loss: 0.45793 | val_0_rmse: 0.43924 |  0:00:22s\n",
      "epoch 113| loss: 0.57448 | val_0_rmse: 0.36535 |  0:00:22s\n",
      "epoch 114| loss: 0.4994  | val_0_rmse: 0.56266 |  0:00:22s\n",
      "epoch 115| loss: 0.60305 | val_0_rmse: 0.43943 |  0:00:22s\n",
      "epoch 116| loss: 0.50074 | val_0_rmse: 0.34536 |  0:00:22s\n",
      "epoch 117| loss: 0.48286 | val_0_rmse: 0.36747 |  0:00:23s\n",
      "epoch 118| loss: 0.41202 | val_0_rmse: 0.40435 |  0:00:23s\n",
      "epoch 119| loss: 0.45922 | val_0_rmse: 0.35479 |  0:00:23s\n",
      "epoch 120| loss: 0.36865 | val_0_rmse: 0.374   |  0:00:23s\n",
      "epoch 121| loss: 0.43059 | val_0_rmse: 0.36035 |  0:00:23s\n",
      "epoch 122| loss: 0.33173 | val_0_rmse: 0.37346 |  0:00:23s\n",
      "epoch 123| loss: 0.37511 | val_0_rmse: 0.38726 |  0:00:24s\n",
      "epoch 124| loss: 0.3847  | val_0_rmse: 0.40416 |  0:00:24s\n",
      "epoch 125| loss: 0.39041 | val_0_rmse: 0.36951 |  0:00:24s\n",
      "epoch 126| loss: 0.35018 | val_0_rmse: 0.37977 |  0:00:24s\n",
      "epoch 127| loss: 0.36998 | val_0_rmse: 0.37072 |  0:00:24s\n",
      "epoch 128| loss: 0.44283 | val_0_rmse: 0.32311 |  0:00:24s\n",
      "epoch 129| loss: 0.32814 | val_0_rmse: 0.40852 |  0:00:25s\n",
      "epoch 130| loss: 0.39817 | val_0_rmse: 0.47452 |  0:00:25s\n",
      "epoch 131| loss: 0.46389 | val_0_rmse: 0.50734 |  0:00:25s\n",
      "epoch 132| loss: 0.55136 | val_0_rmse: 0.34463 |  0:00:25s\n",
      "epoch 133| loss: 0.2816  | val_0_rmse: 0.50148 |  0:00:25s\n",
      "epoch 134| loss: 0.44412 | val_0_rmse: 0.56574 |  0:00:26s\n",
      "epoch 135| loss: 0.54876 | val_0_rmse: 0.48998 |  0:00:26s\n",
      "epoch 136| loss: 0.43239 | val_0_rmse: 0.39386 |  0:00:26s\n",
      "epoch 137| loss: 0.36939 | val_0_rmse: 0.76237 |  0:00:26s\n",
      "epoch 138| loss: 0.97874 | val_0_rmse: 0.96053 |  0:00:26s\n",
      "epoch 139| loss: 1.32666 | val_0_rmse: 0.8702  |  0:00:26s\n",
      "epoch 140| loss: 1.02737 | val_0_rmse: 0.66449 |  0:00:27s\n",
      "epoch 141| loss: 0.57183 | val_0_rmse: 0.36124 |  0:00:27s\n",
      "epoch 142| loss: 0.29361 | val_0_rmse: 0.44535 |  0:00:27s\n",
      "epoch 143| loss: 0.53798 | val_0_rmse: 0.68712 |  0:00:27s\n",
      "epoch 144| loss: 1.06816 | val_0_rmse: 0.48493 |  0:00:27s\n",
      "epoch 145| loss: 0.55774 | val_0_rmse: 0.29064 |  0:00:28s\n",
      "epoch 146| loss: 0.27773 | val_0_rmse: 0.4806  |  0:00:28s\n",
      "epoch 147| loss: 0.40861 | val_0_rmse: 0.71409 |  0:00:28s\n",
      "epoch 148| loss: 0.89341 | val_0_rmse: 0.81787 |  0:00:28s\n",
      "epoch 149| loss: 1.11509 | val_0_rmse: 0.76306 |  0:00:28s\n",
      "epoch 150| loss: 1.01622 | val_0_rmse: 0.46282 |  0:00:29s\n",
      "epoch 151| loss: 0.43466 | val_0_rmse: 0.4188  |  0:00:29s\n",
      "epoch 152| loss: 0.40065 | val_0_rmse: 0.74632 |  0:00:29s\n",
      "epoch 153| loss: 1.23276 | val_0_rmse: 0.79433 |  0:00:29s\n",
      "epoch 154| loss: 1.21374 | val_0_rmse: 0.81656 |  0:00:29s\n",
      "epoch 155| loss: 1.21735 | val_0_rmse: 0.73355 |  0:00:29s\n",
      "epoch 156| loss: 0.99303 | val_0_rmse: 0.45562 |  0:00:30s\n",
      "epoch 157| loss: 0.4529  | val_0_rmse: 0.34078 |  0:00:30s\n",
      "epoch 158| loss: 0.33323 | val_0_rmse: 0.67824 |  0:00:30s\n",
      "epoch 159| loss: 0.68442 | val_0_rmse: 0.92253 |  0:00:30s\n",
      "epoch 160| loss: 1.2681  | val_0_rmse: 0.97649 |  0:00:30s\n",
      "epoch 161| loss: 1.32346 | val_0_rmse: 0.67335 |  0:00:31s\n",
      "epoch 162| loss: 0.57634 | val_0_rmse: 0.36589 |  0:00:31s\n",
      "epoch 163| loss: 0.2634  | val_0_rmse: 0.36374 |  0:00:31s\n",
      "epoch 164| loss: 0.41881 | val_0_rmse: 0.39462 |  0:00:31s\n",
      "epoch 165| loss: 0.51162 | val_0_rmse: 0.36147 |  0:00:31s\n",
      "epoch 166| loss: 0.40847 | val_0_rmse: 0.30932 |  0:00:31s\n",
      "epoch 167| loss: 0.20882 | val_0_rmse: 0.36139 |  0:00:32s\n",
      "epoch 168| loss: 0.26504 | val_0_rmse: 0.44839 |  0:00:32s\n",
      "epoch 169| loss: 0.39458 | val_0_rmse: 0.47188 |  0:00:32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:05:26,300] Trial 2 finished with value: 0.29063923505231687 and parameters: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.7726938193761264, 'n_independent': 1, 'n_shared': 1, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.00968357545711332, 'batch_size': 2048, 'virtual_batch_size': 64}. Best is trial 2 with value: 0.29063923505231687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 170| loss: 0.42729 | val_0_rmse: 0.39332 |  0:00:32s\n",
      "\n",
      "Early stopping occurred at epoch 170 with best_epoch = 145 and best_val_0_rmse = 0.29064\n",
      "Trial 002 | rmse_log=0.29064 | RMSE$=60,460 | MAE$=39,315 | MAPE=23.43% | n_d/n_a=32/32 steps=7 lr=0.00968 batch=2048 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 127.74208| val_0_rmse: 11.71193|  0:00:00s\n",
      "epoch 1  | loss: 120.05452| val_0_rmse: 11.67858|  0:00:00s\n",
      "epoch 2  | loss: 115.73082| val_0_rmse: 11.49935|  0:00:00s\n",
      "epoch 3  | loss: 110.92867| val_0_rmse: 11.38013|  0:00:01s\n",
      "epoch 4  | loss: 105.97562| val_0_rmse: 11.18287|  0:00:01s\n",
      "epoch 5  | loss: 100.37733| val_0_rmse: 10.97919|  0:00:01s\n",
      "epoch 6  | loss: 96.90138| val_0_rmse: 10.748  |  0:00:02s\n",
      "epoch 7  | loss: 92.5079 | val_0_rmse: 10.64506|  0:00:02s\n",
      "epoch 8  | loss: 87.10944| val_0_rmse: 10.39435|  0:00:02s\n",
      "epoch 9  | loss: 84.64632| val_0_rmse: 10.15307|  0:00:02s\n",
      "epoch 10 | loss: 78.84369| val_0_rmse: 9.85056 |  0:00:03s\n",
      "epoch 11 | loss: 73.64527| val_0_rmse: 9.69957 |  0:00:03s\n",
      "epoch 12 | loss: 69.16147| val_0_rmse: 9.57768 |  0:00:03s\n",
      "epoch 13 | loss: 64.39906| val_0_rmse: 9.23904 |  0:00:04s\n",
      "epoch 14 | loss: 63.42773| val_0_rmse: 9.13557 |  0:00:04s\n",
      "epoch 15 | loss: 57.91105| val_0_rmse: 8.98312 |  0:00:04s\n",
      "epoch 16 | loss: 55.1608 | val_0_rmse: 8.87006 |  0:00:05s\n",
      "epoch 17 | loss: 51.92292| val_0_rmse: 8.61838 |  0:00:05s\n",
      "epoch 18 | loss: 45.71949| val_0_rmse: 8.44037 |  0:00:05s\n",
      "epoch 19 | loss: 45.10282| val_0_rmse: 8.14589 |  0:00:05s\n",
      "epoch 20 | loss: 42.15972| val_0_rmse: 7.87016 |  0:00:06s\n",
      "epoch 21 | loss: 40.02327| val_0_rmse: 7.59032 |  0:00:06s\n",
      "epoch 22 | loss: 37.38532| val_0_rmse: 7.40234 |  0:00:06s\n",
      "epoch 23 | loss: 34.05272| val_0_rmse: 7.27885 |  0:00:07s\n",
      "epoch 24 | loss: 32.41808| val_0_rmse: 7.06538 |  0:00:07s\n",
      "epoch 25 | loss: 30.50644| val_0_rmse: 6.81746 |  0:00:07s\n",
      "epoch 26 | loss: 28.11987| val_0_rmse: 6.63532 |  0:00:08s\n",
      "epoch 27 | loss: 24.26665| val_0_rmse: 6.53112 |  0:00:08s\n",
      "epoch 28 | loss: 25.17666| val_0_rmse: 6.34823 |  0:00:08s\n",
      "epoch 29 | loss: 23.19492| val_0_rmse: 6.08532 |  0:00:08s\n",
      "epoch 30 | loss: 22.02488| val_0_rmse: 5.95555 |  0:00:09s\n",
      "epoch 31 | loss: 20.69452| val_0_rmse: 5.8956  |  0:00:09s\n",
      "epoch 32 | loss: 19.94361| val_0_rmse: 5.628   |  0:00:09s\n",
      "epoch 33 | loss: 17.28894| val_0_rmse: 5.29777 |  0:00:10s\n",
      "epoch 34 | loss: 17.43516| val_0_rmse: 5.1203  |  0:00:10s\n",
      "epoch 35 | loss: 17.62812| val_0_rmse: 4.86712 |  0:00:10s\n",
      "epoch 36 | loss: 15.34128| val_0_rmse: 4.58565 |  0:00:10s\n",
      "epoch 37 | loss: 15.7715 | val_0_rmse: 4.52851 |  0:00:11s\n",
      "epoch 38 | loss: 15.26815| val_0_rmse: 4.25315 |  0:00:11s\n",
      "epoch 39 | loss: 13.31263| val_0_rmse: 3.953   |  0:00:11s\n",
      "epoch 40 | loss: 11.34095| val_0_rmse: 3.61999 |  0:00:12s\n",
      "epoch 41 | loss: 11.78909| val_0_rmse: 3.31994 |  0:00:12s\n",
      "epoch 42 | loss: 10.31376| val_0_rmse: 3.13083 |  0:00:12s\n",
      "epoch 43 | loss: 11.07254| val_0_rmse: 3.08537 |  0:00:13s\n",
      "epoch 44 | loss: 9.38086 | val_0_rmse: 2.88647 |  0:00:13s\n",
      "epoch 45 | loss: 9.8756  | val_0_rmse: 2.74114 |  0:00:13s\n",
      "epoch 46 | loss: 9.41536 | val_0_rmse: 2.76543 |  0:00:13s\n",
      "epoch 47 | loss: 8.97103 | val_0_rmse: 2.69535 |  0:00:14s\n",
      "epoch 48 | loss: 8.18388 | val_0_rmse: 2.53356 |  0:00:14s\n",
      "epoch 49 | loss: 7.21956 | val_0_rmse: 2.62816 |  0:00:14s\n",
      "epoch 50 | loss: 6.92027 | val_0_rmse: 2.71078 |  0:00:15s\n",
      "epoch 51 | loss: 6.86331 | val_0_rmse: 2.65621 |  0:00:15s\n",
      "epoch 52 | loss: 6.63169 | val_0_rmse: 2.67012 |  0:00:15s\n",
      "epoch 53 | loss: 7.6391  | val_0_rmse: 2.65247 |  0:00:15s\n",
      "epoch 54 | loss: 5.69396 | val_0_rmse: 2.56326 |  0:00:16s\n",
      "epoch 55 | loss: 5.44499 | val_0_rmse: 2.40691 |  0:00:16s\n",
      "epoch 56 | loss: 5.51754 | val_0_rmse: 2.30739 |  0:00:16s\n",
      "epoch 57 | loss: 5.25375 | val_0_rmse: 2.23221 |  0:00:17s\n",
      "epoch 58 | loss: 4.97349 | val_0_rmse: 2.11097 |  0:00:17s\n",
      "epoch 59 | loss: 4.55291 | val_0_rmse: 1.92996 |  0:00:17s\n",
      "epoch 60 | loss: 4.18586 | val_0_rmse: 1.70506 |  0:00:17s\n",
      "epoch 61 | loss: 3.58291 | val_0_rmse: 1.47469 |  0:00:18s\n",
      "epoch 62 | loss: 3.42386 | val_0_rmse: 1.28617 |  0:00:18s\n",
      "epoch 63 | loss: 3.48688 | val_0_rmse: 1.18816 |  0:00:18s\n",
      "epoch 64 | loss: 3.19649 | val_0_rmse: 1.0415  |  0:00:19s\n",
      "epoch 65 | loss: 2.52569 | val_0_rmse: 1.04736 |  0:00:19s\n",
      "epoch 66 | loss: 2.97429 | val_0_rmse: 1.00328 |  0:00:19s\n",
      "epoch 67 | loss: 2.49363 | val_0_rmse: 0.94116 |  0:00:19s\n",
      "epoch 68 | loss: 2.80384 | val_0_rmse: 0.96828 |  0:00:20s\n",
      "epoch 69 | loss: 2.77738 | val_0_rmse: 1.09365 |  0:00:20s\n",
      "epoch 70 | loss: 2.54016 | val_0_rmse: 1.08301 |  0:00:20s\n",
      "epoch 71 | loss: 2.33272 | val_0_rmse: 1.02164 |  0:00:21s\n",
      "epoch 72 | loss: 2.06895 | val_0_rmse: 0.97639 |  0:00:21s\n",
      "epoch 73 | loss: 2.19812 | val_0_rmse: 0.91988 |  0:00:21s\n",
      "epoch 74 | loss: 1.91863 | val_0_rmse: 0.82514 |  0:00:21s\n",
      "epoch 75 | loss: 2.13435 | val_0_rmse: 0.76022 |  0:00:22s\n",
      "epoch 76 | loss: 1.89475 | val_0_rmse: 0.69388 |  0:00:22s\n",
      "epoch 77 | loss: 1.9571  | val_0_rmse: 0.65594 |  0:00:22s\n",
      "epoch 78 | loss: 1.61576 | val_0_rmse: 0.60593 |  0:00:23s\n",
      "epoch 79 | loss: 1.76456 | val_0_rmse: 0.63838 |  0:00:23s\n",
      "epoch 80 | loss: 1.60438 | val_0_rmse: 0.66144 |  0:00:23s\n",
      "epoch 81 | loss: 1.50215 | val_0_rmse: 0.66336 |  0:00:23s\n",
      "epoch 82 | loss: 1.49332 | val_0_rmse: 0.65337 |  0:00:24s\n",
      "epoch 83 | loss: 1.43919 | val_0_rmse: 0.60691 |  0:00:24s\n",
      "epoch 84 | loss: 1.18527 | val_0_rmse: 0.56635 |  0:00:24s\n",
      "epoch 85 | loss: 1.2647  | val_0_rmse: 0.54405 |  0:00:25s\n",
      "epoch 86 | loss: 1.29262 | val_0_rmse: 0.55091 |  0:00:25s\n",
      "epoch 87 | loss: 1.35286 | val_0_rmse: 0.55249 |  0:00:25s\n",
      "epoch 88 | loss: 1.46819 | val_0_rmse: 0.55864 |  0:00:25s\n",
      "epoch 89 | loss: 1.26775 | val_0_rmse: 0.55571 |  0:00:26s\n",
      "epoch 90 | loss: 1.14458 | val_0_rmse: 0.52667 |  0:00:26s\n",
      "epoch 91 | loss: 1.02581 | val_0_rmse: 0.51578 |  0:00:26s\n",
      "epoch 92 | loss: 1.01508 | val_0_rmse: 0.5625  |  0:00:27s\n",
      "epoch 93 | loss: 1.26775 | val_0_rmse: 0.56756 |  0:00:27s\n",
      "epoch 94 | loss: 1.06456 | val_0_rmse: 0.58497 |  0:00:27s\n",
      "epoch 95 | loss: 1.05125 | val_0_rmse: 0.5431  |  0:00:28s\n",
      "epoch 96 | loss: 1.04462 | val_0_rmse: 0.42065 |  0:00:28s\n",
      "epoch 97 | loss: 1.04899 | val_0_rmse: 0.41978 |  0:00:28s\n",
      "epoch 98 | loss: 0.87641 | val_0_rmse: 0.46485 |  0:00:28s\n",
      "epoch 99 | loss: 0.95685 | val_0_rmse: 0.51885 |  0:00:29s\n",
      "epoch 100| loss: 1.08604 | val_0_rmse: 0.532   |  0:00:29s\n",
      "epoch 101| loss: 0.88529 | val_0_rmse: 0.50609 |  0:00:29s\n",
      "epoch 102| loss: 1.00537 | val_0_rmse: 0.51003 |  0:00:30s\n",
      "epoch 103| loss: 0.93701 | val_0_rmse: 0.51955 |  0:00:30s\n",
      "epoch 104| loss: 0.82041 | val_0_rmse: 0.45458 |  0:00:30s\n",
      "epoch 105| loss: 0.92005 | val_0_rmse: 0.45111 |  0:00:30s\n",
      "epoch 106| loss: 0.82307 | val_0_rmse: 0.46466 |  0:00:31s\n",
      "epoch 107| loss: 0.94368 | val_0_rmse: 0.40062 |  0:00:31s\n",
      "epoch 108| loss: 0.88189 | val_0_rmse: 0.4289  |  0:00:31s\n",
      "epoch 109| loss: 0.70594 | val_0_rmse: 0.55672 |  0:00:32s\n",
      "epoch 110| loss: 0.76804 | val_0_rmse: 0.54956 |  0:00:32s\n",
      "epoch 111| loss: 0.80836 | val_0_rmse: 0.4134  |  0:00:32s\n",
      "epoch 112| loss: 0.69142 | val_0_rmse: 0.45583 |  0:00:32s\n",
      "epoch 113| loss: 0.66935 | val_0_rmse: 0.54132 |  0:00:33s\n",
      "epoch 114| loss: 0.79318 | val_0_rmse: 0.50317 |  0:00:33s\n",
      "epoch 115| loss: 0.58653 | val_0_rmse: 0.4415  |  0:00:33s\n",
      "epoch 116| loss: 0.75198 | val_0_rmse: 0.52361 |  0:00:34s\n",
      "epoch 117| loss: 0.75921 | val_0_rmse: 0.61726 |  0:00:34s\n",
      "epoch 118| loss: 0.83602 | val_0_rmse: 0.56216 |  0:00:34s\n",
      "epoch 119| loss: 0.72737 | val_0_rmse: 0.44731 |  0:00:34s\n",
      "epoch 120| loss: 0.65985 | val_0_rmse: 0.36034 |  0:00:35s\n",
      "epoch 121| loss: 0.57212 | val_0_rmse: 0.36615 |  0:00:35s\n",
      "epoch 122| loss: 0.68888 | val_0_rmse: 0.44109 |  0:00:35s\n",
      "epoch 123| loss: 0.69409 | val_0_rmse: 0.39399 |  0:00:36s\n",
      "epoch 124| loss: 0.67873 | val_0_rmse: 0.37498 |  0:00:36s\n",
      "epoch 125| loss: 0.55025 | val_0_rmse: 0.36351 |  0:00:36s\n",
      "epoch 126| loss: 0.57484 | val_0_rmse: 0.36491 |  0:00:36s\n",
      "epoch 127| loss: 0.53736 | val_0_rmse: 0.39372 |  0:00:37s\n",
      "epoch 128| loss: 0.67308 | val_0_rmse: 0.37934 |  0:00:37s\n",
      "epoch 129| loss: 0.56205 | val_0_rmse: 0.36671 |  0:00:37s\n",
      "epoch 130| loss: 0.4806  | val_0_rmse: 0.37087 |  0:00:38s\n",
      "epoch 131| loss: 0.50951 | val_0_rmse: 0.35237 |  0:00:38s\n",
      "epoch 132| loss: 0.52744 | val_0_rmse: 0.36194 |  0:00:38s\n",
      "epoch 133| loss: 0.49001 | val_0_rmse: 0.34597 |  0:00:38s\n",
      "epoch 134| loss: 0.54354 | val_0_rmse: 0.34546 |  0:00:39s\n",
      "epoch 135| loss: 0.47193 | val_0_rmse: 0.34964 |  0:00:39s\n",
      "epoch 136| loss: 0.51413 | val_0_rmse: 0.34454 |  0:00:39s\n",
      "epoch 137| loss: 0.45277 | val_0_rmse: 0.34024 |  0:00:40s\n",
      "epoch 138| loss: 0.4298  | val_0_rmse: 0.33231 |  0:00:40s\n",
      "epoch 139| loss: 0.43369 | val_0_rmse: 0.33553 |  0:00:40s\n",
      "epoch 140| loss: 0.40195 | val_0_rmse: 0.34546 |  0:00:41s\n",
      "epoch 141| loss: 0.48575 | val_0_rmse: 0.33271 |  0:00:41s\n",
      "epoch 142| loss: 0.4528  | val_0_rmse: 0.32399 |  0:00:41s\n",
      "epoch 143| loss: 0.40619 | val_0_rmse: 0.32661 |  0:00:41s\n",
      "epoch 144| loss: 0.46229 | val_0_rmse: 0.32551 |  0:00:42s\n",
      "epoch 145| loss: 0.4379  | val_0_rmse: 0.33058 |  0:00:42s\n",
      "epoch 146| loss: 0.44065 | val_0_rmse: 0.33342 |  0:00:42s\n",
      "epoch 147| loss: 0.4071  | val_0_rmse: 0.36728 |  0:00:43s\n",
      "epoch 148| loss: 0.40961 | val_0_rmse: 0.37595 |  0:00:43s\n",
      "epoch 149| loss: 0.43221 | val_0_rmse: 0.36852 |  0:00:43s\n",
      "epoch 150| loss: 0.41707 | val_0_rmse: 0.32867 |  0:00:43s\n",
      "epoch 151| loss: 0.41106 | val_0_rmse: 0.33572 |  0:00:44s\n",
      "epoch 152| loss: 0.41775 | val_0_rmse: 0.35401 |  0:00:44s\n",
      "epoch 153| loss: 0.3726  | val_0_rmse: 0.34256 |  0:00:44s\n",
      "epoch 154| loss: 0.43465 | val_0_rmse: 0.34789 |  0:00:44s\n",
      "epoch 155| loss: 0.39568 | val_0_rmse: 0.37512 |  0:00:45s\n",
      "epoch 156| loss: 0.37482 | val_0_rmse: 0.44279 |  0:00:45s\n",
      "epoch 157| loss: 0.36239 | val_0_rmse: 0.50688 |  0:00:45s\n",
      "epoch 158| loss: 0.44835 | val_0_rmse: 0.58439 |  0:00:46s\n",
      "epoch 159| loss: 0.51503 | val_0_rmse: 0.6504  |  0:00:46s\n",
      "epoch 160| loss: 0.56686 | val_0_rmse: 0.46173 |  0:00:46s\n",
      "epoch 161| loss: 0.36507 | val_0_rmse: 0.33952 |  0:00:46s\n",
      "epoch 162| loss: 0.35984 | val_0_rmse: 0.34054 |  0:00:47s\n",
      "epoch 163| loss: 0.37466 | val_0_rmse: 0.42022 |  0:00:47s\n",
      "epoch 164| loss: 0.48816 | val_0_rmse: 0.44778 |  0:00:47s\n",
      "epoch 165| loss: 0.4564  | val_0_rmse: 0.37987 |  0:00:47s\n",
      "epoch 166| loss: 0.41939 | val_0_rmse: 0.35305 |  0:00:48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:06:15,263] Trial 3 finished with value: 0.3239871899540463 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 6, 'gamma': 1.7816193244566283, 'n_independent': 3, 'n_shared': 2, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'lr': 0.0077762959481651375, 'batch_size': 2048, 'virtual_batch_size': 64}. Best is trial 2 with value: 0.29063923505231687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 167| loss: 0.34366 | val_0_rmse: 0.37661 |  0:00:48s\n",
      "\n",
      "Early stopping occurred at epoch 167 with best_epoch = 142 and best_val_0_rmse = 0.32399\n",
      "Trial 003 | rmse_log=0.32399 | RMSE$=71,435 | MAE$=42,492 | MAPE=24.77% | n_d/n_a=16/16 steps=6 lr=0.00778 batch=2048 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 139.70645| val_0_rmse: 11.0462 |  0:00:00s\n",
      "epoch 1  | loss: 107.04324| val_0_rmse: 10.23481|  0:00:00s\n",
      "epoch 2  | loss: 88.25223| val_0_rmse: 9.53829 |  0:00:00s\n",
      "epoch 3  | loss: 68.59499| val_0_rmse: 8.95576 |  0:00:00s\n",
      "epoch 4  | loss: 55.67453| val_0_rmse: 8.42431 |  0:00:00s\n",
      "epoch 5  | loss: 43.72677| val_0_rmse: 7.88898 |  0:00:00s\n",
      "epoch 6  | loss: 38.68722| val_0_rmse: 7.25859 |  0:00:00s\n",
      "epoch 7  | loss: 31.68285| val_0_rmse: 6.57387 |  0:00:01s\n",
      "epoch 8  | loss: 26.21553| val_0_rmse: 5.83211 |  0:00:01s\n",
      "epoch 9  | loss: 25.62432| val_0_rmse: 5.16647 |  0:00:01s\n",
      "epoch 10 | loss: 19.0057 | val_0_rmse: 4.60735 |  0:00:01s\n",
      "epoch 11 | loss: 19.78193| val_0_rmse: 4.14713 |  0:00:01s\n",
      "epoch 12 | loss: 19.71601| val_0_rmse: 3.82844 |  0:00:01s\n",
      "epoch 13 | loss: 16.44369| val_0_rmse: 3.63574 |  0:00:01s\n",
      "epoch 14 | loss: 12.66816| val_0_rmse: 3.56716 |  0:00:02s\n",
      "epoch 15 | loss: 9.09429 | val_0_rmse: 3.54625 |  0:00:02s\n",
      "epoch 16 | loss: 7.26746 | val_0_rmse: 3.56007 |  0:00:02s\n",
      "epoch 17 | loss: 6.50544 | val_0_rmse: 3.48422 |  0:00:02s\n",
      "epoch 18 | loss: 5.58286 | val_0_rmse: 3.28636 |  0:00:02s\n",
      "epoch 19 | loss: 4.74153 | val_0_rmse: 2.99362 |  0:00:02s\n",
      "epoch 20 | loss: 5.04955 | val_0_rmse: 2.38469 |  0:00:02s\n",
      "epoch 21 | loss: 3.54605 | val_0_rmse: 1.8112  |  0:00:02s\n",
      "epoch 22 | loss: 3.37763 | val_0_rmse: 1.37011 |  0:00:03s\n",
      "epoch 23 | loss: 3.33177 | val_0_rmse: 1.25264 |  0:00:03s\n",
      "epoch 24 | loss: 3.76428 | val_0_rmse: 1.15848 |  0:00:03s\n",
      "epoch 25 | loss: 3.03816 | val_0_rmse: 1.1533  |  0:00:03s\n",
      "epoch 26 | loss: 3.18059 | val_0_rmse: 1.34842 |  0:00:03s\n",
      "epoch 27 | loss: 2.51346 | val_0_rmse: 1.46352 |  0:00:03s\n",
      "epoch 28 | loss: 1.86    | val_0_rmse: 1.33557 |  0:00:03s\n",
      "epoch 29 | loss: 1.34834 | val_0_rmse: 1.03962 |  0:00:03s\n",
      "epoch 30 | loss: 1.52208 | val_0_rmse: 0.97782 |  0:00:04s\n",
      "epoch 31 | loss: 1.76465 | val_0_rmse: 1.35045 |  0:00:04s\n",
      "epoch 32 | loss: 1.66098 | val_0_rmse: 1.5344  |  0:00:04s\n",
      "epoch 33 | loss: 1.63634 | val_0_rmse: 1.40455 |  0:00:04s\n",
      "epoch 34 | loss: 1.52058 | val_0_rmse: 1.11558 |  0:00:04s\n",
      "epoch 35 | loss: 1.35881 | val_0_rmse: 0.82405 |  0:00:04s\n",
      "epoch 36 | loss: 1.39538 | val_0_rmse: 0.91783 |  0:00:04s\n",
      "epoch 37 | loss: 1.06507 | val_0_rmse: 1.16633 |  0:00:04s\n",
      "epoch 38 | loss: 0.9689  | val_0_rmse: 0.97093 |  0:00:05s\n",
      "epoch 39 | loss: 0.97687 | val_0_rmse: 0.71586 |  0:00:05s\n",
      "epoch 40 | loss: 1.00567 | val_0_rmse: 0.59446 |  0:00:05s\n",
      "epoch 41 | loss: 1.15066 | val_0_rmse: 0.79925 |  0:00:05s\n",
      "epoch 42 | loss: 1.04285 | val_0_rmse: 1.15288 |  0:00:05s\n",
      "epoch 43 | loss: 1.17304 | val_0_rmse: 1.24621 |  0:00:05s\n",
      "epoch 44 | loss: 1.2259  | val_0_rmse: 1.05458 |  0:00:05s\n",
      "epoch 45 | loss: 0.82993 | val_0_rmse: 0.72264 |  0:00:05s\n",
      "epoch 46 | loss: 0.77018 | val_0_rmse: 0.8519  |  0:00:06s\n",
      "epoch 47 | loss: 0.61119 | val_0_rmse: 1.04076 |  0:00:06s\n",
      "epoch 48 | loss: 0.56516 | val_0_rmse: 1.16261 |  0:00:06s\n",
      "epoch 49 | loss: 0.87557 | val_0_rmse: 1.26334 |  0:00:06s\n",
      "epoch 50 | loss: 1.0867  | val_0_rmse: 0.92211 |  0:00:06s\n",
      "epoch 51 | loss: 0.58773 | val_0_rmse: 0.52607 |  0:00:06s\n",
      "epoch 52 | loss: 1.05985 | val_0_rmse: 0.47083 |  0:00:06s\n",
      "epoch 53 | loss: 1.46713 | val_0_rmse: 0.65732 |  0:00:06s\n",
      "epoch 54 | loss: 0.7134  | val_0_rmse: 1.21118 |  0:00:07s\n",
      "epoch 55 | loss: 0.93654 | val_0_rmse: 1.32649 |  0:00:07s\n",
      "epoch 56 | loss: 1.25877 | val_0_rmse: 1.08695 |  0:00:07s\n",
      "epoch 57 | loss: 0.64658 | val_0_rmse: 0.64307 |  0:00:07s\n",
      "epoch 58 | loss: 0.54312 | val_0_rmse: 0.53328 |  0:00:07s\n",
      "epoch 59 | loss: 0.72642 | val_0_rmse: 0.63312 |  0:00:07s\n",
      "epoch 60 | loss: 0.41048 | val_0_rmse: 0.93864 |  0:00:07s\n",
      "epoch 61 | loss: 0.45974 | val_0_rmse: 1.0037  |  0:00:07s\n",
      "epoch 62 | loss: 0.53532 | val_0_rmse: 0.83445 |  0:00:07s\n",
      "epoch 63 | loss: 0.3599  | val_0_rmse: 0.66995 |  0:00:08s\n",
      "epoch 64 | loss: 0.39951 | val_0_rmse: 0.67263 |  0:00:08s\n",
      "epoch 65 | loss: 0.30493 | val_0_rmse: 0.77976 |  0:00:08s\n",
      "epoch 66 | loss: 0.26253 | val_0_rmse: 0.74622 |  0:00:08s\n",
      "epoch 67 | loss: 0.25575 | val_0_rmse: 0.70785 |  0:00:08s\n",
      "epoch 68 | loss: 0.21896 | val_0_rmse: 0.91008 |  0:00:08s\n",
      "epoch 69 | loss: 0.26482 | val_0_rmse: 0.78689 |  0:00:08s\n",
      "epoch 70 | loss: 0.27496 | val_0_rmse: 0.6113  |  0:00:08s\n",
      "epoch 71 | loss: 0.29422 | val_0_rmse: 0.56417 |  0:00:09s\n",
      "epoch 72 | loss: 0.4407  | val_0_rmse: 0.73503 |  0:00:09s\n",
      "epoch 73 | loss: 0.33204 | val_0_rmse: 0.79399 |  0:00:09s\n",
      "epoch 74 | loss: 0.37252 | val_0_rmse: 0.8289  |  0:00:09s\n",
      "epoch 75 | loss: 0.32376 | val_0_rmse: 0.70509 |  0:00:09s\n",
      "epoch 76 | loss: 0.29402 | val_0_rmse: 0.63351 |  0:00:09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:06:25,385] Trial 4 finished with value: 0.47083246655914923 and parameters: {'n_d': 64, 'n_a': 64, 'n_steps': 5, 'gamma': 1.2750919564216854, 'n_independent': 1, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'sparsemax', 'lr': 0.013923586223763912, 'batch_size': 2048, 'virtual_batch_size': 256}. Best is trial 2 with value: 0.29063923505231687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 77 | loss: 0.33886 | val_0_rmse: 0.73202 |  0:00:09s\n",
      "\n",
      "Early stopping occurred at epoch 77 with best_epoch = 52 and best_val_0_rmse = 0.47083\n",
      "Trial 004 | rmse_log=0.47083 | RMSE$=173,925 | MAE$=70,278 | MAPE=36.81% | n_d/n_a=64/64 steps=5 lr=0.01392 batch=2048 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 224.77509| val_0_rmse: 12.01249|  0:00:00s\n",
      "epoch 1  | loss: 189.27527| val_0_rmse: 11.53159|  0:00:00s\n",
      "epoch 2  | loss: 154.66105| val_0_rmse: 11.07912|  0:00:00s\n",
      "epoch 3  | loss: 124.4631| val_0_rmse: 10.65269|  0:00:01s\n",
      "epoch 4  | loss: 100.95151| val_0_rmse: 10.13909|  0:00:01s\n",
      "epoch 5  | loss: 77.22114| val_0_rmse: 9.6376  |  0:00:01s\n",
      "epoch 6  | loss: 61.13391| val_0_rmse: 9.0389  |  0:00:01s\n",
      "epoch 7  | loss: 47.15521| val_0_rmse: 8.44039 |  0:00:02s\n",
      "epoch 8  | loss: 36.03722| val_0_rmse: 7.77313 |  0:00:02s\n",
      "epoch 9  | loss: 27.87873| val_0_rmse: 7.05686 |  0:00:02s\n",
      "epoch 10 | loss: 22.90979| val_0_rmse: 6.38978 |  0:00:02s\n",
      "epoch 11 | loss: 20.16079| val_0_rmse: 5.74568 |  0:00:03s\n",
      "epoch 12 | loss: 22.50393| val_0_rmse: 5.23144 |  0:00:03s\n",
      "epoch 13 | loss: 19.99204| val_0_rmse: 4.91823 |  0:00:03s\n",
      "epoch 14 | loss: 21.46726| val_0_rmse: 4.60738 |  0:00:04s\n",
      "epoch 15 | loss: 20.10439| val_0_rmse: 4.35933 |  0:00:04s\n",
      "epoch 16 | loss: 19.16465| val_0_rmse: 4.33798 |  0:00:04s\n",
      "epoch 17 | loss: 15.70048| val_0_rmse: 4.24964 |  0:00:05s\n",
      "epoch 18 | loss: 14.9711 | val_0_rmse: 4.26683 |  0:00:05s\n",
      "epoch 19 | loss: 14.95285| val_0_rmse: 4.26363 |  0:00:05s\n",
      "epoch 20 | loss: 12.16362| val_0_rmse: 4.2299  |  0:00:05s\n",
      "epoch 21 | loss: 10.41287| val_0_rmse: 4.16333 |  0:00:06s\n",
      "epoch 22 | loss: 9.82796 | val_0_rmse: 4.04282 |  0:00:06s\n",
      "epoch 23 | loss: 9.60189 | val_0_rmse: 3.96911 |  0:00:06s\n",
      "epoch 24 | loss: 9.80481 | val_0_rmse: 3.63028 |  0:00:06s\n",
      "epoch 25 | loss: 7.79097 | val_0_rmse: 3.25868 |  0:00:07s\n",
      "epoch 26 | loss: 8.11045 | val_0_rmse: 2.9283  |  0:00:07s\n",
      "epoch 27 | loss: 5.39486 | val_0_rmse: 2.47384 |  0:00:07s\n",
      "epoch 28 | loss: 5.4702  | val_0_rmse: 2.1012  |  0:00:08s\n",
      "epoch 29 | loss: 4.93579 | val_0_rmse: 1.79729 |  0:00:08s\n",
      "epoch 30 | loss: 3.6547  | val_0_rmse: 1.54646 |  0:00:08s\n",
      "epoch 31 | loss: 3.71541 | val_0_rmse: 1.30474 |  0:00:08s\n",
      "epoch 32 | loss: 2.96446 | val_0_rmse: 1.16004 |  0:00:09s\n",
      "epoch 33 | loss: 3.10651 | val_0_rmse: 1.06224 |  0:00:09s\n",
      "epoch 34 | loss: 3.05182 | val_0_rmse: 0.95781 |  0:00:09s\n",
      "epoch 35 | loss: 3.03586 | val_0_rmse: 0.85806 |  0:00:09s\n",
      "epoch 36 | loss: 2.37915 | val_0_rmse: 0.75565 |  0:00:10s\n",
      "epoch 37 | loss: 2.20527 | val_0_rmse: 0.65103 |  0:00:10s\n",
      "epoch 38 | loss: 2.38082 | val_0_rmse: 0.63722 |  0:00:10s\n",
      "epoch 39 | loss: 1.94119 | val_0_rmse: 0.65592 |  0:00:10s\n",
      "epoch 40 | loss: 1.96389 | val_0_rmse: 0.6941  |  0:00:11s\n",
      "epoch 41 | loss: 1.93739 | val_0_rmse: 0.67698 |  0:00:11s\n",
      "epoch 42 | loss: 1.98625 | val_0_rmse: 0.63912 |  0:00:11s\n",
      "epoch 43 | loss: 1.75798 | val_0_rmse: 0.52486 |  0:00:12s\n",
      "epoch 44 | loss: 1.44559 | val_0_rmse: 0.49367 |  0:00:12s\n",
      "epoch 45 | loss: 1.81523 | val_0_rmse: 0.55284 |  0:00:12s\n",
      "epoch 46 | loss: 1.71019 | val_0_rmse: 0.62177 |  0:00:12s\n",
      "epoch 47 | loss: 1.54652 | val_0_rmse: 0.53381 |  0:00:13s\n",
      "epoch 48 | loss: 1.54151 | val_0_rmse: 0.4612  |  0:00:13s\n",
      "epoch 49 | loss: 1.40965 | val_0_rmse: 0.48425 |  0:00:13s\n",
      "epoch 50 | loss: 1.29943 | val_0_rmse: 0.55459 |  0:00:13s\n",
      "epoch 51 | loss: 1.20702 | val_0_rmse: 0.48494 |  0:00:14s\n",
      "epoch 52 | loss: 1.31185 | val_0_rmse: 0.44673 |  0:00:14s\n",
      "epoch 53 | loss: 1.34226 | val_0_rmse: 0.4546  |  0:00:14s\n",
      "epoch 54 | loss: 1.37721 | val_0_rmse: 0.49436 |  0:00:14s\n",
      "epoch 55 | loss: 1.14106 | val_0_rmse: 0.57286 |  0:00:15s\n",
      "epoch 56 | loss: 1.15953 | val_0_rmse: 0.60768 |  0:00:15s\n",
      "epoch 57 | loss: 1.01871 | val_0_rmse: 0.50736 |  0:00:15s\n",
      "epoch 58 | loss: 1.16066 | val_0_rmse: 0.47619 |  0:00:15s\n",
      "epoch 59 | loss: 1.04914 | val_0_rmse: 0.55821 |  0:00:16s\n",
      "epoch 60 | loss: 0.94727 | val_0_rmse: 0.49403 |  0:00:16s\n",
      "epoch 61 | loss: 0.91272 | val_0_rmse: 0.43317 |  0:00:16s\n",
      "epoch 62 | loss: 0.93747 | val_0_rmse: 0.42823 |  0:00:17s\n",
      "epoch 63 | loss: 0.98072 | val_0_rmse: 0.44529 |  0:00:17s\n",
      "epoch 64 | loss: 0.80497 | val_0_rmse: 0.46648 |  0:00:17s\n",
      "epoch 65 | loss: 0.83506 | val_0_rmse: 0.42231 |  0:00:17s\n",
      "epoch 66 | loss: 0.80731 | val_0_rmse: 0.48289 |  0:00:18s\n",
      "epoch 67 | loss: 0.78894 | val_0_rmse: 0.52414 |  0:00:18s\n",
      "epoch 68 | loss: 0.8941  | val_0_rmse: 0.40496 |  0:00:18s\n",
      "epoch 69 | loss: 0.96285 | val_0_rmse: 0.41226 |  0:00:18s\n",
      "epoch 70 | loss: 1.0842  | val_0_rmse: 0.40191 |  0:00:19s\n",
      "epoch 71 | loss: 0.70495 | val_0_rmse: 0.44429 |  0:00:19s\n",
      "epoch 72 | loss: 0.71082 | val_0_rmse: 0.49053 |  0:00:19s\n",
      "epoch 73 | loss: 0.74632 | val_0_rmse: 0.4506  |  0:00:19s\n",
      "epoch 74 | loss: 0.73075 | val_0_rmse: 0.44131 |  0:00:20s\n",
      "epoch 75 | loss: 0.701   | val_0_rmse: 0.46322 |  0:00:20s\n",
      "epoch 76 | loss: 0.75428 | val_0_rmse: 0.47827 |  0:00:20s\n",
      "epoch 77 | loss: 0.63595 | val_0_rmse: 0.40722 |  0:00:20s\n",
      "epoch 78 | loss: 0.77062 | val_0_rmse: 0.45116 |  0:00:21s\n",
      "epoch 79 | loss: 0.64869 | val_0_rmse: 0.53218 |  0:00:21s\n",
      "epoch 80 | loss: 0.70726 | val_0_rmse: 0.41947 |  0:00:21s\n",
      "epoch 81 | loss: 0.6725  | val_0_rmse: 0.40686 |  0:00:21s\n",
      "epoch 82 | loss: 0.63086 | val_0_rmse: 0.50674 |  0:00:22s\n",
      "epoch 83 | loss: 0.63857 | val_0_rmse: 0.47117 |  0:00:22s\n",
      "epoch 84 | loss: 0.58821 | val_0_rmse: 0.42862 |  0:00:22s\n",
      "epoch 85 | loss: 0.63591 | val_0_rmse: 0.44923 |  0:00:22s\n",
      "epoch 86 | loss: 0.56164 | val_0_rmse: 0.41086 |  0:00:23s\n",
      "epoch 87 | loss: 0.43465 | val_0_rmse: 0.40754 |  0:00:23s\n",
      "epoch 88 | loss: 0.4555  | val_0_rmse: 0.3978  |  0:00:23s\n",
      "epoch 89 | loss: 0.47494 | val_0_rmse: 0.40049 |  0:00:23s\n",
      "epoch 90 | loss: 0.46124 | val_0_rmse: 0.39819 |  0:00:24s\n",
      "epoch 91 | loss: 0.46737 | val_0_rmse: 0.4024  |  0:00:24s\n",
      "epoch 92 | loss: 0.4535  | val_0_rmse: 0.40791 |  0:00:24s\n",
      "epoch 93 | loss: 0.45787 | val_0_rmse: 0.40176 |  0:00:24s\n",
      "epoch 94 | loss: 0.4143  | val_0_rmse: 0.46064 |  0:00:25s\n",
      "epoch 95 | loss: 0.48222 | val_0_rmse: 0.44202 |  0:00:25s\n",
      "epoch 96 | loss: 0.43843 | val_0_rmse: 0.42509 |  0:00:25s\n",
      "epoch 97 | loss: 0.46597 | val_0_rmse: 0.4705  |  0:00:25s\n",
      "epoch 98 | loss: 0.51615 | val_0_rmse: 0.45343 |  0:00:26s\n",
      "epoch 99 | loss: 0.43252 | val_0_rmse: 0.44157 |  0:00:26s\n",
      "epoch 100| loss: 0.39982 | val_0_rmse: 0.48709 |  0:00:26s\n",
      "epoch 101| loss: 0.41763 | val_0_rmse: 0.44026 |  0:00:26s\n",
      "epoch 102| loss: 0.37234 | val_0_rmse: 0.38944 |  0:00:27s\n",
      "epoch 103| loss: 0.36918 | val_0_rmse: 0.45566 |  0:00:27s\n",
      "epoch 104| loss: 0.44614 | val_0_rmse: 0.39137 |  0:00:27s\n",
      "epoch 105| loss: 0.43369 | val_0_rmse: 0.48584 |  0:00:27s\n",
      "epoch 106| loss: 0.44271 | val_0_rmse: 0.4196  |  0:00:28s\n",
      "epoch 107| loss: 0.38014 | val_0_rmse: 0.40096 |  0:00:28s\n",
      "epoch 108| loss: 0.38109 | val_0_rmse: 0.41625 |  0:00:28s\n",
      "epoch 109| loss: 0.38352 | val_0_rmse: 0.41396 |  0:00:28s\n",
      "epoch 110| loss: 0.35685 | val_0_rmse: 0.50319 |  0:00:29s\n",
      "epoch 111| loss: 0.42411 | val_0_rmse: 0.39872 |  0:00:29s\n",
      "epoch 112| loss: 0.39784 | val_0_rmse: 0.36364 |  0:00:29s\n",
      "epoch 113| loss: 0.31363 | val_0_rmse: 0.39747 |  0:00:29s\n",
      "epoch 114| loss: 0.35778 | val_0_rmse: 0.39157 |  0:00:30s\n",
      "epoch 115| loss: 0.29232 | val_0_rmse: 0.40182 |  0:00:30s\n",
      "epoch 116| loss: 0.30197 | val_0_rmse: 0.42442 |  0:00:30s\n",
      "epoch 117| loss: 0.28275 | val_0_rmse: 0.38879 |  0:00:30s\n",
      "epoch 118| loss: 0.27489 | val_0_rmse: 0.41521 |  0:00:31s\n",
      "epoch 119| loss: 0.29188 | val_0_rmse: 0.39553 |  0:00:31s\n",
      "epoch 120| loss: 0.3205  | val_0_rmse: 0.40441 |  0:00:31s\n",
      "epoch 121| loss: 0.29839 | val_0_rmse: 0.42854 |  0:00:31s\n",
      "epoch 122| loss: 0.32508 | val_0_rmse: 0.36839 |  0:00:32s\n",
      "epoch 123| loss: 0.30465 | val_0_rmse: 0.393   |  0:00:32s\n",
      "epoch 124| loss: 0.3101  | val_0_rmse: 0.45319 |  0:00:32s\n",
      "epoch 125| loss: 0.35863 | val_0_rmse: 0.4085  |  0:00:33s\n",
      "epoch 126| loss: 0.31198 | val_0_rmse: 0.39155 |  0:00:33s\n",
      "epoch 127| loss: 0.31999 | val_0_rmse: 0.38749 |  0:00:33s\n",
      "epoch 128| loss: 0.30332 | val_0_rmse: 0.40524 |  0:00:33s\n",
      "epoch 129| loss: 0.36247 | val_0_rmse: 0.39664 |  0:00:34s\n",
      "epoch 130| loss: 0.28283 | val_0_rmse: 0.37862 |  0:00:34s\n",
      "epoch 131| loss: 0.25939 | val_0_rmse: 0.40091 |  0:00:34s\n",
      "epoch 132| loss: 0.29397 | val_0_rmse: 0.35937 |  0:00:34s\n",
      "epoch 133| loss: 0.26927 | val_0_rmse: 0.37656 |  0:00:35s\n",
      "epoch 134| loss: 0.33391 | val_0_rmse: 0.47251 |  0:00:35s\n",
      "epoch 135| loss: 0.3475  | val_0_rmse: 0.39816 |  0:00:35s\n",
      "epoch 136| loss: 0.26764 | val_0_rmse: 0.42477 |  0:00:35s\n",
      "epoch 137| loss: 0.44545 | val_0_rmse: 0.48289 |  0:00:36s\n",
      "epoch 138| loss: 0.47437 | val_0_rmse: 0.45845 |  0:00:36s\n",
      "epoch 139| loss: 0.36399 | val_0_rmse: 0.50657 |  0:00:36s\n",
      "epoch 140| loss: 0.45145 | val_0_rmse: 0.41731 |  0:00:36s\n",
      "epoch 141| loss: 0.35752 | val_0_rmse: 0.53291 |  0:00:37s\n",
      "epoch 142| loss: 0.45766 | val_0_rmse: 0.36895 |  0:00:37s\n",
      "epoch 143| loss: 0.27101 | val_0_rmse: 0.43297 |  0:00:37s\n",
      "epoch 144| loss: 0.35965 | val_0_rmse: 0.36073 |  0:00:37s\n",
      "epoch 145| loss: 0.24446 | val_0_rmse: 0.4143  |  0:00:38s\n",
      "epoch 146| loss: 0.29059 | val_0_rmse: 0.36894 |  0:00:38s\n",
      "epoch 147| loss: 0.19897 | val_0_rmse: 0.38215 |  0:00:38s\n",
      "epoch 148| loss: 0.2457  | val_0_rmse: 0.364   |  0:00:38s\n",
      "epoch 149| loss: 0.2038  | val_0_rmse: 0.40248 |  0:00:39s\n",
      "epoch 150| loss: 0.24526 | val_0_rmse: 0.36075 |  0:00:39s\n",
      "epoch 151| loss: 0.21229 | val_0_rmse: 0.35593 |  0:00:39s\n",
      "epoch 152| loss: 0.19202 | val_0_rmse: 0.38748 |  0:00:39s\n",
      "epoch 153| loss: 0.22834 | val_0_rmse: 0.36216 |  0:00:40s\n",
      "epoch 154| loss: 0.20414 | val_0_rmse: 0.38796 |  0:00:40s\n",
      "epoch 155| loss: 0.23244 | val_0_rmse: 0.34301 |  0:00:40s\n",
      "epoch 156| loss: 0.1998  | val_0_rmse: 0.38547 |  0:00:41s\n",
      "epoch 157| loss: 0.27956 | val_0_rmse: 0.33362 |  0:00:41s\n",
      "epoch 158| loss: 0.19348 | val_0_rmse: 0.33929 |  0:00:41s\n",
      "epoch 159| loss: 0.18739 | val_0_rmse: 0.34862 |  0:00:41s\n",
      "epoch 160| loss: 0.23241 | val_0_rmse: 0.33543 |  0:00:42s\n",
      "epoch 161| loss: 0.20181 | val_0_rmse: 0.36963 |  0:00:42s\n",
      "epoch 162| loss: 0.23273 | val_0_rmse: 0.34076 |  0:00:42s\n",
      "epoch 163| loss: 0.19543 | val_0_rmse: 0.33141 |  0:00:42s\n",
      "epoch 164| loss: 0.18428 | val_0_rmse: 0.43518 |  0:00:43s\n",
      "epoch 165| loss: 0.27116 | val_0_rmse: 0.33622 |  0:00:43s\n",
      "epoch 166| loss: 0.20559 | val_0_rmse: 0.34017 |  0:00:43s\n",
      "epoch 167| loss: 0.21232 | val_0_rmse: 0.44013 |  0:00:43s\n",
      "epoch 168| loss: 0.34363 | val_0_rmse: 0.37871 |  0:00:44s\n",
      "epoch 169| loss: 0.23582 | val_0_rmse: 0.38024 |  0:00:44s\n",
      "epoch 170| loss: 0.24005 | val_0_rmse: 0.33514 |  0:00:44s\n",
      "epoch 171| loss: 0.19973 | val_0_rmse: 0.36025 |  0:00:44s\n",
      "epoch 172| loss: 0.21554 | val_0_rmse: 0.36063 |  0:00:45s\n",
      "epoch 173| loss: 0.18505 | val_0_rmse: 0.35698 |  0:00:45s\n",
      "epoch 174| loss: 0.19261 | val_0_rmse: 0.34699 |  0:00:45s\n",
      "epoch 175| loss: 0.251   | val_0_rmse: 0.34551 |  0:00:45s\n",
      "epoch 176| loss: 0.20402 | val_0_rmse: 0.41154 |  0:00:46s\n",
      "epoch 177| loss: 0.22772 | val_0_rmse: 0.31973 |  0:00:46s\n",
      "epoch 178| loss: 0.2011  | val_0_rmse: 0.31758 |  0:00:46s\n",
      "epoch 179| loss: 0.18906 | val_0_rmse: 0.36644 |  0:00:46s\n",
      "epoch 180| loss: 0.26199 | val_0_rmse: 0.33405 |  0:00:47s\n",
      "epoch 181| loss: 0.16568 | val_0_rmse: 0.36109 |  0:00:47s\n",
      "epoch 182| loss: 0.18287 | val_0_rmse: 0.36062 |  0:00:47s\n",
      "epoch 183| loss: 0.19309 | val_0_rmse: 0.35697 |  0:00:47s\n",
      "epoch 184| loss: 0.17668 | val_0_rmse: 0.36338 |  0:00:48s\n",
      "epoch 185| loss: 0.17919 | val_0_rmse: 0.35669 |  0:00:48s\n",
      "epoch 186| loss: 0.16294 | val_0_rmse: 0.43282 |  0:00:48s\n",
      "epoch 187| loss: 0.22511 | val_0_rmse: 0.36287 |  0:00:48s\n",
      "epoch 188| loss: 0.15255 | val_0_rmse: 0.38307 |  0:00:49s\n",
      "epoch 189| loss: 0.20232 | val_0_rmse: 0.33439 |  0:00:49s\n",
      "epoch 190| loss: 0.17811 | val_0_rmse: 0.34699 |  0:00:49s\n",
      "epoch 191| loss: 0.16554 | val_0_rmse: 0.44743 |  0:00:50s\n",
      "epoch 192| loss: 0.28524 | val_0_rmse: 0.39779 |  0:00:50s\n",
      "epoch 193| loss: 0.20743 | val_0_rmse: 0.4101  |  0:00:50s\n",
      "epoch 194| loss: 0.28212 | val_0_rmse: 0.35711 |  0:00:50s\n",
      "epoch 195| loss: 0.19244 | val_0_rmse: 0.38154 |  0:00:51s\n",
      "epoch 196| loss: 0.2317  | val_0_rmse: 0.34283 |  0:00:51s\n",
      "epoch 197| loss: 0.15929 | val_0_rmse: 0.37582 |  0:00:51s\n",
      "epoch 198| loss: 0.28087 | val_0_rmse: 0.39939 |  0:00:51s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:07:17,886] Trial 5 finished with value: 0.3175810704859477 and parameters: {'n_d': 64, 'n_a': 32, 'n_steps': 5, 'gamma': 1.6233522215458511, 'n_independent': 1, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.005821119647026179, 'batch_size': 1024, 'virtual_batch_size': 64}. Best is trial 2 with value: 0.29063923505231687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199| loss: 0.25287 | val_0_rmse: 0.32006 |  0:00:52s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 178 and best_val_0_rmse = 0.31758\n",
      "Trial 005 | rmse_log=0.31758 | RMSE$=70,446 | MAE$=42,250 | MAPE=22.51% | n_d/n_a=64/32 steps=5 lr=0.00582 batch=1024 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 175.80131| val_0_rmse: 12.00985|  0:00:00s\n",
      "epoch 1  | loss: 162.18645| val_0_rmse: 11.7793 |  0:00:00s\n",
      "epoch 2  | loss: 149.15716| val_0_rmse: 11.62847|  0:00:00s\n",
      "epoch 3  | loss: 139.15523| val_0_rmse: 11.47204|  0:00:00s\n",
      "epoch 4  | loss: 131.08334| val_0_rmse: 11.3097 |  0:00:00s\n",
      "epoch 5  | loss: 121.9162| val_0_rmse: 11.15746|  0:00:00s\n",
      "epoch 6  | loss: 112.31463| val_0_rmse: 10.99298|  0:00:01s\n",
      "epoch 7  | loss: 104.9654| val_0_rmse: 10.82823|  0:00:01s\n",
      "epoch 8  | loss: 98.81978| val_0_rmse: 10.63588|  0:00:01s\n",
      "epoch 9  | loss: 92.79635| val_0_rmse: 10.42157|  0:00:01s\n",
      "epoch 10 | loss: 86.56544| val_0_rmse: 10.21686|  0:00:01s\n",
      "epoch 11 | loss: 79.66138| val_0_rmse: 9.99903 |  0:00:01s\n",
      "epoch 12 | loss: 74.01886| val_0_rmse: 9.76443 |  0:00:01s\n",
      "epoch 13 | loss: 68.94985| val_0_rmse: 9.5241  |  0:00:02s\n",
      "epoch 14 | loss: 63.91606| val_0_rmse: 9.25861 |  0:00:02s\n",
      "epoch 15 | loss: 57.69718| val_0_rmse: 8.96111 |  0:00:02s\n",
      "epoch 16 | loss: 52.84705| val_0_rmse: 8.66821 |  0:00:02s\n",
      "epoch 17 | loss: 47.3671 | val_0_rmse: 8.36327 |  0:00:02s\n",
      "epoch 18 | loss: 44.33335| val_0_rmse: 8.02088 |  0:00:02s\n",
      "epoch 19 | loss: 37.16967| val_0_rmse: 7.62358 |  0:00:02s\n",
      "epoch 20 | loss: 37.14846| val_0_rmse: 7.2606  |  0:00:03s\n",
      "epoch 21 | loss: 30.14003| val_0_rmse: 6.84868 |  0:00:03s\n",
      "epoch 22 | loss: 26.91555| val_0_rmse: 6.45269 |  0:00:03s\n",
      "epoch 23 | loss: 25.03096| val_0_rmse: 6.03354 |  0:00:03s\n",
      "epoch 24 | loss: 23.13275| val_0_rmse: 5.64142 |  0:00:03s\n",
      "epoch 25 | loss: 19.86628| val_0_rmse: 5.25546 |  0:00:03s\n",
      "epoch 26 | loss: 16.3458 | val_0_rmse: 4.85194 |  0:00:03s\n",
      "epoch 27 | loss: 13.21067| val_0_rmse: 4.49192 |  0:00:04s\n",
      "epoch 28 | loss: 13.48396| val_0_rmse: 4.13132 |  0:00:04s\n",
      "epoch 29 | loss: 12.15403| val_0_rmse: 3.80318 |  0:00:04s\n",
      "epoch 30 | loss: 9.42168 | val_0_rmse: 3.46311 |  0:00:04s\n",
      "epoch 31 | loss: 9.20909 | val_0_rmse: 3.19574 |  0:00:04s\n",
      "epoch 32 | loss: 7.20448 | val_0_rmse: 2.91338 |  0:00:04s\n",
      "epoch 33 | loss: 7.26796 | val_0_rmse: 2.70587 |  0:00:05s\n",
      "epoch 34 | loss: 6.59335 | val_0_rmse: 2.52011 |  0:00:05s\n",
      "epoch 35 | loss: 5.2158  | val_0_rmse: 2.35207 |  0:00:05s\n",
      "epoch 36 | loss: 4.52529 | val_0_rmse: 2.18316 |  0:00:05s\n",
      "epoch 37 | loss: 4.13774 | val_0_rmse: 2.02969 |  0:00:05s\n",
      "epoch 38 | loss: 3.66882 | val_0_rmse: 1.94858 |  0:00:05s\n",
      "epoch 39 | loss: 4.47413 | val_0_rmse: 1.86254 |  0:00:06s\n",
      "epoch 40 | loss: 3.21025 | val_0_rmse: 1.76824 |  0:00:06s\n",
      "epoch 41 | loss: 3.62074 | val_0_rmse: 1.65435 |  0:00:06s\n",
      "epoch 42 | loss: 2.5446  | val_0_rmse: 1.56005 |  0:00:06s\n",
      "epoch 43 | loss: 2.01887 | val_0_rmse: 1.45453 |  0:00:06s\n",
      "epoch 44 | loss: 2.54649 | val_0_rmse: 1.36399 |  0:00:06s\n",
      "epoch 45 | loss: 1.65751 | val_0_rmse: 1.28155 |  0:00:06s\n",
      "epoch 46 | loss: 1.61723 | val_0_rmse: 1.18291 |  0:00:07s\n",
      "epoch 47 | loss: 1.6837  | val_0_rmse: 1.09266 |  0:00:07s\n",
      "epoch 48 | loss: 1.5265  | val_0_rmse: 1.02418 |  0:00:07s\n",
      "epoch 49 | loss: 1.31323 | val_0_rmse: 0.96993 |  0:00:07s\n",
      "epoch 50 | loss: 1.29211 | val_0_rmse: 0.91854 |  0:00:07s\n",
      "epoch 51 | loss: 0.91184 | val_0_rmse: 0.82607 |  0:00:07s\n",
      "epoch 52 | loss: 0.80941 | val_0_rmse: 0.76311 |  0:00:07s\n",
      "epoch 53 | loss: 0.98085 | val_0_rmse: 0.71948 |  0:00:08s\n",
      "epoch 54 | loss: 0.80897 | val_0_rmse: 0.72571 |  0:00:08s\n",
      "epoch 55 | loss: 1.17671 | val_0_rmse: 0.78799 |  0:00:08s\n",
      "epoch 56 | loss: 0.91867 | val_0_rmse: 0.78702 |  0:00:08s\n",
      "epoch 57 | loss: 0.88735 | val_0_rmse: 0.68265 |  0:00:08s\n",
      "epoch 58 | loss: 0.78919 | val_0_rmse: 0.55111 |  0:00:08s\n",
      "epoch 59 | loss: 0.70611 | val_0_rmse: 0.57952 |  0:00:08s\n",
      "epoch 60 | loss: 0.58398 | val_0_rmse: 0.65999 |  0:00:09s\n",
      "epoch 61 | loss: 0.73831 | val_0_rmse: 0.65723 |  0:00:09s\n",
      "epoch 62 | loss: 0.70875 | val_0_rmse: 0.5598  |  0:00:09s\n",
      "epoch 63 | loss: 0.78479 | val_0_rmse: 0.53021 |  0:00:09s\n",
      "epoch 64 | loss: 0.58173 | val_0_rmse: 0.62906 |  0:00:09s\n",
      "epoch 65 | loss: 0.55879 | val_0_rmse: 0.62039 |  0:00:09s\n",
      "epoch 66 | loss: 0.52926 | val_0_rmse: 0.47639 |  0:00:09s\n",
      "epoch 67 | loss: 0.58324 | val_0_rmse: 0.44079 |  0:00:10s\n",
      "epoch 68 | loss: 0.54636 | val_0_rmse: 0.4845  |  0:00:10s\n",
      "epoch 69 | loss: 0.38065 | val_0_rmse: 0.53428 |  0:00:10s\n",
      "epoch 70 | loss: 0.42895 | val_0_rmse: 0.48489 |  0:00:10s\n",
      "epoch 71 | loss: 0.36634 | val_0_rmse: 0.43026 |  0:00:10s\n",
      "epoch 72 | loss: 0.409   | val_0_rmse: 0.43163 |  0:00:10s\n",
      "epoch 73 | loss: 0.43791 | val_0_rmse: 0.57958 |  0:00:10s\n",
      "epoch 74 | loss: 0.34417 | val_0_rmse: 0.48161 |  0:00:11s\n",
      "epoch 75 | loss: 0.31312 | val_0_rmse: 0.36999 |  0:00:11s\n",
      "epoch 76 | loss: 0.35153 | val_0_rmse: 0.38645 |  0:00:11s\n",
      "epoch 77 | loss: 0.26    | val_0_rmse: 0.45368 |  0:00:11s\n",
      "epoch 78 | loss: 0.29518 | val_0_rmse: 0.39621 |  0:00:11s\n",
      "epoch 79 | loss: 0.26298 | val_0_rmse: 0.37855 |  0:00:11s\n",
      "epoch 80 | loss: 0.22331 | val_0_rmse: 0.39806 |  0:00:11s\n",
      "epoch 81 | loss: 0.25964 | val_0_rmse: 0.43177 |  0:00:11s\n",
      "epoch 82 | loss: 0.27564 | val_0_rmse: 0.37857 |  0:00:12s\n",
      "epoch 83 | loss: 0.37144 | val_0_rmse: 0.40777 |  0:00:12s\n",
      "epoch 84 | loss: 0.34155 | val_0_rmse: 0.43388 |  0:00:12s\n",
      "epoch 85 | loss: 0.29051 | val_0_rmse: 0.41433 |  0:00:12s\n",
      "epoch 86 | loss: 0.2527  | val_0_rmse: 0.38149 |  0:00:12s\n",
      "epoch 87 | loss: 0.27165 | val_0_rmse: 0.42428 |  0:00:12s\n",
      "epoch 88 | loss: 0.29785 | val_0_rmse: 0.42582 |  0:00:12s\n",
      "epoch 89 | loss: 0.23911 | val_0_rmse: 0.38602 |  0:00:13s\n",
      "epoch 90 | loss: 0.20865 | val_0_rmse: 0.39863 |  0:00:13s\n",
      "epoch 91 | loss: 0.26039 | val_0_rmse: 0.36842 |  0:00:13s\n",
      "epoch 92 | loss: 0.20075 | val_0_rmse: 0.40764 |  0:00:13s\n",
      "epoch 93 | loss: 0.17464 | val_0_rmse: 0.36223 |  0:00:13s\n",
      "epoch 94 | loss: 0.17024 | val_0_rmse: 0.35626 |  0:00:13s\n",
      "epoch 95 | loss: 0.19345 | val_0_rmse: 0.44981 |  0:00:13s\n",
      "epoch 96 | loss: 0.25991 | val_0_rmse: 0.51431 |  0:00:14s\n",
      "epoch 97 | loss: 0.19032 | val_0_rmse: 0.38633 |  0:00:14s\n",
      "epoch 98 | loss: 0.15771 | val_0_rmse: 0.34105 |  0:00:14s\n",
      "epoch 99 | loss: 0.16119 | val_0_rmse: 0.39394 |  0:00:14s\n",
      "epoch 100| loss: 0.17402 | val_0_rmse: 0.43246 |  0:00:14s\n",
      "epoch 101| loss: 0.14546 | val_0_rmse: 0.37373 |  0:00:14s\n",
      "epoch 102| loss: 0.18776 | val_0_rmse: 0.3416  |  0:00:14s\n",
      "epoch 103| loss: 0.15765 | val_0_rmse: 0.38555 |  0:00:15s\n",
      "epoch 104| loss: 0.12596 | val_0_rmse: 0.35101 |  0:00:15s\n",
      "epoch 105| loss: 0.11454 | val_0_rmse: 0.33043 |  0:00:15s\n",
      "epoch 106| loss: 0.1169  | val_0_rmse: 0.31922 |  0:00:15s\n",
      "epoch 107| loss: 0.1265  | val_0_rmse: 0.3612  |  0:00:15s\n",
      "epoch 108| loss: 0.17212 | val_0_rmse: 0.38189 |  0:00:15s\n",
      "epoch 109| loss: 0.14579 | val_0_rmse: 0.32403 |  0:00:15s\n",
      "epoch 110| loss: 0.13448 | val_0_rmse: 0.30371 |  0:00:15s\n",
      "epoch 111| loss: 0.12042 | val_0_rmse: 0.31127 |  0:00:16s\n",
      "epoch 112| loss: 0.13138 | val_0_rmse: 0.31433 |  0:00:16s\n",
      "epoch 113| loss: 0.12753 | val_0_rmse: 0.32794 |  0:00:16s\n",
      "epoch 114| loss: 0.13868 | val_0_rmse: 0.31104 |  0:00:16s\n",
      "epoch 115| loss: 0.12609 | val_0_rmse: 0.2905  |  0:00:16s\n",
      "epoch 116| loss: 0.10928 | val_0_rmse: 0.34454 |  0:00:16s\n",
      "epoch 117| loss: 0.12744 | val_0_rmse: 0.31001 |  0:00:16s\n",
      "epoch 118| loss: 0.1325  | val_0_rmse: 0.29592 |  0:00:17s\n",
      "epoch 119| loss: 0.12158 | val_0_rmse: 0.3039  |  0:00:17s\n",
      "epoch 120| loss: 0.12226 | val_0_rmse: 0.33046 |  0:00:17s\n",
      "epoch 121| loss: 0.10767 | val_0_rmse: 0.29806 |  0:00:17s\n",
      "epoch 122| loss: 0.1179  | val_0_rmse: 0.28904 |  0:00:17s\n",
      "epoch 123| loss: 0.12075 | val_0_rmse: 0.3031  |  0:00:17s\n",
      "epoch 124| loss: 0.09495 | val_0_rmse: 0.31209 |  0:00:17s\n",
      "epoch 125| loss: 0.10503 | val_0_rmse: 0.28685 |  0:00:18s\n",
      "epoch 126| loss: 0.08764 | val_0_rmse: 0.30949 |  0:00:18s\n",
      "epoch 127| loss: 0.10984 | val_0_rmse: 0.30479 |  0:00:18s\n",
      "epoch 128| loss: 0.10653 | val_0_rmse: 0.279   |  0:00:18s\n",
      "epoch 129| loss: 0.07713 | val_0_rmse: 0.33169 |  0:00:18s\n",
      "epoch 130| loss: 0.09036 | val_0_rmse: 0.31365 |  0:00:18s\n",
      "epoch 131| loss: 0.11515 | val_0_rmse: 0.3077  |  0:00:18s\n",
      "epoch 132| loss: 0.09882 | val_0_rmse: 0.29663 |  0:00:19s\n",
      "epoch 133| loss: 0.11644 | val_0_rmse: 0.3028  |  0:00:19s\n",
      "epoch 134| loss: 0.08518 | val_0_rmse: 0.34448 |  0:00:19s\n",
      "epoch 135| loss: 0.09209 | val_0_rmse: 0.3174  |  0:00:19s\n",
      "epoch 136| loss: 0.08469 | val_0_rmse: 0.2732  |  0:00:19s\n",
      "epoch 137| loss: 0.07468 | val_0_rmse: 0.30949 |  0:00:19s\n",
      "epoch 138| loss: 0.08518 | val_0_rmse: 0.30415 |  0:00:19s\n",
      "epoch 139| loss: 0.08202 | val_0_rmse: 0.31755 |  0:00:20s\n",
      "epoch 140| loss: 0.09728 | val_0_rmse: 0.30257 |  0:00:20s\n",
      "epoch 141| loss: 0.08716 | val_0_rmse: 0.30359 |  0:00:20s\n",
      "epoch 142| loss: 0.0944  | val_0_rmse: 0.31267 |  0:00:20s\n",
      "epoch 143| loss: 0.08441 | val_0_rmse: 0.28627 |  0:00:20s\n",
      "epoch 144| loss: 0.07698 | val_0_rmse: 0.29137 |  0:00:20s\n",
      "epoch 145| loss: 0.07735 | val_0_rmse: 0.27683 |  0:00:20s\n",
      "epoch 146| loss: 0.07655 | val_0_rmse: 0.28142 |  0:00:21s\n",
      "epoch 147| loss: 0.09239 | val_0_rmse: 0.27169 |  0:00:21s\n",
      "epoch 148| loss: 0.06181 | val_0_rmse: 0.27242 |  0:00:21s\n",
      "epoch 149| loss: 0.06282 | val_0_rmse: 0.27358 |  0:00:21s\n",
      "epoch 150| loss: 0.07296 | val_0_rmse: 0.27501 |  0:00:21s\n",
      "epoch 151| loss: 0.07046 | val_0_rmse: 0.27506 |  0:00:21s\n",
      "epoch 152| loss: 0.06878 | val_0_rmse: 0.26763 |  0:00:21s\n",
      "epoch 153| loss: 0.07037 | val_0_rmse: 0.27095 |  0:00:22s\n",
      "epoch 154| loss: 0.06132 | val_0_rmse: 0.27068 |  0:00:22s\n",
      "epoch 155| loss: 0.0614  | val_0_rmse: 0.29296 |  0:00:22s\n",
      "epoch 156| loss: 0.05861 | val_0_rmse: 0.26812 |  0:00:22s\n",
      "epoch 157| loss: 0.06733 | val_0_rmse: 0.26865 |  0:00:22s\n",
      "epoch 158| loss: 0.06719 | val_0_rmse: 0.29656 |  0:00:22s\n",
      "epoch 159| loss: 0.08508 | val_0_rmse: 0.30416 |  0:00:22s\n",
      "epoch 160| loss: 0.07894 | val_0_rmse: 0.2695  |  0:00:23s\n",
      "epoch 161| loss: 0.06777 | val_0_rmse: 0.28569 |  0:00:23s\n",
      "epoch 162| loss: 0.06209 | val_0_rmse: 0.27533 |  0:00:23s\n",
      "epoch 163| loss: 0.05853 | val_0_rmse: 0.28048 |  0:00:23s\n",
      "epoch 164| loss: 0.0711  | val_0_rmse: 0.26602 |  0:00:23s\n",
      "epoch 165| loss: 0.05583 | val_0_rmse: 0.29155 |  0:00:23s\n",
      "epoch 166| loss: 0.07041 | val_0_rmse: 0.28618 |  0:00:23s\n",
      "epoch 167| loss: 0.07185 | val_0_rmse: 0.28292 |  0:00:24s\n",
      "epoch 168| loss: 0.0802  | val_0_rmse: 0.28838 |  0:00:24s\n",
      "epoch 169| loss: 0.07134 | val_0_rmse: 0.295   |  0:00:24s\n",
      "epoch 170| loss: 0.0924  | val_0_rmse: 0.32068 |  0:00:24s\n",
      "epoch 171| loss: 0.08088 | val_0_rmse: 0.26527 |  0:00:24s\n",
      "epoch 172| loss: 0.0639  | val_0_rmse: 0.28086 |  0:00:24s\n",
      "epoch 173| loss: 0.06734 | val_0_rmse: 0.26726 |  0:00:24s\n",
      "epoch 174| loss: 0.05693 | val_0_rmse: 0.28635 |  0:00:25s\n",
      "epoch 175| loss: 0.06787 | val_0_rmse: 0.28836 |  0:00:25s\n",
      "epoch 176| loss: 0.08091 | val_0_rmse: 0.2888  |  0:00:25s\n",
      "epoch 177| loss: 0.06098 | val_0_rmse: 0.28286 |  0:00:25s\n",
      "epoch 178| loss: 0.06412 | val_0_rmse: 0.28964 |  0:00:25s\n",
      "epoch 179| loss: 0.06587 | val_0_rmse: 0.30303 |  0:00:25s\n",
      "epoch 180| loss: 0.08098 | val_0_rmse: 0.31711 |  0:00:25s\n",
      "epoch 181| loss: 0.08448 | val_0_rmse: 0.28764 |  0:00:25s\n",
      "epoch 182| loss: 0.06267 | val_0_rmse: 0.27603 |  0:00:26s\n",
      "epoch 183| loss: 0.06984 | val_0_rmse: 0.29696 |  0:00:26s\n",
      "epoch 184| loss: 0.06364 | val_0_rmse: 0.29683 |  0:00:26s\n",
      "epoch 185| loss: 0.06415 | val_0_rmse: 0.27894 |  0:00:26s\n",
      "epoch 186| loss: 0.07147 | val_0_rmse: 0.26154 |  0:00:26s\n",
      "epoch 187| loss: 0.06679 | val_0_rmse: 0.27403 |  0:00:26s\n",
      "epoch 188| loss: 0.06348 | val_0_rmse: 0.28643 |  0:00:26s\n",
      "epoch 189| loss: 0.06024 | val_0_rmse: 0.29248 |  0:00:27s\n",
      "epoch 190| loss: 0.0623  | val_0_rmse: 0.27357 |  0:00:27s\n",
      "epoch 191| loss: 0.06325 | val_0_rmse: 0.27384 |  0:00:27s\n",
      "epoch 192| loss: 0.05517 | val_0_rmse: 0.25767 |  0:00:27s\n",
      "epoch 193| loss: 0.0513  | val_0_rmse: 0.26364 |  0:00:27s\n",
      "epoch 194| loss: 0.0555  | val_0_rmse: 0.25421 |  0:00:27s\n",
      "epoch 195| loss: 0.04821 | val_0_rmse: 0.26416 |  0:00:27s\n",
      "epoch 196| loss: 0.05694 | val_0_rmse: 0.27504 |  0:00:28s\n",
      "epoch 197| loss: 0.0541  | val_0_rmse: 0.27199 |  0:00:28s\n",
      "epoch 198| loss: 0.05136 | val_0_rmse: 0.2655  |  0:00:28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:07:46,680] Trial 6 finished with value: 0.25421485266912525 and parameters: {'n_d': 48, 'n_a': 64, 'n_steps': 3, 'gamma': 1.8883160382159603, 'n_independent': 1, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.003824215752478609, 'batch_size': 1024, 'virtual_batch_size': 128}. Best is trial 6 with value: 0.25421485266912525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199| loss: 0.05819 | val_0_rmse: 0.26188 |  0:00:28s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 194 and best_val_0_rmse = 0.25421\n",
      "Trial 006 | rmse_log=0.25421 | RMSE$=55,278 | MAE$=32,819 | MAPE=18.73% | n_d/n_a=48/64 steps=3 lr=0.00382 batch=1024 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 298.50053| val_0_rmse: 12.04776|  0:00:00s\n",
      "epoch 1  | loss: 258.67537| val_0_rmse: 11.6878 |  0:00:00s\n",
      "epoch 2  | loss: 223.53333| val_0_rmse: 11.41682|  0:00:01s\n",
      "epoch 3  | loss: 194.26026| val_0_rmse: 11.13981|  0:00:01s\n",
      "epoch 4  | loss: 169.81119| val_0_rmse: 10.79912|  0:00:02s\n",
      "epoch 5  | loss: 142.94268| val_0_rmse: 10.49133|  0:00:02s\n",
      "epoch 6  | loss: 122.80825| val_0_rmse: 10.13844|  0:00:02s\n",
      "epoch 7  | loss: 103.84264| val_0_rmse: 9.81126 |  0:00:03s\n",
      "epoch 8  | loss: 86.3593 | val_0_rmse: 9.44865 |  0:00:03s\n",
      "epoch 9  | loss: 69.57638| val_0_rmse: 9.04561 |  0:00:04s\n",
      "epoch 10 | loss: 54.03169| val_0_rmse: 8.5597  |  0:00:04s\n",
      "epoch 11 | loss: 46.1339 | val_0_rmse: 8.13639 |  0:00:04s\n",
      "epoch 12 | loss: 35.56233| val_0_rmse: 7.68133 |  0:00:05s\n",
      "epoch 13 | loss: 28.19553| val_0_rmse: 7.24818 |  0:00:05s\n",
      "epoch 14 | loss: 27.32426| val_0_rmse: 6.84225 |  0:00:06s\n",
      "epoch 15 | loss: 23.77776| val_0_rmse: 6.4118  |  0:00:06s\n",
      "epoch 16 | loss: 20.78634| val_0_rmse: 6.16565 |  0:00:06s\n",
      "epoch 17 | loss: 25.1555 | val_0_rmse: 5.95171 |  0:00:07s\n",
      "epoch 18 | loss: 19.68612| val_0_rmse: 5.67922 |  0:00:07s\n",
      "epoch 19 | loss: 20.55443| val_0_rmse: 5.45728 |  0:00:07s\n",
      "epoch 20 | loss: 19.29167| val_0_rmse: 5.23529 |  0:00:08s\n",
      "epoch 21 | loss: 21.70374| val_0_rmse: 5.16841 |  0:00:08s\n",
      "epoch 22 | loss: 17.62634| val_0_rmse: 4.99626 |  0:00:09s\n",
      "epoch 23 | loss: 18.52954| val_0_rmse: 4.93526 |  0:00:09s\n",
      "epoch 24 | loss: 17.71337| val_0_rmse: 4.89112 |  0:00:09s\n",
      "epoch 25 | loss: 14.95814| val_0_rmse: 4.88938 |  0:00:10s\n",
      "epoch 26 | loss: 13.9043 | val_0_rmse: 4.8076  |  0:00:10s\n",
      "epoch 27 | loss: 14.22049| val_0_rmse: 4.83314 |  0:00:11s\n",
      "epoch 28 | loss: 13.66783| val_0_rmse: 4.7669  |  0:00:11s\n",
      "epoch 29 | loss: 13.37458| val_0_rmse: 4.68906 |  0:00:12s\n",
      "epoch 30 | loss: 12.33906| val_0_rmse: 4.47617 |  0:00:12s\n",
      "epoch 31 | loss: 12.15256| val_0_rmse: 4.26629 |  0:00:12s\n",
      "epoch 32 | loss: 12.86298| val_0_rmse: 4.10062 |  0:00:13s\n",
      "epoch 33 | loss: 11.49416| val_0_rmse: 3.97308 |  0:00:13s\n",
      "epoch 34 | loss: 9.72207 | val_0_rmse: 3.82649 |  0:00:13s\n",
      "epoch 35 | loss: 9.75968 | val_0_rmse: 3.46753 |  0:00:14s\n",
      "epoch 36 | loss: 8.68148 | val_0_rmse: 3.25217 |  0:00:14s\n",
      "epoch 37 | loss: 8.97994 | val_0_rmse: 2.99562 |  0:00:15s\n",
      "epoch 38 | loss: 8.08124 | val_0_rmse: 2.82365 |  0:00:15s\n",
      "epoch 39 | loss: 7.93441 | val_0_rmse: 2.72288 |  0:00:16s\n",
      "epoch 40 | loss: 7.65436 | val_0_rmse: 2.60972 |  0:00:16s\n",
      "epoch 41 | loss: 7.79441 | val_0_rmse: 2.45642 |  0:00:16s\n",
      "epoch 42 | loss: 6.23927 | val_0_rmse: 2.35087 |  0:00:17s\n",
      "epoch 43 | loss: 6.4115  | val_0_rmse: 2.10186 |  0:00:17s\n",
      "epoch 44 | loss: 5.92719 | val_0_rmse: 1.94109 |  0:00:17s\n",
      "epoch 45 | loss: 5.69019 | val_0_rmse: 1.75456 |  0:00:18s\n",
      "epoch 46 | loss: 5.27524 | val_0_rmse: 1.60842 |  0:00:18s\n",
      "epoch 47 | loss: 5.27307 | val_0_rmse: 1.49498 |  0:00:19s\n",
      "epoch 48 | loss: 4.37382 | val_0_rmse: 1.39923 |  0:00:19s\n",
      "epoch 49 | loss: 4.74021 | val_0_rmse: 1.29964 |  0:00:20s\n",
      "epoch 50 | loss: 4.37706 | val_0_rmse: 1.1339  |  0:00:20s\n",
      "epoch 51 | loss: 3.72771 | val_0_rmse: 1.01979 |  0:00:20s\n",
      "epoch 52 | loss: 3.63063 | val_0_rmse: 0.89932 |  0:00:21s\n",
      "epoch 53 | loss: 3.4268  | val_0_rmse: 0.74612 |  0:00:21s\n",
      "epoch 54 | loss: 3.47192 | val_0_rmse: 0.67393 |  0:00:21s\n",
      "epoch 55 | loss: 3.222   | val_0_rmse: 0.65391 |  0:00:22s\n",
      "epoch 56 | loss: 3.1971  | val_0_rmse: 0.60123 |  0:00:22s\n",
      "epoch 57 | loss: 2.88526 | val_0_rmse: 0.64254 |  0:00:23s\n",
      "epoch 58 | loss: 2.86251 | val_0_rmse: 0.66513 |  0:00:23s\n",
      "epoch 59 | loss: 2.73014 | val_0_rmse: 0.64803 |  0:00:23s\n",
      "epoch 60 | loss: 2.53552 | val_0_rmse: 0.61819 |  0:00:24s\n",
      "epoch 61 | loss: 2.72579 | val_0_rmse: 0.5933  |  0:00:24s\n",
      "epoch 62 | loss: 2.57801 | val_0_rmse: 0.60452 |  0:00:25s\n",
      "epoch 63 | loss: 2.47653 | val_0_rmse: 0.63267 |  0:00:25s\n",
      "epoch 64 | loss: 2.44809 | val_0_rmse: 0.76429 |  0:00:25s\n",
      "epoch 65 | loss: 2.70356 | val_0_rmse: 0.56405 |  0:00:26s\n",
      "epoch 66 | loss: 2.08499 | val_0_rmse: 0.57151 |  0:00:26s\n",
      "epoch 67 | loss: 2.49379 | val_0_rmse: 0.56751 |  0:00:26s\n",
      "epoch 68 | loss: 2.18336 | val_0_rmse: 0.53832 |  0:00:27s\n",
      "epoch 69 | loss: 1.84984 | val_0_rmse: 0.60759 |  0:00:27s\n",
      "epoch 70 | loss: 1.93825 | val_0_rmse: 0.70406 |  0:00:28s\n",
      "epoch 71 | loss: 1.97656 | val_0_rmse: 0.63871 |  0:00:28s\n",
      "epoch 72 | loss: 2.01476 | val_0_rmse: 0.56322 |  0:00:28s\n",
      "epoch 73 | loss: 1.85782 | val_0_rmse: 0.49101 |  0:00:29s\n",
      "epoch 74 | loss: 1.65602 | val_0_rmse: 0.51935 |  0:00:29s\n",
      "epoch 75 | loss: 1.8759  | val_0_rmse: 0.51577 |  0:00:29s\n",
      "epoch 76 | loss: 1.80961 | val_0_rmse: 0.61958 |  0:00:30s\n",
      "epoch 77 | loss: 1.89632 | val_0_rmse: 0.58756 |  0:00:30s\n",
      "epoch 78 | loss: 1.73866 | val_0_rmse: 0.60251 |  0:00:31s\n",
      "epoch 79 | loss: 1.64719 | val_0_rmse: 0.68937 |  0:00:31s\n",
      "epoch 80 | loss: 1.83919 | val_0_rmse: 0.59923 |  0:00:31s\n",
      "epoch 81 | loss: 1.89059 | val_0_rmse: 0.54106 |  0:00:32s\n",
      "epoch 82 | loss: 1.5242  | val_0_rmse: 0.53231 |  0:00:32s\n",
      "epoch 83 | loss: 1.55189 | val_0_rmse: 0.50955 |  0:00:32s\n",
      "epoch 84 | loss: 1.3946  | val_0_rmse: 0.59222 |  0:00:33s\n",
      "epoch 85 | loss: 1.49982 | val_0_rmse: 0.5466  |  0:00:33s\n",
      "epoch 86 | loss: 1.40476 | val_0_rmse: 0.60483 |  0:00:34s\n",
      "epoch 87 | loss: 1.47535 | val_0_rmse: 0.62751 |  0:00:34s\n",
      "epoch 88 | loss: 1.51838 | val_0_rmse: 0.57455 |  0:00:34s\n",
      "epoch 89 | loss: 1.62361 | val_0_rmse: 0.56282 |  0:00:35s\n",
      "epoch 90 | loss: 1.37654 | val_0_rmse: 0.59383 |  0:00:35s\n",
      "epoch 91 | loss: 1.48644 | val_0_rmse: 0.55486 |  0:00:36s\n",
      "epoch 92 | loss: 1.48839 | val_0_rmse: 0.56901 |  0:00:36s\n",
      "epoch 93 | loss: 1.32464 | val_0_rmse: 0.59496 |  0:00:36s\n",
      "epoch 94 | loss: 1.38792 | val_0_rmse: 0.58283 |  0:00:37s\n",
      "epoch 95 | loss: 1.34068 | val_0_rmse: 0.60991 |  0:00:37s\n",
      "epoch 96 | loss: 1.26977 | val_0_rmse: 0.6051  |  0:00:37s\n",
      "epoch 97 | loss: 1.33826 | val_0_rmse: 0.61196 |  0:00:38s\n",
      "epoch 98 | loss: 1.34601 | val_0_rmse: 0.6112  |  0:00:38s\n",
      "\n",
      "Early stopping occurred at epoch 98 with best_epoch = 73 and best_val_0_rmse = 0.49101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:08:25,863] Trial 7 finished with value: 0.49101191963762764 and parameters: {'n_d': 64, 'n_a': 16, 'n_steps': 8, 'gamma': 1.652574380450601, 'n_independent': 2, 'n_shared': 2, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.004124049066749558, 'batch_size': 1024, 'virtual_batch_size': 64}. Best is trial 6 with value: 0.25421485266912525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 007 | rmse_log=0.49101 | RMSE$=99,310 | MAE$=68,157 | MAPE=39.50% | n_d/n_a=64/16 steps=8 lr=0.00412 batch=1024 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 97.12354| val_0_rmse: 11.52162|  0:00:00s\n",
      "epoch 1  | loss: 87.30417| val_0_rmse: 11.27115|  0:00:00s\n",
      "epoch 2  | loss: 79.92341| val_0_rmse: 11.01087|  0:00:00s\n",
      "epoch 3  | loss: 72.02744| val_0_rmse: 10.69353|  0:00:00s\n",
      "epoch 4  | loss: 66.10841| val_0_rmse: 10.34693|  0:00:01s\n",
      "epoch 5  | loss: 60.02202| val_0_rmse: 10.08705|  0:00:01s\n",
      "epoch 6  | loss: 53.21373| val_0_rmse: 9.74476 |  0:00:01s\n",
      "epoch 7  | loss: 47.48664| val_0_rmse: 9.37972 |  0:00:01s\n",
      "epoch 8  | loss: 42.94258| val_0_rmse: 9.0205  |  0:00:01s\n",
      "epoch 9  | loss: 37.43748| val_0_rmse: 8.64365 |  0:00:01s\n",
      "epoch 10 | loss: 34.39565| val_0_rmse: 8.2279  |  0:00:02s\n",
      "epoch 11 | loss: 32.71831| val_0_rmse: 7.79677 |  0:00:02s\n",
      "epoch 12 | loss: 28.05112| val_0_rmse: 7.44593 |  0:00:02s\n",
      "epoch 13 | loss: 28.72369| val_0_rmse: 6.9689  |  0:00:02s\n",
      "epoch 14 | loss: 21.78197| val_0_rmse: 6.47281 |  0:00:02s\n",
      "epoch 15 | loss: 22.49254| val_0_rmse: 6.05448 |  0:00:03s\n",
      "epoch 16 | loss: 22.12493| val_0_rmse: 5.77658 |  0:00:03s\n",
      "epoch 17 | loss: 19.97239| val_0_rmse: 5.50293 |  0:00:03s\n",
      "epoch 18 | loss: 20.82574| val_0_rmse: 5.24306 |  0:00:03s\n",
      "epoch 19 | loss: 22.01721| val_0_rmse: 5.19125 |  0:00:03s\n",
      "epoch 20 | loss: 20.00538| val_0_rmse: 5.00444 |  0:00:04s\n",
      "epoch 21 | loss: 20.83053| val_0_rmse: 4.83906 |  0:00:04s\n",
      "epoch 22 | loss: 17.23884| val_0_rmse: 4.55638 |  0:00:04s\n",
      "epoch 23 | loss: 16.15497| val_0_rmse: 4.60021 |  0:00:04s\n",
      "epoch 24 | loss: 17.78785| val_0_rmse: 4.50671 |  0:00:04s\n",
      "epoch 25 | loss: 16.0655 | val_0_rmse: 4.45097 |  0:00:05s\n",
      "epoch 26 | loss: 14.11574| val_0_rmse: 4.44966 |  0:00:05s\n",
      "epoch 27 | loss: 12.90791| val_0_rmse: 4.43733 |  0:00:05s\n",
      "epoch 28 | loss: 12.04252| val_0_rmse: 4.41066 |  0:00:05s\n",
      "epoch 29 | loss: 12.31377| val_0_rmse: 4.34221 |  0:00:05s\n",
      "epoch 30 | loss: 9.70882 | val_0_rmse: 4.26236 |  0:00:06s\n",
      "epoch 31 | loss: 9.76686 | val_0_rmse: 4.18448 |  0:00:06s\n",
      "epoch 32 | loss: 9.74034 | val_0_rmse: 4.14729 |  0:00:06s\n",
      "epoch 33 | loss: 9.14096 | val_0_rmse: 3.95443 |  0:00:06s\n",
      "epoch 34 | loss: 7.98434 | val_0_rmse: 3.79983 |  0:00:06s\n",
      "epoch 35 | loss: 8.59271 | val_0_rmse: 3.64306 |  0:00:07s\n",
      "epoch 36 | loss: 6.76577 | val_0_rmse: 3.39817 |  0:00:07s\n",
      "epoch 37 | loss: 7.28075 | val_0_rmse: 3.15888 |  0:00:07s\n",
      "epoch 38 | loss: 5.91296 | val_0_rmse: 2.87079 |  0:00:07s\n",
      "epoch 39 | loss: 5.7638  | val_0_rmse: 2.67969 |  0:00:07s\n",
      "epoch 40 | loss: 5.48772 | val_0_rmse: 2.47541 |  0:00:08s\n",
      "epoch 41 | loss: 5.11761 | val_0_rmse: 2.27511 |  0:00:08s\n",
      "epoch 42 | loss: 5.00761 | val_0_rmse: 2.09739 |  0:00:08s\n",
      "epoch 43 | loss: 4.76391 | val_0_rmse: 2.00702 |  0:00:08s\n",
      "epoch 44 | loss: 4.46866 | val_0_rmse: 1.91505 |  0:00:08s\n",
      "epoch 45 | loss: 3.81502 | val_0_rmse: 1.83478 |  0:00:09s\n",
      "epoch 46 | loss: 3.90565 | val_0_rmse: 1.81279 |  0:00:09s\n",
      "epoch 47 | loss: 3.57592 | val_0_rmse: 1.78066 |  0:00:09s\n",
      "epoch 48 | loss: 2.8891  | val_0_rmse: 1.55363 |  0:00:09s\n",
      "epoch 49 | loss: 2.81067 | val_0_rmse: 1.45848 |  0:00:09s\n",
      "epoch 50 | loss: 2.67179 | val_0_rmse: 1.31703 |  0:00:10s\n",
      "epoch 51 | loss: 2.57171 | val_0_rmse: 1.2109  |  0:00:10s\n",
      "epoch 52 | loss: 2.49794 | val_0_rmse: 1.13817 |  0:00:10s\n",
      "epoch 53 | loss: 2.58451 | val_0_rmse: 1.03497 |  0:00:10s\n",
      "epoch 54 | loss: 1.9003  | val_0_rmse: 0.95915 |  0:00:10s\n",
      "epoch 55 | loss: 1.91174 | val_0_rmse: 0.85861 |  0:00:11s\n",
      "epoch 56 | loss: 1.86391 | val_0_rmse: 0.81495 |  0:00:11s\n",
      "epoch 57 | loss: 1.78063 | val_0_rmse: 0.8005  |  0:00:11s\n",
      "epoch 58 | loss: 1.56056 | val_0_rmse: 0.76254 |  0:00:11s\n",
      "epoch 59 | loss: 1.57808 | val_0_rmse: 0.75359 |  0:00:11s\n",
      "epoch 60 | loss: 1.42025 | val_0_rmse: 0.67443 |  0:00:12s\n",
      "epoch 61 | loss: 1.58604 | val_0_rmse: 0.62644 |  0:00:12s\n",
      "epoch 62 | loss: 1.41666 | val_0_rmse: 0.53555 |  0:00:12s\n",
      "epoch 63 | loss: 1.10086 | val_0_rmse: 0.53405 |  0:00:12s\n",
      "epoch 64 | loss: 1.13501 | val_0_rmse: 0.55592 |  0:00:12s\n",
      "epoch 65 | loss: 1.3352  | val_0_rmse: 0.54397 |  0:00:12s\n",
      "epoch 66 | loss: 0.93187 | val_0_rmse: 0.54189 |  0:00:13s\n",
      "epoch 67 | loss: 0.93579 | val_0_rmse: 0.58501 |  0:00:13s\n",
      "epoch 68 | loss: 0.9444  | val_0_rmse: 0.52998 |  0:00:13s\n",
      "epoch 69 | loss: 0.95191 | val_0_rmse: 0.52025 |  0:00:13s\n",
      "epoch 70 | loss: 0.86025 | val_0_rmse: 0.49081 |  0:00:13s\n",
      "epoch 71 | loss: 0.74454 | val_0_rmse: 0.45347 |  0:00:14s\n",
      "epoch 72 | loss: 0.78394 | val_0_rmse: 0.46137 |  0:00:14s\n",
      "epoch 73 | loss: 0.73781 | val_0_rmse: 0.50385 |  0:00:14s\n",
      "epoch 74 | loss: 0.70683 | val_0_rmse: 0.50522 |  0:00:14s\n",
      "epoch 75 | loss: 0.60048 | val_0_rmse: 0.45496 |  0:00:14s\n",
      "epoch 76 | loss: 0.62509 | val_0_rmse: 0.41454 |  0:00:15s\n",
      "epoch 77 | loss: 0.55997 | val_0_rmse: 0.5344  |  0:00:15s\n",
      "epoch 78 | loss: 0.58289 | val_0_rmse: 0.47729 |  0:00:15s\n",
      "epoch 79 | loss: 0.53762 | val_0_rmse: 0.49879 |  0:00:15s\n",
      "epoch 80 | loss: 0.5678  | val_0_rmse: 0.47222 |  0:00:15s\n",
      "epoch 81 | loss: 0.4986  | val_0_rmse: 0.43249 |  0:00:16s\n",
      "epoch 82 | loss: 0.46877 | val_0_rmse: 0.47894 |  0:00:16s\n",
      "epoch 83 | loss: 0.48164 | val_0_rmse: 0.44721 |  0:00:16s\n",
      "epoch 84 | loss: 0.51681 | val_0_rmse: 0.42288 |  0:00:16s\n",
      "epoch 85 | loss: 0.40413 | val_0_rmse: 0.41132 |  0:00:16s\n",
      "epoch 86 | loss: 0.51029 | val_0_rmse: 0.41418 |  0:00:17s\n",
      "epoch 87 | loss: 0.41609 | val_0_rmse: 0.45329 |  0:00:17s\n",
      "epoch 88 | loss: 0.42751 | val_0_rmse: 0.42551 |  0:00:17s\n",
      "epoch 89 | loss: 0.40372 | val_0_rmse: 0.3644  |  0:00:17s\n",
      "epoch 90 | loss: 0.44046 | val_0_rmse: 0.36597 |  0:00:17s\n",
      "epoch 91 | loss: 0.34239 | val_0_rmse: 0.45131 |  0:00:18s\n",
      "epoch 92 | loss: 0.38351 | val_0_rmse: 0.46063 |  0:00:18s\n",
      "epoch 93 | loss: 0.47348 | val_0_rmse: 0.35356 |  0:00:18s\n",
      "epoch 94 | loss: 0.42097 | val_0_rmse: 0.3675  |  0:00:18s\n",
      "epoch 95 | loss: 0.3747  | val_0_rmse: 0.33229 |  0:00:18s\n",
      "epoch 96 | loss: 0.35848 | val_0_rmse: 0.36099 |  0:00:19s\n",
      "epoch 97 | loss: 0.31008 | val_0_rmse: 0.38806 |  0:00:19s\n",
      "epoch 98 | loss: 0.31912 | val_0_rmse: 0.41318 |  0:00:19s\n",
      "epoch 99 | loss: 0.34469 | val_0_rmse: 0.35118 |  0:00:19s\n",
      "epoch 100| loss: 0.26604 | val_0_rmse: 0.37016 |  0:00:19s\n",
      "epoch 101| loss: 0.3268  | val_0_rmse: 0.38465 |  0:00:19s\n",
      "epoch 102| loss: 0.32186 | val_0_rmse: 0.3646  |  0:00:20s\n",
      "epoch 103| loss: 0.36741 | val_0_rmse: 0.39224 |  0:00:20s\n",
      "epoch 104| loss: 0.32528 | val_0_rmse: 0.33551 |  0:00:20s\n",
      "epoch 105| loss: 0.26942 | val_0_rmse: 0.3699  |  0:00:20s\n",
      "epoch 106| loss: 0.29317 | val_0_rmse: 0.33653 |  0:00:20s\n",
      "epoch 107| loss: 0.27645 | val_0_rmse: 0.38526 |  0:00:21s\n",
      "epoch 108| loss: 0.31261 | val_0_rmse: 0.3726  |  0:00:21s\n",
      "epoch 109| loss: 0.23676 | val_0_rmse: 0.39624 |  0:00:21s\n",
      "epoch 110| loss: 0.28385 | val_0_rmse: 0.37533 |  0:00:21s\n",
      "epoch 111| loss: 0.25137 | val_0_rmse: 0.36754 |  0:00:21s\n",
      "epoch 112| loss: 0.28298 | val_0_rmse: 0.34056 |  0:00:22s\n",
      "epoch 113| loss: 0.24638 | val_0_rmse: 0.3653  |  0:00:22s\n",
      "epoch 114| loss: 0.30595 | val_0_rmse: 0.37152 |  0:00:22s\n",
      "epoch 115| loss: 0.25714 | val_0_rmse: 0.40884 |  0:00:22s\n",
      "epoch 116| loss: 0.31847 | val_0_rmse: 0.35988 |  0:00:22s\n",
      "epoch 117| loss: 0.2474  | val_0_rmse: 0.42779 |  0:00:22s\n",
      "epoch 118| loss: 0.30632 | val_0_rmse: 0.53807 |  0:00:23s\n",
      "epoch 119| loss: 0.43176 | val_0_rmse: 0.44349 |  0:00:23s\n",
      "epoch 120| loss: 0.34958 | val_0_rmse: 0.33594 |  0:00:23s\n",
      "\n",
      "Early stopping occurred at epoch 120 with best_epoch = 95 and best_val_0_rmse = 0.33229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:08:49,730] Trial 8 finished with value: 0.33228779891077187 and parameters: {'n_d': 24, 'n_a': 48, 'n_steps': 7, 'gamma': 1.5325430637987236, 'n_independent': 1, 'n_shared': 1, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.009407980842441974, 'batch_size': 2048, 'virtual_batch_size': 64}. Best is trial 6 with value: 0.25421485266912525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 008 | rmse_log=0.33229 | RMSE$=68,489 | MAE$=46,370 | MAPE=27.97% | n_d/n_a=24/48 steps=7 lr=0.00941 batch=2048 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 137.02556| val_0_rmse: 10.08232|  0:00:00s\n",
      "epoch 1  | loss: 80.17325| val_0_rmse: 8.28926 |  0:00:00s\n",
      "epoch 2  | loss: 49.96016| val_0_rmse: 6.67154 |  0:00:00s\n",
      "epoch 3  | loss: 28.81507| val_0_rmse: 5.3726  |  0:00:00s\n",
      "epoch 4  | loss: 21.10056| val_0_rmse: 4.4746  |  0:00:00s\n",
      "epoch 5  | loss: 14.2968 | val_0_rmse: 4.55845 |  0:00:01s\n",
      "epoch 6  | loss: 6.92725 | val_0_rmse: 4.84403 |  0:00:01s\n",
      "epoch 7  | loss: 4.68415 | val_0_rmse: 4.02482 |  0:00:01s\n",
      "epoch 8  | loss: 2.49417 | val_0_rmse: 2.83423 |  0:00:01s\n",
      "epoch 9  | loss: 1.53489 | val_0_rmse: 2.89739 |  0:00:02s\n",
      "epoch 10 | loss: 1.54915 | val_0_rmse: 2.36505 |  0:00:02s\n",
      "epoch 11 | loss: 0.72855 | val_0_rmse: 2.44476 |  0:00:02s\n",
      "epoch 12 | loss: 0.90477 | val_0_rmse: 2.31505 |  0:00:02s\n",
      "epoch 13 | loss: 1.13101 | val_0_rmse: 1.91361 |  0:00:02s\n",
      "epoch 14 | loss: 0.53324 | val_0_rmse: 1.94161 |  0:00:03s\n",
      "epoch 15 | loss: 0.52497 | val_0_rmse: 1.67456 |  0:00:03s\n",
      "epoch 16 | loss: 0.59674 | val_0_rmse: 1.29195 |  0:00:03s\n",
      "epoch 17 | loss: 0.54257 | val_0_rmse: 1.6586  |  0:00:03s\n",
      "epoch 18 | loss: 0.62005 | val_0_rmse: 1.03049 |  0:00:03s\n",
      "epoch 19 | loss: 0.41895 | val_0_rmse: 1.37708 |  0:00:04s\n",
      "epoch 20 | loss: 0.48542 | val_0_rmse: 0.97143 |  0:00:04s\n",
      "epoch 21 | loss: 0.52982 | val_0_rmse: 1.11097 |  0:00:04s\n",
      "epoch 22 | loss: 0.28003 | val_0_rmse: 0.83237 |  0:00:04s\n",
      "epoch 23 | loss: 0.28248 | val_0_rmse: 0.71771 |  0:00:04s\n",
      "epoch 24 | loss: 0.23482 | val_0_rmse: 0.80536 |  0:00:05s\n",
      "epoch 25 | loss: 0.21217 | val_0_rmse: 0.87936 |  0:00:05s\n",
      "epoch 26 | loss: 0.20967 | val_0_rmse: 0.85809 |  0:00:05s\n",
      "epoch 27 | loss: 0.1675  | val_0_rmse: 0.89669 |  0:00:05s\n",
      "epoch 28 | loss: 0.16677 | val_0_rmse: 0.79155 |  0:00:05s\n",
      "epoch 29 | loss: 0.18634 | val_0_rmse: 0.58412 |  0:00:06s\n",
      "epoch 30 | loss: 0.21731 | val_0_rmse: 0.78851 |  0:00:06s\n",
      "epoch 31 | loss: 0.12679 | val_0_rmse: 0.78917 |  0:00:06s\n",
      "epoch 32 | loss: 0.14272 | val_0_rmse: 0.42949 |  0:00:06s\n",
      "epoch 33 | loss: 0.13769 | val_0_rmse: 0.7464  |  0:00:06s\n",
      "epoch 34 | loss: 0.17189 | val_0_rmse: 0.30736 |  0:00:07s\n",
      "epoch 35 | loss: 0.1818  | val_0_rmse: 0.63346 |  0:00:07s\n",
      "epoch 36 | loss: 0.13394 | val_0_rmse: 0.24434 |  0:00:07s\n",
      "epoch 37 | loss: 0.16541 | val_0_rmse: 0.55196 |  0:00:07s\n",
      "epoch 38 | loss: 0.11616 | val_0_rmse: 0.23543 |  0:00:07s\n",
      "epoch 39 | loss: 0.10934 | val_0_rmse: 0.50172 |  0:00:08s\n",
      "epoch 40 | loss: 0.11976 | val_0_rmse: 0.23165 |  0:00:08s\n",
      "epoch 41 | loss: 0.1022  | val_0_rmse: 0.47286 |  0:00:08s\n",
      "epoch 42 | loss: 0.12098 | val_0_rmse: 0.23462 |  0:00:08s\n",
      "epoch 43 | loss: 0.09317 | val_0_rmse: 0.39678 |  0:00:08s\n",
      "epoch 44 | loss: 0.11978 | val_0_rmse: 0.25401 |  0:00:08s\n",
      "epoch 45 | loss: 0.09607 | val_0_rmse: 0.36456 |  0:00:09s\n",
      "epoch 46 | loss: 0.10215 | val_0_rmse: 0.24669 |  0:00:09s\n",
      "epoch 47 | loss: 0.08592 | val_0_rmse: 0.38031 |  0:00:09s\n",
      "epoch 48 | loss: 0.1026  | val_0_rmse: 0.25561 |  0:00:09s\n",
      "epoch 49 | loss: 0.10417 | val_0_rmse: 0.31817 |  0:00:09s\n",
      "epoch 50 | loss: 0.09395 | val_0_rmse: 0.27121 |  0:00:10s\n",
      "epoch 51 | loss: 0.11091 | val_0_rmse: 0.3727  |  0:00:10s\n",
      "epoch 52 | loss: 0.10376 | val_0_rmse: 0.24099 |  0:00:10s\n",
      "epoch 53 | loss: 0.08072 | val_0_rmse: 0.34426 |  0:00:10s\n",
      "epoch 54 | loss: 0.09238 | val_0_rmse: 0.28911 |  0:00:10s\n",
      "epoch 55 | loss: 0.10669 | val_0_rmse: 0.3016  |  0:00:11s\n",
      "epoch 56 | loss: 0.07608 | val_0_rmse: 0.23332 |  0:00:11s\n",
      "epoch 57 | loss: 0.05242 | val_0_rmse: 0.22844 |  0:00:11s\n",
      "epoch 58 | loss: 0.05667 | val_0_rmse: 0.23269 |  0:00:11s\n",
      "epoch 59 | loss: 0.05854 | val_0_rmse: 0.22569 |  0:00:11s\n",
      "epoch 60 | loss: 0.05533 | val_0_rmse: 0.24728 |  0:00:12s\n",
      "epoch 61 | loss: 0.06187 | val_0_rmse: 0.28349 |  0:00:12s\n",
      "epoch 62 | loss: 0.08999 | val_0_rmse: 0.32883 |  0:00:12s\n",
      "epoch 63 | loss: 0.11893 | val_0_rmse: 0.2392  |  0:00:12s\n",
      "epoch 64 | loss: 0.09524 | val_0_rmse: 0.30371 |  0:00:12s\n",
      "epoch 65 | loss: 0.08571 | val_0_rmse: 0.27437 |  0:00:12s\n",
      "epoch 66 | loss: 0.09648 | val_0_rmse: 0.27721 |  0:00:13s\n",
      "epoch 67 | loss: 0.06534 | val_0_rmse: 0.25757 |  0:00:13s\n",
      "epoch 68 | loss: 0.10472 | val_0_rmse: 0.30576 |  0:00:13s\n",
      "epoch 69 | loss: 0.07599 | val_0_rmse: 0.24328 |  0:00:13s\n",
      "epoch 70 | loss: 0.07633 | val_0_rmse: 0.31678 |  0:00:13s\n",
      "epoch 71 | loss: 0.07888 | val_0_rmse: 0.2489  |  0:00:14s\n",
      "epoch 72 | loss: 0.07435 | val_0_rmse: 0.31709 |  0:00:14s\n",
      "epoch 73 | loss: 0.08234 | val_0_rmse: 0.23107 |  0:00:14s\n",
      "epoch 74 | loss: 0.06268 | val_0_rmse: 0.34304 |  0:00:14s\n",
      "epoch 75 | loss: 0.09074 | val_0_rmse: 0.23857 |  0:00:14s\n",
      "epoch 76 | loss: 0.07326 | val_0_rmse: 0.27011 |  0:00:15s\n",
      "epoch 77 | loss: 0.06177 | val_0_rmse: 0.26684 |  0:00:15s\n",
      "epoch 78 | loss: 0.08488 | val_0_rmse: 0.31124 |  0:00:15s\n",
      "epoch 79 | loss: 0.07881 | val_0_rmse: 0.22816 |  0:00:15s\n",
      "epoch 80 | loss: 0.06054 | val_0_rmse: 0.34754 |  0:00:15s\n",
      "epoch 81 | loss: 0.08543 | val_0_rmse: 0.20262 |  0:00:16s\n",
      "epoch 82 | loss: 0.0503  | val_0_rmse: 0.22917 |  0:00:16s\n",
      "epoch 83 | loss: 0.04504 | val_0_rmse: 0.21336 |  0:00:16s\n",
      "epoch 84 | loss: 0.0404  | val_0_rmse: 0.19445 |  0:00:16s\n",
      "epoch 85 | loss: 0.04064 | val_0_rmse: 0.22573 |  0:00:16s\n",
      "epoch 86 | loss: 0.07076 | val_0_rmse: 0.26947 |  0:00:17s\n",
      "epoch 87 | loss: 0.08404 | val_0_rmse: 0.22025 |  0:00:17s\n",
      "epoch 88 | loss: 0.07866 | val_0_rmse: 0.2486  |  0:00:17s\n",
      "epoch 89 | loss: 0.06594 | val_0_rmse: 0.23912 |  0:00:17s\n",
      "epoch 90 | loss: 0.09047 | val_0_rmse: 0.2464  |  0:00:17s\n",
      "epoch 91 | loss: 0.05609 | val_0_rmse: 0.2253  |  0:00:17s\n",
      "epoch 92 | loss: 0.07898 | val_0_rmse: 0.26429 |  0:00:18s\n",
      "epoch 93 | loss: 0.0694  | val_0_rmse: 0.20588 |  0:00:18s\n",
      "epoch 94 | loss: 0.05097 | val_0_rmse: 0.20536 |  0:00:18s\n",
      "epoch 95 | loss: 0.04808 | val_0_rmse: 0.2112  |  0:00:18s\n",
      "epoch 96 | loss: 0.0353  | val_0_rmse: 0.21421 |  0:00:18s\n",
      "epoch 97 | loss: 0.04876 | val_0_rmse: 0.1998  |  0:00:19s\n",
      "epoch 98 | loss: 0.03908 | val_0_rmse: 0.20429 |  0:00:19s\n",
      "epoch 99 | loss: 0.06149 | val_0_rmse: 0.21893 |  0:00:19s\n",
      "epoch 100| loss: 0.04269 | val_0_rmse: 0.18831 |  0:00:19s\n",
      "epoch 101| loss: 0.05861 | val_0_rmse: 0.21027 |  0:00:19s\n",
      "epoch 102| loss: 0.03802 | val_0_rmse: 0.20556 |  0:00:20s\n",
      "epoch 103| loss: 0.04596 | val_0_rmse: 0.26313 |  0:00:20s\n",
      "epoch 104| loss: 0.04346 | val_0_rmse: 0.19471 |  0:00:20s\n",
      "epoch 105| loss: 0.04804 | val_0_rmse: 0.18814 |  0:00:20s\n",
      "epoch 106| loss: 0.05011 | val_0_rmse: 0.28009 |  0:00:20s\n",
      "epoch 107| loss: 0.05823 | val_0_rmse: 0.18852 |  0:00:20s\n",
      "epoch 108| loss: 0.04803 | val_0_rmse: 0.21186 |  0:00:21s\n",
      "epoch 109| loss: 0.05207 | val_0_rmse: 0.21275 |  0:00:21s\n",
      "epoch 110| loss: 0.03588 | val_0_rmse: 0.20403 |  0:00:21s\n",
      "epoch 111| loss: 0.04969 | val_0_rmse: 0.19823 |  0:00:21s\n",
      "epoch 112| loss: 0.03333 | val_0_rmse: 0.2835  |  0:00:21s\n",
      "epoch 113| loss: 0.06835 | val_0_rmse: 0.24007 |  0:00:22s\n",
      "epoch 114| loss: 0.10529 | val_0_rmse: 0.27725 |  0:00:22s\n",
      "epoch 115| loss: 0.0832  | val_0_rmse: 0.20121 |  0:00:22s\n",
      "epoch 116| loss: 0.05378 | val_0_rmse: 0.29121 |  0:00:22s\n",
      "epoch 117| loss: 0.09618 | val_0_rmse: 0.2168  |  0:00:22s\n",
      "epoch 118| loss: 0.06474 | val_0_rmse: 0.24282 |  0:00:23s\n",
      "epoch 119| loss: 0.06226 | val_0_rmse: 0.24964 |  0:00:23s\n",
      "epoch 120| loss: 0.08295 | val_0_rmse: 0.21055 |  0:00:23s\n",
      "epoch 121| loss: 0.04426 | val_0_rmse: 0.2329  |  0:00:23s\n",
      "epoch 122| loss: 0.09208 | val_0_rmse: 0.20877 |  0:00:23s\n",
      "epoch 123| loss: 0.04389 | val_0_rmse: 0.25499 |  0:00:24s\n",
      "epoch 124| loss: 0.08677 | val_0_rmse: 0.20598 |  0:00:24s\n",
      "epoch 125| loss: 0.04321 | val_0_rmse: 0.24406 |  0:00:24s\n",
      "epoch 126| loss: 0.08259 | val_0_rmse: 0.21153 |  0:00:24s\n",
      "epoch 127| loss: 0.05478 | val_0_rmse: 0.23016 |  0:00:24s\n",
      "epoch 128| loss: 0.07261 | val_0_rmse: 0.21067 |  0:00:24s\n",
      "epoch 129| loss: 0.04655 | val_0_rmse: 0.24679 |  0:00:25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:09:15,472] Trial 9 finished with value: 0.18814445496113438 and parameters: {'n_d': 48, 'n_a': 16, 'n_steps': 3, 'gamma': 1.3388248590997978, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.018812324760215642, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 9 with value: 0.18814445496113438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 130| loss: 0.07677 | val_0_rmse: 0.20351 |  0:00:25s\n",
      "\n",
      "Early stopping occurred at epoch 130 with best_epoch = 105 and best_val_0_rmse = 0.18814\n",
      "Trial 009 | rmse_log=0.18814 | RMSE$=33,326 | MAE$=22,062 | MAPE=14.00% | n_d/n_a=48/16 steps=3 lr=0.01881 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 121.95481| val_0_rmse: 9.28955 |  0:00:00s\n",
      "epoch 1  | loss: 64.5177 | val_0_rmse: 7.42557 |  0:00:00s\n",
      "epoch 2  | loss: 37.7768 | val_0_rmse: 5.80085 |  0:00:00s\n",
      "epoch 3  | loss: 23.80904| val_0_rmse: 4.90489 |  0:00:00s\n",
      "epoch 4  | loss: 22.31516| val_0_rmse: 4.48592 |  0:00:01s\n",
      "epoch 5  | loss: 15.46233| val_0_rmse: 4.79316 |  0:00:01s\n",
      "epoch 6  | loss: 5.75375 | val_0_rmse: 4.64059 |  0:00:01s\n",
      "epoch 7  | loss: 3.61923 | val_0_rmse: 3.31073 |  0:00:01s\n",
      "epoch 8  | loss: 3.06995 | val_0_rmse: 2.90319 |  0:00:01s\n",
      "epoch 9  | loss: 1.35883 | val_0_rmse: 2.51507 |  0:00:02s\n",
      "epoch 10 | loss: 1.51562 | val_0_rmse: 1.80996 |  0:00:02s\n",
      "epoch 11 | loss: 0.90265 | val_0_rmse: 2.18811 |  0:00:02s\n",
      "epoch 12 | loss: 0.8883  | val_0_rmse: 1.76509 |  0:00:02s\n",
      "epoch 13 | loss: 0.71677 | val_0_rmse: 1.79679 |  0:00:02s\n",
      "epoch 14 | loss: 0.41924 | val_0_rmse: 1.54304 |  0:00:02s\n",
      "epoch 15 | loss: 0.50275 | val_0_rmse: 1.59781 |  0:00:03s\n",
      "epoch 16 | loss: 0.51962 | val_0_rmse: 1.71713 |  0:00:03s\n",
      "epoch 17 | loss: 0.37789 | val_0_rmse: 1.16688 |  0:00:03s\n",
      "epoch 18 | loss: 0.54673 | val_0_rmse: 1.60772 |  0:00:03s\n",
      "epoch 19 | loss: 0.43111 | val_0_rmse: 1.14961 |  0:00:03s\n",
      "epoch 20 | loss: 0.49767 | val_0_rmse: 1.42446 |  0:00:04s\n",
      "epoch 21 | loss: 0.31619 | val_0_rmse: 0.91104 |  0:00:04s\n",
      "epoch 22 | loss: 0.27967 | val_0_rmse: 0.9753  |  0:00:04s\n",
      "epoch 23 | loss: 0.2457  | val_0_rmse: 1.1221  |  0:00:04s\n",
      "epoch 24 | loss: 0.31261 | val_0_rmse: 0.7332  |  0:00:04s\n",
      "epoch 25 | loss: 0.25385 | val_0_rmse: 1.00902 |  0:00:05s\n",
      "epoch 26 | loss: 0.19556 | val_0_rmse: 0.628   |  0:00:05s\n",
      "epoch 27 | loss: 0.27303 | val_0_rmse: 1.05573 |  0:00:05s\n",
      "epoch 28 | loss: 0.2939  | val_0_rmse: 0.46281 |  0:00:05s\n",
      "epoch 29 | loss: 0.28977 | val_0_rmse: 0.91955 |  0:00:06s\n",
      "epoch 30 | loss: 0.25711 | val_0_rmse: 0.46353 |  0:00:06s\n",
      "epoch 31 | loss: 0.4533  | val_0_rmse: 0.61087 |  0:00:06s\n",
      "epoch 32 | loss: 0.26835 | val_0_rmse: 0.97738 |  0:00:06s\n",
      "epoch 33 | loss: 0.26784 | val_0_rmse: 0.32454 |  0:00:06s\n",
      "epoch 34 | loss: 0.2821  | val_0_rmse: 0.70331 |  0:00:07s\n",
      "epoch 35 | loss: 0.29916 | val_0_rmse: 0.51864 |  0:00:07s\n",
      "epoch 36 | loss: 0.26675 | val_0_rmse: 0.30778 |  0:00:07s\n",
      "epoch 37 | loss: 0.33691 | val_0_rmse: 0.61953 |  0:00:07s\n",
      "epoch 38 | loss: 0.29204 | val_0_rmse: 0.31284 |  0:00:07s\n",
      "epoch 39 | loss: 0.20093 | val_0_rmse: 0.47399 |  0:00:08s\n",
      "epoch 40 | loss: 0.1436  | val_0_rmse: 0.28872 |  0:00:08s\n",
      "epoch 41 | loss: 0.2365  | val_0_rmse: 0.43683 |  0:00:08s\n",
      "epoch 42 | loss: 0.17336 | val_0_rmse: 0.27491 |  0:00:08s\n",
      "epoch 43 | loss: 0.09932 | val_0_rmse: 0.4131  |  0:00:08s\n",
      "epoch 44 | loss: 0.11834 | val_0_rmse: 0.33837 |  0:00:09s\n",
      "epoch 45 | loss: 0.09824 | val_0_rmse: 0.39008 |  0:00:09s\n",
      "epoch 46 | loss: 0.09641 | val_0_rmse: 0.32941 |  0:00:09s\n",
      "epoch 47 | loss: 0.11779 | val_0_rmse: 0.34303 |  0:00:09s\n",
      "epoch 48 | loss: 0.09676 | val_0_rmse: 0.41682 |  0:00:09s\n",
      "epoch 49 | loss: 0.07829 | val_0_rmse: 0.49222 |  0:00:10s\n",
      "epoch 50 | loss: 0.09611 | val_0_rmse: 0.2739  |  0:00:10s\n",
      "epoch 51 | loss: 0.36621 | val_0_rmse: 0.25742 |  0:00:10s\n",
      "epoch 52 | loss: 0.17653 | val_0_rmse: 0.56996 |  0:00:10s\n",
      "epoch 53 | loss: 0.19456 | val_0_rmse: 0.25453 |  0:00:10s\n",
      "epoch 54 | loss: 0.15648 | val_0_rmse: 0.41441 |  0:00:11s\n",
      "epoch 55 | loss: 0.11045 | val_0_rmse: 0.23992 |  0:00:11s\n",
      "epoch 56 | loss: 0.1676  | val_0_rmse: 0.26165 |  0:00:11s\n",
      "epoch 57 | loss: 0.14437 | val_0_rmse: 0.48532 |  0:00:11s\n",
      "epoch 58 | loss: 0.14868 | val_0_rmse: 0.42012 |  0:00:11s\n",
      "epoch 59 | loss: 0.39826 | val_0_rmse: 0.24177 |  0:00:12s\n",
      "epoch 60 | loss: 0.12546 | val_0_rmse: 0.42562 |  0:00:12s\n",
      "epoch 61 | loss: 0.11965 | val_0_rmse: 0.23025 |  0:00:12s\n",
      "epoch 62 | loss: 0.07004 | val_0_rmse: 0.24461 |  0:00:12s\n",
      "epoch 63 | loss: 0.05771 | val_0_rmse: 0.31631 |  0:00:12s\n",
      "epoch 64 | loss: 0.05864 | val_0_rmse: 0.29276 |  0:00:13s\n",
      "epoch 65 | loss: 0.08148 | val_0_rmse: 0.22987 |  0:00:13s\n",
      "epoch 66 | loss: 0.05405 | val_0_rmse: 0.23129 |  0:00:13s\n",
      "epoch 67 | loss: 0.0497  | val_0_rmse: 0.28311 |  0:00:13s\n",
      "epoch 68 | loss: 0.06289 | val_0_rmse: 0.21716 |  0:00:13s\n",
      "epoch 69 | loss: 0.0523  | val_0_rmse: 0.22471 |  0:00:14s\n",
      "epoch 70 | loss: 0.0538  | val_0_rmse: 0.2579  |  0:00:14s\n",
      "epoch 71 | loss: 0.05118 | val_0_rmse: 0.23871 |  0:00:14s\n",
      "epoch 72 | loss: 0.04174 | val_0_rmse: 0.25129 |  0:00:14s\n",
      "epoch 73 | loss: 0.07435 | val_0_rmse: 0.26433 |  0:00:14s\n",
      "epoch 74 | loss: 0.0562  | val_0_rmse: 0.23903 |  0:00:14s\n",
      "epoch 75 | loss: 0.09773 | val_0_rmse: 0.30508 |  0:00:15s\n",
      "epoch 76 | loss: 0.07492 | val_0_rmse: 0.22816 |  0:00:15s\n",
      "epoch 77 | loss: 0.05952 | val_0_rmse: 0.25976 |  0:00:15s\n",
      "epoch 78 | loss: 0.05577 | val_0_rmse: 0.22908 |  0:00:15s\n",
      "epoch 79 | loss: 0.04288 | val_0_rmse: 0.23347 |  0:00:15s\n",
      "epoch 80 | loss: 0.0662  | val_0_rmse: 0.24056 |  0:00:16s\n",
      "epoch 81 | loss: 0.04987 | val_0_rmse: 0.21604 |  0:00:16s\n",
      "epoch 82 | loss: 0.04299 | val_0_rmse: 0.22258 |  0:00:16s\n",
      "epoch 83 | loss: 0.06112 | val_0_rmse: 0.20491 |  0:00:16s\n",
      "epoch 84 | loss: 0.04825 | val_0_rmse: 0.20146 |  0:00:16s\n",
      "epoch 85 | loss: 0.04585 | val_0_rmse: 0.20641 |  0:00:16s\n",
      "epoch 86 | loss: 0.03874 | val_0_rmse: 0.20696 |  0:00:17s\n",
      "epoch 87 | loss: 0.04204 | val_0_rmse: 0.20334 |  0:00:17s\n",
      "epoch 88 | loss: 0.04293 | val_0_rmse: 0.21138 |  0:00:17s\n",
      "epoch 89 | loss: 0.06721 | val_0_rmse: 0.229   |  0:00:17s\n",
      "epoch 90 | loss: 0.03968 | val_0_rmse: 0.19566 |  0:00:17s\n",
      "epoch 91 | loss: 0.04436 | val_0_rmse: 0.20972 |  0:00:18s\n",
      "epoch 92 | loss: 0.03702 | val_0_rmse: 0.23968 |  0:00:18s\n",
      "epoch 93 | loss: 0.04729 | val_0_rmse: 0.24526 |  0:00:18s\n",
      "epoch 94 | loss: 0.06969 | val_0_rmse: 0.42497 |  0:00:18s\n",
      "epoch 95 | loss: 0.15695 | val_0_rmse: 0.31721 |  0:00:18s\n",
      "epoch 96 | loss: 0.1778  | val_0_rmse: 0.21026 |  0:00:19s\n",
      "epoch 97 | loss: 0.13547 | val_0_rmse: 0.48822 |  0:00:19s\n",
      "epoch 98 | loss: 0.22145 | val_0_rmse: 0.28032 |  0:00:19s\n",
      "epoch 99 | loss: 0.08587 | val_0_rmse: 0.22809 |  0:00:19s\n",
      "epoch 100| loss: 0.0415  | val_0_rmse: 0.21849 |  0:00:19s\n",
      "epoch 101| loss: 0.03879 | val_0_rmse: 0.22688 |  0:00:19s\n",
      "epoch 102| loss: 0.03797 | val_0_rmse: 0.24938 |  0:00:20s\n",
      "epoch 103| loss: 0.03651 | val_0_rmse: 0.20586 |  0:00:20s\n",
      "epoch 104| loss: 0.04838 | val_0_rmse: 0.20263 |  0:00:20s\n",
      "epoch 105| loss: 0.04635 | val_0_rmse: 0.22187 |  0:00:20s\n",
      "epoch 106| loss: 0.05122 | val_0_rmse: 0.22275 |  0:00:20s\n",
      "epoch 107| loss: 0.04089 | val_0_rmse: 0.20434 |  0:00:21s\n",
      "epoch 108| loss: 0.04115 | val_0_rmse: 0.25087 |  0:00:21s\n",
      "epoch 109| loss: 0.0415  | val_0_rmse: 0.21923 |  0:00:21s\n",
      "epoch 110| loss: 0.03682 | val_0_rmse: 0.21241 |  0:00:21s\n",
      "epoch 111| loss: 0.05059 | val_0_rmse: 0.20626 |  0:00:21s\n",
      "epoch 112| loss: 0.03882 | val_0_rmse: 0.26532 |  0:00:21s\n",
      "epoch 113| loss: 0.05822 | val_0_rmse: 0.28589 |  0:00:22s\n",
      "epoch 114| loss: 0.14922 | val_0_rmse: 0.20855 |  0:00:22s\n",
      "epoch 115| loss: 0.12111 | val_0_rmse: 0.47313 |  0:00:22s\n",
      "\n",
      "Early stopping occurred at epoch 115 with best_epoch = 90 and best_val_0_rmse = 0.19566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:09:38,457] Trial 10 finished with value: 0.1956561393903216 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2069742687259282, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.019809301195178163, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 9 with value: 0.18814445496113438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 010 | rmse_log=0.19566 | RMSE$=40,028 | MAE$=24,722 | MAPE=14.42% | n_d/n_a=48/24 steps=3 lr=0.01981 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 122.40066| val_0_rmse: 9.69368 |  0:00:00s\n",
      "epoch 1  | loss: 64.25269| val_0_rmse: 7.80499 |  0:00:00s\n",
      "epoch 2  | loss: 36.71102| val_0_rmse: 6.32351 |  0:00:00s\n",
      "epoch 3  | loss: 25.28202| val_0_rmse: 5.13343 |  0:00:00s\n",
      "epoch 4  | loss: 23.49885| val_0_rmse: 4.77159 |  0:00:00s\n",
      "epoch 5  | loss: 15.90525| val_0_rmse: 5.03854 |  0:00:01s\n",
      "epoch 6  | loss: 7.47163 | val_0_rmse: 5.31875 |  0:00:01s\n",
      "epoch 7  | loss: 5.29411 | val_0_rmse: 4.93947 |  0:00:01s\n",
      "epoch 8  | loss: 3.11543 | val_0_rmse: 3.68944 |  0:00:01s\n",
      "epoch 9  | loss: 3.71979 | val_0_rmse: 3.18684 |  0:00:01s\n",
      "epoch 10 | loss: 1.57923 | val_0_rmse: 3.66006 |  0:00:02s\n",
      "epoch 11 | loss: 2.19441 | val_0_rmse: 2.55158 |  0:00:02s\n",
      "epoch 12 | loss: 1.15521 | val_0_rmse: 2.3641  |  0:00:02s\n",
      "epoch 13 | loss: 0.68952 | val_0_rmse: 2.04605 |  0:00:02s\n",
      "epoch 14 | loss: 0.46076 | val_0_rmse: 1.81416 |  0:00:02s\n",
      "epoch 15 | loss: 0.55283 | val_0_rmse: 1.76393 |  0:00:03s\n",
      "epoch 16 | loss: 0.4469  | val_0_rmse: 1.37015 |  0:00:03s\n",
      "epoch 17 | loss: 0.44375 | val_0_rmse: 1.4277  |  0:00:03s\n",
      "epoch 18 | loss: 0.38613 | val_0_rmse: 1.31849 |  0:00:03s\n",
      "epoch 19 | loss: 0.30109 | val_0_rmse: 1.39244 |  0:00:03s\n",
      "epoch 20 | loss: 0.31128 | val_0_rmse: 1.0643  |  0:00:04s\n",
      "epoch 21 | loss: 0.27673 | val_0_rmse: 0.92821 |  0:00:04s\n",
      "epoch 22 | loss: 0.35442 | val_0_rmse: 1.11121 |  0:00:04s\n",
      "epoch 23 | loss: 0.27208 | val_0_rmse: 0.81188 |  0:00:04s\n",
      "epoch 24 | loss: 0.29674 | val_0_rmse: 1.0208  |  0:00:04s\n",
      "epoch 25 | loss: 0.30365 | val_0_rmse: 0.88822 |  0:00:05s\n",
      "epoch 26 | loss: 0.16538 | val_0_rmse: 0.9779  |  0:00:05s\n",
      "epoch 27 | loss: 0.17101 | val_0_rmse: 0.67223 |  0:00:05s\n",
      "epoch 28 | loss: 0.22183 | val_0_rmse: 0.77571 |  0:00:05s\n",
      "epoch 29 | loss: 0.16963 | val_0_rmse: 0.60209 |  0:00:05s\n",
      "epoch 30 | loss: 0.15074 | val_0_rmse: 0.79325 |  0:00:05s\n",
      "epoch 31 | loss: 0.20378 | val_0_rmse: 0.49066 |  0:00:06s\n",
      "epoch 32 | loss: 0.12143 | val_0_rmse: 0.57741 |  0:00:06s\n",
      "epoch 33 | loss: 0.13166 | val_0_rmse: 0.31944 |  0:00:06s\n",
      "epoch 34 | loss: 0.14576 | val_0_rmse: 0.46927 |  0:00:06s\n",
      "epoch 35 | loss: 0.12478 | val_0_rmse: 0.28946 |  0:00:06s\n",
      "epoch 36 | loss: 0.12404 | val_0_rmse: 0.39607 |  0:00:07s\n",
      "epoch 37 | loss: 0.1435  | val_0_rmse: 0.23701 |  0:00:07s\n",
      "epoch 38 | loss: 0.10861 | val_0_rmse: 0.31411 |  0:00:07s\n",
      "epoch 39 | loss: 0.09118 | val_0_rmse: 0.23475 |  0:00:07s\n",
      "epoch 40 | loss: 0.11397 | val_0_rmse: 0.26777 |  0:00:07s\n",
      "epoch 41 | loss: 0.09955 | val_0_rmse: 0.23391 |  0:00:08s\n",
      "epoch 42 | loss: 0.10344 | val_0_rmse: 0.29062 |  0:00:08s\n",
      "epoch 43 | loss: 0.08305 | val_0_rmse: 0.22633 |  0:00:08s\n",
      "epoch 44 | loss: 0.10688 | val_0_rmse: 0.30056 |  0:00:08s\n",
      "epoch 45 | loss: 0.08864 | val_0_rmse: 0.22839 |  0:00:08s\n",
      "epoch 46 | loss: 0.09343 | val_0_rmse: 0.27283 |  0:00:09s\n",
      "epoch 47 | loss: 0.08189 | val_0_rmse: 0.24685 |  0:00:09s\n",
      "epoch 48 | loss: 0.08165 | val_0_rmse: 0.23902 |  0:00:09s\n",
      "epoch 49 | loss: 0.08702 | val_0_rmse: 0.24918 |  0:00:09s\n",
      "epoch 50 | loss: 0.06925 | val_0_rmse: 0.2288  |  0:00:09s\n",
      "epoch 51 | loss: 0.07741 | val_0_rmse: 0.26414 |  0:00:09s\n",
      "epoch 52 | loss: 0.08155 | val_0_rmse: 0.23114 |  0:00:10s\n",
      "epoch 53 | loss: 0.06967 | val_0_rmse: 0.23051 |  0:00:10s\n",
      "epoch 54 | loss: 0.07058 | val_0_rmse: 0.22445 |  0:00:10s\n",
      "epoch 55 | loss: 0.06726 | val_0_rmse: 0.2461  |  0:00:10s\n",
      "epoch 56 | loss: 0.08058 | val_0_rmse: 0.22909 |  0:00:10s\n",
      "epoch 57 | loss: 0.07378 | val_0_rmse: 0.253   |  0:00:10s\n",
      "epoch 58 | loss: 0.07386 | val_0_rmse: 0.22395 |  0:00:11s\n",
      "epoch 59 | loss: 0.07597 | val_0_rmse: 0.25723 |  0:00:11s\n",
      "epoch 60 | loss: 0.07109 | val_0_rmse: 0.21487 |  0:00:11s\n",
      "epoch 61 | loss: 0.06555 | val_0_rmse: 0.24583 |  0:00:11s\n",
      "epoch 62 | loss: 0.06488 | val_0_rmse: 0.21506 |  0:00:11s\n",
      "epoch 63 | loss: 0.06776 | val_0_rmse: 0.23929 |  0:00:12s\n",
      "epoch 64 | loss: 0.06867 | val_0_rmse: 0.21838 |  0:00:12s\n",
      "epoch 65 | loss: 0.06493 | val_0_rmse: 0.23933 |  0:00:12s\n",
      "epoch 66 | loss: 0.06301 | val_0_rmse: 0.20722 |  0:00:12s\n",
      "epoch 67 | loss: 0.06433 | val_0_rmse: 0.24589 |  0:00:12s\n",
      "epoch 68 | loss: 0.05209 | val_0_rmse: 0.20088 |  0:00:13s\n",
      "epoch 69 | loss: 0.06008 | val_0_rmse: 0.26597 |  0:00:13s\n",
      "epoch 70 | loss: 0.05325 | val_0_rmse: 0.20371 |  0:00:13s\n",
      "epoch 71 | loss: 0.06366 | val_0_rmse: 0.25976 |  0:00:13s\n",
      "epoch 72 | loss: 0.06908 | val_0_rmse: 0.21064 |  0:00:13s\n",
      "epoch 73 | loss: 0.07133 | val_0_rmse: 0.23259 |  0:00:14s\n",
      "epoch 74 | loss: 0.05675 | val_0_rmse: 0.2305  |  0:00:14s\n",
      "epoch 75 | loss: 0.06055 | val_0_rmse: 0.23845 |  0:00:14s\n",
      "epoch 76 | loss: 0.06614 | val_0_rmse: 0.22161 |  0:00:14s\n",
      "epoch 77 | loss: 0.06096 | val_0_rmse: 0.24088 |  0:00:14s\n",
      "epoch 78 | loss: 0.05814 | val_0_rmse: 0.22152 |  0:00:14s\n",
      "epoch 79 | loss: 0.06228 | val_0_rmse: 0.24522 |  0:00:15s\n",
      "epoch 80 | loss: 0.06228 | val_0_rmse: 0.22418 |  0:00:15s\n",
      "epoch 81 | loss: 0.0486  | val_0_rmse: 0.24503 |  0:00:15s\n",
      "epoch 82 | loss: 0.05831 | val_0_rmse: 0.2255  |  0:00:15s\n",
      "epoch 83 | loss: 0.05636 | val_0_rmse: 0.23084 |  0:00:15s\n",
      "epoch 84 | loss: 0.05435 | val_0_rmse: 0.22406 |  0:00:16s\n",
      "epoch 85 | loss: 0.0574  | val_0_rmse: 0.22871 |  0:00:16s\n",
      "epoch 86 | loss: 0.05573 | val_0_rmse: 0.2094  |  0:00:16s\n",
      "epoch 87 | loss: 0.04942 | val_0_rmse: 0.23495 |  0:00:16s\n",
      "epoch 88 | loss: 0.0505  | val_0_rmse: 0.21598 |  0:00:16s\n",
      "epoch 89 | loss: 0.04673 | val_0_rmse: 0.22348 |  0:00:17s\n",
      "epoch 90 | loss: 0.05315 | val_0_rmse: 0.2157  |  0:00:17s\n",
      "epoch 91 | loss: 0.05461 | val_0_rmse: 0.2189  |  0:00:17s\n",
      "epoch 92 | loss: 0.04629 | val_0_rmse: 0.22094 |  0:00:17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:09:56,631] Trial 11 finished with value: 0.20087717742150527 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2330765191835589, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.017904475626704417, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 9 with value: 0.18814445496113438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 93 | loss: 0.05622 | val_0_rmse: 0.2413  |  0:00:17s\n",
      "\n",
      "Early stopping occurred at epoch 93 with best_epoch = 68 and best_val_0_rmse = 0.20088\n",
      "Trial 011 | rmse_log=0.20088 | RMSE$=44,125 | MAE$=25,979 | MAPE=14.33% | n_d/n_a=48/24 steps=3 lr=0.01790 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 123.77134| val_0_rmse: 10.04951|  0:00:00s\n",
      "epoch 1  | loss: 65.01661| val_0_rmse: 8.33106 |  0:00:00s\n",
      "epoch 2  | loss: 34.94715| val_0_rmse: 6.36758 |  0:00:00s\n",
      "epoch 3  | loss: 28.0263 | val_0_rmse: 5.26857 |  0:00:00s\n",
      "epoch 4  | loss: 23.70892| val_0_rmse: 4.70938 |  0:00:01s\n",
      "epoch 5  | loss: 19.09701| val_0_rmse: 4.7064  |  0:00:01s\n",
      "epoch 6  | loss: 8.00067 | val_0_rmse: 5.01351 |  0:00:01s\n",
      "epoch 7  | loss: 5.75689 | val_0_rmse: 4.01735 |  0:00:01s\n",
      "epoch 8  | loss: 3.95281 | val_0_rmse: 2.70141 |  0:00:02s\n",
      "epoch 9  | loss: 1.73068 | val_0_rmse: 2.75216 |  0:00:02s\n",
      "epoch 10 | loss: 1.63662 | val_0_rmse: 1.97688 |  0:00:02s\n",
      "epoch 11 | loss: 1.73875 | val_0_rmse: 2.34828 |  0:00:02s\n",
      "epoch 12 | loss: 1.01322 | val_0_rmse: 1.20946 |  0:00:03s\n",
      "epoch 13 | loss: 1.20239 | val_0_rmse: 2.0337  |  0:00:03s\n",
      "epoch 14 | loss: 1.11935 | val_0_rmse: 0.81399 |  0:00:03s\n",
      "epoch 15 | loss: 1.77185 | val_0_rmse: 1.18074 |  0:00:03s\n",
      "epoch 16 | loss: 0.72206 | val_0_rmse: 1.49117 |  0:00:04s\n",
      "epoch 17 | loss: 0.69184 | val_0_rmse: 0.80447 |  0:00:04s\n",
      "epoch 18 | loss: 0.53674 | val_0_rmse: 1.4695  |  0:00:04s\n",
      "epoch 19 | loss: 0.78995 | val_0_rmse: 0.79792 |  0:00:04s\n",
      "epoch 20 | loss: 0.65847 | val_0_rmse: 1.02518 |  0:00:05s\n",
      "epoch 21 | loss: 0.55087 | val_0_rmse: 0.97474 |  0:00:05s\n",
      "epoch 22 | loss: 0.37112 | val_0_rmse: 0.86219 |  0:00:05s\n",
      "epoch 23 | loss: 0.39836 | val_0_rmse: 0.81334 |  0:00:05s\n",
      "epoch 24 | loss: 0.34461 | val_0_rmse: 0.76229 |  0:00:05s\n",
      "epoch 25 | loss: 0.29238 | val_0_rmse: 0.80525 |  0:00:06s\n",
      "epoch 26 | loss: 0.25033 | val_0_rmse: 0.63194 |  0:00:06s\n",
      "epoch 27 | loss: 0.37877 | val_0_rmse: 0.93887 |  0:00:06s\n",
      "epoch 28 | loss: 0.30977 | val_0_rmse: 0.56487 |  0:00:06s\n",
      "epoch 29 | loss: 0.23284 | val_0_rmse: 0.62976 |  0:00:07s\n",
      "epoch 30 | loss: 0.22571 | val_0_rmse: 0.45203 |  0:00:07s\n",
      "epoch 31 | loss: 0.22813 | val_0_rmse: 0.61849 |  0:00:07s\n",
      "epoch 32 | loss: 0.19122 | val_0_rmse: 0.32892 |  0:00:07s\n",
      "epoch 33 | loss: 0.14377 | val_0_rmse: 0.50616 |  0:00:08s\n",
      "epoch 34 | loss: 0.18079 | val_0_rmse: 0.3022  |  0:00:08s\n",
      "epoch 35 | loss: 0.12356 | val_0_rmse: 0.42735 |  0:00:08s\n",
      "epoch 36 | loss: 0.148   | val_0_rmse: 0.28058 |  0:00:08s\n",
      "epoch 37 | loss: 0.18078 | val_0_rmse: 0.35993 |  0:00:09s\n",
      "epoch 38 | loss: 0.13158 | val_0_rmse: 0.27129 |  0:00:09s\n",
      "epoch 39 | loss: 0.1799  | val_0_rmse: 0.47299 |  0:00:09s\n",
      "epoch 40 | loss: 0.18072 | val_0_rmse: 0.30567 |  0:00:09s\n",
      "epoch 41 | loss: 0.11089 | val_0_rmse: 0.26698 |  0:00:09s\n",
      "epoch 42 | loss: 0.12348 | val_0_rmse: 0.49502 |  0:00:10s\n",
      "epoch 43 | loss: 0.17978 | val_0_rmse: 0.29133 |  0:00:10s\n",
      "epoch 44 | loss: 0.15076 | val_0_rmse: 0.38863 |  0:00:10s\n",
      "epoch 45 | loss: 0.12717 | val_0_rmse: 0.31792 |  0:00:11s\n",
      "epoch 46 | loss: 0.26349 | val_0_rmse: 0.35253 |  0:00:11s\n",
      "epoch 47 | loss: 0.13146 | val_0_rmse: 0.29599 |  0:00:11s\n",
      "epoch 48 | loss: 0.10491 | val_0_rmse: 0.25879 |  0:00:11s\n",
      "epoch 49 | loss: 0.088   | val_0_rmse: 0.31634 |  0:00:12s\n",
      "epoch 50 | loss: 0.10298 | val_0_rmse: 0.28512 |  0:00:12s\n",
      "epoch 51 | loss: 0.10007 | val_0_rmse: 0.30477 |  0:00:12s\n",
      "epoch 52 | loss: 0.07963 | val_0_rmse: 0.26893 |  0:00:12s\n",
      "epoch 53 | loss: 0.12968 | val_0_rmse: 0.44539 |  0:00:12s\n",
      "epoch 54 | loss: 0.11508 | val_0_rmse: 0.34758 |  0:00:13s\n",
      "epoch 55 | loss: 0.18952 | val_0_rmse: 0.42909 |  0:00:13s\n",
      "epoch 56 | loss: 0.12698 | val_0_rmse: 0.27334 |  0:00:13s\n",
      "epoch 57 | loss: 0.13568 | val_0_rmse: 0.49759 |  0:00:13s\n",
      "epoch 58 | loss: 0.14439 | val_0_rmse: 0.28767 |  0:00:14s\n",
      "epoch 59 | loss: 0.12864 | val_0_rmse: 0.45176 |  0:00:14s\n",
      "epoch 60 | loss: 0.12007 | val_0_rmse: 0.23971 |  0:00:14s\n",
      "epoch 61 | loss: 0.11015 | val_0_rmse: 0.4113  |  0:00:14s\n",
      "epoch 62 | loss: 0.10006 | val_0_rmse: 0.2888  |  0:00:15s\n",
      "epoch 63 | loss: 0.12826 | val_0_rmse: 0.37135 |  0:00:15s\n",
      "epoch 64 | loss: 0.10436 | val_0_rmse: 0.28673 |  0:00:15s\n",
      "epoch 65 | loss: 0.11678 | val_0_rmse: 0.37712 |  0:00:15s\n",
      "epoch 66 | loss: 0.07922 | val_0_rmse: 0.31343 |  0:00:16s\n",
      "epoch 67 | loss: 0.09742 | val_0_rmse: 0.32786 |  0:00:16s\n",
      "epoch 68 | loss: 0.09266 | val_0_rmse: 0.2968  |  0:00:16s\n",
      "epoch 69 | loss: 0.09049 | val_0_rmse: 0.34489 |  0:00:16s\n",
      "epoch 70 | loss: 0.08887 | val_0_rmse: 0.30857 |  0:00:17s\n",
      "epoch 71 | loss: 0.11456 | val_0_rmse: 0.31768 |  0:00:17s\n",
      "epoch 72 | loss: 0.09387 | val_0_rmse: 0.30231 |  0:00:17s\n",
      "epoch 73 | loss: 0.09122 | val_0_rmse: 0.31403 |  0:00:17s\n",
      "epoch 74 | loss: 0.10039 | val_0_rmse: 0.3052  |  0:00:18s\n",
      "epoch 75 | loss: 0.08935 | val_0_rmse: 0.39465 |  0:00:18s\n",
      "epoch 76 | loss: 0.20325 | val_0_rmse: 0.26821 |  0:00:18s\n",
      "epoch 77 | loss: 0.09037 | val_0_rmse: 0.25275 |  0:00:18s\n",
      "epoch 78 | loss: 0.05585 | val_0_rmse: 0.33424 |  0:00:18s\n",
      "epoch 79 | loss: 0.16828 | val_0_rmse: 0.23281 |  0:00:19s\n",
      "epoch 80 | loss: 0.06389 | val_0_rmse: 0.26947 |  0:00:19s\n",
      "epoch 81 | loss: 0.06037 | val_0_rmse: 0.27907 |  0:00:19s\n",
      "epoch 82 | loss: 0.0801  | val_0_rmse: 0.24827 |  0:00:19s\n",
      "epoch 83 | loss: 0.06124 | val_0_rmse: 0.29653 |  0:00:20s\n",
      "epoch 84 | loss: 0.08261 | val_0_rmse: 0.267   |  0:00:20s\n",
      "epoch 85 | loss: 0.08568 | val_0_rmse: 0.26936 |  0:00:20s\n",
      "epoch 86 | loss: 0.07444 | val_0_rmse: 0.25518 |  0:00:20s\n",
      "epoch 87 | loss: 0.07236 | val_0_rmse: 0.25824 |  0:00:21s\n",
      "epoch 88 | loss: 0.0768  | val_0_rmse: 0.26736 |  0:00:21s\n",
      "epoch 89 | loss: 0.06064 | val_0_rmse: 0.25322 |  0:00:21s\n",
      "epoch 90 | loss: 0.05572 | val_0_rmse: 0.27758 |  0:00:21s\n",
      "epoch 91 | loss: 0.07617 | val_0_rmse: 0.314   |  0:00:21s\n",
      "epoch 92 | loss: 0.09205 | val_0_rmse: 0.35464 |  0:00:22s\n",
      "epoch 93 | loss: 0.10375 | val_0_rmse: 0.24836 |  0:00:22s\n",
      "epoch 94 | loss: 0.05442 | val_0_rmse: 0.23385 |  0:00:22s\n",
      "epoch 95 | loss: 0.06655 | val_0_rmse: 0.22154 |  0:00:22s\n",
      "epoch 96 | loss: 0.11335 | val_0_rmse: 0.35373 |  0:00:23s\n",
      "epoch 97 | loss: 0.101   | val_0_rmse: 0.37793 |  0:00:23s\n",
      "epoch 98 | loss: 0.14617 | val_0_rmse: 0.27475 |  0:00:23s\n",
      "epoch 99 | loss: 0.0654  | val_0_rmse: 0.26816 |  0:00:24s\n",
      "epoch 100| loss: 0.0586  | val_0_rmse: 0.23825 |  0:00:24s\n",
      "epoch 101| loss: 0.05562 | val_0_rmse: 0.23933 |  0:00:24s\n",
      "epoch 102| loss: 0.05848 | val_0_rmse: 0.23441 |  0:00:24s\n",
      "epoch 103| loss: 0.04964 | val_0_rmse: 0.24807 |  0:00:24s\n",
      "epoch 104| loss: 0.05091 | val_0_rmse: 0.23591 |  0:00:25s\n",
      "epoch 105| loss: 0.04446 | val_0_rmse: 0.23413 |  0:00:25s\n",
      "epoch 106| loss: 0.05061 | val_0_rmse: 0.24523 |  0:00:25s\n",
      "epoch 107| loss: 0.06796 | val_0_rmse: 0.22457 |  0:00:25s\n",
      "epoch 108| loss: 0.04264 | val_0_rmse: 0.27476 |  0:00:26s\n",
      "epoch 109| loss: 0.06773 | val_0_rmse: 0.29594 |  0:00:26s\n",
      "epoch 110| loss: 0.08474 | val_0_rmse: 0.25359 |  0:00:26s\n",
      "epoch 111| loss: 0.05627 | val_0_rmse: 0.26077 |  0:00:26s\n",
      "epoch 112| loss: 0.07419 | val_0_rmse: 0.26007 |  0:00:26s\n",
      "epoch 113| loss: 0.06069 | val_0_rmse: 0.24374 |  0:00:27s\n",
      "epoch 114| loss: 0.06354 | val_0_rmse: 0.22109 |  0:00:27s\n",
      "epoch 115| loss: 0.05475 | val_0_rmse: 0.30974 |  0:00:27s\n",
      "epoch 116| loss: 0.07079 | val_0_rmse: 0.24246 |  0:00:28s\n",
      "epoch 117| loss: 0.05772 | val_0_rmse: 0.21918 |  0:00:28s\n",
      "epoch 118| loss: 0.05359 | val_0_rmse: 0.24082 |  0:00:28s\n",
      "epoch 119| loss: 0.05761 | val_0_rmse: 0.22301 |  0:00:28s\n",
      "epoch 120| loss: 0.09997 | val_0_rmse: 0.38124 |  0:00:28s\n",
      "epoch 121| loss: 0.08782 | val_0_rmse: 0.26105 |  0:00:29s\n",
      "epoch 122| loss: 0.07956 | val_0_rmse: 0.35672 |  0:00:29s\n",
      "epoch 123| loss: 0.10757 | val_0_rmse: 0.22904 |  0:00:29s\n",
      "epoch 124| loss: 0.0465  | val_0_rmse: 0.22654 |  0:00:29s\n",
      "epoch 125| loss: 0.05641 | val_0_rmse: 0.2223  |  0:00:30s\n",
      "epoch 126| loss: 0.08263 | val_0_rmse: 0.31079 |  0:00:30s\n",
      "epoch 127| loss: 0.07579 | val_0_rmse: 0.35903 |  0:00:30s\n",
      "epoch 128| loss: 0.0908  | val_0_rmse: 0.26598 |  0:00:30s\n",
      "epoch 129| loss: 0.06654 | val_0_rmse: 0.2981  |  0:00:30s\n",
      "epoch 130| loss: 0.06336 | val_0_rmse: 0.28541 |  0:00:31s\n",
      "epoch 131| loss: 0.08306 | val_0_rmse: 0.28773 |  0:00:31s\n",
      "epoch 132| loss: 0.06509 | val_0_rmse: 0.30759 |  0:00:31s\n",
      "epoch 133| loss: 0.0974  | val_0_rmse: 0.25838 |  0:00:31s\n",
      "epoch 134| loss: 0.04103 | val_0_rmse: 0.20068 |  0:00:32s\n",
      "epoch 135| loss: 0.03725 | val_0_rmse: 0.23745 |  0:00:32s\n",
      "epoch 136| loss: 0.06789 | val_0_rmse: 0.31131 |  0:00:32s\n",
      "epoch 137| loss: 0.11337 | val_0_rmse: 0.20177 |  0:00:32s\n",
      "epoch 138| loss: 0.05041 | val_0_rmse: 0.26068 |  0:00:32s\n",
      "epoch 139| loss: 0.05214 | val_0_rmse: 0.24337 |  0:00:33s\n",
      "epoch 140| loss: 0.07483 | val_0_rmse: 0.31904 |  0:00:33s\n",
      "epoch 141| loss: 0.1423  | val_0_rmse: 0.21322 |  0:00:33s\n",
      "epoch 142| loss: 0.09668 | val_0_rmse: 0.38266 |  0:00:33s\n",
      "epoch 143| loss: 0.13402 | val_0_rmse: 0.4229  |  0:00:34s\n",
      "epoch 144| loss: 0.16559 | val_0_rmse: 0.28083 |  0:00:34s\n",
      "epoch 145| loss: 0.13983 | val_0_rmse: 0.23102 |  0:00:34s\n",
      "epoch 146| loss: 0.08558 | val_0_rmse: 0.37803 |  0:00:34s\n",
      "epoch 147| loss: 0.09983 | val_0_rmse: 0.35799 |  0:00:34s\n",
      "epoch 148| loss: 0.14795 | val_0_rmse: 0.31455 |  0:00:35s\n",
      "epoch 149| loss: 0.15254 | val_0_rmse: 0.2552  |  0:00:35s\n",
      "epoch 150| loss: 0.08323 | val_0_rmse: 0.33854 |  0:00:35s\n",
      "epoch 151| loss: 0.09903 | val_0_rmse: 0.39549 |  0:00:35s\n",
      "epoch 152| loss: 0.14236 | val_0_rmse: 0.26849 |  0:00:36s\n",
      "epoch 153| loss: 0.13927 | val_0_rmse: 0.22959 |  0:00:36s\n",
      "epoch 154| loss: 0.0693  | val_0_rmse: 0.36888 |  0:00:36s\n",
      "epoch 155| loss: 0.08287 | val_0_rmse: 0.37539 |  0:00:36s\n",
      "epoch 156| loss: 0.16541 | val_0_rmse: 0.27051 |  0:00:37s\n",
      "epoch 157| loss: 0.09368 | val_0_rmse: 0.22197 |  0:00:37s\n",
      "epoch 158| loss: 0.09623 | val_0_rmse: 0.36157 |  0:00:37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:10:34,743] Trial 12 finished with value: 0.20068282988782327 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 4, 'gamma': 1.3767635137709182, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.01880754149303489, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 9 with value: 0.18814445496113438.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 159| loss: 0.11512 | val_0_rmse: 0.37048 |  0:00:37s\n",
      "\n",
      "Early stopping occurred at epoch 159 with best_epoch = 134 and best_val_0_rmse = 0.20068\n",
      "Trial 012 | rmse_log=0.20068 | RMSE$=38,975 | MAE$=25,229 | MAPE=15.34% | n_d/n_a=48/24 steps=4 lr=0.01881 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 125.71824| val_0_rmse: 10.1861 |  0:00:00s\n",
      "epoch 1  | loss: 81.96513| val_0_rmse: 8.88728 |  0:00:00s\n",
      "epoch 2  | loss: 57.16079| val_0_rmse: 7.69302 |  0:00:00s\n",
      "epoch 3  | loss: 34.9676 | val_0_rmse: 6.30843 |  0:00:00s\n",
      "epoch 4  | loss: 25.79737| val_0_rmse: 5.16422 |  0:00:00s\n",
      "epoch 5  | loss: 22.52447| val_0_rmse: 4.43357 |  0:00:01s\n",
      "epoch 6  | loss: 13.62942| val_0_rmse: 4.15099 |  0:00:01s\n",
      "epoch 7  | loss: 9.53685 | val_0_rmse: 4.12085 |  0:00:01s\n",
      "epoch 8  | loss: 4.95933 | val_0_rmse: 3.80279 |  0:00:01s\n",
      "epoch 9  | loss: 3.63014 | val_0_rmse: 3.11626 |  0:00:01s\n",
      "epoch 10 | loss: 1.98604 | val_0_rmse: 2.53089 |  0:00:02s\n",
      "epoch 11 | loss: 1.98593 | val_0_rmse: 2.6931  |  0:00:02s\n",
      "epoch 12 | loss: 1.68015 | val_0_rmse: 2.32648 |  0:00:02s\n",
      "epoch 13 | loss: 1.18317 | val_0_rmse: 1.8209  |  0:00:02s\n",
      "epoch 14 | loss: 0.94081 | val_0_rmse: 1.96375 |  0:00:02s\n",
      "epoch 15 | loss: 0.86121 | val_0_rmse: 1.43394 |  0:00:03s\n",
      "epoch 16 | loss: 0.74654 | val_0_rmse: 1.75723 |  0:00:03s\n",
      "epoch 17 | loss: 0.72181 | val_0_rmse: 1.16324 |  0:00:03s\n",
      "epoch 18 | loss: 0.81436 | val_0_rmse: 1.45405 |  0:00:03s\n",
      "epoch 19 | loss: 0.57787 | val_0_rmse: 1.31005 |  0:00:03s\n",
      "epoch 20 | loss: 0.41786 | val_0_rmse: 1.19085 |  0:00:04s\n",
      "epoch 21 | loss: 0.42792 | val_0_rmse: 1.12811 |  0:00:04s\n",
      "epoch 22 | loss: 0.34462 | val_0_rmse: 1.06021 |  0:00:04s\n",
      "epoch 23 | loss: 0.36712 | val_0_rmse: 0.73208 |  0:00:04s\n",
      "epoch 24 | loss: 0.38497 | val_0_rmse: 1.00459 |  0:00:04s\n",
      "epoch 25 | loss: 0.30947 | val_0_rmse: 0.9085  |  0:00:05s\n",
      "epoch 26 | loss: 0.26563 | val_0_rmse: 0.85033 |  0:00:05s\n",
      "epoch 27 | loss: 0.27613 | val_0_rmse: 0.81217 |  0:00:05s\n",
      "epoch 28 | loss: 0.36259 | val_0_rmse: 0.80066 |  0:00:05s\n",
      "epoch 29 | loss: 0.29342 | val_0_rmse: 0.98226 |  0:00:05s\n",
      "epoch 30 | loss: 0.24709 | val_0_rmse: 0.70064 |  0:00:05s\n",
      "epoch 31 | loss: 0.24306 | val_0_rmse: 0.78768 |  0:00:06s\n",
      "epoch 32 | loss: 0.18941 | val_0_rmse: 0.77935 |  0:00:06s\n",
      "epoch 33 | loss: 0.2629  | val_0_rmse: 0.74822 |  0:00:06s\n",
      "epoch 34 | loss: 0.16948 | val_0_rmse: 0.68936 |  0:00:06s\n",
      "epoch 35 | loss: 0.24791 | val_0_rmse: 0.67267 |  0:00:06s\n",
      "epoch 36 | loss: 0.14964 | val_0_rmse: 0.59175 |  0:00:07s\n",
      "epoch 37 | loss: 0.18242 | val_0_rmse: 0.74197 |  0:00:07s\n",
      "epoch 38 | loss: 0.18288 | val_0_rmse: 0.4749  |  0:00:07s\n",
      "epoch 39 | loss: 0.18727 | val_0_rmse: 0.71364 |  0:00:07s\n",
      "epoch 40 | loss: 0.16596 | val_0_rmse: 0.34563 |  0:00:07s\n",
      "epoch 41 | loss: 0.18747 | val_0_rmse: 0.61285 |  0:00:07s\n",
      "epoch 42 | loss: 0.11923 | val_0_rmse: 0.34852 |  0:00:08s\n",
      "epoch 43 | loss: 0.13269 | val_0_rmse: 0.60767 |  0:00:08s\n",
      "epoch 44 | loss: 0.12353 | val_0_rmse: 0.30853 |  0:00:08s\n",
      "epoch 45 | loss: 0.1706  | val_0_rmse: 0.55758 |  0:00:08s\n",
      "epoch 46 | loss: 0.12305 | val_0_rmse: 0.27568 |  0:00:08s\n",
      "epoch 47 | loss: 0.13728 | val_0_rmse: 0.44083 |  0:00:09s\n",
      "epoch 48 | loss: 0.09457 | val_0_rmse: 0.29156 |  0:00:09s\n",
      "epoch 49 | loss: 0.07757 | val_0_rmse: 0.46647 |  0:00:09s\n",
      "epoch 50 | loss: 0.09695 | val_0_rmse: 0.27229 |  0:00:09s\n",
      "epoch 51 | loss: 0.10332 | val_0_rmse: 0.35503 |  0:00:09s\n",
      "epoch 52 | loss: 0.10978 | val_0_rmse: 0.29088 |  0:00:10s\n",
      "epoch 53 | loss: 0.10183 | val_0_rmse: 0.37594 |  0:00:10s\n",
      "epoch 54 | loss: 0.07599 | val_0_rmse: 0.38848 |  0:00:10s\n",
      "epoch 55 | loss: 0.1047  | val_0_rmse: 0.36289 |  0:00:10s\n",
      "epoch 56 | loss: 0.08051 | val_0_rmse: 0.39173 |  0:00:10s\n",
      "epoch 57 | loss: 0.07398 | val_0_rmse: 0.44185 |  0:00:10s\n",
      "epoch 58 | loss: 0.0577  | val_0_rmse: 0.29993 |  0:00:11s\n",
      "epoch 59 | loss: 0.08269 | val_0_rmse: 0.44458 |  0:00:11s\n",
      "epoch 60 | loss: 0.08347 | val_0_rmse: 0.29744 |  0:00:11s\n",
      "epoch 61 | loss: 0.10524 | val_0_rmse: 0.37492 |  0:00:11s\n",
      "epoch 62 | loss: 0.0694  | val_0_rmse: 0.35947 |  0:00:11s\n",
      "epoch 63 | loss: 0.05914 | val_0_rmse: 0.30556 |  0:00:11s\n",
      "epoch 64 | loss: 0.05115 | val_0_rmse: 0.26884 |  0:00:12s\n",
      "epoch 65 | loss: 0.07322 | val_0_rmse: 0.23615 |  0:00:12s\n",
      "epoch 66 | loss: 0.06092 | val_0_rmse: 0.2713  |  0:00:12s\n",
      "epoch 67 | loss: 0.06774 | val_0_rmse: 0.30781 |  0:00:12s\n",
      "epoch 68 | loss: 0.04909 | val_0_rmse: 0.20709 |  0:00:12s\n",
      "epoch 69 | loss: 0.03765 | val_0_rmse: 0.22667 |  0:00:13s\n",
      "epoch 70 | loss: 0.04806 | val_0_rmse: 0.2412  |  0:00:13s\n",
      "epoch 71 | loss: 0.03666 | val_0_rmse: 0.19951 |  0:00:13s\n",
      "epoch 72 | loss: 0.05112 | val_0_rmse: 0.22056 |  0:00:13s\n",
      "epoch 73 | loss: 0.05186 | val_0_rmse: 0.229   |  0:00:13s\n",
      "epoch 74 | loss: 0.05279 | val_0_rmse: 0.20507 |  0:00:13s\n",
      "epoch 75 | loss: 0.04383 | val_0_rmse: 0.22522 |  0:00:14s\n",
      "epoch 76 | loss: 0.03649 | val_0_rmse: 0.18641 |  0:00:14s\n",
      "epoch 77 | loss: 0.03527 | val_0_rmse: 0.27389 |  0:00:14s\n",
      "epoch 78 | loss: 0.04505 | val_0_rmse: 0.17503 |  0:00:14s\n",
      "epoch 79 | loss: 0.03999 | val_0_rmse: 0.23094 |  0:00:14s\n",
      "epoch 80 | loss: 0.0403  | val_0_rmse: 0.20902 |  0:00:15s\n",
      "epoch 81 | loss: 0.05076 | val_0_rmse: 0.18839 |  0:00:15s\n",
      "epoch 82 | loss: 0.03908 | val_0_rmse: 0.1872  |  0:00:15s\n",
      "epoch 83 | loss: 0.03315 | val_0_rmse: 0.19602 |  0:00:15s\n",
      "epoch 84 | loss: 0.03555 | val_0_rmse: 0.19762 |  0:00:15s\n",
      "epoch 85 | loss: 0.03935 | val_0_rmse: 0.19647 |  0:00:16s\n",
      "epoch 86 | loss: 0.0509  | val_0_rmse: 0.19885 |  0:00:16s\n",
      "epoch 87 | loss: 0.04156 | val_0_rmse: 0.19341 |  0:00:16s\n",
      "epoch 88 | loss: 0.03902 | val_0_rmse: 0.20812 |  0:00:16s\n",
      "epoch 89 | loss: 0.03454 | val_0_rmse: 0.18822 |  0:00:16s\n",
      "epoch 90 | loss: 0.04014 | val_0_rmse: 0.19569 |  0:00:17s\n",
      "epoch 91 | loss: 0.03318 | val_0_rmse: 0.20175 |  0:00:17s\n",
      "epoch 92 | loss: 0.03625 | val_0_rmse: 0.21277 |  0:00:17s\n",
      "epoch 93 | loss: 0.03921 | val_0_rmse: 0.20352 |  0:00:17s\n",
      "epoch 94 | loss: 0.04698 | val_0_rmse: 0.1862  |  0:00:17s\n",
      "epoch 95 | loss: 0.03919 | val_0_rmse: 0.21472 |  0:00:17s\n",
      "epoch 96 | loss: 0.04706 | val_0_rmse: 0.25892 |  0:00:18s\n",
      "epoch 97 | loss: 0.11687 | val_0_rmse: 0.19913 |  0:00:18s\n",
      "epoch 98 | loss: 0.09337 | val_0_rmse: 0.4008  |  0:00:18s\n",
      "epoch 99 | loss: 0.1266  | val_0_rmse: 0.22504 |  0:00:18s\n",
      "epoch 100| loss: 0.07846 | val_0_rmse: 0.27925 |  0:00:18s\n",
      "epoch 101| loss: 0.11508 | val_0_rmse: 0.2255  |  0:00:19s\n",
      "epoch 102| loss: 0.08564 | val_0_rmse: 0.35176 |  0:00:19s\n",
      "epoch 103| loss: 0.11865 | val_0_rmse: 0.25273 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 103 with best_epoch = 78 and best_val_0_rmse = 0.17503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:10:54,493] Trial 13 finished with value: 0.17502611623133627 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.398604157507134, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.013399459940475474, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 013 | rmse_log=0.17503 | RMSE$=35,753 | MAE$=21,564 | MAPE=12.86% | n_d/n_a=48/24 steps=3 lr=0.01340 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 188.86174| val_0_rmse: 11.26635|  0:00:00s\n",
      "epoch 1  | loss: 121.21865| val_0_rmse: 10.1513 |  0:00:00s\n",
      "epoch 2  | loss: 74.89179| val_0_rmse: 8.94687 |  0:00:00s\n",
      "epoch 3  | loss: 48.0729 | val_0_rmse: 7.51184 |  0:00:00s\n",
      "epoch 4  | loss: 31.1489 | val_0_rmse: 5.9923  |  0:00:01s\n",
      "epoch 5  | loss: 23.92839| val_0_rmse: 4.56051 |  0:00:01s\n",
      "epoch 6  | loss: 21.73894| val_0_rmse: 3.69046 |  0:00:01s\n",
      "epoch 7  | loss: 15.42482| val_0_rmse: 3.42001 |  0:00:01s\n",
      "epoch 8  | loss: 9.84153 | val_0_rmse: 3.59152 |  0:00:01s\n",
      "epoch 9  | loss: 6.39013 | val_0_rmse: 3.54062 |  0:00:02s\n",
      "epoch 10 | loss: 4.56526 | val_0_rmse: 2.42994 |  0:00:02s\n",
      "epoch 11 | loss: 2.99195 | val_0_rmse: 1.88966 |  0:00:02s\n",
      "epoch 12 | loss: 2.11206 | val_0_rmse: 2.15495 |  0:00:02s\n",
      "epoch 13 | loss: 1.83948 | val_0_rmse: 1.80059 |  0:00:02s\n",
      "epoch 14 | loss: 1.3842  | val_0_rmse: 1.76204 |  0:00:03s\n",
      "epoch 15 | loss: 1.19391 | val_0_rmse: 1.54658 |  0:00:03s\n",
      "epoch 16 | loss: 1.06023 | val_0_rmse: 1.50164 |  0:00:03s\n",
      "epoch 17 | loss: 0.76053 | val_0_rmse: 0.92939 |  0:00:03s\n",
      "epoch 18 | loss: 0.82668 | val_0_rmse: 1.48554 |  0:00:03s\n",
      "epoch 19 | loss: 0.91943 | val_0_rmse: 0.87772 |  0:00:04s\n",
      "epoch 20 | loss: 0.72425 | val_0_rmse: 1.52529 |  0:00:04s\n",
      "epoch 21 | loss: 1.09241 | val_0_rmse: 0.8842  |  0:00:04s\n",
      "epoch 22 | loss: 0.70753 | val_0_rmse: 1.28459 |  0:00:04s\n",
      "epoch 23 | loss: 0.80654 | val_0_rmse: 0.82203 |  0:00:04s\n",
      "epoch 24 | loss: 0.6651  | val_0_rmse: 0.80892 |  0:00:05s\n",
      "epoch 25 | loss: 0.47625 | val_0_rmse: 0.91247 |  0:00:05s\n",
      "epoch 26 | loss: 0.46953 | val_0_rmse: 0.74471 |  0:00:05s\n",
      "epoch 27 | loss: 0.42282 | val_0_rmse: 0.92726 |  0:00:05s\n",
      "epoch 28 | loss: 0.54203 | val_0_rmse: 0.4598  |  0:00:05s\n",
      "epoch 29 | loss: 0.54254 | val_0_rmse: 0.93411 |  0:00:06s\n",
      "epoch 30 | loss: 0.42861 | val_0_rmse: 0.52608 |  0:00:06s\n",
      "epoch 31 | loss: 0.36314 | val_0_rmse: 0.75069 |  0:00:06s\n",
      "epoch 32 | loss: 0.31593 | val_0_rmse: 0.38016 |  0:00:06s\n",
      "epoch 33 | loss: 0.3967  | val_0_rmse: 0.7799  |  0:00:06s\n",
      "epoch 34 | loss: 0.39009 | val_0_rmse: 0.36949 |  0:00:07s\n",
      "epoch 35 | loss: 0.35601 | val_0_rmse: 0.69883 |  0:00:07s\n",
      "epoch 36 | loss: 0.37616 | val_0_rmse: 0.42631 |  0:00:07s\n",
      "epoch 37 | loss: 0.26224 | val_0_rmse: 0.58018 |  0:00:07s\n",
      "epoch 38 | loss: 0.21046 | val_0_rmse: 0.50029 |  0:00:07s\n",
      "epoch 39 | loss: 0.17661 | val_0_rmse: 0.53625 |  0:00:08s\n",
      "epoch 40 | loss: 0.17149 | val_0_rmse: 0.62109 |  0:00:08s\n",
      "epoch 41 | loss: 0.26293 | val_0_rmse: 0.42117 |  0:00:08s\n",
      "epoch 42 | loss: 0.14886 | val_0_rmse: 0.51604 |  0:00:08s\n",
      "epoch 43 | loss: 0.1818  | val_0_rmse: 0.4599  |  0:00:08s\n",
      "epoch 44 | loss: 0.19105 | val_0_rmse: 0.49657 |  0:00:09s\n",
      "epoch 45 | loss: 0.21405 | val_0_rmse: 0.43388 |  0:00:09s\n",
      "epoch 46 | loss: 0.20685 | val_0_rmse: 0.67582 |  0:00:09s\n",
      "epoch 47 | loss: 0.1956  | val_0_rmse: 0.36111 |  0:00:09s\n",
      "epoch 48 | loss: 0.24612 | val_0_rmse: 0.58105 |  0:00:09s\n",
      "epoch 49 | loss: 0.20381 | val_0_rmse: 0.54876 |  0:00:10s\n",
      "epoch 50 | loss: 0.22772 | val_0_rmse: 0.47033 |  0:00:10s\n",
      "epoch 51 | loss: 0.18178 | val_0_rmse: 0.64524 |  0:00:10s\n",
      "epoch 52 | loss: 0.20929 | val_0_rmse: 0.2915  |  0:00:10s\n",
      "epoch 53 | loss: 0.31753 | val_0_rmse: 0.54584 |  0:00:10s\n",
      "epoch 54 | loss: 0.12643 | val_0_rmse: 0.34551 |  0:00:10s\n",
      "epoch 55 | loss: 0.12574 | val_0_rmse: 0.39242 |  0:00:11s\n",
      "epoch 56 | loss: 0.1051  | val_0_rmse: 0.36347 |  0:00:11s\n",
      "epoch 57 | loss: 0.10251 | val_0_rmse: 0.3763  |  0:00:11s\n",
      "epoch 58 | loss: 0.12526 | val_0_rmse: 0.36142 |  0:00:11s\n",
      "epoch 59 | loss: 0.11239 | val_0_rmse: 0.38449 |  0:00:12s\n",
      "epoch 60 | loss: 0.10662 | val_0_rmse: 0.51198 |  0:00:12s\n",
      "epoch 61 | loss: 0.11434 | val_0_rmse: 0.29157 |  0:00:12s\n",
      "epoch 62 | loss: 0.14315 | val_0_rmse: 0.50922 |  0:00:12s\n",
      "epoch 63 | loss: 0.11042 | val_0_rmse: 0.27902 |  0:00:12s\n",
      "epoch 64 | loss: 0.11947 | val_0_rmse: 0.46605 |  0:00:12s\n",
      "epoch 65 | loss: 0.0995  | val_0_rmse: 0.3064  |  0:00:13s\n",
      "epoch 66 | loss: 0.11045 | val_0_rmse: 0.46771 |  0:00:13s\n",
      "epoch 67 | loss: 0.12307 | val_0_rmse: 0.28496 |  0:00:13s\n",
      "epoch 68 | loss: 0.09264 | val_0_rmse: 0.42723 |  0:00:13s\n",
      "epoch 69 | loss: 0.11021 | val_0_rmse: 0.29952 |  0:00:13s\n",
      "epoch 70 | loss: 0.10883 | val_0_rmse: 0.36738 |  0:00:14s\n",
      "epoch 71 | loss: 0.08127 | val_0_rmse: 0.29748 |  0:00:14s\n",
      "epoch 72 | loss: 0.11321 | val_0_rmse: 0.40443 |  0:00:14s\n",
      "epoch 73 | loss: 0.13158 | val_0_rmse: 0.26208 |  0:00:14s\n",
      "epoch 74 | loss: 0.10191 | val_0_rmse: 0.29526 |  0:00:14s\n",
      "epoch 75 | loss: 0.08559 | val_0_rmse: 0.28243 |  0:00:15s\n",
      "epoch 76 | loss: 0.06724 | val_0_rmse: 0.27379 |  0:00:15s\n",
      "epoch 77 | loss: 0.06315 | val_0_rmse: 0.25134 |  0:00:15s\n",
      "epoch 78 | loss: 0.06701 | val_0_rmse: 0.24637 |  0:00:15s\n",
      "epoch 79 | loss: 0.10507 | val_0_rmse: 0.34516 |  0:00:15s\n",
      "epoch 80 | loss: 0.11624 | val_0_rmse: 0.25539 |  0:00:16s\n",
      "epoch 81 | loss: 0.06912 | val_0_rmse: 0.32362 |  0:00:16s\n",
      "epoch 82 | loss: 0.07771 | val_0_rmse: 0.2528  |  0:00:16s\n",
      "epoch 83 | loss: 0.0809  | val_0_rmse: 0.33449 |  0:00:16s\n",
      "epoch 84 | loss: 0.08753 | val_0_rmse: 0.33077 |  0:00:16s\n",
      "epoch 85 | loss: 0.14516 | val_0_rmse: 0.40308 |  0:00:16s\n",
      "epoch 86 | loss: 0.22566 | val_0_rmse: 0.34565 |  0:00:17s\n",
      "epoch 87 | loss: 0.09649 | val_0_rmse: 0.25956 |  0:00:17s\n",
      "epoch 88 | loss: 0.06542 | val_0_rmse: 0.23202 |  0:00:17s\n",
      "epoch 89 | loss: 0.05925 | val_0_rmse: 0.28179 |  0:00:17s\n",
      "epoch 90 | loss: 0.07756 | val_0_rmse: 0.24471 |  0:00:18s\n",
      "epoch 91 | loss: 0.06592 | val_0_rmse: 0.33463 |  0:00:18s\n",
      "epoch 92 | loss: 0.15317 | val_0_rmse: 0.23997 |  0:00:18s\n",
      "epoch 93 | loss: 0.11521 | val_0_rmse: 0.41628 |  0:00:18s\n",
      "epoch 94 | loss: 0.15404 | val_0_rmse: 0.34539 |  0:00:18s\n",
      "epoch 95 | loss: 0.17501 | val_0_rmse: 0.23523 |  0:00:19s\n",
      "epoch 96 | loss: 0.15025 | val_0_rmse: 0.45825 |  0:00:19s\n",
      "epoch 97 | loss: 0.21475 | val_0_rmse: 0.34248 |  0:00:19s\n",
      "epoch 98 | loss: 0.23671 | val_0_rmse: 0.36912 |  0:00:19s\n",
      "epoch 99 | loss: 0.13191 | val_0_rmse: 0.53847 |  0:00:19s\n",
      "epoch 100| loss: 0.42575 | val_0_rmse: 0.32363 |  0:00:20s\n",
      "epoch 101| loss: 0.14268 | val_0_rmse: 0.409   |  0:00:20s\n",
      "epoch 102| loss: 0.15307 | val_0_rmse: 0.2437  |  0:00:20s\n",
      "epoch 103| loss: 0.05922 | val_0_rmse: 0.26259 |  0:00:20s\n",
      "epoch 104| loss: 0.0711  | val_0_rmse: 0.20963 |  0:00:20s\n",
      "epoch 105| loss: 0.04772 | val_0_rmse: 0.2218  |  0:00:21s\n",
      "epoch 106| loss: 0.05506 | val_0_rmse: 0.29123 |  0:00:21s\n",
      "epoch 107| loss: 0.06548 | val_0_rmse: 0.28108 |  0:00:21s\n",
      "epoch 108| loss: 0.13899 | val_0_rmse: 0.22919 |  0:00:21s\n",
      "epoch 109| loss: 0.07676 | val_0_rmse: 0.2811  |  0:00:21s\n",
      "epoch 110| loss: 0.06653 | val_0_rmse: 0.21499 |  0:00:21s\n",
      "epoch 111| loss: 0.04392 | val_0_rmse: 0.22016 |  0:00:22s\n",
      "epoch 112| loss: 0.04976 | val_0_rmse: 0.32378 |  0:00:22s\n",
      "epoch 113| loss: 0.09423 | val_0_rmse: 0.23484 |  0:00:22s\n",
      "epoch 114| loss: 0.07871 | val_0_rmse: 0.24173 |  0:00:22s\n",
      "epoch 115| loss: 0.06738 | val_0_rmse: 0.22221 |  0:00:22s\n",
      "epoch 116| loss: 0.04828 | val_0_rmse: 0.26311 |  0:00:23s\n",
      "epoch 117| loss: 0.04784 | val_0_rmse: 0.23777 |  0:00:23s\n",
      "epoch 118| loss: 0.04454 | val_0_rmse: 0.22663 |  0:00:23s\n",
      "epoch 119| loss: 0.0415  | val_0_rmse: 0.2093  |  0:00:23s\n",
      "epoch 120| loss: 0.05109 | val_0_rmse: 0.24265 |  0:00:23s\n",
      "epoch 121| loss: 0.04584 | val_0_rmse: 0.22008 |  0:00:24s\n",
      "epoch 122| loss: 0.04576 | val_0_rmse: 0.20253 |  0:00:24s\n",
      "epoch 123| loss: 0.04156 | val_0_rmse: 0.23246 |  0:00:24s\n",
      "epoch 124| loss: 0.03893 | val_0_rmse: 0.23464 |  0:00:24s\n",
      "epoch 125| loss: 0.04499 | val_0_rmse: 0.19908 |  0:00:24s\n",
      "epoch 126| loss: 0.03984 | val_0_rmse: 0.22065 |  0:00:25s\n",
      "epoch 127| loss: 0.03504 | val_0_rmse: 0.19913 |  0:00:25s\n",
      "epoch 128| loss: 0.03804 | val_0_rmse: 0.20108 |  0:00:25s\n",
      "epoch 129| loss: 0.0369  | val_0_rmse: 0.2396  |  0:00:25s\n",
      "epoch 130| loss: 0.03842 | val_0_rmse: 0.22225 |  0:00:25s\n",
      "epoch 131| loss: 0.03652 | val_0_rmse: 0.21826 |  0:00:26s\n",
      "epoch 132| loss: 0.04105 | val_0_rmse: 0.20159 |  0:00:26s\n",
      "epoch 133| loss: 0.04037 | val_0_rmse: 0.21796 |  0:00:26s\n",
      "epoch 134| loss: 0.03312 | val_0_rmse: 0.21283 |  0:00:26s\n",
      "epoch 135| loss: 0.03165 | val_0_rmse: 0.21796 |  0:00:26s\n",
      "epoch 136| loss: 0.0409  | val_0_rmse: 0.21217 |  0:00:27s\n",
      "epoch 137| loss: 0.03255 | val_0_rmse: 0.26153 |  0:00:27s\n",
      "epoch 138| loss: 0.06383 | val_0_rmse: 0.2854  |  0:00:27s\n",
      "epoch 139| loss: 0.06442 | val_0_rmse: 0.27888 |  0:00:27s\n",
      "epoch 140| loss: 0.10881 | val_0_rmse: 0.22582 |  0:00:27s\n",
      "epoch 141| loss: 0.07726 | val_0_rmse: 0.4255  |  0:00:28s\n",
      "epoch 142| loss: 0.11958 | val_0_rmse: 0.24908 |  0:00:28s\n",
      "epoch 143| loss: 0.05567 | val_0_rmse: 0.2558  |  0:00:28s\n",
      "epoch 144| loss: 0.05111 | val_0_rmse: 0.23957 |  0:00:28s\n",
      "epoch 145| loss: 0.05114 | val_0_rmse: 0.2434  |  0:00:28s\n",
      "epoch 146| loss: 0.03882 | val_0_rmse: 0.25607 |  0:00:29s\n",
      "epoch 147| loss: 0.0487  | val_0_rmse: 0.20972 |  0:00:29s\n",
      "epoch 148| loss: 0.03362 | val_0_rmse: 0.21465 |  0:00:29s\n",
      "epoch 149| loss: 0.04218 | val_0_rmse: 0.25424 |  0:00:29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:11:24,765] Trial 14 finished with value: 0.19907849643284625 and parameters: {'n_d': 48, 'n_a': 16, 'n_steps': 4, 'gamma': 1.3981829516124344, 'n_independent': 2, 'n_shared': 2, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.013807605584044145, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 150| loss: 0.0467  | val_0_rmse: 0.24634 |  0:00:29s\n",
      "\n",
      "Early stopping occurred at epoch 150 with best_epoch = 125 and best_val_0_rmse = 0.19908\n",
      "Trial 014 | rmse_log=0.19908 | RMSE$=44,601 | MAE$=26,802 | MAPE=15.39% | n_d/n_a=48/16 steps=4 lr=0.01381 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 157.86836| val_0_rmse: 11.6402 |  0:00:00s\n",
      "epoch 1  | loss: 128.20995| val_0_rmse: 10.81449|  0:00:00s\n",
      "epoch 2  | loss: 104.9232| val_0_rmse: 9.94263 |  0:00:00s\n",
      "epoch 3  | loss: 83.59516| val_0_rmse: 8.83967 |  0:00:00s\n",
      "epoch 4  | loss: 68.34405| val_0_rmse: 7.63754 |  0:00:01s\n",
      "epoch 5  | loss: 49.46817| val_0_rmse: 6.25072 |  0:00:01s\n",
      "epoch 6  | loss: 31.36174| val_0_rmse: 4.74332 |  0:00:01s\n",
      "epoch 7  | loss: 22.70379| val_0_rmse: 3.33113 |  0:00:01s\n",
      "epoch 8  | loss: 14.89479| val_0_rmse: 2.23412 |  0:00:01s\n",
      "epoch 9  | loss: 11.86669| val_0_rmse: 1.76314 |  0:00:02s\n",
      "epoch 10 | loss: 10.30914| val_0_rmse: 1.8149  |  0:00:02s\n",
      "epoch 11 | loss: 7.21232 | val_0_rmse: 2.39052 |  0:00:02s\n",
      "epoch 12 | loss: 5.12859 | val_0_rmse: 2.60345 |  0:00:02s\n",
      "epoch 13 | loss: 2.85921 | val_0_rmse: 1.78469 |  0:00:02s\n",
      "epoch 14 | loss: 1.78324 | val_0_rmse: 1.35397 |  0:00:03s\n",
      "epoch 15 | loss: 1.57384 | val_0_rmse: 1.9084  |  0:00:03s\n",
      "epoch 16 | loss: 1.14818 | val_0_rmse: 1.88452 |  0:00:03s\n",
      "epoch 17 | loss: 0.86599 | val_0_rmse: 1.53647 |  0:00:03s\n",
      "epoch 18 | loss: 1.05068 | val_0_rmse: 2.13264 |  0:00:03s\n",
      "epoch 19 | loss: 0.66499 | val_0_rmse: 1.63268 |  0:00:04s\n",
      "epoch 20 | loss: 0.62457 | val_0_rmse: 1.83652 |  0:00:04s\n",
      "epoch 21 | loss: 0.61197 | val_0_rmse: 1.76779 |  0:00:04s\n",
      "epoch 22 | loss: 0.47114 | val_0_rmse: 1.62179 |  0:00:04s\n",
      "epoch 23 | loss: 0.43444 | val_0_rmse: 1.6357  |  0:00:04s\n",
      "epoch 24 | loss: 0.33726 | val_0_rmse: 1.51133 |  0:00:04s\n",
      "epoch 25 | loss: 0.38281 | val_0_rmse: 1.67357 |  0:00:05s\n",
      "epoch 26 | loss: 0.39103 | val_0_rmse: 1.45424 |  0:00:05s\n",
      "epoch 27 | loss: 0.34011 | val_0_rmse: 1.75136 |  0:00:05s\n",
      "epoch 28 | loss: 0.27822 | val_0_rmse: 1.34979 |  0:00:05s\n",
      "epoch 29 | loss: 0.3948  | val_0_rmse: 1.94932 |  0:00:05s\n",
      "epoch 30 | loss: 0.49982 | val_0_rmse: 1.3204  |  0:00:06s\n",
      "epoch 31 | loss: 0.25009 | val_0_rmse: 1.54314 |  0:00:06s\n",
      "epoch 32 | loss: 0.28839 | val_0_rmse: 1.26683 |  0:00:06s\n",
      "epoch 33 | loss: 0.19394 | val_0_rmse: 1.41525 |  0:00:06s\n",
      "epoch 34 | loss: 0.23914 | val_0_rmse: 1.06168 |  0:00:06s\n",
      "epoch 35 | loss: 0.24368 | val_0_rmse: 1.28136 |  0:00:07s\n",
      "epoch 36 | loss: 0.22322 | val_0_rmse: 1.01314 |  0:00:07s\n",
      "epoch 37 | loss: 0.16221 | val_0_rmse: 1.22218 |  0:00:07s\n",
      "epoch 38 | loss: 0.20252 | val_0_rmse: 0.84364 |  0:00:07s\n",
      "epoch 39 | loss: 0.18586 | val_0_rmse: 0.95476 |  0:00:07s\n",
      "epoch 40 | loss: 0.12775 | val_0_rmse: 0.79266 |  0:00:08s\n",
      "epoch 41 | loss: 0.11862 | val_0_rmse: 1.01039 |  0:00:08s\n",
      "epoch 42 | loss: 0.19376 | val_0_rmse: 0.66349 |  0:00:08s\n",
      "epoch 43 | loss: 0.27442 | val_0_rmse: 0.70454 |  0:00:08s\n",
      "epoch 44 | loss: 0.13739 | val_0_rmse: 0.83435 |  0:00:08s\n",
      "epoch 45 | loss: 0.17952 | val_0_rmse: 0.57195 |  0:00:09s\n",
      "epoch 46 | loss: 0.12726 | val_0_rmse: 0.81413 |  0:00:09s\n",
      "epoch 47 | loss: 0.096   | val_0_rmse: 0.60116 |  0:00:09s\n",
      "epoch 48 | loss: 0.12172 | val_0_rmse: 0.82526 |  0:00:09s\n",
      "epoch 49 | loss: 0.10677 | val_0_rmse: 0.59433 |  0:00:09s\n",
      "epoch 50 | loss: 0.12744 | val_0_rmse: 0.873   |  0:00:10s\n",
      "epoch 51 | loss: 0.10802 | val_0_rmse: 0.57773 |  0:00:10s\n",
      "epoch 52 | loss: 0.10325 | val_0_rmse: 0.81938 |  0:00:10s\n",
      "epoch 53 | loss: 0.08936 | val_0_rmse: 0.49551 |  0:00:10s\n",
      "epoch 54 | loss: 0.09998 | val_0_rmse: 0.67352 |  0:00:10s\n",
      "epoch 55 | loss: 0.0785  | val_0_rmse: 0.46521 |  0:00:11s\n",
      "epoch 56 | loss: 0.11832 | val_0_rmse: 0.67421 |  0:00:11s\n",
      "epoch 57 | loss: 0.0931  | val_0_rmse: 0.4097  |  0:00:11s\n",
      "epoch 58 | loss: 0.1042  | val_0_rmse: 0.56069 |  0:00:11s\n",
      "epoch 59 | loss: 0.08132 | val_0_rmse: 0.349   |  0:00:11s\n",
      "epoch 60 | loss: 0.10867 | val_0_rmse: 0.62282 |  0:00:12s\n",
      "epoch 61 | loss: 0.15264 | val_0_rmse: 0.44573 |  0:00:12s\n",
      "epoch 62 | loss: 0.09923 | val_0_rmse: 0.29203 |  0:00:12s\n",
      "epoch 63 | loss: 0.12512 | val_0_rmse: 0.60325 |  0:00:12s\n",
      "epoch 64 | loss: 0.16532 | val_0_rmse: 0.54446 |  0:00:12s\n",
      "epoch 65 | loss: 0.13579 | val_0_rmse: 0.26099 |  0:00:12s\n",
      "epoch 66 | loss: 0.11827 | val_0_rmse: 0.48694 |  0:00:13s\n",
      "epoch 67 | loss: 0.1038  | val_0_rmse: 0.26937 |  0:00:13s\n",
      "epoch 68 | loss: 0.11809 | val_0_rmse: 0.33445 |  0:00:13s\n",
      "epoch 69 | loss: 0.09346 | val_0_rmse: 0.4623  |  0:00:13s\n",
      "epoch 70 | loss: 0.09224 | val_0_rmse: 0.25451 |  0:00:13s\n",
      "epoch 71 | loss: 0.12213 | val_0_rmse: 0.36851 |  0:00:14s\n",
      "epoch 72 | loss: 0.07202 | val_0_rmse: 0.32712 |  0:00:14s\n",
      "epoch 73 | loss: 0.10998 | val_0_rmse: 0.25236 |  0:00:14s\n",
      "epoch 74 | loss: 0.1104  | val_0_rmse: 0.47722 |  0:00:14s\n",
      "epoch 75 | loss: 0.11327 | val_0_rmse: 0.31844 |  0:00:14s\n",
      "epoch 76 | loss: 0.06042 | val_0_rmse: 0.30972 |  0:00:15s\n",
      "epoch 77 | loss: 0.05099 | val_0_rmse: 0.29242 |  0:00:15s\n",
      "epoch 78 | loss: 0.05558 | val_0_rmse: 0.34628 |  0:00:15s\n",
      "epoch 79 | loss: 0.05593 | val_0_rmse: 0.25642 |  0:00:15s\n",
      "epoch 80 | loss: 0.0812  | val_0_rmse: 0.41065 |  0:00:15s\n",
      "epoch 81 | loss: 0.09941 | val_0_rmse: 0.3503  |  0:00:15s\n",
      "epoch 82 | loss: 0.08661 | val_0_rmse: 0.25727 |  0:00:16s\n",
      "epoch 83 | loss: 0.09218 | val_0_rmse: 0.40113 |  0:00:16s\n",
      "epoch 84 | loss: 0.07568 | val_0_rmse: 0.2491  |  0:00:16s\n",
      "epoch 85 | loss: 0.07678 | val_0_rmse: 0.26815 |  0:00:16s\n",
      "epoch 86 | loss: 0.05338 | val_0_rmse: 0.27534 |  0:00:16s\n",
      "epoch 87 | loss: 0.04788 | val_0_rmse: 0.26407 |  0:00:17s\n",
      "epoch 88 | loss: 0.05341 | val_0_rmse: 0.26984 |  0:00:17s\n",
      "epoch 89 | loss: 0.10986 | val_0_rmse: 0.31544 |  0:00:17s\n",
      "epoch 90 | loss: 0.09037 | val_0_rmse: 0.38112 |  0:00:17s\n",
      "epoch 91 | loss: 0.08949 | val_0_rmse: 0.25271 |  0:00:17s\n",
      "epoch 92 | loss: 0.08971 | val_0_rmse: 0.32624 |  0:00:18s\n",
      "epoch 93 | loss: 0.06016 | val_0_rmse: 0.26568 |  0:00:18s\n",
      "epoch 94 | loss: 0.05452 | val_0_rmse: 0.25699 |  0:00:18s\n",
      "epoch 95 | loss: 0.04323 | val_0_rmse: 0.26548 |  0:00:18s\n",
      "epoch 96 | loss: 0.07001 | val_0_rmse: 0.25715 |  0:00:18s\n",
      "epoch 97 | loss: 0.05474 | val_0_rmse: 0.24402 |  0:00:19s\n",
      "epoch 98 | loss: 0.06523 | val_0_rmse: 0.25778 |  0:00:19s\n",
      "epoch 99 | loss: 0.04775 | val_0_rmse: 0.28524 |  0:00:19s\n",
      "epoch 100| loss: 0.04068 | val_0_rmse: 0.24082 |  0:00:19s\n",
      "epoch 101| loss: 0.04194 | val_0_rmse: 0.27052 |  0:00:19s\n",
      "epoch 102| loss: 0.06019 | val_0_rmse: 0.25305 |  0:00:19s\n",
      "epoch 103| loss: 0.05758 | val_0_rmse: 0.24414 |  0:00:20s\n",
      "epoch 104| loss: 0.05396 | val_0_rmse: 0.26886 |  0:00:20s\n",
      "epoch 105| loss: 0.06499 | val_0_rmse: 0.25195 |  0:00:20s\n",
      "epoch 106| loss: 0.05376 | val_0_rmse: 0.30681 |  0:00:20s\n",
      "epoch 107| loss: 0.05608 | val_0_rmse: 0.27006 |  0:00:20s\n",
      "epoch 108| loss: 0.05652 | val_0_rmse: 0.26826 |  0:00:21s\n",
      "epoch 109| loss: 0.05438 | val_0_rmse: 0.28789 |  0:00:21s\n",
      "epoch 110| loss: 0.0496  | val_0_rmse: 0.23485 |  0:00:21s\n",
      "epoch 111| loss: 0.06529 | val_0_rmse: 0.28754 |  0:00:21s\n",
      "epoch 112| loss: 0.04938 | val_0_rmse: 0.27336 |  0:00:21s\n",
      "epoch 113| loss: 0.0456  | val_0_rmse: 0.32455 |  0:00:21s\n",
      "epoch 114| loss: 0.04634 | val_0_rmse: 0.32376 |  0:00:22s\n",
      "epoch 115| loss: 0.05131 | val_0_rmse: 0.3033  |  0:00:22s\n",
      "epoch 116| loss: 0.04037 | val_0_rmse: 0.28559 |  0:00:22s\n",
      "epoch 117| loss: 0.04218 | val_0_rmse: 0.30998 |  0:00:22s\n",
      "epoch 118| loss: 0.04305 | val_0_rmse: 0.30159 |  0:00:23s\n",
      "epoch 119| loss: 0.04135 | val_0_rmse: 0.25539 |  0:00:23s\n",
      "epoch 120| loss: 0.04118 | val_0_rmse: 0.27939 |  0:00:23s\n",
      "epoch 121| loss: 0.03933 | val_0_rmse: 0.26384 |  0:00:23s\n",
      "epoch 122| loss: 0.03889 | val_0_rmse: 0.28848 |  0:00:23s\n",
      "epoch 123| loss: 0.047   | val_0_rmse: 0.23916 |  0:00:23s\n",
      "epoch 124| loss: 0.04509 | val_0_rmse: 0.21245 |  0:00:24s\n",
      "epoch 125| loss: 0.0446  | val_0_rmse: 0.28096 |  0:00:24s\n",
      "epoch 126| loss: 0.04388 | val_0_rmse: 0.20749 |  0:00:24s\n",
      "epoch 127| loss: 0.04035 | val_0_rmse: 0.27589 |  0:00:24s\n",
      "epoch 128| loss: 0.0485  | val_0_rmse: 0.23344 |  0:00:24s\n",
      "epoch 129| loss: 0.05091 | val_0_rmse: 0.25743 |  0:00:25s\n",
      "epoch 130| loss: 0.04223 | val_0_rmse: 0.21504 |  0:00:25s\n",
      "epoch 131| loss: 0.04824 | val_0_rmse: 0.26203 |  0:00:25s\n",
      "epoch 132| loss: 0.04483 | val_0_rmse: 0.21098 |  0:00:25s\n",
      "epoch 133| loss: 0.04237 | val_0_rmse: 0.25386 |  0:00:25s\n",
      "epoch 134| loss: 0.04195 | val_0_rmse: 0.20167 |  0:00:25s\n",
      "epoch 135| loss: 0.04292 | val_0_rmse: 0.24709 |  0:00:26s\n",
      "epoch 136| loss: 0.04015 | val_0_rmse: 0.20392 |  0:00:26s\n",
      "epoch 137| loss: 0.03952 | val_0_rmse: 0.24936 |  0:00:26s\n",
      "epoch 138| loss: 0.04572 | val_0_rmse: 0.20149 |  0:00:26s\n",
      "epoch 139| loss: 0.0397  | val_0_rmse: 0.23503 |  0:00:26s\n",
      "epoch 140| loss: 0.03722 | val_0_rmse: 0.20607 |  0:00:27s\n",
      "epoch 141| loss: 0.03808 | val_0_rmse: 0.23528 |  0:00:27s\n",
      "epoch 142| loss: 0.03596 | val_0_rmse: 0.20269 |  0:00:27s\n",
      "epoch 143| loss: 0.04038 | val_0_rmse: 0.22868 |  0:00:27s\n",
      "epoch 144| loss: 0.03871 | val_0_rmse: 0.21346 |  0:00:27s\n",
      "epoch 145| loss: 0.03701 | val_0_rmse: 0.22531 |  0:00:28s\n",
      "epoch 146| loss: 0.04001 | val_0_rmse: 0.20408 |  0:00:28s\n",
      "epoch 147| loss: 0.0371  | val_0_rmse: 0.22294 |  0:00:28s\n",
      "epoch 148| loss: 0.04356 | val_0_rmse: 0.21746 |  0:00:28s\n",
      "epoch 149| loss: 0.03236 | val_0_rmse: 0.21101 |  0:00:28s\n",
      "epoch 150| loss: 0.02963 | val_0_rmse: 0.20764 |  0:00:29s\n",
      "epoch 151| loss: 0.03307 | val_0_rmse: 0.25797 |  0:00:29s\n",
      "epoch 152| loss: 0.04411 | val_0_rmse: 0.21747 |  0:00:29s\n",
      "epoch 153| loss: 0.04482 | val_0_rmse: 0.24147 |  0:00:29s\n",
      "epoch 154| loss: 0.0372  | val_0_rmse: 0.21679 |  0:00:29s\n",
      "epoch 155| loss: 0.03406 | val_0_rmse: 0.24558 |  0:00:30s\n",
      "epoch 156| loss: 0.04915 | val_0_rmse: 0.20585 |  0:00:30s\n",
      "epoch 157| loss: 0.02752 | val_0_rmse: 0.23955 |  0:00:30s\n",
      "epoch 158| loss: 0.04512 | val_0_rmse: 0.22068 |  0:00:30s\n",
      "epoch 159| loss: 0.03457 | val_0_rmse: 0.24674 |  0:00:30s\n",
      "epoch 160| loss: 0.04215 | val_0_rmse: 0.2445  |  0:00:31s\n",
      "epoch 161| loss: 0.05299 | val_0_rmse: 0.24189 |  0:00:31s\n",
      "epoch 162| loss: 0.06996 | val_0_rmse: 0.24687 |  0:00:31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:11:56,813] Trial 15 finished with value: 0.20149309118876543 and parameters: {'n_d': 24, 'n_a': 24, 'n_steps': 4, 'gamma': 1.4585360470320046, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.01299553239566428, 'batch_size': 512, 'virtual_batch_size': 256}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 163| loss: 0.0514  | val_0_rmse: 0.27437 |  0:00:31s\n",
      "\n",
      "Early stopping occurred at epoch 163 with best_epoch = 138 and best_val_0_rmse = 0.20149\n",
      "Trial 015 | rmse_log=0.20149 | RMSE$=42,103 | MAE$=26,273 | MAPE=14.85% | n_d/n_a=24/24 steps=4 lr=0.01300 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 130.71353| val_0_rmse: 11.30281|  0:00:00s\n",
      "epoch 1  | loss: 102.13527| val_0_rmse: 10.81114|  0:00:00s\n",
      "epoch 2  | loss: 81.40382| val_0_rmse: 10.16527|  0:00:00s\n",
      "epoch 3  | loss: 65.54527| val_0_rmse: 9.48158 |  0:00:00s\n",
      "epoch 4  | loss: 47.53097| val_0_rmse: 8.61465 |  0:00:00s\n",
      "epoch 5  | loss: 37.26608| val_0_rmse: 7.6228  |  0:00:01s\n",
      "epoch 6  | loss: 27.10718| val_0_rmse: 6.50381 |  0:00:01s\n",
      "epoch 7  | loss: 16.92648| val_0_rmse: 5.31743 |  0:00:01s\n",
      "epoch 8  | loss: 15.22853| val_0_rmse: 4.35847 |  0:00:01s\n",
      "epoch 9  | loss: 11.75883| val_0_rmse: 3.78716 |  0:00:01s\n",
      "epoch 10 | loss: 7.28733 | val_0_rmse: 3.6764  |  0:00:02s\n",
      "epoch 11 | loss: 4.35857 | val_0_rmse: 3.70713 |  0:00:02s\n",
      "epoch 12 | loss: 2.56902 | val_0_rmse: 3.23004 |  0:00:02s\n",
      "epoch 13 | loss: 2.00156 | val_0_rmse: 2.59097 |  0:00:02s\n",
      "epoch 14 | loss: 1.91087 | val_0_rmse: 2.1991  |  0:00:02s\n",
      "epoch 15 | loss: 1.86555 | val_0_rmse: 2.06122 |  0:00:03s\n",
      "epoch 16 | loss: 1.26663 | val_0_rmse: 1.92046 |  0:00:03s\n",
      "epoch 17 | loss: 0.84559 | val_0_rmse: 1.63507 |  0:00:03s\n",
      "epoch 18 | loss: 0.72158 | val_0_rmse: 1.56519 |  0:00:03s\n",
      "epoch 19 | loss: 0.64649 | val_0_rmse: 1.41076 |  0:00:03s\n",
      "epoch 20 | loss: 0.49305 | val_0_rmse: 1.31531 |  0:00:04s\n",
      "epoch 21 | loss: 0.42763 | val_0_rmse: 1.24044 |  0:00:04s\n",
      "epoch 22 | loss: 0.38395 | val_0_rmse: 0.81576 |  0:00:04s\n",
      "epoch 23 | loss: 0.48699 | val_0_rmse: 1.2378  |  0:00:04s\n",
      "epoch 24 | loss: 0.34849 | val_0_rmse: 0.70339 |  0:00:04s\n",
      "epoch 25 | loss: 0.29986 | val_0_rmse: 0.79289 |  0:00:04s\n",
      "epoch 26 | loss: 0.26187 | val_0_rmse: 0.6855  |  0:00:05s\n",
      "epoch 27 | loss: 0.25084 | val_0_rmse: 0.83442 |  0:00:05s\n",
      "epoch 28 | loss: 0.23614 | val_0_rmse: 0.58517 |  0:00:05s\n",
      "epoch 29 | loss: 0.30378 | val_0_rmse: 0.88277 |  0:00:05s\n",
      "epoch 30 | loss: 0.27286 | val_0_rmse: 0.48777 |  0:00:05s\n",
      "epoch 31 | loss: 0.20306 | val_0_rmse: 0.67566 |  0:00:06s\n",
      "epoch 32 | loss: 0.19709 | val_0_rmse: 0.38181 |  0:00:06s\n",
      "epoch 33 | loss: 0.23878 | val_0_rmse: 0.53276 |  0:00:06s\n",
      "epoch 34 | loss: 0.26755 | val_0_rmse: 0.40648 |  0:00:06s\n",
      "epoch 35 | loss: 0.21954 | val_0_rmse: 0.33929 |  0:00:06s\n",
      "epoch 36 | loss: 0.24643 | val_0_rmse: 0.49672 |  0:00:06s\n",
      "epoch 37 | loss: 0.1762  | val_0_rmse: 0.31771 |  0:00:07s\n",
      "epoch 38 | loss: 0.20579 | val_0_rmse: 0.39685 |  0:00:07s\n",
      "epoch 39 | loss: 0.14636 | val_0_rmse: 0.29494 |  0:00:07s\n",
      "epoch 40 | loss: 0.14419 | val_0_rmse: 0.41292 |  0:00:07s\n",
      "epoch 41 | loss: 0.13734 | val_0_rmse: 0.30148 |  0:00:07s\n",
      "epoch 42 | loss: 0.09777 | val_0_rmse: 0.37417 |  0:00:08s\n",
      "epoch 43 | loss: 0.12391 | val_0_rmse: 0.27945 |  0:00:08s\n",
      "epoch 44 | loss: 0.10033 | val_0_rmse: 0.32272 |  0:00:08s\n",
      "epoch 45 | loss: 0.09355 | val_0_rmse: 0.26381 |  0:00:08s\n",
      "epoch 46 | loss: 0.12601 | val_0_rmse: 0.28821 |  0:00:08s\n",
      "epoch 47 | loss: 0.09196 | val_0_rmse: 0.29133 |  0:00:09s\n",
      "epoch 48 | loss: 0.09317 | val_0_rmse: 0.25374 |  0:00:09s\n",
      "epoch 49 | loss: 0.08391 | val_0_rmse: 0.26873 |  0:00:09s\n",
      "epoch 50 | loss: 0.09808 | val_0_rmse: 0.2372  |  0:00:09s\n",
      "epoch 51 | loss: 0.06403 | val_0_rmse: 0.23082 |  0:00:09s\n",
      "epoch 52 | loss: 0.05849 | val_0_rmse: 0.25301 |  0:00:09s\n",
      "epoch 53 | loss: 0.07897 | val_0_rmse: 0.25627 |  0:00:10s\n",
      "epoch 54 | loss: 0.08441 | val_0_rmse: 0.23803 |  0:00:10s\n",
      "epoch 55 | loss: 0.06859 | val_0_rmse: 0.28017 |  0:00:10s\n",
      "epoch 56 | loss: 0.08976 | val_0_rmse: 0.23484 |  0:00:10s\n",
      "epoch 57 | loss: 0.06804 | val_0_rmse: 0.22357 |  0:00:10s\n",
      "epoch 58 | loss: 0.07559 | val_0_rmse: 0.23279 |  0:00:10s\n",
      "epoch 59 | loss: 0.06111 | val_0_rmse: 0.227   |  0:00:11s\n",
      "epoch 60 | loss: 0.06479 | val_0_rmse: 0.28897 |  0:00:11s\n",
      "epoch 61 | loss: 0.07854 | val_0_rmse: 0.25019 |  0:00:11s\n",
      "epoch 62 | loss: 0.12604 | val_0_rmse: 0.22734 |  0:00:11s\n",
      "epoch 63 | loss: 0.09    | val_0_rmse: 0.29786 |  0:00:11s\n",
      "epoch 64 | loss: 0.07596 | val_0_rmse: 0.28815 |  0:00:12s\n",
      "epoch 65 | loss: 0.11496 | val_0_rmse: 0.26258 |  0:00:12s\n",
      "epoch 66 | loss: 0.10134 | val_0_rmse: 0.23395 |  0:00:12s\n",
      "epoch 67 | loss: 0.07909 | val_0_rmse: 0.2568  |  0:00:12s\n",
      "epoch 68 | loss: 0.08539 | val_0_rmse: 0.27583 |  0:00:12s\n",
      "epoch 69 | loss: 0.08556 | val_0_rmse: 0.2576  |  0:00:12s\n",
      "epoch 70 | loss: 0.10559 | val_0_rmse: 0.24735 |  0:00:13s\n",
      "epoch 71 | loss: 0.0649  | val_0_rmse: 0.23375 |  0:00:13s\n",
      "epoch 72 | loss: 0.07373 | val_0_rmse: 0.28673 |  0:00:13s\n",
      "epoch 73 | loss: 0.08976 | val_0_rmse: 0.24127 |  0:00:13s\n",
      "epoch 74 | loss: 0.07247 | val_0_rmse: 0.22045 |  0:00:13s\n",
      "epoch 75 | loss: 0.06076 | val_0_rmse: 0.2307  |  0:00:13s\n",
      "epoch 76 | loss: 0.05179 | val_0_rmse: 0.22442 |  0:00:14s\n",
      "epoch 77 | loss: 0.04719 | val_0_rmse: 0.2292  |  0:00:14s\n",
      "epoch 78 | loss: 0.04676 | val_0_rmse: 0.21573 |  0:00:14s\n",
      "epoch 79 | loss: 0.04757 | val_0_rmse: 0.20946 |  0:00:14s\n",
      "epoch 80 | loss: 0.04835 | val_0_rmse: 0.22012 |  0:00:14s\n",
      "epoch 81 | loss: 0.04411 | val_0_rmse: 0.21737 |  0:00:15s\n",
      "epoch 82 | loss: 0.04405 | val_0_rmse: 0.22976 |  0:00:15s\n",
      "epoch 83 | loss: 0.04327 | val_0_rmse: 0.21805 |  0:00:15s\n",
      "epoch 84 | loss: 0.04596 | val_0_rmse: 0.24785 |  0:00:15s\n",
      "epoch 85 | loss: 0.06592 | val_0_rmse: 0.22672 |  0:00:15s\n",
      "epoch 86 | loss: 0.06454 | val_0_rmse: 0.24504 |  0:00:16s\n",
      "epoch 87 | loss: 0.05354 | val_0_rmse: 0.23039 |  0:00:16s\n",
      "epoch 88 | loss: 0.05667 | val_0_rmse: 0.23833 |  0:00:16s\n",
      "epoch 89 | loss: 0.05762 | val_0_rmse: 0.24087 |  0:00:16s\n",
      "epoch 90 | loss: 0.05442 | val_0_rmse: 0.23583 |  0:00:16s\n",
      "epoch 91 | loss: 0.0499  | val_0_rmse: 0.22855 |  0:00:16s\n",
      "epoch 92 | loss: 0.04263 | val_0_rmse: 0.23899 |  0:00:17s\n",
      "epoch 93 | loss: 0.0549  | val_0_rmse: 0.2373  |  0:00:17s\n",
      "epoch 94 | loss: 0.06257 | val_0_rmse: 0.28552 |  0:00:17s\n",
      "epoch 95 | loss: 0.06816 | val_0_rmse: 0.23666 |  0:00:17s\n",
      "epoch 96 | loss: 0.05426 | val_0_rmse: 0.24493 |  0:00:17s\n",
      "epoch 97 | loss: 0.04866 | val_0_rmse: 0.24776 |  0:00:17s\n",
      "epoch 98 | loss: 0.04991 | val_0_rmse: 0.25337 |  0:00:18s\n",
      "epoch 99 | loss: 0.05414 | val_0_rmse: 0.2377  |  0:00:18s\n",
      "epoch 100| loss: 0.05426 | val_0_rmse: 0.25489 |  0:00:18s\n",
      "epoch 101| loss: 0.05559 | val_0_rmse: 0.23976 |  0:00:18s\n",
      "epoch 102| loss: 0.04951 | val_0_rmse: 0.23494 |  0:00:18s\n",
      "epoch 103| loss: 0.0492  | val_0_rmse: 0.23568 |  0:00:18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:12:16,273] Trial 16 finished with value: 0.20945646846156557 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.334616733269399, 'n_independent': 3, 'n_shared': 2, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.011701580380816274, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 104| loss: 0.04699 | val_0_rmse: 0.2386  |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 104 with best_epoch = 79 and best_val_0_rmse = 0.20946\n",
      "Trial 016 | rmse_log=0.20946 | RMSE$=42,297 | MAE$=25,916 | MAPE=15.79% | n_d/n_a=32/16 steps=3 lr=0.01170 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 150.40603| val_0_rmse: 11.17497|  0:00:00s\n",
      "epoch 1  | loss: 99.23717| val_0_rmse: 9.68556 |  0:00:00s\n",
      "epoch 2  | loss: 62.28452| val_0_rmse: 7.84707 |  0:00:00s\n",
      "epoch 3  | loss: 37.1537 | val_0_rmse: 5.74931 |  0:00:01s\n",
      "epoch 4  | loss: 25.30462| val_0_rmse: 4.07508 |  0:00:01s\n",
      "epoch 5  | loss: 23.68837| val_0_rmse: 2.98939 |  0:00:01s\n",
      "epoch 6  | loss: 24.55532| val_0_rmse: 2.67027 |  0:00:01s\n",
      "epoch 7  | loss: 15.07227| val_0_rmse: 2.91603 |  0:00:02s\n",
      "epoch 8  | loss: 8.19273 | val_0_rmse: 3.21956 |  0:00:02s\n",
      "epoch 9  | loss: 7.05116 | val_0_rmse: 2.56866 |  0:00:02s\n",
      "epoch 10 | loss: 4.07299 | val_0_rmse: 1.53284 |  0:00:02s\n",
      "epoch 11 | loss: 2.81741 | val_0_rmse: 1.16031 |  0:00:02s\n",
      "epoch 12 | loss: 2.30615 | val_0_rmse: 1.24315 |  0:00:03s\n",
      "epoch 13 | loss: 1.93807 | val_0_rmse: 0.94615 |  0:00:03s\n",
      "epoch 14 | loss: 1.25771 | val_0_rmse: 1.26126 |  0:00:03s\n",
      "epoch 15 | loss: 1.24416 | val_0_rmse: 0.74346 |  0:00:03s\n",
      "epoch 16 | loss: 1.02365 | val_0_rmse: 1.38519 |  0:00:04s\n",
      "epoch 17 | loss: 0.79236 | val_0_rmse: 0.84337 |  0:00:04s\n",
      "epoch 18 | loss: 0.87792 | val_0_rmse: 1.1465  |  0:00:04s\n",
      "epoch 19 | loss: 0.72433 | val_0_rmse: 0.83364 |  0:00:04s\n",
      "epoch 20 | loss: 0.5576  | val_0_rmse: 1.04799 |  0:00:05s\n",
      "epoch 21 | loss: 0.51662 | val_0_rmse: 1.08187 |  0:00:05s\n",
      "epoch 22 | loss: 0.47454 | val_0_rmse: 0.95834 |  0:00:05s\n",
      "epoch 23 | loss: 0.52684 | val_0_rmse: 0.83478 |  0:00:05s\n",
      "epoch 24 | loss: 0.39483 | val_0_rmse: 0.84473 |  0:00:05s\n",
      "epoch 25 | loss: 0.39366 | val_0_rmse: 0.94447 |  0:00:06s\n",
      "epoch 26 | loss: 0.34868 | val_0_rmse: 0.85081 |  0:00:06s\n",
      "epoch 27 | loss: 0.29294 | val_0_rmse: 0.7155  |  0:00:06s\n",
      "epoch 28 | loss: 0.31684 | val_0_rmse: 0.71114 |  0:00:06s\n",
      "epoch 29 | loss: 0.29644 | val_0_rmse: 0.81155 |  0:00:06s\n",
      "epoch 30 | loss: 0.30984 | val_0_rmse: 0.86813 |  0:00:07s\n",
      "epoch 31 | loss: 0.2953  | val_0_rmse: 0.57133 |  0:00:07s\n",
      "epoch 32 | loss: 0.30255 | val_0_rmse: 0.77445 |  0:00:07s\n",
      "epoch 33 | loss: 0.27381 | val_0_rmse: 0.59532 |  0:00:07s\n",
      "epoch 34 | loss: 0.20748 | val_0_rmse: 0.49923 |  0:00:08s\n",
      "epoch 35 | loss: 0.2297  | val_0_rmse: 0.4555  |  0:00:08s\n",
      "epoch 36 | loss: 0.20205 | val_0_rmse: 0.49856 |  0:00:08s\n",
      "epoch 37 | loss: 0.2618  | val_0_rmse: 0.58471 |  0:00:08s\n",
      "epoch 38 | loss: 0.21462 | val_0_rmse: 0.57757 |  0:00:09s\n",
      "epoch 39 | loss: 0.23661 | val_0_rmse: 0.50001 |  0:00:09s\n",
      "epoch 40 | loss: 0.19303 | val_0_rmse: 0.70935 |  0:00:09s\n",
      "epoch 41 | loss: 0.19828 | val_0_rmse: 0.6678  |  0:00:09s\n",
      "epoch 42 | loss: 0.18383 | val_0_rmse: 0.52568 |  0:00:09s\n",
      "epoch 43 | loss: 0.21918 | val_0_rmse: 0.58252 |  0:00:10s\n",
      "epoch 44 | loss: 0.29656 | val_0_rmse: 0.36291 |  0:00:10s\n",
      "epoch 45 | loss: 0.20722 | val_0_rmse: 0.5381  |  0:00:10s\n",
      "epoch 46 | loss: 0.13868 | val_0_rmse: 0.4659  |  0:00:10s\n",
      "epoch 47 | loss: 0.12362 | val_0_rmse: 0.36876 |  0:00:11s\n",
      "epoch 48 | loss: 0.21028 | val_0_rmse: 0.78979 |  0:00:11s\n",
      "epoch 49 | loss: 0.35633 | val_0_rmse: 0.49328 |  0:00:11s\n",
      "epoch 50 | loss: 0.19976 | val_0_rmse: 0.56319 |  0:00:11s\n",
      "epoch 51 | loss: 0.16731 | val_0_rmse: 0.4787  |  0:00:11s\n",
      "epoch 52 | loss: 0.14837 | val_0_rmse: 0.41425 |  0:00:12s\n",
      "epoch 53 | loss: 0.15857 | val_0_rmse: 0.42625 |  0:00:12s\n",
      "epoch 54 | loss: 0.11308 | val_0_rmse: 0.38865 |  0:00:12s\n",
      "epoch 55 | loss: 0.11125 | val_0_rmse: 0.37941 |  0:00:12s\n",
      "epoch 56 | loss: 0.10363 | val_0_rmse: 0.43122 |  0:00:13s\n",
      "epoch 57 | loss: 0.09272 | val_0_rmse: 0.33215 |  0:00:13s\n",
      "epoch 58 | loss: 0.08994 | val_0_rmse: 0.37783 |  0:00:13s\n",
      "epoch 59 | loss: 0.1014  | val_0_rmse: 0.50088 |  0:00:13s\n",
      "epoch 60 | loss: 0.18963 | val_0_rmse: 0.31203 |  0:00:14s\n",
      "epoch 61 | loss: 0.23695 | val_0_rmse: 0.33669 |  0:00:14s\n",
      "epoch 62 | loss: 0.10401 | val_0_rmse: 0.29748 |  0:00:14s\n",
      "epoch 63 | loss: 0.13478 | val_0_rmse: 0.30376 |  0:00:14s\n",
      "epoch 64 | loss: 0.13871 | val_0_rmse: 0.39918 |  0:00:14s\n",
      "epoch 65 | loss: 0.1441  | val_0_rmse: 0.29189 |  0:00:15s\n",
      "epoch 66 | loss: 0.08757 | val_0_rmse: 0.33563 |  0:00:15s\n",
      "epoch 67 | loss: 0.09403 | val_0_rmse: 0.28848 |  0:00:15s\n",
      "epoch 68 | loss: 0.09553 | val_0_rmse: 0.29959 |  0:00:15s\n",
      "epoch 69 | loss: 0.09677 | val_0_rmse: 0.36364 |  0:00:16s\n",
      "epoch 70 | loss: 0.09208 | val_0_rmse: 0.34753 |  0:00:16s\n",
      "epoch 71 | loss: 0.10045 | val_0_rmse: 0.29841 |  0:00:16s\n",
      "epoch 72 | loss: 0.08864 | val_0_rmse: 0.29671 |  0:00:16s\n",
      "epoch 73 | loss: 0.10191 | val_0_rmse: 0.32452 |  0:00:16s\n",
      "epoch 74 | loss: 0.07966 | val_0_rmse: 0.32078 |  0:00:17s\n",
      "epoch 75 | loss: 0.0833  | val_0_rmse: 0.28044 |  0:00:17s\n",
      "epoch 76 | loss: 0.06561 | val_0_rmse: 0.27795 |  0:00:17s\n",
      "epoch 77 | loss: 0.07811 | val_0_rmse: 0.28877 |  0:00:17s\n",
      "epoch 78 | loss: 0.07528 | val_0_rmse: 0.29166 |  0:00:18s\n",
      "epoch 79 | loss: 0.072   | val_0_rmse: 0.27527 |  0:00:18s\n",
      "epoch 80 | loss: 0.08547 | val_0_rmse: 0.26641 |  0:00:18s\n",
      "epoch 81 | loss: 0.05898 | val_0_rmse: 0.27173 |  0:00:18s\n",
      "epoch 82 | loss: 0.07507 | val_0_rmse: 0.28586 |  0:00:19s\n",
      "epoch 83 | loss: 0.07769 | val_0_rmse: 0.28167 |  0:00:19s\n",
      "epoch 84 | loss: 0.06143 | val_0_rmse: 0.27546 |  0:00:19s\n",
      "epoch 85 | loss: 0.07397 | val_0_rmse: 0.27821 |  0:00:19s\n",
      "epoch 86 | loss: 0.08677 | val_0_rmse: 0.27489 |  0:00:19s\n",
      "epoch 87 | loss: 0.10706 | val_0_rmse: 0.39341 |  0:00:20s\n",
      "epoch 88 | loss: 0.12773 | val_0_rmse: 0.56587 |  0:00:20s\n",
      "epoch 89 | loss: 0.36244 | val_0_rmse: 0.28032 |  0:00:20s\n",
      "epoch 90 | loss: 0.16613 | val_0_rmse: 0.58719 |  0:00:20s\n",
      "epoch 91 | loss: 0.27411 | val_0_rmse: 0.31443 |  0:00:21s\n",
      "epoch 92 | loss: 0.10049 | val_0_rmse: 0.29096 |  0:00:21s\n",
      "epoch 93 | loss: 0.08033 | val_0_rmse: 0.30072 |  0:00:21s\n",
      "epoch 94 | loss: 0.06641 | val_0_rmse: 0.27255 |  0:00:21s\n",
      "epoch 95 | loss: 0.06477 | val_0_rmse: 0.28832 |  0:00:21s\n",
      "epoch 96 | loss: 0.06433 | val_0_rmse: 0.27048 |  0:00:22s\n",
      "epoch 97 | loss: 0.0621  | val_0_rmse: 0.26789 |  0:00:22s\n",
      "epoch 98 | loss: 0.07362 | val_0_rmse: 0.26204 |  0:00:22s\n",
      "epoch 99 | loss: 0.05757 | val_0_rmse: 0.29077 |  0:00:22s\n",
      "epoch 100| loss: 0.07374 | val_0_rmse: 0.27475 |  0:00:23s\n",
      "epoch 101| loss: 0.06587 | val_0_rmse: 0.24413 |  0:00:23s\n",
      "epoch 102| loss: 0.05782 | val_0_rmse: 0.25504 |  0:00:23s\n",
      "epoch 103| loss: 0.06562 | val_0_rmse: 0.28471 |  0:00:23s\n",
      "epoch 104| loss: 0.07527 | val_0_rmse: 0.23863 |  0:00:24s\n",
      "epoch 105| loss: 0.06429 | val_0_rmse: 0.24608 |  0:00:24s\n",
      "epoch 106| loss: 0.04761 | val_0_rmse: 0.24879 |  0:00:24s\n",
      "epoch 107| loss: 0.04464 | val_0_rmse: 0.26071 |  0:00:24s\n",
      "epoch 108| loss: 0.05846 | val_0_rmse: 0.29743 |  0:00:25s\n",
      "epoch 109| loss: 0.07499 | val_0_rmse: 0.38824 |  0:00:25s\n",
      "epoch 110| loss: 0.14697 | val_0_rmse: 0.28857 |  0:00:25s\n",
      "epoch 111| loss: 0.06843 | val_0_rmse: 0.25872 |  0:00:25s\n",
      "epoch 112| loss: 0.05421 | val_0_rmse: 0.27978 |  0:00:25s\n",
      "epoch 113| loss: 0.06324 | val_0_rmse: 0.24326 |  0:00:26s\n",
      "epoch 114| loss: 0.0472  | val_0_rmse: 0.28172 |  0:00:26s\n",
      "epoch 115| loss: 0.06875 | val_0_rmse: 0.23865 |  0:00:26s\n",
      "epoch 116| loss: 0.04481 | val_0_rmse: 0.22924 |  0:00:26s\n",
      "epoch 117| loss: 0.04452 | val_0_rmse: 0.2284  |  0:00:27s\n",
      "epoch 118| loss: 0.0415  | val_0_rmse: 0.26713 |  0:00:27s\n",
      "epoch 119| loss: 0.05281 | val_0_rmse: 0.24842 |  0:00:27s\n",
      "epoch 120| loss: 0.0447  | val_0_rmse: 0.24823 |  0:00:27s\n",
      "epoch 121| loss: 0.05157 | val_0_rmse: 0.21606 |  0:00:28s\n",
      "epoch 122| loss: 0.04162 | val_0_rmse: 0.23217 |  0:00:28s\n",
      "epoch 123| loss: 0.04067 | val_0_rmse: 0.23536 |  0:00:28s\n",
      "epoch 124| loss: 0.04122 | val_0_rmse: 0.23205 |  0:00:28s\n",
      "epoch 125| loss: 0.04456 | val_0_rmse: 0.28762 |  0:00:28s\n",
      "epoch 126| loss: 0.05933 | val_0_rmse: 0.31661 |  0:00:29s\n",
      "epoch 127| loss: 0.07978 | val_0_rmse: 0.32652 |  0:00:29s\n",
      "epoch 128| loss: 0.07607 | val_0_rmse: 0.27674 |  0:00:29s\n",
      "epoch 129| loss: 0.05453 | val_0_rmse: 0.34147 |  0:00:29s\n",
      "epoch 130| loss: 0.09063 | val_0_rmse: 0.25917 |  0:00:30s\n",
      "epoch 131| loss: 0.05173 | val_0_rmse: 0.34441 |  0:00:30s\n",
      "epoch 132| loss: 0.09024 | val_0_rmse: 0.26617 |  0:00:30s\n",
      "epoch 133| loss: 0.04769 | val_0_rmse: 0.34664 |  0:00:30s\n",
      "epoch 134| loss: 0.08463 | val_0_rmse: 0.29828 |  0:00:30s\n",
      "epoch 135| loss: 0.05404 | val_0_rmse: 0.31965 |  0:00:31s\n",
      "epoch 136| loss: 0.06948 | val_0_rmse: 0.30556 |  0:00:31s\n",
      "epoch 137| loss: 0.06138 | val_0_rmse: 0.31163 |  0:00:31s\n",
      "epoch 138| loss: 0.06818 | val_0_rmse: 0.33425 |  0:00:31s\n",
      "epoch 139| loss: 0.05986 | val_0_rmse: 0.30925 |  0:00:32s\n",
      "epoch 140| loss: 0.06028 | val_0_rmse: 0.33093 |  0:00:32s\n",
      "epoch 141| loss: 0.05702 | val_0_rmse: 0.30756 |  0:00:32s\n",
      "epoch 142| loss: 0.05897 | val_0_rmse: 0.32882 |  0:00:32s\n",
      "epoch 143| loss: 0.05952 | val_0_rmse: 0.31216 |  0:00:32s\n",
      "epoch 144| loss: 0.06879 | val_0_rmse: 0.33554 |  0:00:33s\n",
      "epoch 145| loss: 0.05349 | val_0_rmse: 0.30385 |  0:00:33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:12:50,379] Trial 17 finished with value: 0.21605995710186252 and parameters: {'n_d': 48, 'n_a': 16, 'n_steps': 4, 'gamma': 1.543484321138569, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.015262308481585557, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 146| loss: 0.06412 | val_0_rmse: 0.32973 |  0:00:33s\n",
      "\n",
      "Early stopping occurred at epoch 146 with best_epoch = 121 and best_val_0_rmse = 0.21606\n",
      "Trial 017 | rmse_log=0.21606 | RMSE$=48,494 | MAE$=26,976 | MAPE=15.03% | n_d/n_a=48/16 steps=4 lr=0.01526 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 131.74443| val_0_rmse: 10.69774|  0:00:00s\n",
      "epoch 1  | loss: 93.93808| val_0_rmse: 9.76182 |  0:00:00s\n",
      "epoch 2  | loss: 68.39348| val_0_rmse: 8.76914 |  0:00:00s\n",
      "epoch 3  | loss: 47.69331| val_0_rmse: 7.68832 |  0:00:00s\n",
      "epoch 4  | loss: 33.56943| val_0_rmse: 6.53809 |  0:00:00s\n",
      "epoch 5  | loss: 21.16334| val_0_rmse: 5.50773 |  0:00:01s\n",
      "epoch 6  | loss: 17.47298| val_0_rmse: 4.7548  |  0:00:01s\n",
      "epoch 7  | loss: 15.34151| val_0_rmse: 4.22914 |  0:00:01s\n",
      "epoch 8  | loss: 13.43548| val_0_rmse: 3.97893 |  0:00:01s\n",
      "epoch 9  | loss: 8.58247 | val_0_rmse: 4.17154 |  0:00:01s\n",
      "epoch 10 | loss: 5.33296 | val_0_rmse: 4.14363 |  0:00:02s\n",
      "epoch 11 | loss: 4.52639 | val_0_rmse: 3.47155 |  0:00:02s\n",
      "epoch 12 | loss: 3.60161 | val_0_rmse: 2.75177 |  0:00:02s\n",
      "epoch 13 | loss: 2.40996 | val_0_rmse: 2.43464 |  0:00:02s\n",
      "epoch 14 | loss: 2.23902 | val_0_rmse: 2.57891 |  0:00:02s\n",
      "epoch 15 | loss: 1.59355 | val_0_rmse: 2.01389 |  0:00:03s\n",
      "epoch 16 | loss: 1.20351 | val_0_rmse: 1.8995  |  0:00:03s\n",
      "epoch 17 | loss: 0.966   | val_0_rmse: 1.7597  |  0:00:03s\n",
      "epoch 18 | loss: 0.8355  | val_0_rmse: 1.55014 |  0:00:03s\n",
      "epoch 19 | loss: 0.9824  | val_0_rmse: 1.62135 |  0:00:04s\n",
      "epoch 20 | loss: 0.62529 | val_0_rmse: 1.25305 |  0:00:04s\n",
      "epoch 21 | loss: 0.84395 | val_0_rmse: 1.22121 |  0:00:04s\n",
      "epoch 22 | loss: 0.62844 | val_0_rmse: 1.12995 |  0:00:04s\n",
      "epoch 23 | loss: 0.58344 | val_0_rmse: 1.06557 |  0:00:04s\n",
      "epoch 24 | loss: 0.5173  | val_0_rmse: 0.97456 |  0:00:04s\n",
      "epoch 25 | loss: 0.56025 | val_0_rmse: 1.16884 |  0:00:05s\n",
      "epoch 26 | loss: 0.53709 | val_0_rmse: 0.82362 |  0:00:05s\n",
      "epoch 27 | loss: 0.44383 | val_0_rmse: 1.05078 |  0:00:05s\n",
      "epoch 28 | loss: 0.45217 | val_0_rmse: 0.82276 |  0:00:05s\n",
      "epoch 29 | loss: 0.5329  | val_0_rmse: 0.91999 |  0:00:05s\n",
      "epoch 30 | loss: 0.38413 | val_0_rmse: 0.71815 |  0:00:06s\n",
      "epoch 31 | loss: 0.32416 | val_0_rmse: 1.0331  |  0:00:06s\n",
      "epoch 32 | loss: 0.4279  | val_0_rmse: 0.6405  |  0:00:06s\n",
      "epoch 33 | loss: 0.34325 | val_0_rmse: 0.8688  |  0:00:06s\n",
      "epoch 34 | loss: 0.42508 | val_0_rmse: 0.62099 |  0:00:06s\n",
      "epoch 35 | loss: 0.39198 | val_0_rmse: 0.74219 |  0:00:07s\n",
      "epoch 36 | loss: 0.25419 | val_0_rmse: 0.60299 |  0:00:07s\n",
      "epoch 37 | loss: 0.2481  | val_0_rmse: 0.7144  |  0:00:07s\n",
      "epoch 38 | loss: 0.23689 | val_0_rmse: 0.68704 |  0:00:07s\n",
      "epoch 39 | loss: 0.26298 | val_0_rmse: 0.53677 |  0:00:07s\n",
      "epoch 40 | loss: 0.21318 | val_0_rmse: 0.53095 |  0:00:08s\n",
      "epoch 41 | loss: 0.22456 | val_0_rmse: 0.64403 |  0:00:08s\n",
      "epoch 42 | loss: 0.20122 | val_0_rmse: 0.47924 |  0:00:08s\n",
      "epoch 43 | loss: 0.23681 | val_0_rmse: 0.69517 |  0:00:08s\n",
      "epoch 44 | loss: 0.31334 | val_0_rmse: 0.43742 |  0:00:08s\n",
      "epoch 45 | loss: 0.34429 | val_0_rmse: 0.5397  |  0:00:09s\n",
      "epoch 46 | loss: 0.35419 | val_0_rmse: 0.68026 |  0:00:09s\n",
      "epoch 47 | loss: 0.31174 | val_0_rmse: 0.39114 |  0:00:09s\n",
      "epoch 48 | loss: 0.40198 | val_0_rmse: 0.44441 |  0:00:09s\n",
      "epoch 49 | loss: 0.22736 | val_0_rmse: 0.68979 |  0:00:09s\n",
      "epoch 50 | loss: 0.2941  | val_0_rmse: 0.39471 |  0:00:09s\n",
      "epoch 51 | loss: 0.21622 | val_0_rmse: 0.52294 |  0:00:10s\n",
      "epoch 52 | loss: 0.24646 | val_0_rmse: 0.41171 |  0:00:10s\n",
      "epoch 53 | loss: 0.17326 | val_0_rmse: 0.39892 |  0:00:10s\n",
      "epoch 54 | loss: 0.14473 | val_0_rmse: 0.48731 |  0:00:10s\n",
      "epoch 55 | loss: 0.13655 | val_0_rmse: 0.37519 |  0:00:10s\n",
      "epoch 56 | loss: 0.14183 | val_0_rmse: 0.51627 |  0:00:10s\n",
      "epoch 57 | loss: 0.15497 | val_0_rmse: 0.37253 |  0:00:11s\n",
      "epoch 58 | loss: 0.14721 | val_0_rmse: 0.47976 |  0:00:11s\n",
      "epoch 59 | loss: 0.15065 | val_0_rmse: 0.37425 |  0:00:11s\n",
      "epoch 60 | loss: 0.14511 | val_0_rmse: 0.41038 |  0:00:11s\n",
      "epoch 61 | loss: 0.12384 | val_0_rmse: 0.34846 |  0:00:11s\n",
      "epoch 62 | loss: 0.10397 | val_0_rmse: 0.42277 |  0:00:12s\n",
      "epoch 63 | loss: 0.10387 | val_0_rmse: 0.3618  |  0:00:12s\n",
      "epoch 64 | loss: 0.10183 | val_0_rmse: 0.39642 |  0:00:12s\n",
      "epoch 65 | loss: 0.11387 | val_0_rmse: 0.34393 |  0:00:12s\n",
      "epoch 66 | loss: 0.09166 | val_0_rmse: 0.39373 |  0:00:12s\n",
      "epoch 67 | loss: 0.12319 | val_0_rmse: 0.33516 |  0:00:13s\n",
      "epoch 68 | loss: 0.10237 | val_0_rmse: 0.3253  |  0:00:13s\n",
      "epoch 69 | loss: 0.13491 | val_0_rmse: 0.30612 |  0:00:13s\n",
      "epoch 70 | loss: 0.14011 | val_0_rmse: 0.42696 |  0:00:13s\n",
      "epoch 71 | loss: 0.20052 | val_0_rmse: 0.30142 |  0:00:13s\n",
      "epoch 72 | loss: 0.21634 | val_0_rmse: 0.30848 |  0:00:14s\n",
      "epoch 73 | loss: 0.10246 | val_0_rmse: 0.37702 |  0:00:14s\n",
      "epoch 74 | loss: 0.12642 | val_0_rmse: 0.30918 |  0:00:14s\n",
      "epoch 75 | loss: 0.13149 | val_0_rmse: 0.3726  |  0:00:14s\n",
      "epoch 76 | loss: 0.15943 | val_0_rmse: 0.3006  |  0:00:14s\n",
      "epoch 77 | loss: 0.11613 | val_0_rmse: 0.40402 |  0:00:14s\n",
      "epoch 78 | loss: 0.13868 | val_0_rmse: 0.31513 |  0:00:15s\n",
      "epoch 79 | loss: 0.08659 | val_0_rmse: 0.33875 |  0:00:15s\n",
      "epoch 80 | loss: 0.13256 | val_0_rmse: 0.28349 |  0:00:15s\n",
      "epoch 81 | loss: 0.0901  | val_0_rmse: 0.28537 |  0:00:15s\n",
      "epoch 82 | loss: 0.08863 | val_0_rmse: 0.29968 |  0:00:15s\n",
      "epoch 83 | loss: 0.08175 | val_0_rmse: 0.31666 |  0:00:16s\n",
      "epoch 84 | loss: 0.08176 | val_0_rmse: 0.32193 |  0:00:16s\n",
      "epoch 85 | loss: 0.09795 | val_0_rmse: 0.32012 |  0:00:16s\n",
      "epoch 86 | loss: 0.1686  | val_0_rmse: 0.32334 |  0:00:16s\n",
      "epoch 87 | loss: 0.12002 | val_0_rmse: 0.31411 |  0:00:16s\n",
      "epoch 88 | loss: 0.1197  | val_0_rmse: 0.32544 |  0:00:16s\n",
      "epoch 89 | loss: 0.10163 | val_0_rmse: 0.29969 |  0:00:17s\n",
      "epoch 90 | loss: 0.16388 | val_0_rmse: 0.31195 |  0:00:17s\n",
      "epoch 91 | loss: 0.10808 | val_0_rmse: 0.3051  |  0:00:17s\n",
      "epoch 92 | loss: 0.08313 | val_0_rmse: 0.26302 |  0:00:17s\n",
      "epoch 93 | loss: 0.07576 | val_0_rmse: 0.25738 |  0:00:17s\n",
      "epoch 94 | loss: 0.06107 | val_0_rmse: 0.25227 |  0:00:17s\n",
      "epoch 95 | loss: 0.05687 | val_0_rmse: 0.2473  |  0:00:18s\n",
      "epoch 96 | loss: 0.0588  | val_0_rmse: 0.24549 |  0:00:18s\n",
      "epoch 97 | loss: 0.0613  | val_0_rmse: 0.26764 |  0:00:18s\n",
      "epoch 98 | loss: 0.06089 | val_0_rmse: 0.25363 |  0:00:18s\n",
      "epoch 99 | loss: 0.0577  | val_0_rmse: 0.24075 |  0:00:18s\n",
      "epoch 100| loss: 0.05806 | val_0_rmse: 0.27584 |  0:00:19s\n",
      "epoch 101| loss: 0.06709 | val_0_rmse: 0.26342 |  0:00:19s\n",
      "epoch 102| loss: 0.09115 | val_0_rmse: 0.27777 |  0:00:19s\n",
      "epoch 103| loss: 0.08496 | val_0_rmse: 0.24044 |  0:00:19s\n",
      "epoch 104| loss: 0.05565 | val_0_rmse: 0.23728 |  0:00:19s\n",
      "epoch 105| loss: 0.04982 | val_0_rmse: 0.26324 |  0:00:20s\n",
      "epoch 106| loss: 0.08113 | val_0_rmse: 0.27044 |  0:00:20s\n",
      "epoch 107| loss: 0.09085 | val_0_rmse: 0.23944 |  0:00:20s\n",
      "epoch 108| loss: 0.0693  | val_0_rmse: 0.26616 |  0:00:20s\n",
      "epoch 109| loss: 0.06564 | val_0_rmse: 0.2804  |  0:00:20s\n",
      "epoch 110| loss: 0.06072 | val_0_rmse: 0.33011 |  0:00:20s\n",
      "epoch 111| loss: 0.10815 | val_0_rmse: 0.29664 |  0:00:21s\n",
      "epoch 112| loss: 0.11207 | val_0_rmse: 0.28114 |  0:00:21s\n",
      "epoch 113| loss: 0.05781 | val_0_rmse: 0.28564 |  0:00:21s\n",
      "epoch 114| loss: 0.05862 | val_0_rmse: 0.27779 |  0:00:21s\n",
      "epoch 115| loss: 0.05796 | val_0_rmse: 0.32604 |  0:00:21s\n",
      "epoch 116| loss: 0.07481 | val_0_rmse: 0.26951 |  0:00:22s\n",
      "epoch 117| loss: 0.04764 | val_0_rmse: 0.27287 |  0:00:22s\n",
      "epoch 118| loss: 0.04847 | val_0_rmse: 0.27479 |  0:00:22s\n",
      "epoch 119| loss: 0.04776 | val_0_rmse: 0.27026 |  0:00:22s\n",
      "epoch 120| loss: 0.04346 | val_0_rmse: 0.26775 |  0:00:22s\n",
      "epoch 121| loss: 0.04746 | val_0_rmse: 0.27367 |  0:00:22s\n",
      "epoch 122| loss: 0.06295 | val_0_rmse: 0.27949 |  0:00:23s\n",
      "epoch 123| loss: 0.07574 | val_0_rmse: 0.27957 |  0:00:23s\n",
      "epoch 124| loss: 0.06303 | val_0_rmse: 0.26522 |  0:00:23s\n",
      "epoch 125| loss: 0.05371 | val_0_rmse: 0.29693 |  0:00:23s\n",
      "epoch 126| loss: 0.06323 | val_0_rmse: 0.28038 |  0:00:23s\n",
      "epoch 127| loss: 0.04873 | val_0_rmse: 0.27084 |  0:00:23s\n",
      "epoch 128| loss: 0.05361 | val_0_rmse: 0.25505 |  0:00:24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:13:15,100] Trial 18 finished with value: 0.237276540033062 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.452760754669118, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-05, 'mask_type': 'sparsemax', 'lr': 0.010177565908060068, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 129| loss: 0.04628 | val_0_rmse: 0.23852 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 129 with best_epoch = 104 and best_val_0_rmse = 0.23728\n",
      "Trial 018 | rmse_log=0.23728 | RMSE$=44,002 | MAE$=29,430 | MAPE=17.23% | n_d/n_a=48/24 steps=3 lr=0.01018 batch=512 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 117.86676| val_0_rmse: 7.88563 |  0:00:00s\n",
      "epoch 1  | loss: 60.35072| val_0_rmse: 5.15671 |  0:00:00s\n",
      "epoch 2  | loss: 30.75593| val_0_rmse: 3.20893 |  0:00:00s\n",
      "epoch 3  | loss: 28.26275| val_0_rmse: 2.32295 |  0:00:00s\n",
      "epoch 4  | loss: 24.68513| val_0_rmse: 2.36786 |  0:00:01s\n",
      "epoch 5  | loss: 12.91697| val_0_rmse: 3.39271 |  0:00:01s\n",
      "epoch 6  | loss: 7.57915 | val_0_rmse: 3.81734 |  0:00:01s\n",
      "epoch 7  | loss: 4.80762 | val_0_rmse: 2.28811 |  0:00:01s\n",
      "epoch 8  | loss: 2.95686 | val_0_rmse: 2.53907 |  0:00:01s\n",
      "epoch 9  | loss: 1.97312 | val_0_rmse: 2.34552 |  0:00:01s\n",
      "epoch 10 | loss: 2.29427 | val_0_rmse: 2.32194 |  0:00:02s\n",
      "epoch 11 | loss: 1.39972 | val_0_rmse: 2.32902 |  0:00:02s\n",
      "epoch 12 | loss: 1.40767 | val_0_rmse: 2.24727 |  0:00:02s\n",
      "epoch 13 | loss: 1.31843 | val_0_rmse: 1.73462 |  0:00:02s\n",
      "epoch 14 | loss: 0.96813 | val_0_rmse: 1.90961 |  0:00:02s\n",
      "epoch 15 | loss: 0.98073 | val_0_rmse: 1.75125 |  0:00:03s\n",
      "epoch 16 | loss: 1.31832 | val_0_rmse: 1.80748 |  0:00:03s\n",
      "epoch 17 | loss: 0.43683 | val_0_rmse: 2.07469 |  0:00:03s\n",
      "epoch 18 | loss: 0.62288 | val_0_rmse: 1.95208 |  0:00:03s\n",
      "epoch 19 | loss: 0.55835 | val_0_rmse: 1.81545 |  0:00:03s\n",
      "epoch 20 | loss: 0.38867 | val_0_rmse: 1.80832 |  0:00:04s\n",
      "epoch 21 | loss: 0.5011  | val_0_rmse: 1.70902 |  0:00:04s\n",
      "epoch 22 | loss: 0.29214 | val_0_rmse: 1.58385 |  0:00:04s\n",
      "epoch 23 | loss: 0.39237 | val_0_rmse: 1.45586 |  0:00:04s\n",
      "epoch 24 | loss: 0.27242 | val_0_rmse: 1.6081  |  0:00:04s\n",
      "epoch 25 | loss: 0.20935 | val_0_rmse: 1.40549 |  0:00:05s\n",
      "epoch 26 | loss: 0.17445 | val_0_rmse: 1.30296 |  0:00:05s\n",
      "epoch 27 | loss: 0.25011 | val_0_rmse: 1.00818 |  0:00:05s\n",
      "epoch 28 | loss: 0.17008 | val_0_rmse: 1.10054 |  0:00:05s\n",
      "epoch 29 | loss: 0.16078 | val_0_rmse: 1.0584  |  0:00:05s\n",
      "epoch 30 | loss: 0.15551 | val_0_rmse: 0.84725 |  0:00:06s\n",
      "epoch 31 | loss: 0.21409 | val_0_rmse: 1.01054 |  0:00:06s\n",
      "epoch 32 | loss: 0.1789  | val_0_rmse: 0.77785 |  0:00:06s\n",
      "epoch 33 | loss: 0.25319 | val_0_rmse: 0.9361  |  0:00:06s\n",
      "epoch 34 | loss: 0.16676 | val_0_rmse: 0.62896 |  0:00:06s\n",
      "epoch 35 | loss: 0.13932 | val_0_rmse: 0.82229 |  0:00:07s\n",
      "epoch 36 | loss: 0.15528 | val_0_rmse: 0.52753 |  0:00:07s\n",
      "epoch 37 | loss: 0.13486 | val_0_rmse: 0.65107 |  0:00:07s\n",
      "epoch 38 | loss: 0.15348 | val_0_rmse: 0.37714 |  0:00:07s\n",
      "epoch 39 | loss: 0.09515 | val_0_rmse: 0.50526 |  0:00:07s\n",
      "epoch 40 | loss: 0.08267 | val_0_rmse: 0.5139  |  0:00:08s\n",
      "epoch 41 | loss: 0.08767 | val_0_rmse: 0.35778 |  0:00:08s\n",
      "epoch 42 | loss: 0.11235 | val_0_rmse: 0.37483 |  0:00:08s\n",
      "epoch 43 | loss: 0.08572 | val_0_rmse: 0.40943 |  0:00:08s\n",
      "epoch 44 | loss: 0.08143 | val_0_rmse: 0.32083 |  0:00:08s\n",
      "epoch 45 | loss: 0.07514 | val_0_rmse: 0.29023 |  0:00:09s\n",
      "epoch 46 | loss: 0.10023 | val_0_rmse: 0.29115 |  0:00:09s\n",
      "epoch 47 | loss: 0.08699 | val_0_rmse: 0.3092  |  0:00:09s\n",
      "epoch 48 | loss: 0.09509 | val_0_rmse: 0.28418 |  0:00:09s\n",
      "epoch 49 | loss: 0.14711 | val_0_rmse: 0.45296 |  0:00:09s\n",
      "epoch 50 | loss: 0.23453 | val_0_rmse: 0.2272  |  0:00:10s\n",
      "epoch 51 | loss: 0.12066 | val_0_rmse: 0.45085 |  0:00:10s\n",
      "epoch 52 | loss: 0.07868 | val_0_rmse: 0.3662  |  0:00:10s\n",
      "epoch 53 | loss: 0.11436 | val_0_rmse: 0.59016 |  0:00:10s\n",
      "epoch 54 | loss: 0.10497 | val_0_rmse: 0.27277 |  0:00:10s\n",
      "epoch 55 | loss: 0.1408  | val_0_rmse: 0.54719 |  0:00:10s\n",
      "epoch 56 | loss: 0.10817 | val_0_rmse: 0.2749  |  0:00:11s\n",
      "epoch 57 | loss: 0.12701 | val_0_rmse: 0.47882 |  0:00:11s\n",
      "epoch 58 | loss: 0.07551 | val_0_rmse: 0.24835 |  0:00:11s\n",
      "epoch 59 | loss: 0.15441 | val_0_rmse: 0.54526 |  0:00:11s\n",
      "epoch 60 | loss: 0.14439 | val_0_rmse: 0.26132 |  0:00:11s\n",
      "epoch 61 | loss: 0.21853 | val_0_rmse: 0.32048 |  0:00:12s\n",
      "epoch 62 | loss: 0.13417 | val_0_rmse: 0.31135 |  0:00:12s\n",
      "epoch 63 | loss: 0.0987  | val_0_rmse: 0.40887 |  0:00:12s\n",
      "epoch 64 | loss: 0.0862  | val_0_rmse: 0.31351 |  0:00:12s\n",
      "epoch 65 | loss: 0.09767 | val_0_rmse: 0.35598 |  0:00:13s\n",
      "epoch 66 | loss: 0.07133 | val_0_rmse: 0.29109 |  0:00:13s\n",
      "epoch 67 | loss: 0.07448 | val_0_rmse: 0.29781 |  0:00:13s\n",
      "epoch 68 | loss: 0.06525 | val_0_rmse: 0.35641 |  0:00:13s\n",
      "epoch 69 | loss: 0.07159 | val_0_rmse: 0.23685 |  0:00:13s\n",
      "epoch 70 | loss: 0.08264 | val_0_rmse: 0.323   |  0:00:13s\n",
      "epoch 71 | loss: 0.05171 | val_0_rmse: 0.42424 |  0:00:14s\n",
      "epoch 72 | loss: 0.09485 | val_0_rmse: 0.27061 |  0:00:14s\n",
      "epoch 73 | loss: 0.10552 | val_0_rmse: 0.29272 |  0:00:14s\n",
      "epoch 74 | loss: 0.06629 | val_0_rmse: 0.33464 |  0:00:14s\n",
      "epoch 75 | loss: 0.07354 | val_0_rmse: 0.37722 |  0:00:14s\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 50 and best_val_0_rmse = 0.2272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:13:30,377] Trial 19 finished with value: 0.22719747467690155 and parameters: {'n_d': 48, 'n_a': 48, 'n_steps': 4, 'gamma': 1.3223883462042147, 'n_independent': 3, 'n_shared': 2, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.016900223379241124, 'batch_size': 512, 'virtual_batch_size': 256}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 019 | rmse_log=0.22720 | RMSE$=45,561 | MAE$=29,896 | MAPE=16.95% | n_d/n_a=48/48 steps=4 lr=0.01690 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 120.36312| val_0_rmse: 11.54076|  0:00:00s\n",
      "epoch 1  | loss: 109.36808| val_0_rmse: 11.29788|  0:00:00s\n",
      "epoch 2  | loss: 100.25301| val_0_rmse: 10.99339|  0:00:00s\n",
      "epoch 3  | loss: 88.96401| val_0_rmse: 10.69206|  0:00:00s\n",
      "epoch 4  | loss: 82.86058| val_0_rmse: 10.37208|  0:00:00s\n",
      "epoch 5  | loss: 72.36551| val_0_rmse: 10.02212|  0:00:00s\n",
      "epoch 6  | loss: 64.35681| val_0_rmse: 9.6398  |  0:00:01s\n",
      "epoch 7  | loss: 55.45577| val_0_rmse: 9.23486 |  0:00:01s\n",
      "epoch 8  | loss: 51.36382| val_0_rmse: 8.81752 |  0:00:01s\n",
      "epoch 9  | loss: 41.45112| val_0_rmse: 8.36674 |  0:00:01s\n",
      "epoch 10 | loss: 36.44636| val_0_rmse: 7.89543 |  0:00:01s\n",
      "epoch 11 | loss: 26.04185| val_0_rmse: 7.36931 |  0:00:01s\n",
      "epoch 12 | loss: 25.32649| val_0_rmse: 6.86061 |  0:00:01s\n",
      "epoch 13 | loss: 20.65628| val_0_rmse: 6.33893 |  0:00:02s\n",
      "epoch 14 | loss: 18.97131| val_0_rmse: 5.74267 |  0:00:02s\n",
      "epoch 15 | loss: 12.28029| val_0_rmse: 5.02827 |  0:00:02s\n",
      "epoch 16 | loss: 9.76919 | val_0_rmse: 4.37576 |  0:00:02s\n",
      "epoch 17 | loss: 8.04984 | val_0_rmse: 3.76302 |  0:00:02s\n",
      "epoch 18 | loss: 6.1365  | val_0_rmse: 3.32202 |  0:00:02s\n",
      "epoch 19 | loss: 5.68001 | val_0_rmse: 3.10636 |  0:00:02s\n",
      "epoch 20 | loss: 4.43869 | val_0_rmse: 3.02732 |  0:00:03s\n",
      "epoch 21 | loss: 3.01684 | val_0_rmse: 2.82734 |  0:00:03s\n",
      "epoch 22 | loss: 3.00348 | val_0_rmse: 2.53247 |  0:00:03s\n",
      "epoch 23 | loss: 2.03133 | val_0_rmse: 2.13689 |  0:00:03s\n",
      "epoch 24 | loss: 1.70833 | val_0_rmse: 1.75789 |  0:00:03s\n",
      "epoch 25 | loss: 1.10829 | val_0_rmse: 1.58683 |  0:00:03s\n",
      "epoch 26 | loss: 0.98866 | val_0_rmse: 1.46747 |  0:00:03s\n",
      "epoch 27 | loss: 0.7653  | val_0_rmse: 1.2063  |  0:00:04s\n",
      "epoch 28 | loss: 0.55075 | val_0_rmse: 1.07185 |  0:00:04s\n",
      "epoch 29 | loss: 0.67214 | val_0_rmse: 0.90631 |  0:00:04s\n",
      "epoch 30 | loss: 0.57873 | val_0_rmse: 0.73762 |  0:00:04s\n",
      "epoch 31 | loss: 0.43022 | val_0_rmse: 0.67753 |  0:00:04s\n",
      "epoch 32 | loss: 0.54084 | val_0_rmse: 0.68048 |  0:00:04s\n",
      "epoch 33 | loss: 0.47008 | val_0_rmse: 0.6364  |  0:00:04s\n",
      "epoch 34 | loss: 0.43799 | val_0_rmse: 0.51678 |  0:00:04s\n",
      "epoch 35 | loss: 0.37617 | val_0_rmse: 0.64867 |  0:00:05s\n",
      "epoch 36 | loss: 0.35199 | val_0_rmse: 0.44567 |  0:00:05s\n",
      "epoch 37 | loss: 0.27075 | val_0_rmse: 0.50214 |  0:00:05s\n",
      "epoch 38 | loss: 0.28788 | val_0_rmse: 0.41433 |  0:00:05s\n",
      "epoch 39 | loss: 0.28801 | val_0_rmse: 0.39993 |  0:00:05s\n",
      "epoch 40 | loss: 0.28348 | val_0_rmse: 0.44838 |  0:00:05s\n",
      "epoch 41 | loss: 0.21651 | val_0_rmse: 0.36784 |  0:00:06s\n",
      "epoch 42 | loss: 0.24079 | val_0_rmse: 0.49018 |  0:00:06s\n",
      "epoch 43 | loss: 0.21991 | val_0_rmse: 0.44636 |  0:00:06s\n",
      "epoch 44 | loss: 0.20473 | val_0_rmse: 0.29046 |  0:00:06s\n",
      "epoch 45 | loss: 0.26231 | val_0_rmse: 0.32256 |  0:00:06s\n",
      "epoch 46 | loss: 0.25071 | val_0_rmse: 0.42943 |  0:00:06s\n",
      "epoch 47 | loss: 0.26055 | val_0_rmse: 0.38963 |  0:00:06s\n",
      "epoch 48 | loss: 0.20108 | val_0_rmse: 0.32092 |  0:00:06s\n",
      "epoch 49 | loss: 0.26641 | val_0_rmse: 0.26427 |  0:00:07s\n",
      "epoch 50 | loss: 0.23774 | val_0_rmse: 0.4215  |  0:00:07s\n",
      "epoch 51 | loss: 0.22136 | val_0_rmse: 0.43782 |  0:00:07s\n",
      "epoch 52 | loss: 0.21817 | val_0_rmse: 0.25234 |  0:00:07s\n",
      "epoch 53 | loss: 0.17114 | val_0_rmse: 0.2645  |  0:00:07s\n",
      "epoch 54 | loss: 0.14425 | val_0_rmse: 0.3088  |  0:00:07s\n",
      "epoch 55 | loss: 0.1253  | val_0_rmse: 0.30348 |  0:00:07s\n",
      "epoch 56 | loss: 0.12644 | val_0_rmse: 0.27033 |  0:00:07s\n",
      "epoch 57 | loss: 0.12471 | val_0_rmse: 0.28482 |  0:00:08s\n",
      "epoch 58 | loss: 0.14367 | val_0_rmse: 0.30381 |  0:00:08s\n",
      "epoch 59 | loss: 0.13189 | val_0_rmse: 0.2726  |  0:00:08s\n",
      "epoch 60 | loss: 0.13722 | val_0_rmse: 0.31171 |  0:00:08s\n",
      "epoch 61 | loss: 0.16114 | val_0_rmse: 0.32472 |  0:00:08s\n",
      "epoch 62 | loss: 0.1735  | val_0_rmse: 0.28985 |  0:00:08s\n",
      "epoch 63 | loss: 0.16518 | val_0_rmse: 0.27885 |  0:00:08s\n",
      "epoch 64 | loss: 0.1718  | val_0_rmse: 0.26797 |  0:00:09s\n",
      "epoch 65 | loss: 0.13417 | val_0_rmse: 0.30989 |  0:00:09s\n",
      "epoch 66 | loss: 0.17168 | val_0_rmse: 0.27416 |  0:00:09s\n",
      "epoch 67 | loss: 0.13207 | val_0_rmse: 0.28481 |  0:00:09s\n",
      "epoch 68 | loss: 0.12395 | val_0_rmse: 0.25403 |  0:00:09s\n",
      "epoch 69 | loss: 0.12795 | val_0_rmse: 0.33083 |  0:00:09s\n",
      "epoch 70 | loss: 0.12958 | val_0_rmse: 0.23728 |  0:00:09s\n",
      "epoch 71 | loss: 0.08679 | val_0_rmse: 0.26818 |  0:00:10s\n",
      "epoch 72 | loss: 0.1141  | val_0_rmse: 0.29279 |  0:00:10s\n",
      "epoch 73 | loss: 0.08923 | val_0_rmse: 0.24564 |  0:00:10s\n",
      "epoch 74 | loss: 0.08359 | val_0_rmse: 0.24054 |  0:00:10s\n",
      "epoch 75 | loss: 0.07585 | val_0_rmse: 0.2664  |  0:00:10s\n",
      "epoch 76 | loss: 0.06866 | val_0_rmse: 0.23019 |  0:00:10s\n",
      "epoch 77 | loss: 0.07689 | val_0_rmse: 0.24358 |  0:00:10s\n",
      "epoch 78 | loss: 0.08062 | val_0_rmse: 0.25274 |  0:00:10s\n",
      "epoch 79 | loss: 0.06263 | val_0_rmse: 0.21692 |  0:00:11s\n",
      "epoch 80 | loss: 0.09237 | val_0_rmse: 0.20748 |  0:00:11s\n",
      "epoch 81 | loss: 0.0927  | val_0_rmse: 0.2599  |  0:00:11s\n",
      "epoch 82 | loss: 0.09396 | val_0_rmse: 0.24363 |  0:00:11s\n",
      "epoch 83 | loss: 0.08408 | val_0_rmse: 0.23285 |  0:00:11s\n",
      "epoch 84 | loss: 0.10275 | val_0_rmse: 0.22834 |  0:00:11s\n",
      "epoch 85 | loss: 0.08625 | val_0_rmse: 0.23493 |  0:00:11s\n",
      "epoch 86 | loss: 0.06197 | val_0_rmse: 0.23332 |  0:00:11s\n",
      "epoch 87 | loss: 0.0664  | val_0_rmse: 0.21835 |  0:00:12s\n",
      "epoch 88 | loss: 0.07583 | val_0_rmse: 0.23225 |  0:00:12s\n",
      "epoch 89 | loss: 0.05957 | val_0_rmse: 0.22885 |  0:00:12s\n",
      "epoch 90 | loss: 0.06223 | val_0_rmse: 0.24212 |  0:00:12s\n",
      "epoch 91 | loss: 0.06273 | val_0_rmse: 0.23692 |  0:00:12s\n",
      "epoch 92 | loss: 0.07048 | val_0_rmse: 0.22277 |  0:00:12s\n",
      "epoch 93 | loss: 0.06439 | val_0_rmse: 0.25148 |  0:00:12s\n",
      "epoch 94 | loss: 0.05877 | val_0_rmse: 0.23877 |  0:00:13s\n",
      "epoch 95 | loss: 0.05243 | val_0_rmse: 0.22935 |  0:00:13s\n",
      "epoch 96 | loss: 0.07219 | val_0_rmse: 0.22681 |  0:00:13s\n",
      "epoch 97 | loss: 0.06703 | val_0_rmse: 0.27656 |  0:00:13s\n",
      "epoch 98 | loss: 0.06814 | val_0_rmse: 0.26021 |  0:00:13s\n",
      "epoch 99 | loss: 0.07814 | val_0_rmse: 0.24408 |  0:00:13s\n",
      "epoch 100| loss: 0.0528  | val_0_rmse: 0.27769 |  0:00:13s\n",
      "epoch 101| loss: 0.07719 | val_0_rmse: 0.28046 |  0:00:13s\n",
      "epoch 102| loss: 0.06923 | val_0_rmse: 0.26125 |  0:00:14s\n",
      "epoch 103| loss: 0.08384 | val_0_rmse: 0.26673 |  0:00:14s\n",
      "epoch 104| loss: 0.06394 | val_0_rmse: 0.28055 |  0:00:14s\n",
      "epoch 105| loss: 0.06203 | val_0_rmse: 0.2688  |  0:00:14s\n",
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 80 and best_val_0_rmse = 0.20748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:13:45,237] Trial 20 finished with value: 0.20747753480071662 and parameters: {'n_d': 24, 'n_a': 64, 'n_steps': 3, 'gamma': 1.4665809338740259, 'n_independent': 2, 'n_shared': 2, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.007334816794757803, 'batch_size': 1024, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 020 | rmse_log=0.20748 | RMSE$=43,121 | MAE$=26,959 | MAPE=16.14% | n_d/n_a=24/64 steps=3 lr=0.00733 batch=1024 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 120.96378| val_0_rmse: 9.45215 |  0:00:00s\n",
      "epoch 1  | loss: 64.38444| val_0_rmse: 7.62656 |  0:00:00s\n",
      "epoch 2  | loss: 35.44335| val_0_rmse: 5.89544 |  0:00:00s\n",
      "epoch 3  | loss: 26.43951| val_0_rmse: 4.67395 |  0:00:00s\n",
      "epoch 4  | loss: 23.09068| val_0_rmse: 4.41772 |  0:00:01s\n",
      "epoch 5  | loss: 13.05305| val_0_rmse: 4.71007 |  0:00:01s\n",
      "epoch 6  | loss: 6.84637 | val_0_rmse: 4.85706 |  0:00:01s\n",
      "epoch 7  | loss: 3.70834 | val_0_rmse: 3.61449 |  0:00:01s\n",
      "epoch 8  | loss: 1.83455 | val_0_rmse: 3.34957 |  0:00:01s\n",
      "epoch 9  | loss: 1.57875 | val_0_rmse: 2.90485 |  0:00:01s\n",
      "epoch 10 | loss: 1.20462 | val_0_rmse: 2.49128 |  0:00:02s\n",
      "epoch 11 | loss: 1.03726 | val_0_rmse: 2.31056 |  0:00:02s\n",
      "epoch 12 | loss: 1.14037 | val_0_rmse: 1.98697 |  0:00:02s\n",
      "epoch 13 | loss: 0.76154 | val_0_rmse: 1.92798 |  0:00:02s\n",
      "epoch 14 | loss: 0.71101 | val_0_rmse: 1.95664 |  0:00:02s\n",
      "epoch 15 | loss: 0.49109 | val_0_rmse: 1.63945 |  0:00:03s\n",
      "epoch 16 | loss: 0.50158 | val_0_rmse: 1.25458 |  0:00:03s\n",
      "epoch 17 | loss: 0.37432 | val_0_rmse: 1.69096 |  0:00:03s\n",
      "epoch 18 | loss: 0.54211 | val_0_rmse: 0.76381 |  0:00:03s\n",
      "epoch 19 | loss: 0.4827  | val_0_rmse: 1.25649 |  0:00:03s\n",
      "epoch 20 | loss: 0.27729 | val_0_rmse: 0.78556 |  0:00:04s\n",
      "epoch 21 | loss: 0.36403 | val_0_rmse: 1.0032  |  0:00:04s\n",
      "epoch 22 | loss: 0.34173 | val_0_rmse: 1.08998 |  0:00:04s\n",
      "epoch 23 | loss: 0.21364 | val_0_rmse: 0.94598 |  0:00:04s\n",
      "epoch 24 | loss: 0.19379 | val_0_rmse: 0.89352 |  0:00:04s\n",
      "epoch 25 | loss: 0.18782 | val_0_rmse: 0.85482 |  0:00:04s\n",
      "epoch 26 | loss: 0.38238 | val_0_rmse: 1.05534 |  0:00:05s\n",
      "epoch 27 | loss: 0.31535 | val_0_rmse: 0.67815 |  0:00:05s\n",
      "epoch 28 | loss: 0.30037 | val_0_rmse: 1.006   |  0:00:05s\n",
      "epoch 29 | loss: 0.33778 | val_0_rmse: 0.72324 |  0:00:05s\n",
      "epoch 30 | loss: 0.23261 | val_0_rmse: 0.59917 |  0:00:05s\n",
      "epoch 31 | loss: 0.27416 | val_0_rmse: 0.91378 |  0:00:06s\n",
      "epoch 32 | loss: 0.25663 | val_0_rmse: 0.6543  |  0:00:06s\n",
      "epoch 33 | loss: 0.14222 | val_0_rmse: 0.63092 |  0:00:06s\n",
      "epoch 34 | loss: 0.21408 | val_0_rmse: 0.36959 |  0:00:06s\n",
      "epoch 35 | loss: 0.15519 | val_0_rmse: 0.53336 |  0:00:06s\n",
      "epoch 36 | loss: 0.12683 | val_0_rmse: 0.39058 |  0:00:06s\n",
      "epoch 37 | loss: 0.12745 | val_0_rmse: 0.55718 |  0:00:07s\n",
      "epoch 38 | loss: 0.12696 | val_0_rmse: 0.32953 |  0:00:07s\n",
      "epoch 39 | loss: 0.10433 | val_0_rmse: 0.446   |  0:00:07s\n",
      "epoch 40 | loss: 0.1068  | val_0_rmse: 0.27128 |  0:00:07s\n",
      "epoch 41 | loss: 0.10025 | val_0_rmse: 0.39196 |  0:00:07s\n",
      "epoch 42 | loss: 0.10094 | val_0_rmse: 0.26018 |  0:00:08s\n",
      "epoch 43 | loss: 0.09647 | val_0_rmse: 0.35435 |  0:00:08s\n",
      "epoch 44 | loss: 0.08965 | val_0_rmse: 0.24788 |  0:00:08s\n",
      "epoch 45 | loss: 0.08265 | val_0_rmse: 0.29683 |  0:00:08s\n",
      "epoch 46 | loss: 0.08866 | val_0_rmse: 0.24686 |  0:00:08s\n",
      "epoch 47 | loss: 0.08931 | val_0_rmse: 0.28604 |  0:00:09s\n",
      "epoch 48 | loss: 0.08639 | val_0_rmse: 0.26934 |  0:00:09s\n",
      "epoch 49 | loss: 0.08916 | val_0_rmse: 0.24005 |  0:00:09s\n",
      "epoch 50 | loss: 0.05198 | val_0_rmse: 0.23808 |  0:00:09s\n",
      "epoch 51 | loss: 0.05451 | val_0_rmse: 0.24712 |  0:00:09s\n",
      "epoch 52 | loss: 0.05651 | val_0_rmse: 0.24395 |  0:00:09s\n",
      "epoch 53 | loss: 0.06419 | val_0_rmse: 0.2957  |  0:00:10s\n",
      "epoch 54 | loss: 0.10856 | val_0_rmse: 0.28467 |  0:00:10s\n",
      "epoch 55 | loss: 0.12949 | val_0_rmse: 0.32483 |  0:00:10s\n",
      "epoch 56 | loss: 0.13179 | val_0_rmse: 0.22522 |  0:00:10s\n",
      "epoch 57 | loss: 0.07088 | val_0_rmse: 0.34586 |  0:00:10s\n",
      "epoch 58 | loss: 0.12766 | val_0_rmse: 0.27541 |  0:00:11s\n",
      "epoch 59 | loss: 0.15847 | val_0_rmse: 0.25663 |  0:00:11s\n",
      "epoch 60 | loss: 0.07172 | val_0_rmse: 0.23277 |  0:00:11s\n",
      "epoch 61 | loss: 0.06475 | val_0_rmse: 0.2631  |  0:00:11s\n",
      "epoch 62 | loss: 0.07084 | val_0_rmse: 0.25351 |  0:00:11s\n",
      "epoch 63 | loss: 0.07507 | val_0_rmse: 0.23347 |  0:00:12s\n",
      "epoch 64 | loss: 0.05849 | val_0_rmse: 0.33119 |  0:00:12s\n",
      "epoch 65 | loss: 0.07924 | val_0_rmse: 0.27408 |  0:00:12s\n",
      "epoch 66 | loss: 0.09069 | val_0_rmse: 0.37645 |  0:00:12s\n",
      "epoch 67 | loss: 0.1291  | val_0_rmse: 0.28436 |  0:00:12s\n",
      "epoch 68 | loss: 0.10589 | val_0_rmse: 0.39589 |  0:00:12s\n",
      "epoch 69 | loss: 0.17615 | val_0_rmse: 0.25093 |  0:00:13s\n",
      "epoch 70 | loss: 0.11821 | val_0_rmse: 0.26577 |  0:00:13s\n",
      "epoch 71 | loss: 0.09345 | val_0_rmse: 0.26936 |  0:00:13s\n",
      "epoch 72 | loss: 0.06827 | val_0_rmse: 0.25282 |  0:00:13s\n",
      "epoch 73 | loss: 0.09213 | val_0_rmse: 0.28592 |  0:00:13s\n",
      "epoch 74 | loss: 0.07484 | val_0_rmse: 0.2292  |  0:00:14s\n",
      "epoch 75 | loss: 0.0937  | val_0_rmse: 0.31443 |  0:00:14s\n",
      "epoch 76 | loss: 0.07886 | val_0_rmse: 0.22289 |  0:00:14s\n",
      "epoch 77 | loss: 0.07143 | val_0_rmse: 0.29352 |  0:00:14s\n",
      "epoch 78 | loss: 0.06642 | val_0_rmse: 0.23062 |  0:00:14s\n",
      "epoch 79 | loss: 0.06415 | val_0_rmse: 0.26116 |  0:00:14s\n",
      "epoch 80 | loss: 0.07352 | val_0_rmse: 0.26011 |  0:00:15s\n",
      "epoch 81 | loss: 0.06041 | val_0_rmse: 0.24078 |  0:00:15s\n",
      "epoch 82 | loss: 0.06963 | val_0_rmse: 0.26357 |  0:00:15s\n",
      "epoch 83 | loss: 0.06482 | val_0_rmse: 0.24209 |  0:00:15s\n",
      "epoch 84 | loss: 0.05807 | val_0_rmse: 0.25575 |  0:00:15s\n",
      "epoch 85 | loss: 0.06349 | val_0_rmse: 0.23673 |  0:00:16s\n",
      "epoch 86 | loss: 0.05895 | val_0_rmse: 0.24868 |  0:00:16s\n",
      "epoch 87 | loss: 0.0648  | val_0_rmse: 0.2437  |  0:00:16s\n",
      "epoch 88 | loss: 0.05952 | val_0_rmse: 0.24178 |  0:00:16s\n",
      "epoch 89 | loss: 0.06419 | val_0_rmse: 0.24099 |  0:00:16s\n",
      "epoch 90 | loss: 0.04641 | val_0_rmse: 0.24185 |  0:00:17s\n",
      "epoch 91 | loss: 0.0581  | val_0_rmse: 0.24434 |  0:00:17s\n",
      "epoch 92 | loss: 0.0553  | val_0_rmse: 0.23815 |  0:00:17s\n",
      "epoch 93 | loss: 0.06665 | val_0_rmse: 0.24769 |  0:00:17s\n",
      "epoch 94 | loss: 0.05581 | val_0_rmse: 0.23838 |  0:00:17s\n",
      "epoch 95 | loss: 0.06857 | val_0_rmse: 0.24912 |  0:00:17s\n",
      "epoch 96 | loss: 0.05208 | val_0_rmse: 0.22195 |  0:00:18s\n",
      "epoch 97 | loss: 0.05145 | val_0_rmse: 0.22907 |  0:00:18s\n",
      "epoch 98 | loss: 0.05817 | val_0_rmse: 0.27119 |  0:00:18s\n",
      "epoch 99 | loss: 0.05499 | val_0_rmse: 0.21483 |  0:00:18s\n",
      "epoch 100| loss: 0.05762 | val_0_rmse: 0.21849 |  0:00:18s\n",
      "epoch 101| loss: 0.08895 | val_0_rmse: 0.34691 |  0:00:19s\n",
      "epoch 102| loss: 0.09678 | val_0_rmse: 0.46786 |  0:00:19s\n",
      "epoch 103| loss: 0.29013 | val_0_rmse: 0.22872 |  0:00:19s\n",
      "epoch 104| loss: 0.05598 | val_0_rmse: 0.2411  |  0:00:19s\n",
      "epoch 105| loss: 0.04102 | val_0_rmse: 0.22432 |  0:00:19s\n",
      "epoch 106| loss: 0.0535  | val_0_rmse: 0.21424 |  0:00:19s\n",
      "epoch 107| loss: 0.05335 | val_0_rmse: 0.24405 |  0:00:20s\n",
      "epoch 108| loss: 0.08523 | val_0_rmse: 0.34822 |  0:00:20s\n",
      "epoch 109| loss: 0.09995 | val_0_rmse: 0.43525 |  0:00:20s\n",
      "epoch 110| loss: 0.20985 | val_0_rmse: 0.28028 |  0:00:20s\n",
      "epoch 111| loss: 0.11231 | val_0_rmse: 0.21797 |  0:00:20s\n",
      "epoch 112| loss: 0.09316 | val_0_rmse: 0.41688 |  0:00:21s\n",
      "epoch 113| loss: 0.12189 | val_0_rmse: 0.32197 |  0:00:21s\n",
      "epoch 114| loss: 0.11752 | val_0_rmse: 0.33863 |  0:00:21s\n",
      "epoch 115| loss: 0.17643 | val_0_rmse: 0.28477 |  0:00:21s\n",
      "epoch 116| loss: 0.08241 | val_0_rmse: 0.30675 |  0:00:21s\n",
      "epoch 117| loss: 0.0863  | val_0_rmse: 0.43423 |  0:00:21s\n",
      "epoch 118| loss: 0.18078 | val_0_rmse: 0.25139 |  0:00:22s\n",
      "epoch 119| loss: 0.10663 | val_0_rmse: 0.2167  |  0:00:22s\n",
      "epoch 120| loss: 0.09627 | val_0_rmse: 0.4137  |  0:00:22s\n",
      "epoch 121| loss: 0.12884 | val_0_rmse: 0.3084  |  0:00:22s\n",
      "epoch 122| loss: 0.10245 | val_0_rmse: 0.35107 |  0:00:22s\n",
      "epoch 123| loss: 0.19165 | val_0_rmse: 0.30684 |  0:00:23s\n",
      "epoch 124| loss: 0.07545 | val_0_rmse: 0.28614 |  0:00:23s\n",
      "epoch 125| loss: 0.06345 | val_0_rmse: 0.36116 |  0:00:23s\n",
      "epoch 126| loss: 0.11041 | val_0_rmse: 0.3259  |  0:00:23s\n",
      "epoch 127| loss: 0.18985 | val_0_rmse: 0.25869 |  0:00:23s\n",
      "epoch 128| loss: 0.08907 | val_0_rmse: 0.37126 |  0:00:24s\n",
      "epoch 129| loss: 0.10436 | val_0_rmse: 0.33261 |  0:00:24s\n",
      "epoch 130| loss: 0.10198 | val_0_rmse: 0.29208 |  0:00:24s\n",
      "epoch 131| loss: 0.11607 | val_0_rmse: 0.21743 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 131 with best_epoch = 106 and best_val_0_rmse = 0.21424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:14:10,183] Trial 21 finished with value: 0.21423535766251217 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2161409907476204, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.019564836720493545, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 021 | rmse_log=0.21424 | RMSE$=45,271 | MAE$=26,141 | MAPE=15.30% | n_d/n_a=48/24 steps=3 lr=0.01956 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 125.71525| val_0_rmse: 10.37806|  0:00:00s\n",
      "epoch 1  | loss: 73.11718| val_0_rmse: 8.95612 |  0:00:00s\n",
      "epoch 2  | loss: 47.27103| val_0_rmse: 7.44247 |  0:00:00s\n",
      "epoch 3  | loss: 32.80534| val_0_rmse: 5.8955  |  0:00:00s\n",
      "epoch 4  | loss: 28.24853| val_0_rmse: 4.54045 |  0:00:01s\n",
      "epoch 5  | loss: 24.11241| val_0_rmse: 3.89752 |  0:00:01s\n",
      "epoch 6  | loss: 17.91254| val_0_rmse: 3.98195 |  0:00:01s\n",
      "epoch 7  | loss: 10.40707| val_0_rmse: 4.20882 |  0:00:01s\n",
      "epoch 8  | loss: 6.66921 | val_0_rmse: 3.7367  |  0:00:02s\n",
      "epoch 9  | loss: 4.61781 | val_0_rmse: 2.71657 |  0:00:02s\n",
      "epoch 10 | loss: 3.78222 | val_0_rmse: 2.17025 |  0:00:02s\n",
      "epoch 11 | loss: 2.21966 | val_0_rmse: 2.35049 |  0:00:02s\n",
      "epoch 12 | loss: 2.01265 | val_0_rmse: 1.49211 |  0:00:03s\n",
      "epoch 13 | loss: 1.7257  | val_0_rmse: 1.94177 |  0:00:03s\n",
      "epoch 14 | loss: 1.17614 | val_0_rmse: 1.27667 |  0:00:03s\n",
      "epoch 15 | loss: 1.26579 | val_0_rmse: 1.41682 |  0:00:03s\n",
      "epoch 16 | loss: 0.73461 | val_0_rmse: 1.40857 |  0:00:04s\n",
      "epoch 17 | loss: 0.71767 | val_0_rmse: 1.17679 |  0:00:04s\n",
      "epoch 18 | loss: 0.5497  | val_0_rmse: 1.08012 |  0:00:04s\n",
      "epoch 19 | loss: 0.61911 | val_0_rmse: 1.32795 |  0:00:04s\n",
      "epoch 20 | loss: 0.66148 | val_0_rmse: 0.51575 |  0:00:05s\n",
      "epoch 21 | loss: 0.74864 | val_0_rmse: 1.21204 |  0:00:05s\n",
      "epoch 22 | loss: 0.61221 | val_0_rmse: 0.4382  |  0:00:05s\n",
      "epoch 23 | loss: 0.86651 | val_0_rmse: 0.88807 |  0:00:05s\n",
      "epoch 24 | loss: 0.62508 | val_0_rmse: 0.5951  |  0:00:05s\n",
      "epoch 25 | loss: 0.4381  | val_0_rmse: 0.82068 |  0:00:06s\n",
      "epoch 26 | loss: 0.41782 | val_0_rmse: 0.49761 |  0:00:06s\n",
      "epoch 27 | loss: 0.36003 | val_0_rmse: 0.75288 |  0:00:06s\n",
      "epoch 28 | loss: 0.31464 | val_0_rmse: 0.52138 |  0:00:06s\n",
      "epoch 29 | loss: 0.59506 | val_0_rmse: 0.91239 |  0:00:06s\n",
      "epoch 30 | loss: 0.4615  | val_0_rmse: 0.42368 |  0:00:07s\n",
      "epoch 31 | loss: 0.38607 | val_0_rmse: 0.83177 |  0:00:07s\n",
      "epoch 32 | loss: 0.31187 | val_0_rmse: 0.52076 |  0:00:07s\n",
      "epoch 33 | loss: 0.26242 | val_0_rmse: 0.86475 |  0:00:07s\n",
      "epoch 34 | loss: 0.28677 | val_0_rmse: 0.36264 |  0:00:08s\n",
      "epoch 35 | loss: 0.27564 | val_0_rmse: 0.8428  |  0:00:08s\n",
      "epoch 36 | loss: 0.35888 | val_0_rmse: 0.43868 |  0:00:08s\n",
      "epoch 37 | loss: 0.36793 | val_0_rmse: 0.72961 |  0:00:08s\n",
      "epoch 38 | loss: 0.29746 | val_0_rmse: 0.5218  |  0:00:09s\n",
      "epoch 39 | loss: 0.17765 | val_0_rmse: 0.66872 |  0:00:09s\n",
      "epoch 40 | loss: 0.16578 | val_0_rmse: 0.59311 |  0:00:09s\n",
      "epoch 41 | loss: 0.24512 | val_0_rmse: 0.60934 |  0:00:09s\n",
      "epoch 42 | loss: 0.23723 | val_0_rmse: 0.47896 |  0:00:10s\n",
      "epoch 43 | loss: 0.26699 | val_0_rmse: 0.8313  |  0:00:10s\n",
      "epoch 44 | loss: 0.28439 | val_0_rmse: 0.28547 |  0:00:10s\n",
      "epoch 45 | loss: 0.31652 | val_0_rmse: 0.72858 |  0:00:10s\n",
      "epoch 46 | loss: 0.37282 | val_0_rmse: 0.65415 |  0:00:10s\n",
      "epoch 47 | loss: 0.18952 | val_0_rmse: 0.28427 |  0:00:11s\n",
      "epoch 48 | loss: 0.20538 | val_0_rmse: 0.80501 |  0:00:11s\n",
      "epoch 49 | loss: 0.32966 | val_0_rmse: 0.27549 |  0:00:11s\n",
      "epoch 50 | loss: 0.25742 | val_0_rmse: 0.27128 |  0:00:11s\n",
      "epoch 51 | loss: 0.15835 | val_0_rmse: 0.60209 |  0:00:12s\n",
      "epoch 52 | loss: 0.1841  | val_0_rmse: 0.26398 |  0:00:12s\n",
      "epoch 53 | loss: 0.28402 | val_0_rmse: 0.44689 |  0:00:12s\n",
      "epoch 54 | loss: 0.19596 | val_0_rmse: 0.35551 |  0:00:12s\n",
      "epoch 55 | loss: 0.13278 | val_0_rmse: 0.24471 |  0:00:13s\n",
      "epoch 56 | loss: 0.11975 | val_0_rmse: 0.42728 |  0:00:13s\n",
      "epoch 57 | loss: 0.1136  | val_0_rmse: 0.27205 |  0:00:13s\n",
      "epoch 58 | loss: 0.19041 | val_0_rmse: 0.33624 |  0:00:13s\n",
      "epoch 59 | loss: 0.08851 | val_0_rmse: 0.24326 |  0:00:13s\n",
      "epoch 60 | loss: 0.11674 | val_0_rmse: 0.2689  |  0:00:14s\n",
      "epoch 61 | loss: 0.09717 | val_0_rmse: 0.31621 |  0:00:14s\n",
      "epoch 62 | loss: 0.11904 | val_0_rmse: 0.34303 |  0:00:14s\n",
      "epoch 63 | loss: 0.09429 | val_0_rmse: 0.24156 |  0:00:14s\n",
      "epoch 64 | loss: 0.14627 | val_0_rmse: 0.39959 |  0:00:15s\n",
      "epoch 65 | loss: 0.21789 | val_0_rmse: 0.30969 |  0:00:15s\n",
      "epoch 66 | loss: 0.14278 | val_0_rmse: 0.23891 |  0:00:15s\n",
      "epoch 67 | loss: 0.13864 | val_0_rmse: 0.46543 |  0:00:15s\n",
      "epoch 68 | loss: 0.11798 | val_0_rmse: 0.22642 |  0:00:16s\n",
      "epoch 69 | loss: 0.13074 | val_0_rmse: 0.48828 |  0:00:16s\n",
      "epoch 70 | loss: 0.16026 | val_0_rmse: 0.24596 |  0:00:16s\n",
      "epoch 71 | loss: 0.08154 | val_0_rmse: 0.32342 |  0:00:16s\n",
      "epoch 72 | loss: 0.0785  | val_0_rmse: 0.28142 |  0:00:17s\n",
      "epoch 73 | loss: 0.08517 | val_0_rmse: 0.2271  |  0:00:17s\n",
      "epoch 74 | loss: 0.06532 | val_0_rmse: 0.40777 |  0:00:17s\n",
      "epoch 75 | loss: 0.11387 | val_0_rmse: 0.25866 |  0:00:17s\n",
      "epoch 76 | loss: 0.07999 | val_0_rmse: 0.34115 |  0:00:17s\n",
      "epoch 77 | loss: 0.10799 | val_0_rmse: 0.28771 |  0:00:18s\n",
      "epoch 78 | loss: 0.0812  | val_0_rmse: 0.21753 |  0:00:18s\n",
      "epoch 79 | loss: 0.1042  | val_0_rmse: 0.45857 |  0:00:18s\n",
      "epoch 80 | loss: 0.17303 | val_0_rmse: 0.29224 |  0:00:18s\n",
      "epoch 81 | loss: 0.12612 | val_0_rmse: 0.20289 |  0:00:19s\n",
      "epoch 82 | loss: 0.08218 | val_0_rmse: 0.33505 |  0:00:19s\n",
      "epoch 83 | loss: 0.07758 | val_0_rmse: 0.20693 |  0:00:19s\n",
      "epoch 84 | loss: 0.08962 | val_0_rmse: 0.42769 |  0:00:19s\n",
      "epoch 85 | loss: 0.12014 | val_0_rmse: 0.2951  |  0:00:20s\n",
      "epoch 86 | loss: 0.28725 | val_0_rmse: 0.2792  |  0:00:20s\n",
      "epoch 87 | loss: 0.10906 | val_0_rmse: 0.41472 |  0:00:20s\n",
      "epoch 88 | loss: 0.11474 | val_0_rmse: 0.31182 |  0:00:20s\n",
      "epoch 89 | loss: 0.30189 | val_0_rmse: 0.29513 |  0:00:20s\n",
      "epoch 90 | loss: 0.11808 | val_0_rmse: 0.42603 |  0:00:21s\n",
      "epoch 91 | loss: 0.1431  | val_0_rmse: 0.27831 |  0:00:21s\n",
      "epoch 92 | loss: 0.21427 | val_0_rmse: 0.28385 |  0:00:21s\n",
      "epoch 93 | loss: 0.08834 | val_0_rmse: 0.23031 |  0:00:21s\n",
      "epoch 94 | loss: 0.04889 | val_0_rmse: 0.19528 |  0:00:22s\n",
      "epoch 95 | loss: 0.04471 | val_0_rmse: 0.21009 |  0:00:22s\n",
      "epoch 96 | loss: 0.06584 | val_0_rmse: 0.27003 |  0:00:22s\n",
      "epoch 97 | loss: 0.07366 | val_0_rmse: 0.19628 |  0:00:22s\n",
      "epoch 98 | loss: 0.04438 | val_0_rmse: 0.21479 |  0:00:22s\n",
      "epoch 99 | loss: 0.04988 | val_0_rmse: 0.19383 |  0:00:23s\n",
      "epoch 100| loss: 0.04368 | val_0_rmse: 0.18178 |  0:00:23s\n",
      "epoch 101| loss: 0.04497 | val_0_rmse: 0.19601 |  0:00:23s\n",
      "epoch 102| loss: 0.04938 | val_0_rmse: 0.23611 |  0:00:23s\n",
      "epoch 103| loss: 0.05554 | val_0_rmse: 0.21455 |  0:00:24s\n",
      "epoch 104| loss: 0.04934 | val_0_rmse: 0.2119  |  0:00:24s\n",
      "epoch 105| loss: 0.05792 | val_0_rmse: 0.21753 |  0:00:24s\n",
      "epoch 106| loss: 0.04474 | val_0_rmse: 0.24217 |  0:00:24s\n",
      "epoch 107| loss: 0.04759 | val_0_rmse: 0.23116 |  0:00:25s\n",
      "epoch 108| loss: 0.03814 | val_0_rmse: 0.20988 |  0:00:25s\n",
      "epoch 109| loss: 0.03484 | val_0_rmse: 0.2504  |  0:00:25s\n",
      "epoch 110| loss: 0.04712 | val_0_rmse: 0.19584 |  0:00:25s\n",
      "epoch 111| loss: 0.0328  | val_0_rmse: 0.23676 |  0:00:25s\n",
      "epoch 112| loss: 0.03518 | val_0_rmse: 0.18789 |  0:00:26s\n",
      "epoch 113| loss: 0.05825 | val_0_rmse: 0.23154 |  0:00:26s\n",
      "epoch 114| loss: 0.05813 | val_0_rmse: 0.21038 |  0:00:26s\n",
      "epoch 115| loss: 0.03944 | val_0_rmse: 0.19227 |  0:00:26s\n",
      "epoch 116| loss: 0.03625 | val_0_rmse: 0.25148 |  0:00:26s\n",
      "epoch 117| loss: 0.05091 | val_0_rmse: 0.19811 |  0:00:27s\n",
      "epoch 118| loss: 0.04472 | val_0_rmse: 0.20175 |  0:00:27s\n",
      "epoch 119| loss: 0.03046 | val_0_rmse: 0.20054 |  0:00:27s\n",
      "epoch 120| loss: 0.02601 | val_0_rmse: 0.18652 |  0:00:28s\n",
      "epoch 121| loss: 0.03716 | val_0_rmse: 0.19329 |  0:00:28s\n",
      "epoch 122| loss: 0.04875 | val_0_rmse: 0.18928 |  0:00:28s\n",
      "epoch 123| loss: 0.03433 | val_0_rmse: 0.25904 |  0:00:28s\n",
      "epoch 124| loss: 0.06196 | val_0_rmse: 0.23037 |  0:00:29s\n",
      "epoch 125| loss: 0.06319 | val_0_rmse: 0.26814 |  0:00:29s\n",
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 100 and best_val_0_rmse = 0.18178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:14:39,913] Trial 22 finished with value: 0.18178072620615593 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 4, 'gamma': 1.297605859412522, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.015646577983031294, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 022 | rmse_log=0.18178 | RMSE$=34,972 | MAE$=22,857 | MAPE=13.72% | n_d/n_a=48/24 steps=4 lr=0.01565 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 125.64241| val_0_rmse: 10.37676|  0:00:00s\n",
      "epoch 1  | loss: 75.64519| val_0_rmse: 8.82779 |  0:00:00s\n",
      "epoch 2  | loss: 43.33322| val_0_rmse: 7.32015 |  0:00:00s\n",
      "epoch 3  | loss: 30.62196| val_0_rmse: 5.84609 |  0:00:00s\n",
      "epoch 4  | loss: 23.23388| val_0_rmse: 4.69046 |  0:00:01s\n",
      "epoch 5  | loss: 20.41205| val_0_rmse: 4.13957 |  0:00:01s\n",
      "epoch 6  | loss: 14.25287| val_0_rmse: 4.30037 |  0:00:01s\n",
      "epoch 7  | loss: 8.87331 | val_0_rmse: 4.46074 |  0:00:01s\n",
      "epoch 8  | loss: 5.46926 | val_0_rmse: 3.43949 |  0:00:02s\n",
      "epoch 9  | loss: 2.90254 | val_0_rmse: 2.23217 |  0:00:02s\n",
      "epoch 10 | loss: 2.39979 | val_0_rmse: 2.1884  |  0:00:02s\n",
      "epoch 11 | loss: 2.23814 | val_0_rmse: 1.44352 |  0:00:02s\n",
      "epoch 12 | loss: 1.92702 | val_0_rmse: 1.79774 |  0:00:03s\n",
      "epoch 13 | loss: 1.61719 | val_0_rmse: 1.78419 |  0:00:03s\n",
      "epoch 14 | loss: 1.19578 | val_0_rmse: 0.88072 |  0:00:03s\n",
      "epoch 15 | loss: 1.13816 | val_0_rmse: 1.47652 |  0:00:03s\n",
      "epoch 16 | loss: 0.86543 | val_0_rmse: 0.83769 |  0:00:04s\n",
      "epoch 17 | loss: 0.86979 | val_0_rmse: 1.32974 |  0:00:04s\n",
      "epoch 18 | loss: 0.77339 | val_0_rmse: 0.77797 |  0:00:04s\n",
      "epoch 19 | loss: 0.81846 | val_0_rmse: 1.1963  |  0:00:04s\n",
      "epoch 20 | loss: 0.59505 | val_0_rmse: 0.71015 |  0:00:04s\n",
      "epoch 21 | loss: 0.56624 | val_0_rmse: 1.16967 |  0:00:05s\n",
      "epoch 22 | loss: 0.45303 | val_0_rmse: 0.84996 |  0:00:05s\n",
      "epoch 23 | loss: 0.46342 | val_0_rmse: 0.98917 |  0:00:05s\n",
      "epoch 24 | loss: 0.42614 | val_0_rmse: 0.77503 |  0:00:05s\n",
      "epoch 25 | loss: 0.31406 | val_0_rmse: 0.81756 |  0:00:05s\n",
      "epoch 26 | loss: 0.33147 | val_0_rmse: 0.90572 |  0:00:06s\n",
      "epoch 27 | loss: 0.36942 | val_0_rmse: 0.53509 |  0:00:06s\n",
      "epoch 28 | loss: 0.36945 | val_0_rmse: 0.79392 |  0:00:06s\n",
      "epoch 29 | loss: 0.29502 | val_0_rmse: 0.77469 |  0:00:06s\n",
      "epoch 30 | loss: 0.23356 | val_0_rmse: 0.50238 |  0:00:07s\n",
      "epoch 31 | loss: 0.29437 | val_0_rmse: 0.54314 |  0:00:07s\n",
      "epoch 32 | loss: 0.18766 | val_0_rmse: 0.64515 |  0:00:07s\n",
      "epoch 33 | loss: 0.19314 | val_0_rmse: 0.70721 |  0:00:07s\n",
      "epoch 34 | loss: 0.20967 | val_0_rmse: 0.4478  |  0:00:08s\n",
      "epoch 35 | loss: 0.20582 | val_0_rmse: 0.44778 |  0:00:08s\n",
      "epoch 36 | loss: 0.15903 | val_0_rmse: 0.60728 |  0:00:08s\n",
      "epoch 37 | loss: 0.20717 | val_0_rmse: 0.51338 |  0:00:08s\n",
      "epoch 38 | loss: 0.12822 | val_0_rmse: 0.34644 |  0:00:08s\n",
      "epoch 39 | loss: 0.13043 | val_0_rmse: 0.42612 |  0:00:09s\n",
      "epoch 40 | loss: 0.13516 | val_0_rmse: 0.47851 |  0:00:09s\n",
      "epoch 41 | loss: 0.14675 | val_0_rmse: 0.37773 |  0:00:09s\n",
      "epoch 42 | loss: 0.11224 | val_0_rmse: 0.43935 |  0:00:09s\n",
      "epoch 43 | loss: 0.12473 | val_0_rmse: 0.47549 |  0:00:10s\n",
      "epoch 44 | loss: 0.11111 | val_0_rmse: 0.37489 |  0:00:10s\n",
      "epoch 45 | loss: 0.12453 | val_0_rmse: 0.31789 |  0:00:10s\n",
      "epoch 46 | loss: 0.11413 | val_0_rmse: 0.44749 |  0:00:10s\n",
      "epoch 47 | loss: 0.10832 | val_0_rmse: 0.43964 |  0:00:11s\n",
      "epoch 48 | loss: 0.10506 | val_0_rmse: 0.29493 |  0:00:11s\n",
      "epoch 49 | loss: 0.10617 | val_0_rmse: 0.32801 |  0:00:11s\n",
      "epoch 50 | loss: 0.06997 | val_0_rmse: 0.23984 |  0:00:11s\n",
      "epoch 51 | loss: 0.11025 | val_0_rmse: 0.34677 |  0:00:12s\n",
      "epoch 52 | loss: 0.08078 | val_0_rmse: 0.34598 |  0:00:12s\n",
      "epoch 53 | loss: 0.087   | val_0_rmse: 0.3003  |  0:00:12s\n",
      "epoch 54 | loss: 0.15514 | val_0_rmse: 0.51216 |  0:00:12s\n",
      "epoch 55 | loss: 0.14938 | val_0_rmse: 0.48914 |  0:00:12s\n",
      "epoch 56 | loss: 0.45221 | val_0_rmse: 0.28059 |  0:00:13s\n",
      "epoch 57 | loss: 0.18688 | val_0_rmse: 0.66999 |  0:00:13s\n",
      "epoch 58 | loss: 0.31543 | val_0_rmse: 0.32618 |  0:00:13s\n",
      "epoch 59 | loss: 0.16761 | val_0_rmse: 0.40683 |  0:00:13s\n",
      "epoch 60 | loss: 0.1842  | val_0_rmse: 0.26516 |  0:00:13s\n",
      "epoch 61 | loss: 0.13835 | val_0_rmse: 0.30986 |  0:00:14s\n",
      "epoch 62 | loss: 0.12886 | val_0_rmse: 0.50835 |  0:00:14s\n",
      "epoch 63 | loss: 0.2516  | val_0_rmse: 0.40948 |  0:00:14s\n",
      "epoch 64 | loss: 0.28055 | val_0_rmse: 0.29063 |  0:00:14s\n",
      "epoch 65 | loss: 0.11534 | val_0_rmse: 0.33913 |  0:00:15s\n",
      "epoch 66 | loss: 0.09688 | val_0_rmse: 0.2587  |  0:00:15s\n",
      "epoch 67 | loss: 0.09896 | val_0_rmse: 0.19852 |  0:00:15s\n",
      "epoch 68 | loss: 0.05898 | val_0_rmse: 0.28345 |  0:00:15s\n",
      "epoch 69 | loss: 0.05971 | val_0_rmse: 0.33233 |  0:00:16s\n",
      "epoch 70 | loss: 0.09755 | val_0_rmse: 0.25596 |  0:00:16s\n",
      "epoch 71 | loss: 0.16391 | val_0_rmse: 0.28802 |  0:00:16s\n",
      "epoch 72 | loss: 0.14217 | val_0_rmse: 0.48472 |  0:00:16s\n",
      "epoch 73 | loss: 0.16246 | val_0_rmse: 0.34058 |  0:00:16s\n",
      "epoch 74 | loss: 0.15339 | val_0_rmse: 0.33011 |  0:00:17s\n",
      "epoch 75 | loss: 0.15036 | val_0_rmse: 0.27839 |  0:00:17s\n",
      "epoch 76 | loss: 0.13134 | val_0_rmse: 0.36235 |  0:00:17s\n",
      "epoch 77 | loss: 0.13208 | val_0_rmse: 0.37867 |  0:00:17s\n",
      "epoch 78 | loss: 0.12987 | val_0_rmse: 0.27533 |  0:00:17s\n",
      "epoch 79 | loss: 0.18267 | val_0_rmse: 0.24155 |  0:00:18s\n",
      "epoch 80 | loss: 0.09114 | val_0_rmse: 0.35416 |  0:00:18s\n",
      "epoch 81 | loss: 0.08695 | val_0_rmse: 0.3901  |  0:00:18s\n",
      "epoch 82 | loss: 0.17893 | val_0_rmse: 0.28379 |  0:00:18s\n",
      "epoch 83 | loss: 0.11878 | val_0_rmse: 0.24633 |  0:00:19s\n",
      "epoch 84 | loss: 0.09344 | val_0_rmse: 0.35361 |  0:00:19s\n",
      "epoch 85 | loss: 0.12065 | val_0_rmse: 0.38552 |  0:00:19s\n",
      "epoch 86 | loss: 0.13718 | val_0_rmse: 0.28282 |  0:00:19s\n",
      "epoch 87 | loss: 0.14411 | val_0_rmse: 0.24996 |  0:00:19s\n",
      "epoch 88 | loss: 0.09008 | val_0_rmse: 0.37774 |  0:00:20s\n",
      "epoch 89 | loss: 0.0951  | val_0_rmse: 0.36379 |  0:00:20s\n",
      "epoch 90 | loss: 0.13907 | val_0_rmse: 0.30665 |  0:00:20s\n",
      "epoch 91 | loss: 0.1404  | val_0_rmse: 0.25587 |  0:00:20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:15:01,346] Trial 23 finished with value: 0.19852362518193947 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 4, 'gamma': 1.2966301830591072, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.015737359259393617, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 92 | loss: 0.07966 | val_0_rmse: 0.3326  |  0:00:21s\n",
      "\n",
      "Early stopping occurred at epoch 92 with best_epoch = 67 and best_val_0_rmse = 0.19852\n",
      "Trial 023 | rmse_log=0.19852 | RMSE$=40,659 | MAE$=27,014 | MAPE=15.34% | n_d/n_a=48/24 steps=4 lr=0.01574 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 130.34336| val_0_rmse: 10.80642|  0:00:00s\n",
      "epoch 1  | loss: 88.84271| val_0_rmse: 9.89251 |  0:00:00s\n",
      "epoch 2  | loss: 59.4507 | val_0_rmse: 8.86297 |  0:00:00s\n",
      "epoch 3  | loss: 42.44775| val_0_rmse: 7.70491 |  0:00:00s\n",
      "epoch 4  | loss: 32.08885| val_0_rmse: 6.51811 |  0:00:01s\n",
      "epoch 5  | loss: 26.29765| val_0_rmse: 5.44849 |  0:00:01s\n",
      "epoch 6  | loss: 26.57097| val_0_rmse: 4.62268 |  0:00:01s\n",
      "epoch 7  | loss: 22.47376| val_0_rmse: 4.15896 |  0:00:01s\n",
      "epoch 8  | loss: 16.74764| val_0_rmse: 4.01396 |  0:00:02s\n",
      "epoch 9  | loss: 14.60613| val_0_rmse: 4.00619 |  0:00:02s\n",
      "epoch 10 | loss: 8.52704 | val_0_rmse: 3.86451 |  0:00:02s\n",
      "epoch 11 | loss: 7.43459 | val_0_rmse: 3.35774 |  0:00:02s\n",
      "epoch 12 | loss: 4.20885 | val_0_rmse: 2.40767 |  0:00:03s\n",
      "epoch 13 | loss: 3.00388 | val_0_rmse: 2.04581 |  0:00:03s\n",
      "epoch 14 | loss: 1.81205 | val_0_rmse: 2.11572 |  0:00:03s\n",
      "epoch 15 | loss: 1.63194 | val_0_rmse: 1.78847 |  0:00:03s\n",
      "epoch 16 | loss: 1.89387 | val_0_rmse: 1.68198 |  0:00:04s\n",
      "epoch 17 | loss: 1.24538 | val_0_rmse: 1.36983 |  0:00:04s\n",
      "epoch 18 | loss: 0.95178 | val_0_rmse: 1.28583 |  0:00:04s\n",
      "epoch 19 | loss: 0.87929 | val_0_rmse: 1.40359 |  0:00:04s\n",
      "epoch 20 | loss: 0.82298 | val_0_rmse: 1.12012 |  0:00:04s\n",
      "epoch 21 | loss: 0.74527 | val_0_rmse: 1.01362 |  0:00:05s\n",
      "epoch 22 | loss: 0.59787 | val_0_rmse: 0.76937 |  0:00:05s\n",
      "epoch 23 | loss: 0.63429 | val_0_rmse: 1.07494 |  0:00:05s\n",
      "epoch 24 | loss: 0.59286 | val_0_rmse: 0.84771 |  0:00:05s\n",
      "epoch 25 | loss: 0.50341 | val_0_rmse: 1.16216 |  0:00:06s\n",
      "epoch 26 | loss: 0.38332 | val_0_rmse: 1.05928 |  0:00:06s\n",
      "epoch 27 | loss: 0.49147 | val_0_rmse: 0.79269 |  0:00:06s\n",
      "epoch 28 | loss: 0.36927 | val_0_rmse: 0.97768 |  0:00:06s\n",
      "epoch 29 | loss: 0.38638 | val_0_rmse: 0.73299 |  0:00:07s\n",
      "epoch 30 | loss: 0.27656 | val_0_rmse: 0.65384 |  0:00:07s\n",
      "epoch 31 | loss: 0.26185 | val_0_rmse: 0.64799 |  0:00:07s\n",
      "epoch 32 | loss: 0.28623 | val_0_rmse: 0.75412 |  0:00:07s\n",
      "epoch 33 | loss: 0.36241 | val_0_rmse: 0.63312 |  0:00:07s\n",
      "epoch 34 | loss: 0.37045 | val_0_rmse: 0.77744 |  0:00:08s\n",
      "epoch 35 | loss: 0.31182 | val_0_rmse: 0.42969 |  0:00:08s\n",
      "epoch 36 | loss: 0.30745 | val_0_rmse: 0.56401 |  0:00:08s\n",
      "epoch 37 | loss: 0.2742  | val_0_rmse: 0.48045 |  0:00:08s\n",
      "epoch 38 | loss: 0.30844 | val_0_rmse: 0.64921 |  0:00:09s\n",
      "epoch 39 | loss: 0.22001 | val_0_rmse: 0.36841 |  0:00:09s\n",
      "epoch 40 | loss: 0.22168 | val_0_rmse: 0.47434 |  0:00:09s\n",
      "epoch 41 | loss: 0.20278 | val_0_rmse: 0.47716 |  0:00:09s\n",
      "epoch 42 | loss: 0.19439 | val_0_rmse: 0.35645 |  0:00:09s\n",
      "epoch 43 | loss: 0.21016 | val_0_rmse: 0.5081  |  0:00:10s\n",
      "epoch 44 | loss: 0.17482 | val_0_rmse: 0.43637 |  0:00:10s\n",
      "epoch 45 | loss: 0.18959 | val_0_rmse: 0.34245 |  0:00:10s\n",
      "epoch 46 | loss: 0.17566 | val_0_rmse: 0.31659 |  0:00:10s\n",
      "epoch 47 | loss: 0.13091 | val_0_rmse: 0.4862  |  0:00:11s\n",
      "epoch 48 | loss: 0.14579 | val_0_rmse: 0.28886 |  0:00:11s\n",
      "epoch 49 | loss: 0.17206 | val_0_rmse: 0.40516 |  0:00:11s\n",
      "epoch 50 | loss: 0.18851 | val_0_rmse: 0.29998 |  0:00:11s\n",
      "epoch 51 | loss: 0.27317 | val_0_rmse: 0.43365 |  0:00:11s\n",
      "epoch 52 | loss: 0.13835 | val_0_rmse: 0.4118  |  0:00:12s\n",
      "epoch 53 | loss: 0.14911 | val_0_rmse: 0.3694  |  0:00:12s\n",
      "epoch 54 | loss: 0.15742 | val_0_rmse: 0.5349  |  0:00:12s\n",
      "epoch 55 | loss: 0.16224 | val_0_rmse: 0.3792  |  0:00:12s\n",
      "epoch 56 | loss: 0.33901 | val_0_rmse: 0.41711 |  0:00:13s\n",
      "epoch 57 | loss: 0.16615 | val_0_rmse: 0.39156 |  0:00:13s\n",
      "epoch 58 | loss: 0.15185 | val_0_rmse: 0.27426 |  0:00:13s\n",
      "epoch 59 | loss: 0.12616 | val_0_rmse: 0.6323  |  0:00:13s\n",
      "epoch 60 | loss: 0.32849 | val_0_rmse: 0.30034 |  0:00:14s\n",
      "epoch 61 | loss: 0.14931 | val_0_rmse: 0.27777 |  0:00:14s\n",
      "epoch 62 | loss: 0.12581 | val_0_rmse: 0.37954 |  0:00:14s\n",
      "epoch 63 | loss: 0.11526 | val_0_rmse: 0.26831 |  0:00:14s\n",
      "epoch 64 | loss: 0.09156 | val_0_rmse: 0.32242 |  0:00:15s\n",
      "epoch 65 | loss: 0.1212  | val_0_rmse: 0.28481 |  0:00:15s\n",
      "epoch 66 | loss: 0.09225 | val_0_rmse: 0.28822 |  0:00:15s\n",
      "epoch 67 | loss: 0.08528 | val_0_rmse: 0.32056 |  0:00:15s\n",
      "epoch 68 | loss: 0.11627 | val_0_rmse: 0.29838 |  0:00:15s\n",
      "epoch 69 | loss: 0.14925 | val_0_rmse: 0.40292 |  0:00:16s\n",
      "epoch 70 | loss: 0.20611 | val_0_rmse: 0.28446 |  0:00:16s\n",
      "epoch 71 | loss: 0.12822 | val_0_rmse: 0.34948 |  0:00:16s\n",
      "epoch 72 | loss: 0.09354 | val_0_rmse: 0.32003 |  0:00:16s\n",
      "epoch 73 | loss: 0.09192 | val_0_rmse: 0.25307 |  0:00:16s\n",
      "epoch 74 | loss: 0.11597 | val_0_rmse: 0.31596 |  0:00:17s\n",
      "epoch 75 | loss: 0.09709 | val_0_rmse: 0.32861 |  0:00:17s\n",
      "epoch 76 | loss: 0.15161 | val_0_rmse: 0.359   |  0:00:17s\n",
      "epoch 77 | loss: 0.11143 | val_0_rmse: 0.30178 |  0:00:17s\n",
      "epoch 78 | loss: 0.09559 | val_0_rmse: 0.2522  |  0:00:18s\n",
      "epoch 79 | loss: 0.09633 | val_0_rmse: 0.49762 |  0:00:18s\n",
      "epoch 80 | loss: 0.16253 | val_0_rmse: 0.30303 |  0:00:18s\n",
      "epoch 81 | loss: 0.1071  | val_0_rmse: 0.31292 |  0:00:18s\n",
      "epoch 82 | loss: 0.07671 | val_0_rmse: 0.3176  |  0:00:18s\n",
      "epoch 83 | loss: 0.07623 | val_0_rmse: 0.26977 |  0:00:19s\n",
      "epoch 84 | loss: 0.07725 | val_0_rmse: 0.29728 |  0:00:19s\n",
      "epoch 85 | loss: 0.07054 | val_0_rmse: 0.2533  |  0:00:19s\n",
      "epoch 86 | loss: 0.06179 | val_0_rmse: 0.25173 |  0:00:19s\n",
      "epoch 87 | loss: 0.06752 | val_0_rmse: 0.26177 |  0:00:20s\n",
      "epoch 88 | loss: 0.08223 | val_0_rmse: 0.29043 |  0:00:20s\n",
      "epoch 89 | loss: 0.06519 | val_0_rmse: 0.25569 |  0:00:20s\n",
      "epoch 90 | loss: 0.07064 | val_0_rmse: 0.3243  |  0:00:20s\n",
      "epoch 91 | loss: 0.09136 | val_0_rmse: 0.3101  |  0:00:21s\n",
      "epoch 92 | loss: 0.10694 | val_0_rmse: 0.26535 |  0:00:21s\n",
      "epoch 93 | loss: 0.07077 | val_0_rmse: 0.25091 |  0:00:21s\n",
      "epoch 94 | loss: 0.06533 | val_0_rmse: 0.24612 |  0:00:21s\n",
      "epoch 95 | loss: 0.07271 | val_0_rmse: 0.24933 |  0:00:22s\n",
      "epoch 96 | loss: 0.06888 | val_0_rmse: 0.29388 |  0:00:22s\n",
      "epoch 97 | loss: 0.07425 | val_0_rmse: 0.24953 |  0:00:22s\n",
      "epoch 98 | loss: 0.05808 | val_0_rmse: 0.24985 |  0:00:22s\n",
      "epoch 99 | loss: 0.05533 | val_0_rmse: 0.28216 |  0:00:23s\n",
      "epoch 100| loss: 0.05389 | val_0_rmse: 0.24381 |  0:00:23s\n",
      "epoch 101| loss: 0.08778 | val_0_rmse: 0.27942 |  0:00:23s\n",
      "epoch 102| loss: 0.06989 | val_0_rmse: 0.27406 |  0:00:24s\n",
      "epoch 103| loss: 0.0624  | val_0_rmse: 0.26861 |  0:00:24s\n",
      "epoch 104| loss: 0.06427 | val_0_rmse: 0.27966 |  0:00:24s\n",
      "epoch 105| loss: 0.06291 | val_0_rmse: 0.36994 |  0:00:24s\n",
      "epoch 106| loss: 0.07487 | val_0_rmse: 0.27751 |  0:00:24s\n",
      "epoch 107| loss: 0.06093 | val_0_rmse: 0.31072 |  0:00:25s\n",
      "epoch 108| loss: 0.05873 | val_0_rmse: 0.26563 |  0:00:25s\n",
      "epoch 109| loss: 0.06786 | val_0_rmse: 0.3535  |  0:00:25s\n",
      "epoch 110| loss: 0.07238 | val_0_rmse: 0.26977 |  0:00:25s\n",
      "epoch 111| loss: 0.10368 | val_0_rmse: 0.32441 |  0:00:25s\n",
      "epoch 112| loss: 0.07108 | val_0_rmse: 0.23771 |  0:00:26s\n",
      "epoch 113| loss: 0.06277 | val_0_rmse: 0.24459 |  0:00:26s\n",
      "epoch 114| loss: 0.04677 | val_0_rmse: 0.25044 |  0:00:26s\n",
      "epoch 115| loss: 0.05032 | val_0_rmse: 0.26965 |  0:00:26s\n",
      "epoch 116| loss: 0.04703 | val_0_rmse: 0.24169 |  0:00:27s\n",
      "epoch 117| loss: 0.05229 | val_0_rmse: 0.26149 |  0:00:27s\n",
      "epoch 118| loss: 0.07219 | val_0_rmse: 0.26358 |  0:00:27s\n",
      "epoch 119| loss: 0.071   | val_0_rmse: 0.24687 |  0:00:27s\n",
      "epoch 120| loss: 0.05107 | val_0_rmse: 0.24648 |  0:00:28s\n",
      "epoch 121| loss: 0.0726  | val_0_rmse: 0.26478 |  0:00:28s\n",
      "epoch 122| loss: 0.04897 | val_0_rmse: 0.23969 |  0:00:28s\n",
      "epoch 123| loss: 0.06424 | val_0_rmse: 0.27452 |  0:00:28s\n",
      "epoch 124| loss: 0.05426 | val_0_rmse: 0.25157 |  0:00:29s\n",
      "epoch 125| loss: 0.061   | val_0_rmse: 0.26492 |  0:00:29s\n",
      "epoch 126| loss: 0.0538  | val_0_rmse: 0.25105 |  0:00:29s\n",
      "epoch 127| loss: 0.05786 | val_0_rmse: 0.2524  |  0:00:29s\n",
      "epoch 128| loss: 0.05298 | val_0_rmse: 0.26367 |  0:00:29s\n",
      "epoch 129| loss: 0.04799 | val_0_rmse: 0.24507 |  0:00:30s\n",
      "epoch 130| loss: 0.05006 | val_0_rmse: 0.26799 |  0:00:30s\n",
      "epoch 131| loss: 0.04161 | val_0_rmse: 0.23838 |  0:00:30s\n",
      "epoch 132| loss: 0.048   | val_0_rmse: 0.25716 |  0:00:30s\n",
      "epoch 133| loss: 0.04823 | val_0_rmse: 0.23121 |  0:00:30s\n",
      "epoch 134| loss: 0.04684 | val_0_rmse: 0.25427 |  0:00:31s\n",
      "epoch 135| loss: 0.05018 | val_0_rmse: 0.2325  |  0:00:31s\n",
      "epoch 136| loss: 0.04775 | val_0_rmse: 0.25391 |  0:00:31s\n",
      "epoch 137| loss: 0.04038 | val_0_rmse: 0.22829 |  0:00:31s\n",
      "epoch 138| loss: 0.04879 | val_0_rmse: 0.25215 |  0:00:32s\n",
      "epoch 139| loss: 0.04645 | val_0_rmse: 0.25602 |  0:00:32s\n",
      "epoch 140| loss: 0.04742 | val_0_rmse: 0.24117 |  0:00:32s\n",
      "epoch 141| loss: 0.04869 | val_0_rmse: 0.2389  |  0:00:32s\n",
      "epoch 142| loss: 0.04823 | val_0_rmse: 0.23534 |  0:00:33s\n",
      "epoch 143| loss: 0.04449 | val_0_rmse: 0.23913 |  0:00:33s\n",
      "epoch 144| loss: 0.04183 | val_0_rmse: 0.24066 |  0:00:33s\n",
      "epoch 145| loss: 0.05465 | val_0_rmse: 0.23897 |  0:00:33s\n",
      "epoch 146| loss: 0.03935 | val_0_rmse: 0.2427  |  0:00:33s\n",
      "epoch 147| loss: 0.04815 | val_0_rmse: 0.24217 |  0:00:34s\n",
      "epoch 148| loss: 0.04585 | val_0_rmse: 0.24394 |  0:00:34s\n",
      "epoch 149| loss: 0.04836 | val_0_rmse: 0.2464  |  0:00:34s\n",
      "epoch 150| loss: 0.04398 | val_0_rmse: 0.24778 |  0:00:34s\n",
      "epoch 151| loss: 0.03991 | val_0_rmse: 0.23769 |  0:00:35s\n",
      "epoch 152| loss: 0.05029 | val_0_rmse: 0.2485  |  0:00:35s\n",
      "epoch 153| loss: 0.04042 | val_0_rmse: 0.23332 |  0:00:35s\n",
      "epoch 154| loss: 0.04947 | val_0_rmse: 0.25636 |  0:00:35s\n",
      "epoch 155| loss: 0.03796 | val_0_rmse: 0.22029 |  0:00:35s\n",
      "epoch 156| loss: 0.04888 | val_0_rmse: 0.25924 |  0:00:36s\n",
      "epoch 157| loss: 0.0464  | val_0_rmse: 0.22418 |  0:00:36s\n",
      "epoch 158| loss: 0.04435 | val_0_rmse: 0.24421 |  0:00:36s\n",
      "epoch 159| loss: 0.04399 | val_0_rmse: 0.22913 |  0:00:36s\n",
      "epoch 160| loss: 0.04296 | val_0_rmse: 0.23657 |  0:00:37s\n",
      "epoch 161| loss: 0.04878 | val_0_rmse: 0.22848 |  0:00:37s\n",
      "epoch 162| loss: 0.04421 | val_0_rmse: 0.23638 |  0:00:37s\n",
      "epoch 163| loss: 0.04609 | val_0_rmse: 0.23086 |  0:00:37s\n",
      "epoch 164| loss: 0.03875 | val_0_rmse: 0.23749 |  0:00:37s\n",
      "epoch 165| loss: 0.05048 | val_0_rmse: 0.22962 |  0:00:38s\n",
      "epoch 166| loss: 0.04233 | val_0_rmse: 0.23166 |  0:00:38s\n",
      "epoch 167| loss: 0.03963 | val_0_rmse: 0.22768 |  0:00:38s\n",
      "epoch 168| loss: 0.04675 | val_0_rmse: 0.22822 |  0:00:38s\n",
      "epoch 169| loss: 0.05325 | val_0_rmse: 0.2288  |  0:00:39s\n",
      "epoch 170| loss: 0.03917 | val_0_rmse: 0.22978 |  0:00:39s\n",
      "epoch 171| loss: 0.04454 | val_0_rmse: 0.22909 |  0:00:39s\n",
      "epoch 172| loss: 0.04104 | val_0_rmse: 0.22748 |  0:00:39s\n",
      "epoch 173| loss: 0.04891 | val_0_rmse: 0.23468 |  0:00:40s\n",
      "epoch 174| loss: 0.04253 | val_0_rmse: 0.22207 |  0:00:40s\n",
      "epoch 175| loss: 0.04147 | val_0_rmse: 0.23215 |  0:00:40s\n",
      "epoch 176| loss: 0.04252 | val_0_rmse: 0.22383 |  0:00:40s\n",
      "epoch 177| loss: 0.04151 | val_0_rmse: 0.24117 |  0:00:40s\n",
      "epoch 178| loss: 0.03724 | val_0_rmse: 0.22133 |  0:00:41s\n",
      "epoch 179| loss: 0.03514 | val_0_rmse: 0.21962 |  0:00:41s\n",
      "epoch 180| loss: 0.03927 | val_0_rmse: 0.22127 |  0:00:41s\n",
      "epoch 181| loss: 0.03119 | val_0_rmse: 0.2334  |  0:00:41s\n",
      "epoch 182| loss: 0.03318 | val_0_rmse: 0.24117 |  0:00:42s\n",
      "epoch 183| loss: 0.0394  | val_0_rmse: 0.23974 |  0:00:42s\n",
      "epoch 184| loss: 0.0515  | val_0_rmse: 0.31196 |  0:00:42s\n",
      "epoch 185| loss: 0.12307 | val_0_rmse: 0.25517 |  0:00:42s\n",
      "epoch 186| loss: 0.0572  | val_0_rmse: 0.2955  |  0:00:42s\n",
      "epoch 187| loss: 0.06218 | val_0_rmse: 0.32586 |  0:00:43s\n",
      "epoch 188| loss: 0.08658 | val_0_rmse: 0.2588  |  0:00:43s\n",
      "epoch 189| loss: 0.09665 | val_0_rmse: 0.24108 |  0:00:43s\n",
      "epoch 190| loss: 0.05982 | val_0_rmse: 0.28671 |  0:00:43s\n",
      "epoch 191| loss: 0.05468 | val_0_rmse: 0.33155 |  0:00:44s\n",
      "epoch 192| loss: 0.11302 | val_0_rmse: 0.23704 |  0:00:44s\n",
      "epoch 193| loss: 0.06251 | val_0_rmse: 0.22805 |  0:00:44s\n",
      "epoch 194| loss: 0.06262 | val_0_rmse: 0.32337 |  0:00:44s\n",
      "epoch 195| loss: 0.07422 | val_0_rmse: 0.30257 |  0:00:45s\n",
      "epoch 196| loss: 0.07174 | val_0_rmse: 0.23351 |  0:00:45s\n",
      "epoch 197| loss: 0.06078 | val_0_rmse: 0.22759 |  0:00:45s\n",
      "epoch 198| loss: 0.0325  | val_0_rmse: 0.2083  |  0:00:45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:15:47,818] Trial 24 finished with value: 0.20830291244072727 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 4, 'gamma': 1.3984171889603345, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.011610647714657701, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199| loss: 0.02842 | val_0_rmse: 0.22359 |  0:00:46s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 198 and best_val_0_rmse = 0.2083\n",
      "Trial 024 | rmse_log=0.20830 | RMSE$=43,491 | MAE$=26,628 | MAPE=15.54% | n_d/n_a=48/24 steps=4 lr=0.01161 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 149.49449| val_0_rmse: 11.46228|  0:00:00s\n",
      "epoch 1  | loss: 120.83267| val_0_rmse: 10.89843|  0:00:00s\n",
      "epoch 2  | loss: 99.25109| val_0_rmse: 10.32132|  0:00:00s\n",
      "epoch 3  | loss: 77.19413| val_0_rmse: 9.62633 |  0:00:01s\n",
      "epoch 4  | loss: 57.86076| val_0_rmse: 8.81266 |  0:00:01s\n",
      "epoch 5  | loss: 42.51304| val_0_rmse: 7.88251 |  0:00:01s\n",
      "epoch 6  | loss: 30.30712| val_0_rmse: 6.93281 |  0:00:02s\n",
      "epoch 7  | loss: 21.57767| val_0_rmse: 6.05952 |  0:00:02s\n",
      "epoch 8  | loss: 17.18973| val_0_rmse: 5.14588 |  0:00:02s\n",
      "epoch 9  | loss: 14.84283| val_0_rmse: 4.32539 |  0:00:03s\n",
      "epoch 10 | loss: 10.95407| val_0_rmse: 3.64052 |  0:00:03s\n",
      "epoch 11 | loss: 10.05191| val_0_rmse: 3.32804 |  0:00:03s\n",
      "epoch 12 | loss: 7.30613 | val_0_rmse: 3.38936 |  0:00:03s\n",
      "epoch 13 | loss: 5.6489  | val_0_rmse: 3.30246 |  0:00:04s\n",
      "epoch 14 | loss: 3.92688 | val_0_rmse: 2.94602 |  0:00:04s\n",
      "epoch 15 | loss: 3.96749 | val_0_rmse: 2.51381 |  0:00:04s\n",
      "epoch 16 | loss: 3.36201 | val_0_rmse: 2.0988  |  0:00:04s\n",
      "epoch 17 | loss: 2.35357 | val_0_rmse: 1.87319 |  0:00:05s\n",
      "epoch 18 | loss: 2.05445 | val_0_rmse: 1.69855 |  0:00:05s\n",
      "epoch 19 | loss: 1.30751 | val_0_rmse: 1.23139 |  0:00:06s\n",
      "epoch 20 | loss: 1.1339  | val_0_rmse: 1.27178 |  0:00:06s\n",
      "epoch 21 | loss: 1.00413 | val_0_rmse: 1.10269 |  0:00:06s\n",
      "epoch 22 | loss: 0.91704 | val_0_rmse: 0.95889 |  0:00:07s\n",
      "epoch 23 | loss: 0.85125 | val_0_rmse: 1.00235 |  0:00:07s\n",
      "epoch 24 | loss: 0.73321 | val_0_rmse: 1.01186 |  0:00:07s\n",
      "epoch 25 | loss: 0.78001 | val_0_rmse: 0.84054 |  0:00:07s\n",
      "epoch 26 | loss: 0.64003 | val_0_rmse: 0.87633 |  0:00:08s\n",
      "epoch 27 | loss: 0.56219 | val_0_rmse: 0.82292 |  0:00:08s\n",
      "epoch 28 | loss: 0.47635 | val_0_rmse: 0.92933 |  0:00:08s\n",
      "epoch 29 | loss: 0.55829 | val_0_rmse: 0.78773 |  0:00:09s\n",
      "epoch 30 | loss: 0.46155 | val_0_rmse: 0.90652 |  0:00:09s\n",
      "epoch 31 | loss: 0.40126 | val_0_rmse: 0.67246 |  0:00:09s\n",
      "epoch 32 | loss: 0.45399 | val_0_rmse: 0.90022 |  0:00:10s\n",
      "epoch 33 | loss: 0.53216 | val_0_rmse: 0.71635 |  0:00:10s\n",
      "epoch 34 | loss: 0.38223 | val_0_rmse: 0.74458 |  0:00:10s\n",
      "epoch 35 | loss: 0.36224 | val_0_rmse: 0.67544 |  0:00:10s\n",
      "epoch 36 | loss: 0.33704 | val_0_rmse: 0.72346 |  0:00:11s\n",
      "epoch 37 | loss: 0.34343 | val_0_rmse: 0.43467 |  0:00:11s\n",
      "epoch 38 | loss: 0.41953 | val_0_rmse: 0.86001 |  0:00:11s\n",
      "epoch 39 | loss: 0.33466 | val_0_rmse: 0.40171 |  0:00:12s\n",
      "epoch 40 | loss: 0.40766 | val_0_rmse: 0.72027 |  0:00:12s\n",
      "epoch 41 | loss: 0.35005 | val_0_rmse: 0.38131 |  0:00:12s\n",
      "epoch 42 | loss: 0.35773 | val_0_rmse: 0.44287 |  0:00:13s\n",
      "epoch 43 | loss: 0.28654 | val_0_rmse: 0.45323 |  0:00:13s\n",
      "epoch 44 | loss: 0.24832 | val_0_rmse: 0.39895 |  0:00:13s\n",
      "epoch 45 | loss: 0.27054 | val_0_rmse: 0.53339 |  0:00:13s\n",
      "epoch 46 | loss: 0.26285 | val_0_rmse: 0.36706 |  0:00:14s\n",
      "epoch 47 | loss: 0.27924 | val_0_rmse: 0.45025 |  0:00:14s\n",
      "epoch 48 | loss: 0.21915 | val_0_rmse: 0.43501 |  0:00:14s\n",
      "epoch 49 | loss: 0.25854 | val_0_rmse: 0.5385  |  0:00:15s\n",
      "epoch 50 | loss: 0.22879 | val_0_rmse: 0.50175 |  0:00:15s\n",
      "epoch 51 | loss: 0.23083 | val_0_rmse: 0.46852 |  0:00:15s\n",
      "epoch 52 | loss: 0.20472 | val_0_rmse: 0.44056 |  0:00:15s\n",
      "epoch 53 | loss: 0.20328 | val_0_rmse: 0.57076 |  0:00:16s\n",
      "epoch 54 | loss: 0.26624 | val_0_rmse: 0.35465 |  0:00:16s\n",
      "epoch 55 | loss: 0.28567 | val_0_rmse: 0.57359 |  0:00:16s\n",
      "epoch 56 | loss: 0.24378 | val_0_rmse: 0.37182 |  0:00:17s\n",
      "epoch 57 | loss: 0.23433 | val_0_rmse: 0.56595 |  0:00:17s\n",
      "epoch 58 | loss: 0.21612 | val_0_rmse: 0.32255 |  0:00:17s\n",
      "epoch 59 | loss: 0.22945 | val_0_rmse: 0.66794 |  0:00:17s\n",
      "epoch 60 | loss: 0.31177 | val_0_rmse: 0.32542 |  0:00:18s\n",
      "epoch 61 | loss: 0.15346 | val_0_rmse: 0.40241 |  0:00:18s\n",
      "epoch 62 | loss: 0.14499 | val_0_rmse: 0.32644 |  0:00:18s\n",
      "epoch 63 | loss: 0.14132 | val_0_rmse: 0.36239 |  0:00:19s\n",
      "epoch 64 | loss: 0.12392 | val_0_rmse: 0.4716  |  0:00:19s\n",
      "epoch 65 | loss: 0.17064 | val_0_rmse: 0.30494 |  0:00:19s\n",
      "epoch 66 | loss: 0.24541 | val_0_rmse: 0.47142 |  0:00:19s\n",
      "epoch 67 | loss: 0.23034 | val_0_rmse: 0.34005 |  0:00:20s\n",
      "epoch 68 | loss: 0.16366 | val_0_rmse: 0.33432 |  0:00:20s\n",
      "epoch 69 | loss: 0.16795 | val_0_rmse: 0.37401 |  0:00:20s\n",
      "epoch 70 | loss: 0.14971 | val_0_rmse: 0.2927  |  0:00:20s\n",
      "epoch 71 | loss: 0.16001 | val_0_rmse: 0.43807 |  0:00:21s\n",
      "epoch 72 | loss: 0.16171 | val_0_rmse: 0.33029 |  0:00:21s\n",
      "epoch 73 | loss: 0.26906 | val_0_rmse: 0.26713 |  0:00:21s\n",
      "epoch 74 | loss: 0.17328 | val_0_rmse: 0.54694 |  0:00:22s\n",
      "epoch 75 | loss: 0.24095 | val_0_rmse: 0.26698 |  0:00:22s\n",
      "epoch 76 | loss: 0.1736  | val_0_rmse: 0.27084 |  0:00:22s\n",
      "epoch 77 | loss: 0.17614 | val_0_rmse: 0.47874 |  0:00:22s\n",
      "epoch 78 | loss: 0.18934 | val_0_rmse: 0.32851 |  0:00:23s\n",
      "epoch 79 | loss: 0.23278 | val_0_rmse: 0.31294 |  0:00:23s\n",
      "epoch 80 | loss: 0.12399 | val_0_rmse: 0.31071 |  0:00:23s\n",
      "epoch 81 | loss: 0.12039 | val_0_rmse: 0.27959 |  0:00:24s\n",
      "epoch 82 | loss: 0.11629 | val_0_rmse: 0.34383 |  0:00:24s\n",
      "epoch 83 | loss: 0.11415 | val_0_rmse: 0.28728 |  0:00:24s\n",
      "epoch 84 | loss: 0.11459 | val_0_rmse: 0.30215 |  0:00:24s\n",
      "epoch 85 | loss: 0.10111 | val_0_rmse: 0.3398  |  0:00:25s\n",
      "epoch 86 | loss: 0.11297 | val_0_rmse: 0.30942 |  0:00:25s\n",
      "epoch 87 | loss: 0.12469 | val_0_rmse: 0.32341 |  0:00:25s\n",
      "epoch 88 | loss: 0.11802 | val_0_rmse: 0.40217 |  0:00:25s\n",
      "epoch 89 | loss: 0.11237 | val_0_rmse: 0.29813 |  0:00:26s\n",
      "epoch 90 | loss: 0.11632 | val_0_rmse: 0.38936 |  0:00:26s\n",
      "epoch 91 | loss: 0.11395 | val_0_rmse: 0.27906 |  0:00:26s\n",
      "epoch 92 | loss: 0.14277 | val_0_rmse: 0.2691  |  0:00:26s\n",
      "epoch 93 | loss: 0.10835 | val_0_rmse: 0.32634 |  0:00:27s\n",
      "epoch 94 | loss: 0.09874 | val_0_rmse: 0.28969 |  0:00:27s\n",
      "epoch 95 | loss: 0.08921 | val_0_rmse: 0.28675 |  0:00:27s\n",
      "epoch 96 | loss: 0.0882  | val_0_rmse: 0.3377  |  0:00:27s\n",
      "epoch 97 | loss: 0.10605 | val_0_rmse: 0.27356 |  0:00:28s\n",
      "epoch 98 | loss: 0.09721 | val_0_rmse: 0.32701 |  0:00:28s\n",
      "epoch 99 | loss: 0.10814 | val_0_rmse: 0.25025 |  0:00:28s\n",
      "epoch 100| loss: 0.09111 | val_0_rmse: 0.24951 |  0:00:28s\n",
      "epoch 101| loss: 0.09824 | val_0_rmse: 0.26588 |  0:00:29s\n",
      "epoch 102| loss: 0.09906 | val_0_rmse: 0.28371 |  0:00:29s\n",
      "epoch 103| loss: 0.10173 | val_0_rmse: 0.35306 |  0:00:29s\n",
      "epoch 104| loss: 0.113   | val_0_rmse: 0.28581 |  0:00:30s\n",
      "epoch 105| loss: 0.08463 | val_0_rmse: 0.29291 |  0:00:30s\n",
      "epoch 106| loss: 0.08308 | val_0_rmse: 0.28363 |  0:00:30s\n",
      "epoch 107| loss: 0.08797 | val_0_rmse: 0.32793 |  0:00:30s\n",
      "epoch 108| loss: 0.08784 | val_0_rmse: 0.26745 |  0:00:31s\n",
      "epoch 109| loss: 0.10971 | val_0_rmse: 0.37717 |  0:00:31s\n",
      "epoch 110| loss: 0.0945  | val_0_rmse: 0.26456 |  0:00:31s\n",
      "epoch 111| loss: 0.08694 | val_0_rmse: 0.29914 |  0:00:31s\n",
      "epoch 112| loss: 0.07398 | val_0_rmse: 0.27161 |  0:00:32s\n",
      "epoch 113| loss: 0.08782 | val_0_rmse: 0.29415 |  0:00:32s\n",
      "epoch 114| loss: 0.08574 | val_0_rmse: 0.25128 |  0:00:32s\n",
      "epoch 115| loss: 0.08905 | val_0_rmse: 0.28235 |  0:00:32s\n",
      "epoch 116| loss: 0.08139 | val_0_rmse: 0.25654 |  0:00:33s\n",
      "epoch 117| loss: 0.08748 | val_0_rmse: 0.3205  |  0:00:33s\n",
      "epoch 118| loss: 0.09478 | val_0_rmse: 0.24015 |  0:00:33s\n",
      "epoch 119| loss: 0.07721 | val_0_rmse: 0.33443 |  0:00:33s\n",
      "epoch 120| loss: 0.1256  | val_0_rmse: 0.24662 |  0:00:34s\n",
      "epoch 121| loss: 0.10338 | val_0_rmse: 0.24257 |  0:00:34s\n",
      "epoch 122| loss: 0.08583 | val_0_rmse: 0.26131 |  0:00:34s\n",
      "epoch 123| loss: 0.08868 | val_0_rmse: 0.24264 |  0:00:35s\n",
      "epoch 124| loss: 0.0873  | val_0_rmse: 0.27335 |  0:00:35s\n",
      "epoch 125| loss: 0.08667 | val_0_rmse: 0.25445 |  0:00:35s\n",
      "epoch 126| loss: 0.06853 | val_0_rmse: 0.27556 |  0:00:35s\n",
      "epoch 127| loss: 0.08085 | val_0_rmse: 0.2502  |  0:00:36s\n",
      "epoch 128| loss: 0.0846  | val_0_rmse: 0.27922 |  0:00:36s\n",
      "epoch 129| loss: 0.08175 | val_0_rmse: 0.245   |  0:00:36s\n",
      "epoch 130| loss: 0.08503 | val_0_rmse: 0.25945 |  0:00:37s\n",
      "epoch 131| loss: 0.07549 | val_0_rmse: 0.26281 |  0:00:37s\n",
      "epoch 132| loss: 0.07726 | val_0_rmse: 0.27699 |  0:00:37s\n",
      "epoch 133| loss: 0.08038 | val_0_rmse: 0.31077 |  0:00:37s\n",
      "epoch 134| loss: 0.09017 | val_0_rmse: 0.25168 |  0:00:38s\n",
      "epoch 135| loss: 0.07648 | val_0_rmse: 0.25414 |  0:00:38s\n",
      "epoch 136| loss: 0.06927 | val_0_rmse: 0.24872 |  0:00:38s\n",
      "epoch 137| loss: 0.09189 | val_0_rmse: 0.2446  |  0:00:38s\n",
      "epoch 138| loss: 0.07534 | val_0_rmse: 0.26622 |  0:00:39s\n",
      "epoch 139| loss: 0.07874 | val_0_rmse: 0.32959 |  0:00:39s\n",
      "epoch 140| loss: 0.09397 | val_0_rmse: 0.30138 |  0:00:39s\n",
      "epoch 141| loss: 0.07772 | val_0_rmse: 0.26744 |  0:00:39s\n",
      "epoch 142| loss: 0.08813 | val_0_rmse: 0.29258 |  0:00:40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:16:28,744] Trial 25 finished with value: 0.2401515115309635 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 5, 'gamma': 1.3479709698628477, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.011801338904894248, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 143| loss: 0.07852 | val_0_rmse: 0.28272 |  0:00:40s\n",
      "\n",
      "Early stopping occurred at epoch 143 with best_epoch = 118 and best_val_0_rmse = 0.24015\n",
      "Trial 025 | rmse_log=0.24015 | RMSE$=48,969 | MAE$=31,331 | MAPE=18.44% | n_d/n_a=16/16 steps=5 lr=0.01180 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 161.32621| val_0_rmse: 11.5185 |  0:00:00s\n",
      "epoch 1  | loss: 121.56759| val_0_rmse: 10.21138|  0:00:00s\n",
      "epoch 2  | loss: 94.41964| val_0_rmse: 9.19118 |  0:00:00s\n",
      "epoch 3  | loss: 70.21309| val_0_rmse: 8.01925 |  0:00:00s\n",
      "epoch 4  | loss: 47.75617| val_0_rmse: 6.5738  |  0:00:01s\n",
      "epoch 5  | loss: 27.75876| val_0_rmse: 5.15541 |  0:00:01s\n",
      "epoch 6  | loss: 13.55388| val_0_rmse: 3.72511 |  0:00:01s\n",
      "epoch 7  | loss: 8.634   | val_0_rmse: 3.46804 |  0:00:01s\n",
      "epoch 8  | loss: 5.27216 | val_0_rmse: 3.87209 |  0:00:01s\n",
      "epoch 9  | loss: 2.94773 | val_0_rmse: 3.87554 |  0:00:02s\n",
      "epoch 10 | loss: 2.09698 | val_0_rmse: 2.96717 |  0:00:02s\n",
      "epoch 11 | loss: 1.3226  | val_0_rmse: 3.35357 |  0:00:02s\n",
      "epoch 12 | loss: 1.05725 | val_0_rmse: 3.50575 |  0:00:02s\n",
      "epoch 13 | loss: 1.06982 | val_0_rmse: 3.42792 |  0:00:02s\n",
      "epoch 14 | loss: 0.68042 | val_0_rmse: 3.41178 |  0:00:02s\n",
      "epoch 15 | loss: 0.82155 | val_0_rmse: 3.28509 |  0:00:03s\n",
      "epoch 16 | loss: 0.56765 | val_0_rmse: 2.77767 |  0:00:03s\n",
      "epoch 17 | loss: 0.66358 | val_0_rmse: 2.84902 |  0:00:03s\n",
      "epoch 18 | loss: 0.37293 | val_0_rmse: 2.50331 |  0:00:03s\n",
      "epoch 19 | loss: 0.39015 | val_0_rmse: 2.48973 |  0:00:03s\n",
      "epoch 20 | loss: 0.43398 | val_0_rmse: 2.21504 |  0:00:04s\n",
      "epoch 21 | loss: 0.43373 | val_0_rmse: 2.26758 |  0:00:04s\n",
      "epoch 22 | loss: 0.39165 | val_0_rmse: 1.92239 |  0:00:04s\n",
      "epoch 23 | loss: 0.38433 | val_0_rmse: 2.07287 |  0:00:04s\n",
      "epoch 24 | loss: 0.4406  | val_0_rmse: 1.75687 |  0:00:04s\n",
      "epoch 25 | loss: 0.23057 | val_0_rmse: 1.92137 |  0:00:04s\n",
      "epoch 26 | loss: 0.2163  | val_0_rmse: 1.64331 |  0:00:05s\n",
      "epoch 27 | loss: 0.18161 | val_0_rmse: 1.38694 |  0:00:05s\n",
      "epoch 28 | loss: 0.39278 | val_0_rmse: 1.2616  |  0:00:05s\n",
      "epoch 29 | loss: 0.20771 | val_0_rmse: 1.36083 |  0:00:05s\n",
      "epoch 30 | loss: 0.24948 | val_0_rmse: 1.19179 |  0:00:05s\n",
      "epoch 31 | loss: 0.16898 | val_0_rmse: 1.00623 |  0:00:05s\n",
      "epoch 32 | loss: 0.2197  | val_0_rmse: 1.06573 |  0:00:06s\n",
      "epoch 33 | loss: 0.14084 | val_0_rmse: 0.83094 |  0:00:06s\n",
      "epoch 34 | loss: 0.19437 | val_0_rmse: 1.08448 |  0:00:06s\n",
      "epoch 35 | loss: 0.17625 | val_0_rmse: 0.84966 |  0:00:06s\n",
      "epoch 36 | loss: 0.21129 | val_0_rmse: 1.02593 |  0:00:06s\n",
      "epoch 37 | loss: 0.1547  | val_0_rmse: 0.67297 |  0:00:07s\n",
      "epoch 38 | loss: 0.20723 | val_0_rmse: 1.04786 |  0:00:07s\n",
      "epoch 39 | loss: 0.29456 | val_0_rmse: 0.53038 |  0:00:07s\n",
      "epoch 40 | loss: 0.4128  | val_0_rmse: 0.62055 |  0:00:07s\n",
      "epoch 41 | loss: 0.30527 | val_0_rmse: 0.8751  |  0:00:07s\n",
      "epoch 42 | loss: 0.20035 | val_0_rmse: 0.46377 |  0:00:08s\n",
      "epoch 43 | loss: 0.25611 | val_0_rmse: 0.81262 |  0:00:08s\n",
      "epoch 44 | loss: 0.28869 | val_0_rmse: 0.73855 |  0:00:08s\n",
      "epoch 45 | loss: 0.19099 | val_0_rmse: 0.45129 |  0:00:08s\n",
      "epoch 46 | loss: 0.19555 | val_0_rmse: 0.82328 |  0:00:08s\n",
      "epoch 47 | loss: 0.19102 | val_0_rmse: 0.40175 |  0:00:08s\n",
      "epoch 48 | loss: 0.23003 | val_0_rmse: 0.45891 |  0:00:09s\n",
      "epoch 49 | loss: 0.14849 | val_0_rmse: 0.68367 |  0:00:09s\n",
      "epoch 50 | loss: 0.16209 | val_0_rmse: 0.29521 |  0:00:09s\n",
      "epoch 51 | loss: 0.17426 | val_0_rmse: 0.5405  |  0:00:09s\n",
      "epoch 52 | loss: 0.1567  | val_0_rmse: 0.42814 |  0:00:09s\n",
      "epoch 53 | loss: 0.08669 | val_0_rmse: 0.37101 |  0:00:09s\n",
      "epoch 54 | loss: 0.09464 | val_0_rmse: 0.38851 |  0:00:10s\n",
      "epoch 55 | loss: 0.08991 | val_0_rmse: 0.40626 |  0:00:10s\n",
      "epoch 56 | loss: 0.09654 | val_0_rmse: 0.40075 |  0:00:10s\n",
      "epoch 57 | loss: 0.09816 | val_0_rmse: 0.38211 |  0:00:10s\n",
      "epoch 58 | loss: 0.10514 | val_0_rmse: 0.36506 |  0:00:10s\n",
      "epoch 59 | loss: 0.08274 | val_0_rmse: 0.40269 |  0:00:10s\n",
      "epoch 60 | loss: 0.07511 | val_0_rmse: 0.29257 |  0:00:11s\n",
      "epoch 61 | loss: 0.07345 | val_0_rmse: 0.42822 |  0:00:11s\n",
      "epoch 62 | loss: 0.09241 | val_0_rmse: 0.28772 |  0:00:11s\n",
      "epoch 63 | loss: 0.08456 | val_0_rmse: 0.42915 |  0:00:11s\n",
      "epoch 64 | loss: 0.0903  | val_0_rmse: 0.30787 |  0:00:11s\n",
      "epoch 65 | loss: 0.07318 | val_0_rmse: 0.40251 |  0:00:12s\n",
      "epoch 66 | loss: 0.08787 | val_0_rmse: 0.28228 |  0:00:12s\n",
      "epoch 67 | loss: 0.07496 | val_0_rmse: 0.37532 |  0:00:12s\n",
      "epoch 68 | loss: 0.09344 | val_0_rmse: 0.28072 |  0:00:12s\n",
      "epoch 69 | loss: 0.08985 | val_0_rmse: 0.34527 |  0:00:12s\n",
      "epoch 70 | loss: 0.06921 | val_0_rmse: 0.29223 |  0:00:12s\n",
      "epoch 71 | loss: 0.12571 | val_0_rmse: 0.30456 |  0:00:13s\n",
      "epoch 72 | loss: 0.0741  | val_0_rmse: 0.28656 |  0:00:13s\n",
      "epoch 73 | loss: 0.10571 | val_0_rmse: 0.35156 |  0:00:13s\n",
      "epoch 74 | loss: 0.09677 | val_0_rmse: 0.29291 |  0:00:13s\n",
      "epoch 75 | loss: 0.14384 | val_0_rmse: 0.44408 |  0:00:13s\n",
      "epoch 76 | loss: 0.12624 | val_0_rmse: 0.363   |  0:00:13s\n",
      "epoch 77 | loss: 0.09725 | val_0_rmse: 0.33991 |  0:00:14s\n",
      "epoch 78 | loss: 0.07275 | val_0_rmse: 0.35233 |  0:00:14s\n",
      "epoch 79 | loss: 0.06341 | val_0_rmse: 0.37278 |  0:00:14s\n",
      "epoch 80 | loss: 0.07455 | val_0_rmse: 0.26135 |  0:00:14s\n",
      "epoch 81 | loss: 0.06436 | val_0_rmse: 0.3173  |  0:00:14s\n",
      "epoch 82 | loss: 0.06906 | val_0_rmse: 0.25772 |  0:00:14s\n",
      "epoch 83 | loss: 0.08294 | val_0_rmse: 0.42035 |  0:00:15s\n",
      "epoch 84 | loss: 0.12002 | val_0_rmse: 0.29952 |  0:00:15s\n",
      "epoch 85 | loss: 0.20881 | val_0_rmse: 0.26408 |  0:00:15s\n",
      "epoch 86 | loss: 0.10559 | val_0_rmse: 0.39561 |  0:00:15s\n",
      "epoch 87 | loss: 0.10606 | val_0_rmse: 0.27461 |  0:00:15s\n",
      "epoch 88 | loss: 0.10998 | val_0_rmse: 0.37679 |  0:00:15s\n",
      "epoch 89 | loss: 0.15364 | val_0_rmse: 0.3311  |  0:00:16s\n",
      "epoch 90 | loss: 0.08401 | val_0_rmse: 0.26195 |  0:00:16s\n",
      "epoch 91 | loss: 0.08085 | val_0_rmse: 0.35517 |  0:00:16s\n",
      "epoch 92 | loss: 0.10918 | val_0_rmse: 0.2491  |  0:00:16s\n",
      "epoch 93 | loss: 0.06387 | val_0_rmse: 0.28024 |  0:00:16s\n",
      "epoch 94 | loss: 0.05923 | val_0_rmse: 0.26177 |  0:00:16s\n",
      "epoch 95 | loss: 0.05759 | val_0_rmse: 0.25536 |  0:00:17s\n",
      "epoch 96 | loss: 0.06728 | val_0_rmse: 0.27046 |  0:00:17s\n",
      "epoch 97 | loss: 0.06174 | val_0_rmse: 0.25175 |  0:00:17s\n",
      "epoch 98 | loss: 0.06634 | val_0_rmse: 0.26454 |  0:00:17s\n",
      "epoch 99 | loss: 0.06966 | val_0_rmse: 0.2569  |  0:00:17s\n",
      "epoch 100| loss: 0.06014 | val_0_rmse: 0.26318 |  0:00:18s\n",
      "epoch 101| loss: 0.06006 | val_0_rmse: 0.26834 |  0:00:18s\n",
      "epoch 102| loss: 0.05819 | val_0_rmse: 0.25993 |  0:00:18s\n",
      "epoch 103| loss: 0.05834 | val_0_rmse: 0.25688 |  0:00:18s\n",
      "epoch 104| loss: 0.05112 | val_0_rmse: 0.24979 |  0:00:18s\n",
      "epoch 105| loss: 0.04994 | val_0_rmse: 0.24425 |  0:00:18s\n",
      "epoch 106| loss: 0.04457 | val_0_rmse: 0.25277 |  0:00:19s\n",
      "epoch 107| loss: 0.04163 | val_0_rmse: 0.26049 |  0:00:19s\n",
      "epoch 108| loss: 0.05616 | val_0_rmse: 0.26643 |  0:00:19s\n",
      "epoch 109| loss: 0.04955 | val_0_rmse: 0.257   |  0:00:19s\n",
      "epoch 110| loss: 0.04353 | val_0_rmse: 0.25758 |  0:00:19s\n",
      "epoch 111| loss: 0.04416 | val_0_rmse: 0.25879 |  0:00:19s\n",
      "epoch 112| loss: 0.05458 | val_0_rmse: 0.2725  |  0:00:20s\n",
      "epoch 113| loss: 0.05127 | val_0_rmse: 0.28294 |  0:00:20s\n",
      "epoch 114| loss: 0.05382 | val_0_rmse: 0.33174 |  0:00:20s\n",
      "epoch 115| loss: 0.0916  | val_0_rmse: 0.27642 |  0:00:20s\n",
      "epoch 116| loss: 0.06747 | val_0_rmse: 0.30883 |  0:00:20s\n",
      "epoch 117| loss: 0.08058 | val_0_rmse: 0.24538 |  0:00:20s\n",
      "epoch 118| loss: 0.04771 | val_0_rmse: 0.25773 |  0:00:21s\n",
      "epoch 119| loss: 0.04644 | val_0_rmse: 0.23896 |  0:00:21s\n",
      "epoch 120| loss: 0.03933 | val_0_rmse: 0.25551 |  0:00:21s\n",
      "epoch 121| loss: 0.04102 | val_0_rmse: 0.24205 |  0:00:21s\n",
      "epoch 122| loss: 0.04053 | val_0_rmse: 0.23837 |  0:00:21s\n",
      "epoch 123| loss: 0.03996 | val_0_rmse: 0.24407 |  0:00:21s\n",
      "epoch 124| loss: 0.04754 | val_0_rmse: 0.24156 |  0:00:22s\n",
      "epoch 125| loss: 0.04679 | val_0_rmse: 0.25053 |  0:00:22s\n",
      "epoch 126| loss: 0.05567 | val_0_rmse: 0.26975 |  0:00:22s\n",
      "epoch 127| loss: 0.06632 | val_0_rmse: 0.23004 |  0:00:22s\n",
      "epoch 128| loss: 0.04992 | val_0_rmse: 0.23985 |  0:00:22s\n",
      "epoch 129| loss: 0.04562 | val_0_rmse: 0.25293 |  0:00:22s\n",
      "epoch 130| loss: 0.05775 | val_0_rmse: 0.24375 |  0:00:23s\n",
      "epoch 131| loss: 0.03695 | val_0_rmse: 0.23795 |  0:00:23s\n",
      "epoch 132| loss: 0.03636 | val_0_rmse: 0.25696 |  0:00:23s\n",
      "epoch 133| loss: 0.03967 | val_0_rmse: 0.25648 |  0:00:23s\n",
      "epoch 134| loss: 0.03988 | val_0_rmse: 0.25954 |  0:00:23s\n",
      "epoch 135| loss: 0.04148 | val_0_rmse: 0.27448 |  0:00:23s\n",
      "epoch 136| loss: 0.0458  | val_0_rmse: 0.26227 |  0:00:24s\n",
      "epoch 137| loss: 0.04345 | val_0_rmse: 0.25437 |  0:00:24s\n",
      "epoch 138| loss: 0.04606 | val_0_rmse: 0.30821 |  0:00:24s\n",
      "epoch 139| loss: 0.07317 | val_0_rmse: 0.26214 |  0:00:24s\n",
      "epoch 140| loss: 0.06978 | val_0_rmse: 0.30509 |  0:00:24s\n",
      "epoch 141| loss: 0.0756  | val_0_rmse: 0.25291 |  0:00:25s\n",
      "epoch 142| loss: 0.04771 | val_0_rmse: 0.28818 |  0:00:25s\n",
      "epoch 143| loss: 0.06773 | val_0_rmse: 0.27655 |  0:00:25s\n",
      "epoch 144| loss: 0.08855 | val_0_rmse: 0.29343 |  0:00:25s\n",
      "epoch 145| loss: 0.07363 | val_0_rmse: 0.29915 |  0:00:25s\n",
      "epoch 146| loss: 0.06515 | val_0_rmse: 0.25181 |  0:00:25s\n",
      "epoch 147| loss: 0.04356 | val_0_rmse: 0.26127 |  0:00:26s\n",
      "epoch 148| loss: 0.04398 | val_0_rmse: 0.25087 |  0:00:26s\n",
      "epoch 149| loss: 0.04994 | val_0_rmse: 0.27746 |  0:00:26s\n",
      "epoch 150| loss: 0.04968 | val_0_rmse: 0.2391  |  0:00:26s\n",
      "epoch 151| loss: 0.04489 | val_0_rmse: 0.25575 |  0:00:26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:16:55,938] Trial 26 finished with value: 0.23004482469629114 and parameters: {'n_d': 32, 'n_a': 24, 'n_steps': 3, 'gamma': 1.5333414673942687, 'n_independent': 3, 'n_shared': 3, 'lambda_sparse': 1e-06, 'mask_type': 'sparsemax', 'lr': 0.015701441431149887, 'batch_size': 512, 'virtual_batch_size': 256}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 152| loss: 0.05208 | val_0_rmse: 0.23529 |  0:00:26s\n",
      "\n",
      "Early stopping occurred at epoch 152 with best_epoch = 127 and best_val_0_rmse = 0.23004\n",
      "Trial 026 | rmse_log=0.23004 | RMSE$=41,029 | MAE$=29,043 | MAPE=17.49% | n_d/n_a=32/24 steps=3 lr=0.01570 batch=512 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 198.24679| val_0_rmse: 11.59553|  0:00:00s\n",
      "epoch 1  | loss: 157.68423| val_0_rmse: 10.93786|  0:00:00s\n",
      "epoch 2  | loss: 119.59957| val_0_rmse: 10.21995|  0:00:00s\n",
      "epoch 3  | loss: 90.11728| val_0_rmse: 9.45801 |  0:00:00s\n",
      "epoch 4  | loss: 68.59241| val_0_rmse: 8.66711 |  0:00:00s\n",
      "epoch 5  | loss: 52.80978| val_0_rmse: 7.82338 |  0:00:01s\n",
      "epoch 6  | loss: 39.92301| val_0_rmse: 6.89022 |  0:00:01s\n",
      "epoch 7  | loss: 32.67033| val_0_rmse: 5.9686  |  0:00:01s\n",
      "epoch 8  | loss: 23.99207| val_0_rmse: 5.14438 |  0:00:01s\n",
      "epoch 9  | loss: 21.58397| val_0_rmse: 4.42294 |  0:00:01s\n",
      "epoch 10 | loss: 20.11553| val_0_rmse: 3.88341 |  0:00:02s\n",
      "epoch 11 | loss: 14.7577 | val_0_rmse: 3.44923 |  0:00:02s\n",
      "epoch 12 | loss: 14.46184| val_0_rmse: 3.23138 |  0:00:02s\n",
      "epoch 13 | loss: 12.59216| val_0_rmse: 3.09588 |  0:00:02s\n",
      "epoch 14 | loss: 8.77954 | val_0_rmse: 2.9584  |  0:00:02s\n",
      "epoch 15 | loss: 8.01542 | val_0_rmse: 2.79263 |  0:00:02s\n",
      "epoch 16 | loss: 5.28338 | val_0_rmse: 2.53131 |  0:00:03s\n",
      "epoch 17 | loss: 4.8085  | val_0_rmse: 2.25983 |  0:00:03s\n",
      "epoch 18 | loss: 3.39382 | val_0_rmse: 1.7751  |  0:00:03s\n",
      "epoch 19 | loss: 2.56917 | val_0_rmse: 1.37785 |  0:00:03s\n",
      "epoch 20 | loss: 1.86955 | val_0_rmse: 1.29123 |  0:00:03s\n",
      "epoch 21 | loss: 2.40469 | val_0_rmse: 1.32117 |  0:00:04s\n",
      "epoch 22 | loss: 1.65965 | val_0_rmse: 1.14963 |  0:00:04s\n",
      "epoch 23 | loss: 1.30373 | val_0_rmse: 0.9711  |  0:00:04s\n",
      "epoch 24 | loss: 1.29904 | val_0_rmse: 1.01275 |  0:00:04s\n",
      "epoch 25 | loss: 1.16986 | val_0_rmse: 0.77838 |  0:00:04s\n",
      "epoch 26 | loss: 1.21462 | val_0_rmse: 0.70967 |  0:00:04s\n",
      "epoch 27 | loss: 0.96057 | val_0_rmse: 1.25246 |  0:00:05s\n",
      "epoch 28 | loss: 1.0592  | val_0_rmse: 0.72014 |  0:00:05s\n",
      "epoch 29 | loss: 1.18064 | val_0_rmse: 0.54299 |  0:00:05s\n",
      "epoch 30 | loss: 0.84986 | val_0_rmse: 1.03128 |  0:00:05s\n",
      "epoch 31 | loss: 0.91829 | val_0_rmse: 0.66169 |  0:00:05s\n",
      "epoch 32 | loss: 0.74095 | val_0_rmse: 0.60015 |  0:00:05s\n",
      "epoch 33 | loss: 0.63356 | val_0_rmse: 0.73298 |  0:00:06s\n",
      "epoch 34 | loss: 0.47982 | val_0_rmse: 0.45338 |  0:00:06s\n",
      "epoch 35 | loss: 0.61507 | val_0_rmse: 0.7251  |  0:00:06s\n",
      "epoch 36 | loss: 0.65765 | val_0_rmse: 0.65292 |  0:00:06s\n",
      "epoch 37 | loss: 0.49841 | val_0_rmse: 0.43214 |  0:00:06s\n",
      "epoch 38 | loss: 0.44694 | val_0_rmse: 0.70267 |  0:00:06s\n",
      "epoch 39 | loss: 0.46882 | val_0_rmse: 0.5851  |  0:00:07s\n",
      "epoch 40 | loss: 0.54576 | val_0_rmse: 0.34969 |  0:00:07s\n",
      "epoch 41 | loss: 0.56696 | val_0_rmse: 0.55998 |  0:00:07s\n",
      "epoch 42 | loss: 0.42156 | val_0_rmse: 0.36334 |  0:00:07s\n",
      "epoch 43 | loss: 0.49883 | val_0_rmse: 0.44918 |  0:00:07s\n",
      "epoch 44 | loss: 0.39334 | val_0_rmse: 0.48921 |  0:00:07s\n",
      "epoch 45 | loss: 0.33735 | val_0_rmse: 0.36865 |  0:00:08s\n",
      "epoch 46 | loss: 0.27839 | val_0_rmse: 0.58098 |  0:00:08s\n",
      "epoch 47 | loss: 0.30722 | val_0_rmse: 0.34675 |  0:00:08s\n",
      "epoch 48 | loss: 0.32698 | val_0_rmse: 0.3918  |  0:00:08s\n",
      "epoch 49 | loss: 0.24871 | val_0_rmse: 0.44487 |  0:00:08s\n",
      "epoch 50 | loss: 0.29853 | val_0_rmse: 0.40121 |  0:00:08s\n",
      "epoch 51 | loss: 0.25714 | val_0_rmse: 0.45873 |  0:00:09s\n",
      "epoch 52 | loss: 0.226   | val_0_rmse: 0.34699 |  0:00:09s\n",
      "epoch 53 | loss: 0.24724 | val_0_rmse: 0.4833  |  0:00:09s\n",
      "epoch 54 | loss: 0.21093 | val_0_rmse: 0.36034 |  0:00:09s\n",
      "epoch 55 | loss: 0.19677 | val_0_rmse: 0.40115 |  0:00:09s\n",
      "epoch 56 | loss: 0.16079 | val_0_rmse: 0.45165 |  0:00:10s\n",
      "epoch 57 | loss: 0.17411 | val_0_rmse: 0.35095 |  0:00:10s\n",
      "epoch 58 | loss: 0.17204 | val_0_rmse: 0.42056 |  0:00:10s\n",
      "epoch 59 | loss: 0.15558 | val_0_rmse: 0.34294 |  0:00:10s\n",
      "epoch 60 | loss: 0.19893 | val_0_rmse: 0.45997 |  0:00:10s\n",
      "epoch 61 | loss: 0.19509 | val_0_rmse: 0.31829 |  0:00:10s\n",
      "epoch 62 | loss: 0.21222 | val_0_rmse: 0.41795 |  0:00:10s\n",
      "epoch 63 | loss: 0.19429 | val_0_rmse: 0.33431 |  0:00:11s\n",
      "epoch 64 | loss: 0.20966 | val_0_rmse: 0.42632 |  0:00:11s\n",
      "epoch 65 | loss: 0.17605 | val_0_rmse: 0.34408 |  0:00:11s\n",
      "epoch 66 | loss: 0.24153 | val_0_rmse: 0.36156 |  0:00:11s\n",
      "epoch 67 | loss: 0.18012 | val_0_rmse: 0.32108 |  0:00:11s\n",
      "epoch 68 | loss: 0.1529  | val_0_rmse: 0.31987 |  0:00:11s\n",
      "epoch 69 | loss: 0.15923 | val_0_rmse: 0.37581 |  0:00:12s\n",
      "epoch 70 | loss: 0.14433 | val_0_rmse: 0.31823 |  0:00:12s\n",
      "epoch 71 | loss: 0.12392 | val_0_rmse: 0.37488 |  0:00:12s\n",
      "epoch 72 | loss: 0.12623 | val_0_rmse: 0.30483 |  0:00:12s\n",
      "epoch 73 | loss: 0.13962 | val_0_rmse: 0.31515 |  0:00:12s\n",
      "epoch 74 | loss: 0.11549 | val_0_rmse: 0.28271 |  0:00:12s\n",
      "epoch 75 | loss: 0.12035 | val_0_rmse: 0.30464 |  0:00:13s\n",
      "epoch 76 | loss: 0.10542 | val_0_rmse: 0.26562 |  0:00:13s\n",
      "epoch 77 | loss: 0.11086 | val_0_rmse: 0.30779 |  0:00:13s\n",
      "epoch 78 | loss: 0.09622 | val_0_rmse: 0.27661 |  0:00:13s\n",
      "epoch 79 | loss: 0.10282 | val_0_rmse: 0.30958 |  0:00:13s\n",
      "epoch 80 | loss: 0.11263 | val_0_rmse: 0.27218 |  0:00:14s\n",
      "epoch 81 | loss: 0.11264 | val_0_rmse: 0.28103 |  0:00:14s\n",
      "epoch 82 | loss: 0.10353 | val_0_rmse: 0.30223 |  0:00:14s\n",
      "epoch 83 | loss: 0.10858 | val_0_rmse: 0.35018 |  0:00:14s\n",
      "epoch 84 | loss: 0.1314  | val_0_rmse: 0.29811 |  0:00:14s\n",
      "epoch 85 | loss: 0.10849 | val_0_rmse: 0.28852 |  0:00:14s\n",
      "epoch 86 | loss: 0.10304 | val_0_rmse: 0.314   |  0:00:15s\n",
      "epoch 87 | loss: 0.09765 | val_0_rmse: 0.28351 |  0:00:15s\n",
      "epoch 88 | loss: 0.09754 | val_0_rmse: 0.27828 |  0:00:15s\n",
      "epoch 89 | loss: 0.07958 | val_0_rmse: 0.26146 |  0:00:15s\n",
      "epoch 90 | loss: 0.09579 | val_0_rmse: 0.28885 |  0:00:15s\n",
      "epoch 91 | loss: 0.12104 | val_0_rmse: 0.28292 |  0:00:15s\n",
      "epoch 92 | loss: 0.10114 | val_0_rmse: 0.34282 |  0:00:16s\n",
      "epoch 93 | loss: 0.13388 | val_0_rmse: 0.2898  |  0:00:16s\n",
      "epoch 94 | loss: 0.10224 | val_0_rmse: 0.30901 |  0:00:16s\n",
      "epoch 95 | loss: 0.14464 | val_0_rmse: 0.291   |  0:00:16s\n",
      "epoch 96 | loss: 0.09386 | val_0_rmse: 0.28063 |  0:00:16s\n",
      "epoch 97 | loss: 0.1139  | val_0_rmse: 0.27514 |  0:00:16s\n",
      "epoch 98 | loss: 0.08346 | val_0_rmse: 0.28202 |  0:00:17s\n",
      "epoch 99 | loss: 0.09008 | val_0_rmse: 0.29169 |  0:00:17s\n",
      "epoch 100| loss: 0.07477 | val_0_rmse: 0.27169 |  0:00:17s\n",
      "epoch 101| loss: 0.06553 | val_0_rmse: 0.28176 |  0:00:17s\n",
      "epoch 102| loss: 0.0704  | val_0_rmse: 0.28378 |  0:00:17s\n",
      "epoch 103| loss: 0.08481 | val_0_rmse: 0.28976 |  0:00:18s\n",
      "epoch 104| loss: 0.0849  | val_0_rmse: 0.27118 |  0:00:18s\n",
      "epoch 105| loss: 0.06858 | val_0_rmse: 0.27841 |  0:00:18s\n",
      "epoch 106| loss: 0.07063 | val_0_rmse: 0.2669  |  0:00:18s\n",
      "epoch 107| loss: 0.07433 | val_0_rmse: 0.25578 |  0:00:18s\n",
      "epoch 108| loss: 0.06391 | val_0_rmse: 0.252   |  0:00:18s\n",
      "epoch 109| loss: 0.06463 | val_0_rmse: 0.27237 |  0:00:19s\n",
      "epoch 110| loss: 0.0678  | val_0_rmse: 0.26698 |  0:00:19s\n",
      "epoch 111| loss: 0.06412 | val_0_rmse: 0.25237 |  0:00:19s\n",
      "epoch 112| loss: 0.06129 | val_0_rmse: 0.24731 |  0:00:19s\n",
      "epoch 113| loss: 0.06173 | val_0_rmse: 0.26541 |  0:00:19s\n",
      "epoch 114| loss: 0.06161 | val_0_rmse: 0.25311 |  0:00:19s\n",
      "epoch 115| loss: 0.05199 | val_0_rmse: 0.25496 |  0:00:20s\n",
      "epoch 116| loss: 0.05988 | val_0_rmse: 0.26306 |  0:00:20s\n",
      "epoch 117| loss: 0.05381 | val_0_rmse: 0.2598  |  0:00:20s\n",
      "epoch 118| loss: 0.06548 | val_0_rmse: 0.27231 |  0:00:20s\n",
      "epoch 119| loss: 0.06508 | val_0_rmse: 0.25819 |  0:00:20s\n",
      "epoch 120| loss: 0.06215 | val_0_rmse: 0.24376 |  0:00:20s\n",
      "epoch 121| loss: 0.05559 | val_0_rmse: 0.26625 |  0:00:21s\n",
      "epoch 122| loss: 0.05761 | val_0_rmse: 0.24572 |  0:00:21s\n",
      "epoch 123| loss: 0.06002 | val_0_rmse: 0.24365 |  0:00:21s\n",
      "epoch 124| loss: 0.05888 | val_0_rmse: 0.25024 |  0:00:21s\n",
      "epoch 125| loss: 0.05871 | val_0_rmse: 0.26681 |  0:00:21s\n",
      "epoch 126| loss: 0.06785 | val_0_rmse: 0.23777 |  0:00:21s\n",
      "epoch 127| loss: 0.0548  | val_0_rmse: 0.26844 |  0:00:22s\n",
      "epoch 128| loss: 0.05646 | val_0_rmse: 0.24969 |  0:00:22s\n",
      "epoch 129| loss: 0.05374 | val_0_rmse: 0.23875 |  0:00:22s\n",
      "epoch 130| loss: 0.05142 | val_0_rmse: 0.26114 |  0:00:22s\n",
      "epoch 131| loss: 0.06283 | val_0_rmse: 0.24761 |  0:00:22s\n",
      "epoch 132| loss: 0.08636 | val_0_rmse: 0.25986 |  0:00:23s\n",
      "epoch 133| loss: 0.06371 | val_0_rmse: 0.23792 |  0:00:23s\n",
      "epoch 134| loss: 0.04578 | val_0_rmse: 0.23683 |  0:00:23s\n",
      "epoch 135| loss: 0.06867 | val_0_rmse: 0.28302 |  0:00:23s\n",
      "epoch 136| loss: 0.06292 | val_0_rmse: 0.22816 |  0:00:23s\n",
      "epoch 137| loss: 0.05213 | val_0_rmse: 0.26328 |  0:00:23s\n",
      "epoch 138| loss: 0.05992 | val_0_rmse: 0.21397 |  0:00:24s\n",
      "epoch 139| loss: 0.05188 | val_0_rmse: 0.22981 |  0:00:24s\n",
      "epoch 140| loss: 0.04439 | val_0_rmse: 0.23043 |  0:00:24s\n",
      "epoch 141| loss: 0.04414 | val_0_rmse: 0.21317 |  0:00:24s\n",
      "epoch 142| loss: 0.03896 | val_0_rmse: 0.21561 |  0:00:24s\n",
      "epoch 143| loss: 0.04906 | val_0_rmse: 0.21735 |  0:00:24s\n",
      "epoch 144| loss: 0.04258 | val_0_rmse: 0.22887 |  0:00:25s\n",
      "epoch 145| loss: 0.04521 | val_0_rmse: 0.23507 |  0:00:25s\n",
      "epoch 146| loss: 0.03831 | val_0_rmse: 0.22555 |  0:00:25s\n",
      "epoch 147| loss: 0.04021 | val_0_rmse: 0.27264 |  0:00:25s\n",
      "epoch 148| loss: 0.04941 | val_0_rmse: 0.23408 |  0:00:25s\n",
      "epoch 149| loss: 0.05126 | val_0_rmse: 0.25912 |  0:00:25s\n",
      "epoch 150| loss: 0.04304 | val_0_rmse: 0.22762 |  0:00:26s\n",
      "epoch 151| loss: 0.0331  | val_0_rmse: 0.24626 |  0:00:26s\n",
      "epoch 152| loss: 0.04972 | val_0_rmse: 0.22619 |  0:00:26s\n",
      "epoch 153| loss: 0.03507 | val_0_rmse: 0.23518 |  0:00:26s\n",
      "epoch 154| loss: 0.04137 | val_0_rmse: 0.21863 |  0:00:26s\n",
      "epoch 155| loss: 0.03692 | val_0_rmse: 0.2217  |  0:00:27s\n",
      "epoch 156| loss: 0.03864 | val_0_rmse: 0.23786 |  0:00:27s\n",
      "epoch 157| loss: 0.03988 | val_0_rmse: 0.23908 |  0:00:27s\n",
      "epoch 158| loss: 0.05224 | val_0_rmse: 0.23511 |  0:00:27s\n",
      "epoch 159| loss: 0.05721 | val_0_rmse: 0.21845 |  0:00:27s\n",
      "epoch 160| loss: 0.05757 | val_0_rmse: 0.21928 |  0:00:27s\n",
      "epoch 161| loss: 0.04549 | val_0_rmse: 0.23674 |  0:00:28s\n",
      "epoch 162| loss: 0.04536 | val_0_rmse: 0.22722 |  0:00:28s\n",
      "epoch 163| loss: 0.03939 | val_0_rmse: 0.21687 |  0:00:28s\n",
      "epoch 164| loss: 0.04643 | val_0_rmse: 0.21774 |  0:00:28s\n",
      "epoch 165| loss: 0.03705 | val_0_rmse: 0.22456 |  0:00:28s\n",
      "epoch 166| loss: 0.03429 | val_0_rmse: 0.21187 |  0:00:28s\n",
      "epoch 167| loss: 0.0363  | val_0_rmse: 0.21793 |  0:00:29s\n",
      "epoch 168| loss: 0.03311 | val_0_rmse: 0.21924 |  0:00:29s\n",
      "epoch 169| loss: 0.03178 | val_0_rmse: 0.22205 |  0:00:29s\n",
      "epoch 170| loss: 0.0352  | val_0_rmse: 0.21213 |  0:00:29s\n",
      "epoch 171| loss: 0.03187 | val_0_rmse: 0.22523 |  0:00:29s\n",
      "epoch 172| loss: 0.03241 | val_0_rmse: 0.2269  |  0:00:29s\n",
      "epoch 173| loss: 0.03503 | val_0_rmse: 0.2263  |  0:00:29s\n",
      "epoch 174| loss: 0.04674 | val_0_rmse: 0.23079 |  0:00:30s\n",
      "epoch 175| loss: 0.03542 | val_0_rmse: 0.23208 |  0:00:30s\n",
      "epoch 176| loss: 0.04098 | val_0_rmse: 0.2224  |  0:00:30s\n",
      "epoch 177| loss: 0.03684 | val_0_rmse: 0.20774 |  0:00:30s\n",
      "epoch 178| loss: 0.03371 | val_0_rmse: 0.21625 |  0:00:30s\n",
      "epoch 179| loss: 0.03114 | val_0_rmse: 0.2117  |  0:00:30s\n",
      "epoch 180| loss: 0.03089 | val_0_rmse: 0.21059 |  0:00:31s\n",
      "epoch 181| loss: 0.03266 | val_0_rmse: 0.23413 |  0:00:31s\n",
      "epoch 182| loss: 0.04163 | val_0_rmse: 0.21798 |  0:00:31s\n",
      "epoch 183| loss: 0.02962 | val_0_rmse: 0.20953 |  0:00:31s\n",
      "epoch 184| loss: 0.02871 | val_0_rmse: 0.21307 |  0:00:31s\n",
      "epoch 185| loss: 0.02961 | val_0_rmse: 0.21461 |  0:00:31s\n",
      "epoch 186| loss: 0.02889 | val_0_rmse: 0.23237 |  0:00:32s\n",
      "epoch 187| loss: 0.03926 | val_0_rmse: 0.21823 |  0:00:32s\n",
      "epoch 188| loss: 0.04821 | val_0_rmse: 0.21352 |  0:00:32s\n",
      "epoch 189| loss: 0.04504 | val_0_rmse: 0.22212 |  0:00:32s\n",
      "epoch 190| loss: 0.03245 | val_0_rmse: 0.2118  |  0:00:32s\n",
      "epoch 191| loss: 0.03178 | val_0_rmse: 0.20419 |  0:00:32s\n",
      "epoch 192| loss: 0.02848 | val_0_rmse: 0.20607 |  0:00:33s\n",
      "epoch 193| loss: 0.02966 | val_0_rmse: 0.21241 |  0:00:33s\n",
      "epoch 194| loss: 0.02988 | val_0_rmse: 0.2164  |  0:00:33s\n",
      "epoch 195| loss: 0.03138 | val_0_rmse: 0.20214 |  0:00:33s\n",
      "epoch 196| loss: 0.03004 | val_0_rmse: 0.19641 |  0:00:33s\n",
      "epoch 197| loss: 0.02626 | val_0_rmse: 0.19017 |  0:00:34s\n",
      "epoch 198| loss: 0.02583 | val_0_rmse: 0.19697 |  0:00:34s\n",
      "epoch 199| loss: 0.0241  | val_0_rmse: 0.19289 |  0:00:34s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 197 and best_val_0_rmse = 0.19017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:17:30,667] Trial 27 finished with value: 0.1901743358288376 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 4, 'gamma': 1.268832759462925, 'n_independent': 1, 'n_shared': 2, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.008071128088578676, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 027 | rmse_log=0.19017 | RMSE$=41,466 | MAE$=24,570 | MAPE=14.14% | n_d/n_a=48/24 steps=4 lr=0.00807 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 152.62848| val_0_rmse: 11.17572|  0:00:00s\n",
      "epoch 1  | loss: 117.45031| val_0_rmse: 10.41048|  0:00:00s\n",
      "epoch 2  | loss: 90.64967| val_0_rmse: 9.57784 |  0:00:00s\n",
      "epoch 3  | loss: 71.79923| val_0_rmse: 8.76813 |  0:00:00s\n",
      "epoch 4  | loss: 52.01056| val_0_rmse: 7.94707 |  0:00:00s\n",
      "epoch 5  | loss: 41.53391| val_0_rmse: 7.10334 |  0:00:01s\n",
      "epoch 6  | loss: 31.77468| val_0_rmse: 6.33356 |  0:00:01s\n",
      "epoch 7  | loss: 28.1546 | val_0_rmse: 5.58297 |  0:00:01s\n",
      "epoch 8  | loss: 26.10384| val_0_rmse: 4.90903 |  0:00:01s\n",
      "epoch 9  | loss: 21.62765| val_0_rmse: 4.29599 |  0:00:01s\n",
      "epoch 10 | loss: 16.69138| val_0_rmse: 3.86921 |  0:00:01s\n",
      "epoch 11 | loss: 14.7213 | val_0_rmse: 3.71456 |  0:00:01s\n",
      "epoch 12 | loss: 12.30901| val_0_rmse: 3.69708 |  0:00:02s\n",
      "epoch 13 | loss: 7.09148 | val_0_rmse: 3.6643  |  0:00:02s\n",
      "epoch 14 | loss: 5.47842 | val_0_rmse: 3.42967 |  0:00:02s\n",
      "epoch 15 | loss: 4.44488 | val_0_rmse: 2.97968 |  0:00:02s\n",
      "epoch 16 | loss: 3.64145 | val_0_rmse: 2.33402 |  0:00:02s\n",
      "epoch 17 | loss: 2.71676 | val_0_rmse: 1.95183 |  0:00:02s\n",
      "epoch 18 | loss: 1.61026 | val_0_rmse: 1.8841  |  0:00:03s\n",
      "epoch 19 | loss: 1.65766 | val_0_rmse: 1.61723 |  0:00:03s\n",
      "epoch 20 | loss: 1.20278 | val_0_rmse: 1.35647 |  0:00:03s\n",
      "epoch 21 | loss: 1.11439 | val_0_rmse: 1.38374 |  0:00:03s\n",
      "epoch 22 | loss: 0.95988 | val_0_rmse: 1.15257 |  0:00:03s\n",
      "epoch 23 | loss: 0.64378 | val_0_rmse: 1.20006 |  0:00:03s\n",
      "epoch 24 | loss: 0.73966 | val_0_rmse: 1.145   |  0:00:04s\n",
      "epoch 25 | loss: 0.74312 | val_0_rmse: 1.32186 |  0:00:04s\n",
      "epoch 26 | loss: 0.66295 | val_0_rmse: 0.97936 |  0:00:04s\n",
      "epoch 27 | loss: 0.70808 | val_0_rmse: 0.96829 |  0:00:04s\n",
      "epoch 28 | loss: 0.51736 | val_0_rmse: 1.13378 |  0:00:04s\n",
      "epoch 29 | loss: 0.45932 | val_0_rmse: 0.62547 |  0:00:04s\n",
      "epoch 30 | loss: 0.54604 | val_0_rmse: 0.87421 |  0:00:05s\n",
      "epoch 31 | loss: 0.39293 | val_0_rmse: 0.91895 |  0:00:05s\n",
      "epoch 32 | loss: 0.38134 | val_0_rmse: 0.56832 |  0:00:05s\n",
      "epoch 33 | loss: 0.52517 | val_0_rmse: 0.78836 |  0:00:05s\n",
      "epoch 34 | loss: 0.49198 | val_0_rmse: 0.85187 |  0:00:05s\n",
      "epoch 35 | loss: 0.37993 | val_0_rmse: 0.67346 |  0:00:05s\n",
      "epoch 36 | loss: 0.28351 | val_0_rmse: 1.01794 |  0:00:05s\n",
      "epoch 37 | loss: 0.47434 | val_0_rmse: 0.67257 |  0:00:06s\n",
      "epoch 38 | loss: 0.4521  | val_0_rmse: 0.59071 |  0:00:06s\n",
      "epoch 39 | loss: 0.29916 | val_0_rmse: 0.88059 |  0:00:06s\n",
      "epoch 40 | loss: 0.36283 | val_0_rmse: 0.61659 |  0:00:06s\n",
      "epoch 41 | loss: 0.22051 | val_0_rmse: 0.55286 |  0:00:06s\n",
      "epoch 42 | loss: 0.19402 | val_0_rmse: 0.57545 |  0:00:06s\n",
      "epoch 43 | loss: 0.21242 | val_0_rmse: 0.45442 |  0:00:06s\n",
      "epoch 44 | loss: 0.20843 | val_0_rmse: 0.64557 |  0:00:07s\n",
      "epoch 45 | loss: 0.2531  | val_0_rmse: 0.31264 |  0:00:07s\n",
      "epoch 46 | loss: 0.25943 | val_0_rmse: 0.3592  |  0:00:07s\n",
      "epoch 47 | loss: 0.17692 | val_0_rmse: 0.65336 |  0:00:07s\n",
      "epoch 48 | loss: 0.22336 | val_0_rmse: 0.38002 |  0:00:07s\n",
      "epoch 49 | loss: 0.20485 | val_0_rmse: 0.38745 |  0:00:07s\n",
      "epoch 50 | loss: 0.15977 | val_0_rmse: 0.63131 |  0:00:08s\n",
      "epoch 51 | loss: 0.17723 | val_0_rmse: 0.30352 |  0:00:08s\n",
      "epoch 52 | loss: 0.14936 | val_0_rmse: 0.2918  |  0:00:08s\n",
      "epoch 53 | loss: 0.13795 | val_0_rmse: 0.55532 |  0:00:08s\n",
      "epoch 54 | loss: 0.16774 | val_0_rmse: 0.32021 |  0:00:08s\n",
      "epoch 55 | loss: 0.15037 | val_0_rmse: 0.28845 |  0:00:08s\n",
      "epoch 56 | loss: 0.1207  | val_0_rmse: 0.47778 |  0:00:09s\n",
      "epoch 57 | loss: 0.15315 | val_0_rmse: 0.26733 |  0:00:09s\n",
      "epoch 58 | loss: 0.13644 | val_0_rmse: 0.24708 |  0:00:09s\n",
      "epoch 59 | loss: 0.13394 | val_0_rmse: 0.37716 |  0:00:09s\n",
      "epoch 60 | loss: 0.14391 | val_0_rmse: 0.23442 |  0:00:09s\n",
      "epoch 61 | loss: 0.14139 | val_0_rmse: 0.25873 |  0:00:09s\n",
      "epoch 62 | loss: 0.13717 | val_0_rmse: 0.35574 |  0:00:10s\n",
      "epoch 63 | loss: 0.15224 | val_0_rmse: 0.22605 |  0:00:10s\n",
      "epoch 64 | loss: 0.11994 | val_0_rmse: 0.23917 |  0:00:10s\n",
      "epoch 65 | loss: 0.14111 | val_0_rmse: 0.34202 |  0:00:10s\n",
      "epoch 66 | loss: 0.10978 | val_0_rmse: 0.24494 |  0:00:10s\n",
      "epoch 67 | loss: 0.14664 | val_0_rmse: 0.2444  |  0:00:10s\n",
      "epoch 68 | loss: 0.13002 | val_0_rmse: 0.31442 |  0:00:10s\n",
      "epoch 69 | loss: 0.10758 | val_0_rmse: 0.2372  |  0:00:11s\n",
      "epoch 70 | loss: 0.08136 | val_0_rmse: 0.26762 |  0:00:11s\n",
      "epoch 71 | loss: 0.08119 | val_0_rmse: 0.24288 |  0:00:11s\n",
      "epoch 72 | loss: 0.12637 | val_0_rmse: 0.23811 |  0:00:11s\n",
      "epoch 73 | loss: 0.10841 | val_0_rmse: 0.3988  |  0:00:11s\n",
      "epoch 74 | loss: 0.18404 | val_0_rmse: 0.24508 |  0:00:11s\n",
      "epoch 75 | loss: 0.07911 | val_0_rmse: 0.26717 |  0:00:11s\n",
      "epoch 76 | loss: 0.10757 | val_0_rmse: 0.27864 |  0:00:12s\n",
      "epoch 77 | loss: 0.11786 | val_0_rmse: 0.25377 |  0:00:12s\n",
      "epoch 78 | loss: 0.14786 | val_0_rmse: 0.25418 |  0:00:12s\n",
      "epoch 79 | loss: 0.12207 | val_0_rmse: 0.3519  |  0:00:12s\n",
      "epoch 80 | loss: 0.16441 | val_0_rmse: 0.22579 |  0:00:12s\n",
      "epoch 81 | loss: 0.08098 | val_0_rmse: 0.23831 |  0:00:12s\n",
      "epoch 82 | loss: 0.08642 | val_0_rmse: 0.31466 |  0:00:13s\n",
      "epoch 83 | loss: 0.10518 | val_0_rmse: 0.22848 |  0:00:13s\n",
      "epoch 84 | loss: 0.09501 | val_0_rmse: 0.2758  |  0:00:13s\n",
      "epoch 85 | loss: 0.10528 | val_0_rmse: 0.29151 |  0:00:13s\n",
      "epoch 86 | loss: 0.08398 | val_0_rmse: 0.25615 |  0:00:13s\n",
      "epoch 87 | loss: 0.08867 | val_0_rmse: 0.23642 |  0:00:13s\n",
      "epoch 88 | loss: 0.0753  | val_0_rmse: 0.23526 |  0:00:13s\n",
      "epoch 89 | loss: 0.07017 | val_0_rmse: 0.27509 |  0:00:14s\n",
      "epoch 90 | loss: 0.07964 | val_0_rmse: 0.24417 |  0:00:14s\n",
      "epoch 91 | loss: 0.0997  | val_0_rmse: 0.28671 |  0:00:14s\n",
      "epoch 92 | loss: 0.12478 | val_0_rmse: 0.24213 |  0:00:14s\n",
      "epoch 93 | loss: 0.06734 | val_0_rmse: 0.24862 |  0:00:14s\n",
      "epoch 94 | loss: 0.11723 | val_0_rmse: 0.32238 |  0:00:14s\n",
      "epoch 95 | loss: 0.09557 | val_0_rmse: 0.24096 |  0:00:14s\n",
      "epoch 96 | loss: 0.06773 | val_0_rmse: 0.27321 |  0:00:15s\n",
      "epoch 97 | loss: 0.07861 | val_0_rmse: 0.26279 |  0:00:15s\n",
      "epoch 98 | loss: 0.09469 | val_0_rmse: 0.27123 |  0:00:15s\n",
      "epoch 99 | loss: 0.07156 | val_0_rmse: 0.22405 |  0:00:15s\n",
      "epoch 100| loss: 0.07006 | val_0_rmse: 0.26566 |  0:00:15s\n",
      "epoch 101| loss: 0.07464 | val_0_rmse: 0.22663 |  0:00:15s\n",
      "epoch 102| loss: 0.05577 | val_0_rmse: 0.28133 |  0:00:16s\n",
      "epoch 103| loss: 0.06474 | val_0_rmse: 0.25309 |  0:00:16s\n",
      "epoch 104| loss: 0.05688 | val_0_rmse: 0.2425  |  0:00:16s\n",
      "epoch 105| loss: 0.07586 | val_0_rmse: 0.27422 |  0:00:16s\n",
      "epoch 106| loss: 0.06176 | val_0_rmse: 0.22807 |  0:00:16s\n",
      "epoch 107| loss: 0.06975 | val_0_rmse: 0.23299 |  0:00:16s\n",
      "epoch 108| loss: 0.05196 | val_0_rmse: 0.23758 |  0:00:17s\n",
      "epoch 109| loss: 0.05744 | val_0_rmse: 0.24059 |  0:00:17s\n",
      "epoch 110| loss: 0.07587 | val_0_rmse: 0.30508 |  0:00:17s\n",
      "epoch 111| loss: 0.09246 | val_0_rmse: 0.30484 |  0:00:17s\n",
      "epoch 112| loss: 0.074   | val_0_rmse: 0.2144  |  0:00:17s\n",
      "epoch 113| loss: 0.05865 | val_0_rmse: 0.22216 |  0:00:17s\n",
      "epoch 114| loss: 0.05023 | val_0_rmse: 0.22133 |  0:00:17s\n",
      "epoch 115| loss: 0.05098 | val_0_rmse: 0.22394 |  0:00:18s\n",
      "epoch 116| loss: 0.04971 | val_0_rmse: 0.21406 |  0:00:18s\n",
      "epoch 117| loss: 0.06063 | val_0_rmse: 0.24222 |  0:00:18s\n",
      "epoch 118| loss: 0.05566 | val_0_rmse: 0.20676 |  0:00:18s\n",
      "epoch 119| loss: 0.04253 | val_0_rmse: 0.21529 |  0:00:18s\n",
      "epoch 120| loss: 0.04195 | val_0_rmse: 0.21968 |  0:00:18s\n",
      "epoch 121| loss: 0.04864 | val_0_rmse: 0.23471 |  0:00:18s\n",
      "epoch 122| loss: 0.0628  | val_0_rmse: 0.21355 |  0:00:19s\n",
      "epoch 123| loss: 0.04182 | val_0_rmse: 0.21564 |  0:00:19s\n",
      "epoch 124| loss: 0.04251 | val_0_rmse: 0.20982 |  0:00:19s\n",
      "epoch 125| loss: 0.04643 | val_0_rmse: 0.20624 |  0:00:19s\n",
      "epoch 126| loss: 0.04048 | val_0_rmse: 0.2223  |  0:00:19s\n",
      "epoch 127| loss: 0.04631 | val_0_rmse: 0.21432 |  0:00:19s\n",
      "epoch 128| loss: 0.04978 | val_0_rmse: 0.20487 |  0:00:20s\n",
      "epoch 129| loss: 0.04324 | val_0_rmse: 0.22226 |  0:00:20s\n",
      "epoch 130| loss: 0.04887 | val_0_rmse: 0.24499 |  0:00:20s\n",
      "epoch 131| loss: 0.05509 | val_0_rmse: 0.22369 |  0:00:20s\n",
      "epoch 132| loss: 0.04841 | val_0_rmse: 0.21685 |  0:00:20s\n",
      "epoch 133| loss: 0.03896 | val_0_rmse: 0.20932 |  0:00:20s\n",
      "epoch 134| loss: 0.0435  | val_0_rmse: 0.23209 |  0:00:20s\n",
      "epoch 135| loss: 0.05298 | val_0_rmse: 0.22261 |  0:00:21s\n",
      "epoch 136| loss: 0.04807 | val_0_rmse: 0.21801 |  0:00:21s\n",
      "epoch 137| loss: 0.03951 | val_0_rmse: 0.20557 |  0:00:21s\n",
      "epoch 138| loss: 0.04124 | val_0_rmse: 0.23741 |  0:00:21s\n",
      "epoch 139| loss: 0.05694 | val_0_rmse: 0.21144 |  0:00:21s\n",
      "epoch 140| loss: 0.03744 | val_0_rmse: 0.21049 |  0:00:21s\n",
      "epoch 141| loss: 0.03757 | val_0_rmse: 0.20994 |  0:00:22s\n",
      "epoch 142| loss: 0.03998 | val_0_rmse: 0.20874 |  0:00:22s\n",
      "epoch 143| loss: 0.04055 | val_0_rmse: 0.23037 |  0:00:22s\n",
      "epoch 144| loss: 0.04802 | val_0_rmse: 0.24115 |  0:00:22s\n",
      "epoch 145| loss: 0.05738 | val_0_rmse: 0.21896 |  0:00:22s\n",
      "epoch 146| loss: 0.05241 | val_0_rmse: 0.20817 |  0:00:22s\n",
      "epoch 147| loss: 0.04433 | val_0_rmse: 0.24484 |  0:00:22s\n",
      "epoch 148| loss: 0.05258 | val_0_rmse: 0.21685 |  0:00:23s\n",
      "epoch 149| loss: 0.03657 | val_0_rmse: 0.24021 |  0:00:23s\n",
      "epoch 150| loss: 0.04473 | val_0_rmse: 0.23553 |  0:00:23s\n",
      "epoch 151| loss: 0.04259 | val_0_rmse: 0.2209  |  0:00:23s\n",
      "epoch 152| loss: 0.03645 | val_0_rmse: 0.22256 |  0:00:23s\n",
      "epoch 153| loss: 0.05047 | val_0_rmse: 0.22962 |  0:00:23s\n",
      "\n",
      "Early stopping occurred at epoch 153 with best_epoch = 128 and best_val_0_rmse = 0.20487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:17:54,813] Trial 28 finished with value: 0.20487062462460215 and parameters: {'n_d': 48, 'n_a': 16, 'n_steps': 3, 'gamma': 1.4627067458144607, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.013897389215412774, 'batch_size': 1024, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 028 | rmse_log=0.20487 | RMSE$=44,678 | MAE$=26,567 | MAPE=15.04% | n_d/n_a=48/16 steps=3 lr=0.01390 batch=1024 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 118.3209| val_0_rmse: 10.83817|  0:00:00s\n",
      "epoch 1  | loss: 95.39224| val_0_rmse: 10.53042|  0:00:00s\n",
      "epoch 2  | loss: 80.30591| val_0_rmse: 10.14377|  0:00:00s\n",
      "epoch 3  | loss: 63.55337| val_0_rmse: 9.61765 |  0:00:01s\n",
      "epoch 4  | loss: 51.67456| val_0_rmse: 8.98626 |  0:00:01s\n",
      "epoch 5  | loss: 42.98328| val_0_rmse: 8.40057 |  0:00:01s\n",
      "epoch 6  | loss: 33.52482| val_0_rmse: 7.74622 |  0:00:02s\n",
      "epoch 7  | loss: 26.72998| val_0_rmse: 7.0242  |  0:00:02s\n",
      "epoch 8  | loss: 26.83084| val_0_rmse: 6.38971 |  0:00:02s\n",
      "epoch 9  | loss: 21.42695| val_0_rmse: 5.77007 |  0:00:02s\n",
      "epoch 10 | loss: 21.58331| val_0_rmse: 5.32571 |  0:00:03s\n",
      "epoch 11 | loss: 16.92756| val_0_rmse: 4.94716 |  0:00:03s\n",
      "epoch 12 | loss: 13.67617| val_0_rmse: 4.74979 |  0:00:03s\n",
      "epoch 13 | loss: 15.85065| val_0_rmse: 4.55635 |  0:00:03s\n",
      "epoch 14 | loss: 11.32472| val_0_rmse: 4.24667 |  0:00:04s\n",
      "epoch 15 | loss: 9.40759 | val_0_rmse: 3.86949 |  0:00:04s\n",
      "epoch 16 | loss: 7.20028 | val_0_rmse: 3.30513 |  0:00:04s\n",
      "epoch 17 | loss: 6.62888 | val_0_rmse: 2.57992 |  0:00:05s\n",
      "epoch 18 | loss: 6.3949  | val_0_rmse: 1.94549 |  0:00:05s\n",
      "epoch 19 | loss: 4.50893 | val_0_rmse: 1.77356 |  0:00:05s\n",
      "epoch 20 | loss: 3.93519 | val_0_rmse: 1.79905 |  0:00:05s\n",
      "epoch 21 | loss: 3.32089 | val_0_rmse: 1.70439 |  0:00:06s\n",
      "epoch 22 | loss: 3.83495 | val_0_rmse: 1.13232 |  0:00:06s\n",
      "epoch 23 | loss: 3.21485 | val_0_rmse: 0.84818 |  0:00:06s\n",
      "epoch 24 | loss: 2.93689 | val_0_rmse: 1.07175 |  0:00:07s\n",
      "epoch 25 | loss: 2.25938 | val_0_rmse: 1.20064 |  0:00:07s\n",
      "epoch 26 | loss: 2.07486 | val_0_rmse: 0.76036 |  0:00:07s\n",
      "epoch 27 | loss: 2.13415 | val_0_rmse: 0.67767 |  0:00:07s\n",
      "epoch 28 | loss: 2.2272  | val_0_rmse: 0.82028 |  0:00:07s\n",
      "epoch 29 | loss: 1.73537 | val_0_rmse: 0.98381 |  0:00:08s\n",
      "epoch 30 | loss: 1.91489 | val_0_rmse: 0.82643 |  0:00:08s\n",
      "epoch 31 | loss: 1.78222 | val_0_rmse: 0.56873 |  0:00:08s\n",
      "epoch 32 | loss: 1.8428  | val_0_rmse: 0.56716 |  0:00:09s\n",
      "epoch 33 | loss: 1.86943 | val_0_rmse: 0.85807 |  0:00:09s\n",
      "epoch 34 | loss: 1.6596  | val_0_rmse: 0.94167 |  0:00:09s\n",
      "epoch 35 | loss: 1.7525  | val_0_rmse: 0.50709 |  0:00:09s\n",
      "epoch 36 | loss: 1.31092 | val_0_rmse: 0.60805 |  0:00:10s\n",
      "epoch 37 | loss: 1.37351 | val_0_rmse: 0.79944 |  0:00:10s\n",
      "epoch 38 | loss: 1.46067 | val_0_rmse: 0.65934 |  0:00:10s\n",
      "epoch 39 | loss: 1.01135 | val_0_rmse: 0.54324 |  0:00:10s\n",
      "epoch 40 | loss: 1.37134 | val_0_rmse: 0.97918 |  0:00:11s\n",
      "epoch 41 | loss: 1.40245 | val_0_rmse: 0.93773 |  0:00:11s\n",
      "epoch 42 | loss: 1.00791 | val_0_rmse: 0.52972 |  0:00:11s\n",
      "epoch 43 | loss: 1.08157 | val_0_rmse: 0.52504 |  0:00:12s\n",
      "epoch 44 | loss: 0.93697 | val_0_rmse: 0.73735 |  0:00:12s\n",
      "epoch 45 | loss: 1.19536 | val_0_rmse: 0.56375 |  0:00:12s\n",
      "epoch 46 | loss: 1.03986 | val_0_rmse: 0.58553 |  0:00:12s\n",
      "epoch 47 | loss: 1.26931 | val_0_rmse: 0.55084 |  0:00:13s\n",
      "epoch 48 | loss: 1.14982 | val_0_rmse: 1.07726 |  0:00:13s\n",
      "epoch 49 | loss: 1.22263 | val_0_rmse: 0.59868 |  0:00:13s\n",
      "epoch 50 | loss: 1.15712 | val_0_rmse: 0.48508 |  0:00:13s\n",
      "epoch 51 | loss: 0.85427 | val_0_rmse: 0.69131 |  0:00:14s\n",
      "epoch 52 | loss: 1.04945 | val_0_rmse: 0.68225 |  0:00:14s\n",
      "epoch 53 | loss: 0.90947 | val_0_rmse: 0.50081 |  0:00:14s\n",
      "epoch 54 | loss: 0.72747 | val_0_rmse: 0.6671  |  0:00:14s\n",
      "epoch 55 | loss: 0.80475 | val_0_rmse: 0.59489 |  0:00:15s\n",
      "epoch 56 | loss: 0.85821 | val_0_rmse: 0.42783 |  0:00:15s\n",
      "epoch 57 | loss: 0.79446 | val_0_rmse: 0.47132 |  0:00:15s\n",
      "epoch 58 | loss: 0.68919 | val_0_rmse: 0.51967 |  0:00:15s\n",
      "epoch 59 | loss: 0.69741 | val_0_rmse: 0.48986 |  0:00:16s\n",
      "epoch 60 | loss: 0.67541 | val_0_rmse: 0.49347 |  0:00:16s\n",
      "epoch 61 | loss: 0.52554 | val_0_rmse: 0.50559 |  0:00:16s\n",
      "epoch 62 | loss: 0.63576 | val_0_rmse: 0.50389 |  0:00:17s\n",
      "epoch 63 | loss: 0.92032 | val_0_rmse: 0.46753 |  0:00:17s\n",
      "epoch 64 | loss: 0.64236 | val_0_rmse: 0.45591 |  0:00:17s\n",
      "epoch 65 | loss: 0.5964  | val_0_rmse: 0.42815 |  0:00:17s\n",
      "epoch 66 | loss: 0.54546 | val_0_rmse: 0.4223  |  0:00:18s\n",
      "epoch 67 | loss: 0.64234 | val_0_rmse: 0.5592  |  0:00:18s\n",
      "epoch 68 | loss: 0.80729 | val_0_rmse: 0.44326 |  0:00:18s\n",
      "epoch 69 | loss: 0.86158 | val_0_rmse: 0.4304  |  0:00:18s\n",
      "epoch 70 | loss: 0.81899 | val_0_rmse: 0.71757 |  0:00:19s\n",
      "epoch 71 | loss: 0.96207 | val_0_rmse: 0.61856 |  0:00:19s\n",
      "epoch 72 | loss: 0.6238  | val_0_rmse: 0.45656 |  0:00:19s\n",
      "epoch 73 | loss: 0.66785 | val_0_rmse: 0.66147 |  0:00:19s\n",
      "epoch 74 | loss: 0.69463 | val_0_rmse: 0.61505 |  0:00:20s\n",
      "epoch 75 | loss: 0.59268 | val_0_rmse: 0.49572 |  0:00:20s\n",
      "epoch 76 | loss: 1.0445  | val_0_rmse: 0.61707 |  0:00:20s\n",
      "epoch 77 | loss: 0.92194 | val_0_rmse: 0.53256 |  0:00:20s\n",
      "epoch 78 | loss: 0.5478  | val_0_rmse: 0.57885 |  0:00:21s\n",
      "epoch 79 | loss: 0.60491 | val_0_rmse: 0.40934 |  0:00:21s\n",
      "epoch 80 | loss: 0.55757 | val_0_rmse: 0.42689 |  0:00:21s\n",
      "epoch 81 | loss: 0.44781 | val_0_rmse: 0.48385 |  0:00:22s\n",
      "epoch 82 | loss: 0.57434 | val_0_rmse: 0.49666 |  0:00:22s\n",
      "epoch 83 | loss: 0.4034  | val_0_rmse: 0.42385 |  0:00:22s\n",
      "epoch 84 | loss: 0.51117 | val_0_rmse: 0.49157 |  0:00:22s\n",
      "epoch 85 | loss: 0.58459 | val_0_rmse: 0.53193 |  0:00:23s\n",
      "epoch 86 | loss: 0.46792 | val_0_rmse: 0.45893 |  0:00:23s\n",
      "epoch 87 | loss: 0.57097 | val_0_rmse: 0.42481 |  0:00:23s\n",
      "epoch 88 | loss: 0.43322 | val_0_rmse: 0.53405 |  0:00:23s\n",
      "epoch 89 | loss: 0.43159 | val_0_rmse: 0.46066 |  0:00:24s\n",
      "epoch 90 | loss: 0.49912 | val_0_rmse: 0.42687 |  0:00:24s\n",
      "epoch 91 | loss: 0.41734 | val_0_rmse: 0.44418 |  0:00:24s\n",
      "epoch 92 | loss: 0.39942 | val_0_rmse: 0.57764 |  0:00:24s\n",
      "epoch 93 | loss: 0.49361 | val_0_rmse: 0.60042 |  0:00:25s\n",
      "epoch 94 | loss: 0.49413 | val_0_rmse: 0.51909 |  0:00:25s\n",
      "epoch 95 | loss: 0.39682 | val_0_rmse: 0.46927 |  0:00:25s\n",
      "epoch 96 | loss: 0.38855 | val_0_rmse: 0.43904 |  0:00:26s\n",
      "epoch 97 | loss: 0.3296  | val_0_rmse: 0.45098 |  0:00:26s\n",
      "epoch 98 | loss: 0.43709 | val_0_rmse: 0.42853 |  0:00:26s\n",
      "epoch 99 | loss: 0.3224  | val_0_rmse: 0.36281 |  0:00:26s\n",
      "epoch 100| loss: 0.41337 | val_0_rmse: 0.3491  |  0:00:27s\n",
      "epoch 101| loss: 0.32518 | val_0_rmse: 0.46678 |  0:00:27s\n",
      "epoch 102| loss: 0.41393 | val_0_rmse: 0.43188 |  0:00:27s\n",
      "epoch 103| loss: 0.3458  | val_0_rmse: 0.44829 |  0:00:27s\n",
      "epoch 104| loss: 0.4066  | val_0_rmse: 0.38613 |  0:00:28s\n",
      "epoch 105| loss: 0.29075 | val_0_rmse: 0.41357 |  0:00:28s\n",
      "epoch 106| loss: 0.39539 | val_0_rmse: 0.40081 |  0:00:28s\n",
      "epoch 107| loss: 0.36563 | val_0_rmse: 0.42007 |  0:00:29s\n",
      "epoch 108| loss: 0.30338 | val_0_rmse: 0.39518 |  0:00:29s\n",
      "epoch 109| loss: 0.28277 | val_0_rmse: 0.44162 |  0:00:29s\n",
      "epoch 110| loss: 0.34657 | val_0_rmse: 0.41938 |  0:00:29s\n",
      "epoch 111| loss: 0.3141  | val_0_rmse: 0.39027 |  0:00:30s\n",
      "epoch 112| loss: 0.32873 | val_0_rmse: 0.44672 |  0:00:30s\n",
      "epoch 113| loss: 0.3375  | val_0_rmse: 0.51227 |  0:00:30s\n",
      "epoch 114| loss: 0.25962 | val_0_rmse: 0.4452  |  0:00:30s\n",
      "epoch 115| loss: 0.29038 | val_0_rmse: 0.54152 |  0:00:31s\n",
      "epoch 116| loss: 0.34576 | val_0_rmse: 0.72596 |  0:00:31s\n",
      "epoch 117| loss: 0.40038 | val_0_rmse: 0.54434 |  0:00:31s\n",
      "epoch 118| loss: 0.44577 | val_0_rmse: 0.50268 |  0:00:31s\n",
      "epoch 119| loss: 0.32291 | val_0_rmse: 0.44763 |  0:00:32s\n",
      "epoch 120| loss: 0.24833 | val_0_rmse: 0.47155 |  0:00:32s\n",
      "epoch 121| loss: 0.26239 | val_0_rmse: 0.59335 |  0:00:32s\n",
      "epoch 122| loss: 0.29044 | val_0_rmse: 0.50414 |  0:00:33s\n",
      "epoch 123| loss: 0.36639 | val_0_rmse: 0.48237 |  0:00:33s\n",
      "epoch 124| loss: 0.29936 | val_0_rmse: 0.4859  |  0:00:33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:18:29,127] Trial 29 finished with value: 0.3490970961778581 and parameters: {'n_d': 48, 'n_a': 48, 'n_steps': 5, 'gamma': 1.6694425566637432, 'n_independent': 3, 'n_shared': 2, 'lambda_sparse': 1e-06, 'mask_type': 'sparsemax', 'lr': 0.005126552197212433, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 125| loss: 0.23947 | val_0_rmse: 0.42836 |  0:00:33s\n",
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 100 and best_val_0_rmse = 0.3491\n",
      "Trial 029 | rmse_log=0.34910 | RMSE$=74,758 | MAE$=44,954 | MAPE=25.99% | n_d/n_a=48/48 steps=5 lr=0.00513 batch=512 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 178.39455| val_0_rmse: 11.96659|  0:00:00s\n",
      "epoch 1  | loss: 155.93143| val_0_rmse: 11.49849|  0:00:00s\n",
      "epoch 2  | loss: 139.71091| val_0_rmse: 11.06653|  0:00:00s\n",
      "epoch 3  | loss: 125.92018| val_0_rmse: 10.79683|  0:00:00s\n",
      "epoch 4  | loss: 113.7534| val_0_rmse: 10.5312 |  0:00:00s\n",
      "epoch 5  | loss: 102.30856| val_0_rmse: 10.24133|  0:00:00s\n",
      "epoch 6  | loss: 89.82764| val_0_rmse: 9.93307 |  0:00:01s\n",
      "epoch 7  | loss: 79.90727| val_0_rmse: 9.56691 |  0:00:01s\n",
      "epoch 8  | loss: 69.50249| val_0_rmse: 9.15812 |  0:00:01s\n",
      "epoch 9  | loss: 61.46241| val_0_rmse: 8.68515 |  0:00:01s\n",
      "epoch 10 | loss: 50.56424| val_0_rmse: 8.16918 |  0:00:01s\n",
      "epoch 11 | loss: 41.48265| val_0_rmse: 7.62403 |  0:00:01s\n",
      "epoch 12 | loss: 35.62968| val_0_rmse: 7.04106 |  0:00:01s\n",
      "epoch 13 | loss: 27.95767| val_0_rmse: 6.39853 |  0:00:02s\n",
      "epoch 14 | loss: 22.49967| val_0_rmse: 5.7702  |  0:00:02s\n",
      "epoch 15 | loss: 19.98667| val_0_rmse: 5.13976 |  0:00:02s\n",
      "epoch 16 | loss: 18.43485| val_0_rmse: 4.52816 |  0:00:02s\n",
      "epoch 17 | loss: 16.55011| val_0_rmse: 3.94323 |  0:00:02s\n",
      "epoch 18 | loss: 14.82077| val_0_rmse: 3.50864 |  0:00:02s\n",
      "epoch 19 | loss: 14.97607| val_0_rmse: 3.25465 |  0:00:02s\n",
      "epoch 20 | loss: 13.6737 | val_0_rmse: 3.18211 |  0:00:03s\n",
      "epoch 21 | loss: 11.43509| val_0_rmse: 3.2173  |  0:00:03s\n",
      "epoch 22 | loss: 7.72686 | val_0_rmse: 3.30923 |  0:00:03s\n",
      "epoch 23 | loss: 6.38695 | val_0_rmse: 3.44049 |  0:00:03s\n",
      "epoch 24 | loss: 4.15496 | val_0_rmse: 3.55167 |  0:00:03s\n",
      "epoch 25 | loss: 3.7908  | val_0_rmse: 3.51624 |  0:00:03s\n",
      "epoch 26 | loss: 4.19614 | val_0_rmse: 3.35471 |  0:00:03s\n",
      "epoch 27 | loss: 2.7309  | val_0_rmse: 3.12313 |  0:00:03s\n",
      "epoch 28 | loss: 2.1481  | val_0_rmse: 2.6566  |  0:00:04s\n",
      "epoch 29 | loss: 2.06497 | val_0_rmse: 2.17824 |  0:00:04s\n",
      "epoch 30 | loss: 1.92459 | val_0_rmse: 1.96599 |  0:00:04s\n",
      "epoch 31 | loss: 1.4941  | val_0_rmse: 2.06995 |  0:00:04s\n",
      "epoch 32 | loss: 1.34428 | val_0_rmse: 2.281   |  0:00:04s\n",
      "epoch 33 | loss: 1.2615  | val_0_rmse: 2.34302 |  0:00:04s\n",
      "epoch 34 | loss: 1.11396 | val_0_rmse: 2.09164 |  0:00:04s\n",
      "epoch 35 | loss: 1.02179 | val_0_rmse: 1.75311 |  0:00:05s\n",
      "epoch 36 | loss: 0.95016 | val_0_rmse: 1.53751 |  0:00:05s\n",
      "epoch 37 | loss: 1.14322 | val_0_rmse: 1.64021 |  0:00:05s\n",
      "epoch 38 | loss: 0.71355 | val_0_rmse: 1.83434 |  0:00:05s\n",
      "epoch 39 | loss: 0.7094  | val_0_rmse: 1.89328 |  0:00:05s\n",
      "epoch 40 | loss: 0.9889  | val_0_rmse: 1.75474 |  0:00:05s\n",
      "epoch 41 | loss: 0.63487 | val_0_rmse: 1.40366 |  0:00:05s\n",
      "epoch 42 | loss: 0.4858  | val_0_rmse: 1.24073 |  0:00:06s\n",
      "epoch 43 | loss: 0.49255 | val_0_rmse: 1.39488 |  0:00:06s\n",
      "epoch 44 | loss: 0.49089 | val_0_rmse: 1.37441 |  0:00:06s\n",
      "epoch 45 | loss: 0.44104 | val_0_rmse: 1.22402 |  0:00:06s\n",
      "epoch 46 | loss: 0.35298 | val_0_rmse: 1.1824  |  0:00:06s\n",
      "epoch 47 | loss: 0.44806 | val_0_rmse: 1.07566 |  0:00:06s\n",
      "epoch 48 | loss: 0.4073  | val_0_rmse: 0.96822 |  0:00:06s\n",
      "epoch 49 | loss: 0.39513 | val_0_rmse: 0.97159 |  0:00:07s\n",
      "epoch 50 | loss: 0.30723 | val_0_rmse: 0.9106  |  0:00:07s\n",
      "epoch 51 | loss: 0.3063  | val_0_rmse: 0.70042 |  0:00:07s\n",
      "epoch 52 | loss: 0.37871 | val_0_rmse: 0.80446 |  0:00:07s\n",
      "epoch 53 | loss: 0.28251 | val_0_rmse: 0.81057 |  0:00:07s\n",
      "epoch 54 | loss: 0.25745 | val_0_rmse: 0.71836 |  0:00:07s\n",
      "epoch 55 | loss: 0.22532 | val_0_rmse: 0.90252 |  0:00:07s\n",
      "epoch 56 | loss: 0.22006 | val_0_rmse: 0.81511 |  0:00:08s\n",
      "epoch 57 | loss: 0.25243 | val_0_rmse: 0.58715 |  0:00:08s\n",
      "epoch 58 | loss: 0.25982 | val_0_rmse: 0.66491 |  0:00:08s\n",
      "epoch 59 | loss: 0.23647 | val_0_rmse: 0.96423 |  0:00:08s\n",
      "epoch 60 | loss: 0.33502 | val_0_rmse: 0.97176 |  0:00:08s\n",
      "epoch 61 | loss: 0.28386 | val_0_rmse: 0.71792 |  0:00:08s\n",
      "epoch 62 | loss: 0.18016 | val_0_rmse: 0.67163 |  0:00:08s\n",
      "epoch 63 | loss: 0.16135 | val_0_rmse: 0.80254 |  0:00:08s\n",
      "epoch 64 | loss: 0.19523 | val_0_rmse: 0.70952 |  0:00:09s\n",
      "epoch 65 | loss: 0.16832 | val_0_rmse: 0.58313 |  0:00:09s\n",
      "epoch 66 | loss: 0.20034 | val_0_rmse: 0.62477 |  0:00:09s\n",
      "epoch 67 | loss: 0.10846 | val_0_rmse: 0.66511 |  0:00:09s\n",
      "epoch 68 | loss: 0.20082 | val_0_rmse: 0.52392 |  0:00:09s\n",
      "epoch 69 | loss: 0.14781 | val_0_rmse: 0.55854 |  0:00:09s\n",
      "epoch 70 | loss: 0.1437  | val_0_rmse: 0.49941 |  0:00:09s\n",
      "epoch 71 | loss: 0.11548 | val_0_rmse: 0.6263  |  0:00:10s\n",
      "epoch 72 | loss: 0.12207 | val_0_rmse: 0.56011 |  0:00:10s\n",
      "epoch 73 | loss: 0.12813 | val_0_rmse: 0.39083 |  0:00:10s\n",
      "epoch 74 | loss: 0.16103 | val_0_rmse: 0.41672 |  0:00:10s\n",
      "epoch 75 | loss: 0.17254 | val_0_rmse: 0.56648 |  0:00:10s\n",
      "epoch 76 | loss: 0.14561 | val_0_rmse: 0.56497 |  0:00:10s\n",
      "epoch 77 | loss: 0.15629 | val_0_rmse: 0.42151 |  0:00:10s\n",
      "epoch 78 | loss: 0.10849 | val_0_rmse: 0.41244 |  0:00:11s\n",
      "epoch 79 | loss: 0.09803 | val_0_rmse: 0.5207  |  0:00:11s\n",
      "epoch 80 | loss: 0.1138  | val_0_rmse: 0.47469 |  0:00:11s\n",
      "epoch 81 | loss: 0.1271  | val_0_rmse: 0.32685 |  0:00:11s\n",
      "epoch 82 | loss: 0.12206 | val_0_rmse: 0.32616 |  0:00:11s\n",
      "epoch 83 | loss: 0.14755 | val_0_rmse: 0.45645 |  0:00:11s\n",
      "epoch 84 | loss: 0.11525 | val_0_rmse: 0.4298  |  0:00:11s\n",
      "epoch 85 | loss: 0.0841  | val_0_rmse: 0.30918 |  0:00:12s\n",
      "epoch 86 | loss: 0.11513 | val_0_rmse: 0.32156 |  0:00:12s\n",
      "epoch 87 | loss: 0.10763 | val_0_rmse: 0.44018 |  0:00:12s\n",
      "epoch 88 | loss: 0.12529 | val_0_rmse: 0.4429  |  0:00:12s\n",
      "epoch 89 | loss: 0.13622 | val_0_rmse: 0.33884 |  0:00:12s\n",
      "epoch 90 | loss: 0.09674 | val_0_rmse: 0.34141 |  0:00:12s\n",
      "epoch 91 | loss: 0.08071 | val_0_rmse: 0.3464  |  0:00:12s\n",
      "epoch 92 | loss: 0.07324 | val_0_rmse: 0.33604 |  0:00:12s\n",
      "epoch 93 | loss: 0.06384 | val_0_rmse: 0.32876 |  0:00:13s\n",
      "epoch 94 | loss: 0.08006 | val_0_rmse: 0.33433 |  0:00:13s\n",
      "epoch 95 | loss: 0.072   | val_0_rmse: 0.36911 |  0:00:13s\n",
      "epoch 96 | loss: 0.06814 | val_0_rmse: 0.29572 |  0:00:13s\n",
      "epoch 97 | loss: 0.07706 | val_0_rmse: 0.32184 |  0:00:13s\n",
      "epoch 98 | loss: 0.06612 | val_0_rmse: 0.43787 |  0:00:13s\n",
      "epoch 99 | loss: 0.08976 | val_0_rmse: 0.42218 |  0:00:13s\n",
      "epoch 100| loss: 0.09496 | val_0_rmse: 0.29993 |  0:00:13s\n",
      "epoch 101| loss: 0.07987 | val_0_rmse: 0.2791  |  0:00:14s\n",
      "epoch 102| loss: 0.08523 | val_0_rmse: 0.33226 |  0:00:14s\n",
      "epoch 103| loss: 0.07243 | val_0_rmse: 0.30241 |  0:00:14s\n",
      "epoch 104| loss: 0.05119 | val_0_rmse: 0.26496 |  0:00:14s\n",
      "epoch 105| loss: 0.06842 | val_0_rmse: 0.29116 |  0:00:14s\n",
      "epoch 106| loss: 0.06982 | val_0_rmse: 0.37888 |  0:00:14s\n",
      "epoch 107| loss: 0.09809 | val_0_rmse: 0.3784  |  0:00:14s\n",
      "epoch 108| loss: 0.10401 | val_0_rmse: 0.28829 |  0:00:15s\n",
      "epoch 109| loss: 0.06354 | val_0_rmse: 0.25438 |  0:00:15s\n",
      "epoch 110| loss: 0.07794 | val_0_rmse: 0.25879 |  0:00:15s\n",
      "epoch 111| loss: 0.06462 | val_0_rmse: 0.37129 |  0:00:15s\n",
      "epoch 112| loss: 0.08845 | val_0_rmse: 0.3979  |  0:00:15s\n",
      "epoch 113| loss: 0.11891 | val_0_rmse: 0.31367 |  0:00:15s\n",
      "epoch 114| loss: 0.07173 | val_0_rmse: 0.2806  |  0:00:15s\n",
      "epoch 115| loss: 0.13585 | val_0_rmse: 0.33381 |  0:00:16s\n",
      "epoch 116| loss: 0.20063 | val_0_rmse: 0.29755 |  0:00:16s\n",
      "epoch 117| loss: 0.13143 | val_0_rmse: 0.25417 |  0:00:16s\n",
      "epoch 118| loss: 0.05127 | val_0_rmse: 0.39041 |  0:00:16s\n",
      "epoch 119| loss: 0.13822 | val_0_rmse: 0.45347 |  0:00:16s\n",
      "epoch 120| loss: 0.20621 | val_0_rmse: 0.39302 |  0:00:16s\n",
      "epoch 121| loss: 0.13905 | val_0_rmse: 0.28032 |  0:00:16s\n",
      "epoch 122| loss: 0.05659 | val_0_rmse: 0.30822 |  0:00:16s\n",
      "epoch 123| loss: 0.1392  | val_0_rmse: 0.3558  |  0:00:17s\n",
      "epoch 124| loss: 0.19575 | val_0_rmse: 0.30744 |  0:00:17s\n",
      "epoch 125| loss: 0.13512 | val_0_rmse: 0.27075 |  0:00:17s\n",
      "epoch 126| loss: 0.08872 | val_0_rmse: 0.34361 |  0:00:17s\n",
      "epoch 127| loss: 0.08106 | val_0_rmse: 0.38703 |  0:00:17s\n",
      "epoch 128| loss: 0.11493 | val_0_rmse: 0.3526  |  0:00:17s\n",
      "epoch 129| loss: 0.11821 | val_0_rmse: 0.31528 |  0:00:17s\n",
      "epoch 130| loss: 0.09449 | val_0_rmse: 0.26264 |  0:00:18s\n",
      "epoch 131| loss: 0.06081 | val_0_rmse: 0.30318 |  0:00:18s\n",
      "epoch 132| loss: 0.1044  | val_0_rmse: 0.32537 |  0:00:18s\n",
      "epoch 133| loss: 0.12185 | val_0_rmse: 0.28319 |  0:00:18s\n",
      "epoch 134| loss: 0.0853  | val_0_rmse: 0.28991 |  0:00:18s\n",
      "epoch 135| loss: 0.09894 | val_0_rmse: 0.33638 |  0:00:18s\n",
      "epoch 136| loss: 0.1471  | val_0_rmse: 0.33364 |  0:00:18s\n",
      "epoch 137| loss: 0.12557 | val_0_rmse: 0.351   |  0:00:19s\n",
      "epoch 138| loss: 0.08991 | val_0_rmse: 0.31611 |  0:00:19s\n",
      "epoch 139| loss: 0.07057 | val_0_rmse: 0.26951 |  0:00:19s\n",
      "epoch 140| loss: 0.05572 | val_0_rmse: 0.29026 |  0:00:19s\n",
      "epoch 141| loss: 0.10032 | val_0_rmse: 0.3014  |  0:00:19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:18:49,207] Trial 30 finished with value: 0.2541724549197001 and parameters: {'n_d': 24, 'n_a': 32, 'n_steps': 5, 'gamma': 1.4139860376965099, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.01737257020718449, 'batch_size': 2048, 'virtual_batch_size': 256}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 142| loss: 0.11412 | val_0_rmse: 0.26564 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 142 with best_epoch = 117 and best_val_0_rmse = 0.25417\n",
      "Trial 030 | rmse_log=0.25417 | RMSE$=55,136 | MAE$=33,577 | MAPE=18.94% | n_d/n_a=24/32 steps=5 lr=0.01737 batch=2048 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 197.68231| val_0_rmse: 11.57778|  0:00:00s\n",
      "epoch 1  | loss: 157.16127| val_0_rmse: 10.9125 |  0:00:00s\n",
      "epoch 2  | loss: 121.36202| val_0_rmse: 10.30484|  0:00:00s\n",
      "epoch 3  | loss: 95.91591| val_0_rmse: 9.58029 |  0:00:00s\n",
      "epoch 4  | loss: 73.0447 | val_0_rmse: 8.8678  |  0:00:00s\n",
      "epoch 5  | loss: 57.33181| val_0_rmse: 8.12592 |  0:00:01s\n",
      "epoch 6  | loss: 42.81165| val_0_rmse: 7.35775 |  0:00:01s\n",
      "epoch 7  | loss: 35.78515| val_0_rmse: 6.62487 |  0:00:01s\n",
      "epoch 8  | loss: 25.89735| val_0_rmse: 5.92565 |  0:00:01s\n",
      "epoch 9  | loss: 26.91248| val_0_rmse: 5.29819 |  0:00:01s\n",
      "epoch 10 | loss: 21.02076| val_0_rmse: 4.73091 |  0:00:02s\n",
      "epoch 11 | loss: 17.13262| val_0_rmse: 4.25287 |  0:00:02s\n",
      "epoch 12 | loss: 20.29563| val_0_rmse: 3.96076 |  0:00:02s\n",
      "epoch 13 | loss: 12.2496 | val_0_rmse: 3.67351 |  0:00:02s\n",
      "epoch 14 | loss: 9.14386 | val_0_rmse: 3.4814  |  0:00:02s\n",
      "epoch 15 | loss: 7.72896 | val_0_rmse: 3.27898 |  0:00:02s\n",
      "epoch 16 | loss: 6.4542  | val_0_rmse: 2.94867 |  0:00:03s\n",
      "epoch 17 | loss: 7.59153 | val_0_rmse: 2.62454 |  0:00:03s\n",
      "epoch 18 | loss: 4.98602 | val_0_rmse: 2.23013 |  0:00:03s\n",
      "epoch 19 | loss: 3.33452 | val_0_rmse: 1.79538 |  0:00:03s\n",
      "epoch 20 | loss: 3.09719 | val_0_rmse: 1.48657 |  0:00:03s\n",
      "epoch 21 | loss: 2.06237 | val_0_rmse: 1.36533 |  0:00:03s\n",
      "epoch 22 | loss: 2.05478 | val_0_rmse: 1.40537 |  0:00:04s\n",
      "epoch 23 | loss: 2.301   | val_0_rmse: 1.13292 |  0:00:04s\n",
      "epoch 24 | loss: 1.9257  | val_0_rmse: 0.98157 |  0:00:04s\n",
      "epoch 25 | loss: 1.3061  | val_0_rmse: 1.18911 |  0:00:04s\n",
      "epoch 26 | loss: 1.50115 | val_0_rmse: 1.20287 |  0:00:04s\n",
      "epoch 27 | loss: 1.44664 | val_0_rmse: 0.86616 |  0:00:05s\n",
      "epoch 28 | loss: 1.07309 | val_0_rmse: 0.92865 |  0:00:05s\n",
      "epoch 29 | loss: 1.04593 | val_0_rmse: 0.74244 |  0:00:05s\n",
      "epoch 30 | loss: 1.0326  | val_0_rmse: 0.68077 |  0:00:05s\n",
      "epoch 31 | loss: 0.89538 | val_0_rmse: 0.7867  |  0:00:05s\n",
      "epoch 32 | loss: 1.04265 | val_0_rmse: 0.7999  |  0:00:05s\n",
      "epoch 33 | loss: 0.88565 | val_0_rmse: 0.73912 |  0:00:06s\n",
      "epoch 34 | loss: 0.73858 | val_0_rmse: 0.84947 |  0:00:06s\n",
      "epoch 35 | loss: 0.6307  | val_0_rmse: 0.78225 |  0:00:06s\n",
      "epoch 36 | loss: 0.68208 | val_0_rmse: 0.4388  |  0:00:06s\n",
      "epoch 37 | loss: 0.68738 | val_0_rmse: 0.68997 |  0:00:06s\n",
      "epoch 38 | loss: 0.65601 | val_0_rmse: 0.61682 |  0:00:06s\n",
      "epoch 39 | loss: 0.5055  | val_0_rmse: 0.56842 |  0:00:06s\n",
      "epoch 40 | loss: 0.53843 | val_0_rmse: 0.55869 |  0:00:07s\n",
      "epoch 41 | loss: 0.44705 | val_0_rmse: 0.60646 |  0:00:07s\n",
      "epoch 42 | loss: 0.44898 | val_0_rmse: 0.54217 |  0:00:07s\n",
      "epoch 43 | loss: 0.42498 | val_0_rmse: 0.58295 |  0:00:07s\n",
      "epoch 44 | loss: 0.47165 | val_0_rmse: 0.61924 |  0:00:07s\n",
      "epoch 45 | loss: 0.43404 | val_0_rmse: 0.64478 |  0:00:07s\n",
      "epoch 46 | loss: 0.35828 | val_0_rmse: 0.36574 |  0:00:08s\n",
      "epoch 47 | loss: 0.5472  | val_0_rmse: 0.52601 |  0:00:08s\n",
      "epoch 48 | loss: 0.3456  | val_0_rmse: 0.46333 |  0:00:08s\n",
      "epoch 49 | loss: 0.35756 | val_0_rmse: 0.57504 |  0:00:08s\n",
      "epoch 50 | loss: 0.40069 | val_0_rmse: 0.44292 |  0:00:08s\n",
      "epoch 51 | loss: 0.3565  | val_0_rmse: 0.50692 |  0:00:09s\n",
      "epoch 52 | loss: 0.33929 | val_0_rmse: 0.39945 |  0:00:09s\n",
      "epoch 53 | loss: 0.26549 | val_0_rmse: 0.39552 |  0:00:09s\n",
      "epoch 54 | loss: 0.2521  | val_0_rmse: 0.37395 |  0:00:09s\n",
      "epoch 55 | loss: 0.2593  | val_0_rmse: 0.46691 |  0:00:09s\n",
      "epoch 56 | loss: 0.18767 | val_0_rmse: 0.46504 |  0:00:09s\n",
      "epoch 57 | loss: 0.24126 | val_0_rmse: 0.31011 |  0:00:10s\n",
      "epoch 58 | loss: 0.30093 | val_0_rmse: 0.44575 |  0:00:10s\n",
      "epoch 59 | loss: 0.20349 | val_0_rmse: 0.50061 |  0:00:10s\n",
      "epoch 60 | loss: 0.22784 | val_0_rmse: 0.33652 |  0:00:10s\n",
      "epoch 61 | loss: 0.17224 | val_0_rmse: 0.42376 |  0:00:10s\n",
      "epoch 62 | loss: 0.16354 | val_0_rmse: 0.36449 |  0:00:10s\n",
      "epoch 63 | loss: 0.15909 | val_0_rmse: 0.37566 |  0:00:11s\n",
      "epoch 64 | loss: 0.12075 | val_0_rmse: 0.32224 |  0:00:11s\n",
      "epoch 65 | loss: 0.18806 | val_0_rmse: 0.47912 |  0:00:11s\n",
      "epoch 66 | loss: 0.15618 | val_0_rmse: 0.30349 |  0:00:11s\n",
      "epoch 67 | loss: 0.15607 | val_0_rmse: 0.38154 |  0:00:11s\n",
      "epoch 68 | loss: 0.13332 | val_0_rmse: 0.25425 |  0:00:11s\n",
      "epoch 69 | loss: 0.16564 | val_0_rmse: 0.42254 |  0:00:12s\n",
      "epoch 70 | loss: 0.15735 | val_0_rmse: 0.3198  |  0:00:12s\n",
      "epoch 71 | loss: 0.1331  | val_0_rmse: 0.30703 |  0:00:12s\n",
      "epoch 72 | loss: 0.10769 | val_0_rmse: 0.29508 |  0:00:12s\n",
      "epoch 73 | loss: 0.10231 | val_0_rmse: 0.31938 |  0:00:12s\n",
      "epoch 74 | loss: 0.10843 | val_0_rmse: 0.29942 |  0:00:12s\n",
      "epoch 75 | loss: 0.10356 | val_0_rmse: 0.2523  |  0:00:13s\n",
      "epoch 76 | loss: 0.08408 | val_0_rmse: 0.23604 |  0:00:13s\n",
      "epoch 77 | loss: 0.11711 | val_0_rmse: 0.36316 |  0:00:13s\n",
      "epoch 78 | loss: 0.12696 | val_0_rmse: 0.2656  |  0:00:13s\n",
      "epoch 79 | loss: 0.11625 | val_0_rmse: 0.29866 |  0:00:13s\n",
      "epoch 80 | loss: 0.12746 | val_0_rmse: 0.26332 |  0:00:14s\n",
      "epoch 81 | loss: 0.08139 | val_0_rmse: 0.29281 |  0:00:14s\n",
      "epoch 82 | loss: 0.07016 | val_0_rmse: 0.23752 |  0:00:14s\n",
      "epoch 83 | loss: 0.10366 | val_0_rmse: 0.33934 |  0:00:14s\n",
      "epoch 84 | loss: 0.10348 | val_0_rmse: 0.29557 |  0:00:14s\n",
      "epoch 85 | loss: 0.24879 | val_0_rmse: 0.30155 |  0:00:14s\n",
      "epoch 86 | loss: 0.13778 | val_0_rmse: 0.40425 |  0:00:15s\n",
      "epoch 87 | loss: 0.11975 | val_0_rmse: 0.26791 |  0:00:15s\n",
      "epoch 88 | loss: 0.11247 | val_0_rmse: 0.28725 |  0:00:15s\n",
      "epoch 89 | loss: 0.0869  | val_0_rmse: 0.27515 |  0:00:15s\n",
      "epoch 90 | loss: 0.08318 | val_0_rmse: 0.28736 |  0:00:15s\n",
      "epoch 91 | loss: 0.07465 | val_0_rmse: 0.23197 |  0:00:15s\n",
      "epoch 92 | loss: 0.07694 | val_0_rmse: 0.33449 |  0:00:16s\n",
      "epoch 93 | loss: 0.12048 | val_0_rmse: 0.26261 |  0:00:16s\n",
      "epoch 94 | loss: 0.08784 | val_0_rmse: 0.32737 |  0:00:16s\n",
      "epoch 95 | loss: 0.09647 | val_0_rmse: 0.26226 |  0:00:16s\n",
      "epoch 96 | loss: 0.07386 | val_0_rmse: 0.28063 |  0:00:16s\n",
      "epoch 97 | loss: 0.0737  | val_0_rmse: 0.26177 |  0:00:16s\n",
      "epoch 98 | loss: 0.07405 | val_0_rmse: 0.28204 |  0:00:17s\n",
      "epoch 99 | loss: 0.06685 | val_0_rmse: 0.25194 |  0:00:17s\n",
      "epoch 100| loss: 0.06086 | val_0_rmse: 0.23471 |  0:00:17s\n",
      "epoch 101| loss: 0.06843 | val_0_rmse: 0.27291 |  0:00:17s\n",
      "epoch 102| loss: 0.06648 | val_0_rmse: 0.22384 |  0:00:17s\n",
      "epoch 103| loss: 0.05696 | val_0_rmse: 0.24254 |  0:00:17s\n",
      "epoch 104| loss: 0.05572 | val_0_rmse: 0.25383 |  0:00:18s\n",
      "epoch 105| loss: 0.04978 | val_0_rmse: 0.22067 |  0:00:18s\n",
      "epoch 106| loss: 0.05412 | val_0_rmse: 0.22447 |  0:00:18s\n",
      "epoch 107| loss: 0.06808 | val_0_rmse: 0.21467 |  0:00:18s\n",
      "epoch 108| loss: 0.05834 | val_0_rmse: 0.23124 |  0:00:18s\n",
      "epoch 109| loss: 0.05627 | val_0_rmse: 0.23138 |  0:00:18s\n",
      "epoch 110| loss: 0.04334 | val_0_rmse: 0.21378 |  0:00:19s\n",
      "epoch 111| loss: 0.05192 | val_0_rmse: 0.26931 |  0:00:19s\n",
      "epoch 112| loss: 0.0587  | val_0_rmse: 0.23542 |  0:00:19s\n",
      "epoch 113| loss: 0.05947 | val_0_rmse: 0.31438 |  0:00:19s\n",
      "epoch 114| loss: 0.1037  | val_0_rmse: 0.24232 |  0:00:19s\n",
      "epoch 115| loss: 0.06097 | val_0_rmse: 0.26884 |  0:00:19s\n",
      "epoch 116| loss: 0.06588 | val_0_rmse: 0.32034 |  0:00:20s\n",
      "epoch 117| loss: 0.07985 | val_0_rmse: 0.25427 |  0:00:20s\n",
      "epoch 118| loss: 0.07519 | val_0_rmse: 0.24267 |  0:00:20s\n",
      "epoch 119| loss: 0.06236 | val_0_rmse: 0.2594  |  0:00:20s\n",
      "epoch 120| loss: 0.05259 | val_0_rmse: 0.23386 |  0:00:20s\n",
      "epoch 121| loss: 0.04711 | val_0_rmse: 0.22712 |  0:00:20s\n",
      "epoch 122| loss: 0.05722 | val_0_rmse: 0.22923 |  0:00:21s\n",
      "epoch 123| loss: 0.04835 | val_0_rmse: 0.22964 |  0:00:21s\n",
      "epoch 124| loss: 0.04335 | val_0_rmse: 0.22733 |  0:00:21s\n",
      "epoch 125| loss: 0.04405 | val_0_rmse: 0.23294 |  0:00:21s\n",
      "epoch 126| loss: 0.0491  | val_0_rmse: 0.23751 |  0:00:21s\n",
      "epoch 127| loss: 0.05602 | val_0_rmse: 0.26332 |  0:00:22s\n",
      "epoch 128| loss: 0.05347 | val_0_rmse: 0.23135 |  0:00:22s\n",
      "epoch 129| loss: 0.04059 | val_0_rmse: 0.22506 |  0:00:22s\n",
      "epoch 130| loss: 0.04876 | val_0_rmse: 0.21672 |  0:00:22s\n",
      "epoch 131| loss: 0.05518 | val_0_rmse: 0.24629 |  0:00:22s\n",
      "epoch 132| loss: 0.04792 | val_0_rmse: 0.22572 |  0:00:22s\n",
      "epoch 133| loss: 0.04187 | val_0_rmse: 0.22317 |  0:00:23s\n",
      "epoch 134| loss: 0.03953 | val_0_rmse: 0.23695 |  0:00:23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:19:12,936] Trial 31 finished with value: 0.21378120194595898 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 4, 'gamma': 1.2787962957730958, 'n_independent': 1, 'n_shared': 2, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.007900553029059822, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 135| loss: 0.04881 | val_0_rmse: 0.22679 |  0:00:23s\n",
      "\n",
      "Early stopping occurred at epoch 135 with best_epoch = 110 and best_val_0_rmse = 0.21378\n",
      "Trial 031 | rmse_log=0.21378 | RMSE$=41,000 | MAE$=27,296 | MAPE=16.60% | n_d/n_a=48/24 steps=4 lr=0.00790 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 116.8154| val_0_rmse: 11.34239|  0:00:00s\n",
      "epoch 1  | loss: 90.81445| val_0_rmse: 10.87809|  0:00:00s\n",
      "epoch 2  | loss: 71.92201| val_0_rmse: 10.31673|  0:00:00s\n",
      "epoch 3  | loss: 56.58511| val_0_rmse: 9.67788 |  0:00:00s\n",
      "epoch 4  | loss: 43.93554| val_0_rmse: 9.00752 |  0:00:00s\n",
      "epoch 5  | loss: 34.51024| val_0_rmse: 8.31902 |  0:00:00s\n",
      "epoch 6  | loss: 30.23117| val_0_rmse: 7.60051 |  0:00:01s\n",
      "epoch 7  | loss: 26.50619| val_0_rmse: 6.93318 |  0:00:01s\n",
      "epoch 8  | loss: 26.98889| val_0_rmse: 6.32316 |  0:00:01s\n",
      "epoch 9  | loss: 23.36011| val_0_rmse: 5.80537 |  0:00:01s\n",
      "epoch 10 | loss: 18.10552| val_0_rmse: 5.36442 |  0:00:01s\n",
      "epoch 11 | loss: 16.29896| val_0_rmse: 5.02344 |  0:00:01s\n",
      "epoch 12 | loss: 12.49769| val_0_rmse: 4.75864 |  0:00:01s\n",
      "epoch 13 | loss: 12.36896| val_0_rmse: 4.55478 |  0:00:02s\n",
      "epoch 14 | loss: 10.56036| val_0_rmse: 4.29712 |  0:00:02s\n",
      "epoch 15 | loss: 8.24715 | val_0_rmse: 3.93663 |  0:00:02s\n",
      "epoch 16 | loss: 6.97152 | val_0_rmse: 3.53549 |  0:00:02s\n",
      "epoch 17 | loss: 6.47272 | val_0_rmse: 2.98615 |  0:00:02s\n",
      "epoch 18 | loss: 5.17564 | val_0_rmse: 2.52437 |  0:00:02s\n",
      "epoch 19 | loss: 3.33308 | val_0_rmse: 2.22855 |  0:00:02s\n",
      "epoch 20 | loss: 3.06251 | val_0_rmse: 1.89537 |  0:00:03s\n",
      "epoch 21 | loss: 2.44707 | val_0_rmse: 1.76186 |  0:00:03s\n",
      "epoch 22 | loss: 2.07625 | val_0_rmse: 1.51998 |  0:00:03s\n",
      "epoch 23 | loss: 1.53769 | val_0_rmse: 1.30376 |  0:00:03s\n",
      "epoch 24 | loss: 1.60285 | val_0_rmse: 1.21217 |  0:00:03s\n",
      "epoch 25 | loss: 1.40298 | val_0_rmse: 1.20582 |  0:00:03s\n",
      "epoch 26 | loss: 1.23022 | val_0_rmse: 1.14986 |  0:00:04s\n",
      "epoch 27 | loss: 1.04343 | val_0_rmse: 0.80639 |  0:00:04s\n",
      "epoch 28 | loss: 0.9549  | val_0_rmse: 1.00412 |  0:00:04s\n",
      "epoch 29 | loss: 1.00407 | val_0_rmse: 0.97687 |  0:00:04s\n",
      "epoch 30 | loss: 0.82919 | val_0_rmse: 0.7226  |  0:00:04s\n",
      "epoch 31 | loss: 1.19979 | val_0_rmse: 1.08506 |  0:00:04s\n",
      "epoch 32 | loss: 1.03208 | val_0_rmse: 0.68934 |  0:00:04s\n",
      "epoch 33 | loss: 0.6736  | val_0_rmse: 0.71969 |  0:00:04s\n",
      "epoch 34 | loss: 0.61207 | val_0_rmse: 0.84235 |  0:00:05s\n",
      "epoch 35 | loss: 0.7499  | val_0_rmse: 0.7052  |  0:00:05s\n",
      "epoch 36 | loss: 0.55376 | val_0_rmse: 0.76543 |  0:00:05s\n",
      "epoch 37 | loss: 0.43935 | val_0_rmse: 0.63721 |  0:00:05s\n",
      "epoch 38 | loss: 0.50727 | val_0_rmse: 0.69891 |  0:00:05s\n",
      "epoch 39 | loss: 0.50454 | val_0_rmse: 0.53476 |  0:00:05s\n",
      "epoch 40 | loss: 0.44701 | val_0_rmse: 0.7469  |  0:00:05s\n",
      "epoch 41 | loss: 0.42107 | val_0_rmse: 0.73856 |  0:00:06s\n",
      "epoch 42 | loss: 0.44876 | val_0_rmse: 0.5361  |  0:00:06s\n",
      "epoch 43 | loss: 0.36563 | val_0_rmse: 0.69285 |  0:00:06s\n",
      "epoch 44 | loss: 0.39351 | val_0_rmse: 0.58429 |  0:00:06s\n",
      "epoch 45 | loss: 0.35607 | val_0_rmse: 0.51879 |  0:00:06s\n",
      "epoch 46 | loss: 0.32894 | val_0_rmse: 0.60311 |  0:00:06s\n",
      "epoch 47 | loss: 0.27908 | val_0_rmse: 0.43934 |  0:00:06s\n",
      "epoch 48 | loss: 0.32448 | val_0_rmse: 0.55125 |  0:00:07s\n",
      "epoch 49 | loss: 0.30087 | val_0_rmse: 0.46409 |  0:00:07s\n",
      "epoch 50 | loss: 0.29831 | val_0_rmse: 0.52487 |  0:00:07s\n",
      "epoch 51 | loss: 0.22741 | val_0_rmse: 0.377   |  0:00:07s\n",
      "epoch 52 | loss: 0.28763 | val_0_rmse: 0.5553  |  0:00:07s\n",
      "epoch 53 | loss: 0.31914 | val_0_rmse: 0.46101 |  0:00:07s\n",
      "epoch 54 | loss: 0.20531 | val_0_rmse: 0.46601 |  0:00:07s\n",
      "epoch 55 | loss: 0.20978 | val_0_rmse: 0.49247 |  0:00:08s\n",
      "epoch 56 | loss: 0.20093 | val_0_rmse: 0.28079 |  0:00:08s\n",
      "epoch 57 | loss: 0.19306 | val_0_rmse: 0.45611 |  0:00:08s\n",
      "epoch 58 | loss: 0.17021 | val_0_rmse: 0.3213  |  0:00:08s\n",
      "epoch 59 | loss: 0.24394 | val_0_rmse: 0.53596 |  0:00:08s\n",
      "epoch 60 | loss: 0.23429 | val_0_rmse: 0.41187 |  0:00:08s\n",
      "epoch 61 | loss: 0.20929 | val_0_rmse: 0.28561 |  0:00:08s\n",
      "epoch 62 | loss: 0.20039 | val_0_rmse: 0.48197 |  0:00:08s\n",
      "epoch 63 | loss: 0.21538 | val_0_rmse: 0.33483 |  0:00:09s\n",
      "epoch 64 | loss: 0.17208 | val_0_rmse: 0.27242 |  0:00:09s\n",
      "epoch 65 | loss: 0.18817 | val_0_rmse: 0.41972 |  0:00:09s\n",
      "epoch 66 | loss: 0.20272 | val_0_rmse: 0.29024 |  0:00:09s\n",
      "epoch 67 | loss: 0.15186 | val_0_rmse: 0.43288 |  0:00:09s\n",
      "epoch 68 | loss: 0.16579 | val_0_rmse: 0.28906 |  0:00:09s\n",
      "epoch 69 | loss: 0.15208 | val_0_rmse: 0.33775 |  0:00:09s\n",
      "epoch 70 | loss: 0.14779 | val_0_rmse: 0.43249 |  0:00:10s\n",
      "epoch 71 | loss: 0.1857  | val_0_rmse: 0.27836 |  0:00:10s\n",
      "epoch 72 | loss: 0.17029 | val_0_rmse: 0.45855 |  0:00:10s\n",
      "epoch 73 | loss: 0.26207 | val_0_rmse: 0.36354 |  0:00:10s\n",
      "epoch 74 | loss: 0.16073 | val_0_rmse: 0.25586 |  0:00:10s\n",
      "epoch 75 | loss: 0.12121 | val_0_rmse: 0.34547 |  0:00:10s\n",
      "epoch 76 | loss: 0.12623 | val_0_rmse: 0.27749 |  0:00:10s\n",
      "epoch 77 | loss: 0.15049 | val_0_rmse: 0.25032 |  0:00:11s\n",
      "epoch 78 | loss: 0.12082 | val_0_rmse: 0.36251 |  0:00:11s\n",
      "epoch 79 | loss: 0.17929 | val_0_rmse: 0.27211 |  0:00:11s\n",
      "epoch 80 | loss: 0.11011 | val_0_rmse: 0.31843 |  0:00:11s\n",
      "epoch 81 | loss: 0.14524 | val_0_rmse: 0.29884 |  0:00:11s\n",
      "epoch 82 | loss: 0.11512 | val_0_rmse: 0.26618 |  0:00:11s\n",
      "epoch 83 | loss: 0.13332 | val_0_rmse: 0.34057 |  0:00:11s\n",
      "epoch 84 | loss: 0.10125 | val_0_rmse: 0.2633  |  0:00:12s\n",
      "epoch 85 | loss: 0.14272 | val_0_rmse: 0.28133 |  0:00:12s\n",
      "epoch 86 | loss: 0.11118 | val_0_rmse: 0.26796 |  0:00:12s\n",
      "epoch 87 | loss: 0.10605 | val_0_rmse: 0.241   |  0:00:12s\n",
      "epoch 88 | loss: 0.09781 | val_0_rmse: 0.38805 |  0:00:12s\n",
      "epoch 89 | loss: 0.14487 | val_0_rmse: 0.25044 |  0:00:12s\n",
      "epoch 90 | loss: 0.09308 | val_0_rmse: 0.2384  |  0:00:12s\n",
      "epoch 91 | loss: 0.08661 | val_0_rmse: 0.25383 |  0:00:12s\n",
      "epoch 92 | loss: 0.086   | val_0_rmse: 0.23003 |  0:00:13s\n",
      "epoch 93 | loss: 0.08506 | val_0_rmse: 0.24519 |  0:00:13s\n",
      "epoch 94 | loss: 0.07322 | val_0_rmse: 0.23351 |  0:00:13s\n",
      "epoch 95 | loss: 0.07375 | val_0_rmse: 0.26005 |  0:00:13s\n",
      "epoch 96 | loss: 0.0748  | val_0_rmse: 0.24005 |  0:00:13s\n",
      "epoch 97 | loss: 0.08794 | val_0_rmse: 0.31503 |  0:00:13s\n",
      "epoch 98 | loss: 0.09113 | val_0_rmse: 0.26018 |  0:00:13s\n",
      "epoch 99 | loss: 0.09934 | val_0_rmse: 0.27664 |  0:00:14s\n",
      "epoch 100| loss: 0.0799  | val_0_rmse: 0.2434  |  0:00:14s\n",
      "epoch 101| loss: 0.07351 | val_0_rmse: 0.29046 |  0:00:14s\n",
      "epoch 102| loss: 0.08263 | val_0_rmse: 0.22976 |  0:00:14s\n",
      "epoch 103| loss: 0.0804  | val_0_rmse: 0.26426 |  0:00:14s\n",
      "epoch 104| loss: 0.06587 | val_0_rmse: 0.23265 |  0:00:14s\n",
      "epoch 105| loss: 0.07169 | val_0_rmse: 0.27665 |  0:00:14s\n",
      "epoch 106| loss: 0.06909 | val_0_rmse: 0.22698 |  0:00:15s\n",
      "epoch 107| loss: 0.06838 | val_0_rmse: 0.24008 |  0:00:15s\n",
      "epoch 108| loss: 0.07014 | val_0_rmse: 0.28527 |  0:00:15s\n",
      "epoch 109| loss: 0.07281 | val_0_rmse: 0.24575 |  0:00:15s\n",
      "epoch 110| loss: 0.07079 | val_0_rmse: 0.32591 |  0:00:15s\n",
      "epoch 111| loss: 0.10448 | val_0_rmse: 0.27838 |  0:00:15s\n",
      "epoch 112| loss: 0.10252 | val_0_rmse: 0.30872 |  0:00:15s\n",
      "epoch 113| loss: 0.11    | val_0_rmse: 0.31346 |  0:00:16s\n",
      "epoch 114| loss: 0.10687 | val_0_rmse: 0.34078 |  0:00:16s\n",
      "epoch 115| loss: 0.0781  | val_0_rmse: 0.30667 |  0:00:16s\n",
      "epoch 116| loss: 0.14107 | val_0_rmse: 0.23654 |  0:00:16s\n",
      "epoch 117| loss: 0.08029 | val_0_rmse: 0.37489 |  0:00:16s\n",
      "epoch 118| loss: 0.1085  | val_0_rmse: 0.23456 |  0:00:16s\n",
      "epoch 119| loss: 0.07454 | val_0_rmse: 0.23737 |  0:00:16s\n",
      "epoch 120| loss: 0.05632 | val_0_rmse: 0.25898 |  0:00:17s\n",
      "epoch 121| loss: 0.06821 | val_0_rmse: 0.2275  |  0:00:17s\n",
      "epoch 122| loss: 0.04767 | val_0_rmse: 0.22919 |  0:00:17s\n",
      "epoch 123| loss: 0.05125 | val_0_rmse: 0.22306 |  0:00:17s\n",
      "epoch 124| loss: 0.07114 | val_0_rmse: 0.26218 |  0:00:17s\n",
      "epoch 125| loss: 0.05224 | val_0_rmse: 0.24386 |  0:00:17s\n",
      "epoch 126| loss: 0.09576 | val_0_rmse: 0.2809  |  0:00:17s\n",
      "epoch 127| loss: 0.08983 | val_0_rmse: 0.24954 |  0:00:18s\n",
      "epoch 128| loss: 0.06884 | val_0_rmse: 0.25285 |  0:00:18s\n",
      "epoch 129| loss: 0.06687 | val_0_rmse: 0.27737 |  0:00:18s\n",
      "epoch 130| loss: 0.06242 | val_0_rmse: 0.23837 |  0:00:18s\n",
      "epoch 131| loss: 0.08405 | val_0_rmse: 0.2295  |  0:00:18s\n",
      "epoch 132| loss: 0.05679 | val_0_rmse: 0.28418 |  0:00:18s\n",
      "epoch 133| loss: 0.06218 | val_0_rmse: 0.26902 |  0:00:18s\n",
      "epoch 134| loss: 0.09285 | val_0_rmse: 0.24289 |  0:00:19s\n",
      "epoch 135| loss: 0.0692  | val_0_rmse: 0.23738 |  0:00:19s\n",
      "epoch 136| loss: 0.0488  | val_0_rmse: 0.22427 |  0:00:19s\n",
      "epoch 137| loss: 0.05831 | val_0_rmse: 0.23914 |  0:00:19s\n",
      "epoch 138| loss: 0.04839 | val_0_rmse: 0.2215  |  0:00:19s\n",
      "epoch 139| loss: 0.04817 | val_0_rmse: 0.23879 |  0:00:19s\n",
      "epoch 140| loss: 0.05    | val_0_rmse: 0.2304  |  0:00:19s\n",
      "epoch 141| loss: 0.04865 | val_0_rmse: 0.25492 |  0:00:20s\n",
      "epoch 142| loss: 0.04413 | val_0_rmse: 0.22793 |  0:00:20s\n",
      "epoch 143| loss: 0.04272 | val_0_rmse: 0.22955 |  0:00:20s\n",
      "epoch 144| loss: 0.04268 | val_0_rmse: 0.21872 |  0:00:20s\n",
      "epoch 145| loss: 0.0377  | val_0_rmse: 0.25497 |  0:00:20s\n",
      "epoch 146| loss: 0.04875 | val_0_rmse: 0.22655 |  0:00:20s\n",
      "epoch 147| loss: 0.05223 | val_0_rmse: 0.23643 |  0:00:20s\n",
      "epoch 148| loss: 0.05016 | val_0_rmse: 0.22663 |  0:00:20s\n",
      "epoch 149| loss: 0.05226 | val_0_rmse: 0.23166 |  0:00:21s\n",
      "epoch 150| loss: 0.04774 | val_0_rmse: 0.24267 |  0:00:21s\n",
      "epoch 151| loss: 0.05406 | val_0_rmse: 0.2511  |  0:00:21s\n",
      "epoch 152| loss: 0.04919 | val_0_rmse: 0.21866 |  0:00:21s\n",
      "epoch 153| loss: 0.04123 | val_0_rmse: 0.22338 |  0:00:21s\n",
      "epoch 154| loss: 0.03828 | val_0_rmse: 0.23923 |  0:00:21s\n",
      "epoch 155| loss: 0.03686 | val_0_rmse: 0.24387 |  0:00:22s\n",
      "epoch 156| loss: 0.04353 | val_0_rmse: 0.23186 |  0:00:22s\n",
      "epoch 157| loss: 0.04028 | val_0_rmse: 0.2414  |  0:00:22s\n",
      "epoch 158| loss: 0.04333 | val_0_rmse: 0.23533 |  0:00:22s\n",
      "epoch 159| loss: 0.04203 | val_0_rmse: 0.23765 |  0:00:22s\n",
      "epoch 160| loss: 0.0452  | val_0_rmse: 0.2285  |  0:00:22s\n",
      "epoch 161| loss: 0.03979 | val_0_rmse: 0.22113 |  0:00:22s\n",
      "epoch 162| loss: 0.0352  | val_0_rmse: 0.22941 |  0:00:23s\n",
      "epoch 163| loss: 0.06155 | val_0_rmse: 0.22718 |  0:00:23s\n",
      "epoch 164| loss: 0.03824 | val_0_rmse: 0.23609 |  0:00:23s\n",
      "epoch 165| loss: 0.03448 | val_0_rmse: 0.21853 |  0:00:23s\n",
      "epoch 166| loss: 0.04326 | val_0_rmse: 0.21285 |  0:00:23s\n",
      "epoch 167| loss: 0.0336  | val_0_rmse: 0.23834 |  0:00:23s\n",
      "epoch 168| loss: 0.0492  | val_0_rmse: 0.2174  |  0:00:23s\n",
      "epoch 169| loss: 0.03179 | val_0_rmse: 0.20949 |  0:00:23s\n",
      "epoch 170| loss: 0.04329 | val_0_rmse: 0.22295 |  0:00:24s\n",
      "epoch 171| loss: 0.0482  | val_0_rmse: 0.23784 |  0:00:24s\n",
      "epoch 172| loss: 0.03958 | val_0_rmse: 0.20099 |  0:00:24s\n",
      "epoch 173| loss: 0.03564 | val_0_rmse: 0.21261 |  0:00:24s\n",
      "epoch 174| loss: 0.03479 | val_0_rmse: 0.2099  |  0:00:24s\n",
      "epoch 175| loss: 0.03275 | val_0_rmse: 0.22405 |  0:00:24s\n",
      "epoch 176| loss: 0.04006 | val_0_rmse: 0.2201  |  0:00:24s\n",
      "epoch 177| loss: 0.04267 | val_0_rmse: 0.26711 |  0:00:25s\n",
      "epoch 178| loss: 0.05867 | val_0_rmse: 0.21729 |  0:00:25s\n",
      "epoch 179| loss: 0.048   | val_0_rmse: 0.22572 |  0:00:25s\n",
      "epoch 180| loss: 0.04341 | val_0_rmse: 0.26238 |  0:00:25s\n",
      "epoch 181| loss: 0.04615 | val_0_rmse: 0.20377 |  0:00:25s\n",
      "epoch 182| loss: 0.03652 | val_0_rmse: 0.23655 |  0:00:25s\n",
      "epoch 183| loss: 0.03436 | val_0_rmse: 0.19879 |  0:00:25s\n",
      "epoch 184| loss: 0.04597 | val_0_rmse: 0.2325  |  0:00:26s\n",
      "epoch 185| loss: 0.032   | val_0_rmse: 0.21081 |  0:00:26s\n",
      "epoch 186| loss: 0.04002 | val_0_rmse: 0.21647 |  0:00:26s\n",
      "epoch 187| loss: 0.03044 | val_0_rmse: 0.201   |  0:00:26s\n",
      "epoch 188| loss: 0.03501 | val_0_rmse: 0.19983 |  0:00:26s\n",
      "epoch 189| loss: 0.03783 | val_0_rmse: 0.19482 |  0:00:26s\n",
      "epoch 190| loss: 0.02945 | val_0_rmse: 0.19316 |  0:00:26s\n",
      "epoch 191| loss: 0.02728 | val_0_rmse: 0.2067  |  0:00:27s\n",
      "epoch 192| loss: 0.0272  | val_0_rmse: 0.19213 |  0:00:27s\n",
      "epoch 193| loss: 0.03248 | val_0_rmse: 0.20964 |  0:00:27s\n",
      "epoch 194| loss: 0.0286  | val_0_rmse: 0.20896 |  0:00:27s\n",
      "epoch 195| loss: 0.02579 | val_0_rmse: 0.20508 |  0:00:27s\n",
      "epoch 196| loss: 0.03024 | val_0_rmse: 0.22985 |  0:00:27s\n",
      "epoch 197| loss: 0.03693 | val_0_rmse: 0.20643 |  0:00:27s\n",
      "epoch 198| loss: 0.03692 | val_0_rmse: 0.21489 |  0:00:28s\n",
      "epoch 199| loss: 0.03021 | val_0_rmse: 0.20146 |  0:00:28s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 192 and best_val_0_rmse = 0.19213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:19:41,451] Trial 32 finished with value: 0.1921343333991922 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 4, 'gamma': 1.2786028273733157, 'n_independent': 1, 'n_shared': 1, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.006626101236757061, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 032 | rmse_log=0.19213 | RMSE$=35,593 | MAE$=23,824 | MAPE=14.02% | n_d/n_a=48/24 steps=4 lr=0.00663 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 151.07056| val_0_rmse: 11.76484|  0:00:00s\n",
      "epoch 1  | loss: 136.91172| val_0_rmse: 11.51434|  0:00:00s\n",
      "epoch 2  | loss: 125.11737| val_0_rmse: 11.25142|  0:00:00s\n",
      "epoch 3  | loss: 114.15277| val_0_rmse: 10.95612|  0:00:00s\n",
      "epoch 4  | loss: 101.51376| val_0_rmse: 10.62557|  0:00:00s\n",
      "epoch 5  | loss: 90.21083| val_0_rmse: 10.24507|  0:00:00s\n",
      "epoch 6  | loss: 79.41896| val_0_rmse: 9.8099  |  0:00:01s\n",
      "epoch 7  | loss: 65.54152| val_0_rmse: 9.29865 |  0:00:01s\n",
      "epoch 8  | loss: 56.25126| val_0_rmse: 8.6644  |  0:00:01s\n",
      "epoch 9  | loss: 43.94994| val_0_rmse: 7.88775 |  0:00:01s\n",
      "epoch 10 | loss: 35.87298| val_0_rmse: 7.09634 |  0:00:01s\n",
      "epoch 11 | loss: 26.32593| val_0_rmse: 6.20585 |  0:00:01s\n",
      "epoch 12 | loss: 17.06944| val_0_rmse: 5.19504 |  0:00:01s\n",
      "epoch 13 | loss: 12.70952| val_0_rmse: 4.28539 |  0:00:02s\n",
      "epoch 14 | loss: 8.23416 | val_0_rmse: 3.50338 |  0:00:02s\n",
      "epoch 15 | loss: 6.13091 | val_0_rmse: 2.89354 |  0:00:02s\n",
      "epoch 16 | loss: 5.70562 | val_0_rmse: 2.60385 |  0:00:02s\n",
      "epoch 17 | loss: 3.97594 | val_0_rmse: 2.60876 |  0:00:02s\n",
      "epoch 18 | loss: 2.98033 | val_0_rmse: 2.78413 |  0:00:02s\n",
      "epoch 19 | loss: 2.3956  | val_0_rmse: 2.97684 |  0:00:03s\n",
      "epoch 20 | loss: 3.26619 | val_0_rmse: 2.90501 |  0:00:03s\n",
      "epoch 21 | loss: 1.62512 | val_0_rmse: 2.46779 |  0:00:03s\n",
      "epoch 22 | loss: 1.21922 | val_0_rmse: 2.1468  |  0:00:03s\n",
      "epoch 23 | loss: 0.99911 | val_0_rmse: 2.04369 |  0:00:03s\n",
      "epoch 24 | loss: 1.21934 | val_0_rmse: 1.9275  |  0:00:03s\n",
      "epoch 25 | loss: 0.75543 | val_0_rmse: 1.85125 |  0:00:03s\n",
      "epoch 26 | loss: 0.78087 | val_0_rmse: 1.60785 |  0:00:04s\n",
      "epoch 27 | loss: 0.60012 | val_0_rmse: 1.49335 |  0:00:04s\n",
      "epoch 28 | loss: 0.51766 | val_0_rmse: 1.60685 |  0:00:04s\n",
      "epoch 29 | loss: 0.45424 | val_0_rmse: 1.44706 |  0:00:04s\n",
      "epoch 30 | loss: 0.40147 | val_0_rmse: 1.21967 |  0:00:04s\n",
      "epoch 31 | loss: 0.42542 | val_0_rmse: 1.22528 |  0:00:04s\n",
      "epoch 32 | loss: 0.33921 | val_0_rmse: 1.10167 |  0:00:04s\n",
      "epoch 33 | loss: 0.34074 | val_0_rmse: 0.95155 |  0:00:04s\n",
      "epoch 34 | loss: 0.32008 | val_0_rmse: 1.07123 |  0:00:05s\n",
      "epoch 35 | loss: 0.28487 | val_0_rmse: 0.93228 |  0:00:05s\n",
      "epoch 36 | loss: 0.27003 | val_0_rmse: 0.75032 |  0:00:05s\n",
      "epoch 37 | loss: 0.24602 | val_0_rmse: 0.77433 |  0:00:05s\n",
      "epoch 38 | loss: 0.21133 | val_0_rmse: 0.79742 |  0:00:05s\n",
      "epoch 39 | loss: 0.19651 | val_0_rmse: 0.58594 |  0:00:05s\n",
      "epoch 40 | loss: 0.2577  | val_0_rmse: 0.64297 |  0:00:05s\n",
      "epoch 41 | loss: 0.19038 | val_0_rmse: 0.70724 |  0:00:06s\n",
      "epoch 42 | loss: 0.20996 | val_0_rmse: 0.49167 |  0:00:06s\n",
      "epoch 43 | loss: 0.18989 | val_0_rmse: 0.56674 |  0:00:06s\n",
      "epoch 44 | loss: 0.17628 | val_0_rmse: 0.72675 |  0:00:06s\n",
      "epoch 45 | loss: 0.14167 | val_0_rmse: 0.43943 |  0:00:06s\n",
      "epoch 46 | loss: 0.19819 | val_0_rmse: 0.46039 |  0:00:06s\n",
      "epoch 47 | loss: 0.16345 | val_0_rmse: 0.59274 |  0:00:06s\n",
      "epoch 48 | loss: 0.2154  | val_0_rmse: 0.47717 |  0:00:07s\n",
      "epoch 49 | loss: 0.20291 | val_0_rmse: 0.34365 |  0:00:07s\n",
      "epoch 50 | loss: 0.14357 | val_0_rmse: 0.37415 |  0:00:07s\n",
      "epoch 51 | loss: 0.1213  | val_0_rmse: 0.31885 |  0:00:07s\n",
      "epoch 52 | loss: 0.13292 | val_0_rmse: 0.31524 |  0:00:07s\n",
      "epoch 53 | loss: 0.14676 | val_0_rmse: 0.32242 |  0:00:07s\n",
      "epoch 54 | loss: 0.12343 | val_0_rmse: 0.29452 |  0:00:07s\n",
      "epoch 55 | loss: 0.10261 | val_0_rmse: 0.31976 |  0:00:08s\n",
      "epoch 56 | loss: 0.10397 | val_0_rmse: 0.28542 |  0:00:08s\n",
      "epoch 57 | loss: 0.09071 | val_0_rmse: 0.2882  |  0:00:08s\n",
      "epoch 58 | loss: 0.09845 | val_0_rmse: 0.26468 |  0:00:08s\n",
      "epoch 59 | loss: 0.08955 | val_0_rmse: 0.28312 |  0:00:08s\n",
      "epoch 60 | loss: 0.08087 | val_0_rmse: 0.27371 |  0:00:08s\n",
      "epoch 61 | loss: 0.08999 | val_0_rmse: 0.28196 |  0:00:08s\n",
      "epoch 62 | loss: 0.081   | val_0_rmse: 0.26912 |  0:00:08s\n",
      "epoch 63 | loss: 0.15978 | val_0_rmse: 0.29427 |  0:00:09s\n",
      "epoch 64 | loss: 0.1033  | val_0_rmse: 0.27928 |  0:00:09s\n",
      "epoch 65 | loss: 0.0907  | val_0_rmse: 0.27386 |  0:00:09s\n",
      "epoch 66 | loss: 0.0919  | val_0_rmse: 0.27876 |  0:00:09s\n",
      "epoch 67 | loss: 0.09091 | val_0_rmse: 0.2953  |  0:00:09s\n",
      "epoch 68 | loss: 0.09825 | val_0_rmse: 0.27034 |  0:00:09s\n",
      "epoch 69 | loss: 0.09031 | val_0_rmse: 0.27966 |  0:00:09s\n",
      "epoch 70 | loss: 0.0792  | val_0_rmse: 0.26088 |  0:00:10s\n",
      "epoch 71 | loss: 0.07937 | val_0_rmse: 0.24552 |  0:00:10s\n",
      "epoch 72 | loss: 0.06715 | val_0_rmse: 0.26696 |  0:00:10s\n",
      "epoch 73 | loss: 0.07619 | val_0_rmse: 0.26555 |  0:00:10s\n",
      "epoch 74 | loss: 0.07631 | val_0_rmse: 0.26046 |  0:00:10s\n",
      "epoch 75 | loss: 0.06966 | val_0_rmse: 0.25275 |  0:00:10s\n",
      "epoch 76 | loss: 0.06227 | val_0_rmse: 0.26139 |  0:00:11s\n",
      "epoch 77 | loss: 0.06432 | val_0_rmse: 0.25057 |  0:00:11s\n",
      "epoch 78 | loss: 0.06218 | val_0_rmse: 0.25826 |  0:00:11s\n",
      "epoch 79 | loss: 0.069   | val_0_rmse: 0.26134 |  0:00:11s\n",
      "epoch 80 | loss: 0.07038 | val_0_rmse: 0.23756 |  0:00:11s\n",
      "epoch 81 | loss: 0.05729 | val_0_rmse: 0.26139 |  0:00:11s\n",
      "epoch 82 | loss: 0.06992 | val_0_rmse: 0.23045 |  0:00:11s\n",
      "epoch 83 | loss: 0.05946 | val_0_rmse: 0.24163 |  0:00:12s\n",
      "epoch 84 | loss: 0.05647 | val_0_rmse: 0.24988 |  0:00:12s\n",
      "epoch 85 | loss: 0.06696 | val_0_rmse: 0.25086 |  0:00:12s\n",
      "epoch 86 | loss: 0.06129 | val_0_rmse: 0.24521 |  0:00:12s\n",
      "epoch 87 | loss: 0.05125 | val_0_rmse: 0.24937 |  0:00:12s\n",
      "epoch 88 | loss: 0.06611 | val_0_rmse: 0.24981 |  0:00:12s\n",
      "epoch 89 | loss: 0.0539  | val_0_rmse: 0.25716 |  0:00:12s\n",
      "epoch 90 | loss: 0.05282 | val_0_rmse: 0.21589 |  0:00:12s\n",
      "epoch 91 | loss: 0.05683 | val_0_rmse: 0.24853 |  0:00:13s\n",
      "epoch 92 | loss: 0.05582 | val_0_rmse: 0.24748 |  0:00:13s\n",
      "epoch 93 | loss: 0.05168 | val_0_rmse: 0.23247 |  0:00:13s\n",
      "epoch 94 | loss: 0.0589  | val_0_rmse: 0.23212 |  0:00:13s\n",
      "epoch 95 | loss: 0.04686 | val_0_rmse: 0.23594 |  0:00:13s\n",
      "epoch 96 | loss: 0.06669 | val_0_rmse: 0.21943 |  0:00:13s\n",
      "epoch 97 | loss: 0.06075 | val_0_rmse: 0.25014 |  0:00:13s\n",
      "epoch 98 | loss: 0.05586 | val_0_rmse: 0.23722 |  0:00:14s\n",
      "epoch 99 | loss: 0.05153 | val_0_rmse: 0.24345 |  0:00:14s\n",
      "epoch 100| loss: 0.0591  | val_0_rmse: 0.24502 |  0:00:14s\n",
      "epoch 101| loss: 0.05417 | val_0_rmse: 0.22039 |  0:00:14s\n",
      "epoch 102| loss: 0.05546 | val_0_rmse: 0.25371 |  0:00:14s\n",
      "epoch 103| loss: 0.05186 | val_0_rmse: 0.22561 |  0:00:14s\n",
      "epoch 104| loss: 0.04544 | val_0_rmse: 0.25253 |  0:00:15s\n",
      "epoch 105| loss: 0.06462 | val_0_rmse: 0.23029 |  0:00:15s\n",
      "epoch 106| loss: 0.04631 | val_0_rmse: 0.23909 |  0:00:15s\n",
      "epoch 107| loss: 0.04554 | val_0_rmse: 0.24026 |  0:00:15s\n",
      "epoch 108| loss: 0.04846 | val_0_rmse: 0.24413 |  0:00:15s\n",
      "epoch 109| loss: 0.06632 | val_0_rmse: 0.2453  |  0:00:15s\n",
      "epoch 110| loss: 0.0449  | val_0_rmse: 0.25225 |  0:00:15s\n",
      "epoch 111| loss: 0.06156 | val_0_rmse: 0.22327 |  0:00:16s\n",
      "epoch 112| loss: 0.05093 | val_0_rmse: 0.22576 |  0:00:16s\n",
      "epoch 113| loss: 0.04253 | val_0_rmse: 0.23606 |  0:00:16s\n",
      "epoch 114| loss: 0.04448 | val_0_rmse: 0.25434 |  0:00:16s\n",
      "epoch 115| loss: 0.06398 | val_0_rmse: 0.23017 |  0:00:16s\n",
      "\n",
      "Early stopping occurred at epoch 115 with best_epoch = 90 and best_val_0_rmse = 0.21589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:19:58,479] Trial 33 finished with value: 0.21589088538492635 and parameters: {'n_d': 16, 'n_a': 24, 'n_steps': 3, 'gamma': 1.9917821053746154, 'n_independent': 1, 'n_shared': 2, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.008884106292305858, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 033 | rmse_log=0.21589 | RMSE$=44,915 | MAE$=28,847 | MAPE=16.56% | n_d/n_a=16/24 steps=3 lr=0.00888 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 159.15641| val_0_rmse: 11.43365|  0:00:00s\n",
      "epoch 1  | loss: 119.7557| val_0_rmse: 10.759  |  0:00:00s\n",
      "epoch 2  | loss: 88.00822| val_0_rmse: 9.91975 |  0:00:00s\n",
      "epoch 3  | loss: 62.36786| val_0_rmse: 9.04045 |  0:00:00s\n",
      "epoch 4  | loss: 44.95249| val_0_rmse: 8.08621 |  0:00:01s\n",
      "epoch 5  | loss: 33.98449| val_0_rmse: 6.97814 |  0:00:01s\n",
      "epoch 6  | loss: 20.1763 | val_0_rmse: 5.75659 |  0:00:01s\n",
      "epoch 7  | loss: 20.1724 | val_0_rmse: 4.69611 |  0:00:01s\n",
      "epoch 8  | loss: 19.0658 | val_0_rmse: 4.02909 |  0:00:02s\n",
      "epoch 9  | loss: 16.02043| val_0_rmse: 3.69746 |  0:00:02s\n",
      "epoch 10 | loss: 15.06419| val_0_rmse: 3.67892 |  0:00:02s\n",
      "epoch 11 | loss: 11.27534| val_0_rmse: 3.77564 |  0:00:02s\n",
      "epoch 12 | loss: 8.9931  | val_0_rmse: 3.72927 |  0:00:03s\n",
      "epoch 13 | loss: 7.12196 | val_0_rmse: 3.29778 |  0:00:03s\n",
      "epoch 14 | loss: 5.33792 | val_0_rmse: 2.55643 |  0:00:03s\n",
      "epoch 15 | loss: 3.97112 | val_0_rmse: 1.97735 |  0:00:03s\n",
      "epoch 16 | loss: 3.37996 | val_0_rmse: 1.95619 |  0:00:04s\n",
      "epoch 17 | loss: 3.38641 | val_0_rmse: 2.07522 |  0:00:04s\n",
      "epoch 18 | loss: 1.90431 | val_0_rmse: 1.18927 |  0:00:04s\n",
      "epoch 19 | loss: 1.85839 | val_0_rmse: 1.16755 |  0:00:04s\n",
      "epoch 20 | loss: 1.47407 | val_0_rmse: 1.46327 |  0:00:05s\n",
      "epoch 21 | loss: 1.49786 | val_0_rmse: 0.81933 |  0:00:05s\n",
      "epoch 22 | loss: 1.12696 | val_0_rmse: 0.74376 |  0:00:05s\n",
      "epoch 23 | loss: 0.94496 | val_0_rmse: 1.08985 |  0:00:05s\n",
      "epoch 24 | loss: 0.93556 | val_0_rmse: 0.61302 |  0:00:06s\n",
      "epoch 25 | loss: 1.06379 | val_0_rmse: 0.73458 |  0:00:06s\n",
      "epoch 26 | loss: 0.79351 | val_0_rmse: 0.93332 |  0:00:06s\n",
      "epoch 27 | loss: 0.73117 | val_0_rmse: 0.73225 |  0:00:06s\n",
      "epoch 28 | loss: 0.64577 | val_0_rmse: 0.71236 |  0:00:07s\n",
      "epoch 29 | loss: 0.70274 | val_0_rmse: 0.85815 |  0:00:07s\n",
      "epoch 30 | loss: 0.67334 | val_0_rmse: 0.6316  |  0:00:07s\n",
      "epoch 31 | loss: 0.56327 | val_0_rmse: 0.65458 |  0:00:07s\n",
      "epoch 32 | loss: 0.57834 | val_0_rmse: 0.45239 |  0:00:07s\n",
      "epoch 33 | loss: 0.50281 | val_0_rmse: 0.8723  |  0:00:08s\n",
      "epoch 34 | loss: 0.52149 | val_0_rmse: 0.56131 |  0:00:08s\n",
      "epoch 35 | loss: 0.52028 | val_0_rmse: 0.74061 |  0:00:08s\n",
      "epoch 36 | loss: 0.47355 | val_0_rmse: 0.62855 |  0:00:08s\n",
      "epoch 37 | loss: 0.34142 | val_0_rmse: 0.45932 |  0:00:09s\n",
      "epoch 38 | loss: 0.35922 | val_0_rmse: 0.60794 |  0:00:09s\n",
      "epoch 39 | loss: 0.37107 | val_0_rmse: 0.54897 |  0:00:09s\n",
      "epoch 40 | loss: 0.3482  | val_0_rmse: 0.49924 |  0:00:09s\n",
      "epoch 41 | loss: 0.33093 | val_0_rmse: 0.42474 |  0:00:10s\n",
      "epoch 42 | loss: 0.37997 | val_0_rmse: 0.66088 |  0:00:10s\n",
      "epoch 43 | loss: 0.38229 | val_0_rmse: 0.40495 |  0:00:10s\n",
      "epoch 44 | loss: 0.48658 | val_0_rmse: 0.80531 |  0:00:10s\n",
      "epoch 45 | loss: 0.44886 | val_0_rmse: 0.63642 |  0:00:11s\n",
      "epoch 46 | loss: 1.00071 | val_0_rmse: 0.83079 |  0:00:11s\n",
      "epoch 47 | loss: 0.68156 | val_0_rmse: 0.45027 |  0:00:11s\n",
      "epoch 48 | loss: 0.26901 | val_0_rmse: 0.50534 |  0:00:11s\n",
      "epoch 49 | loss: 0.26299 | val_0_rmse: 0.46169 |  0:00:11s\n",
      "epoch 50 | loss: 0.32271 | val_0_rmse: 0.63102 |  0:00:12s\n",
      "epoch 51 | loss: 0.46366 | val_0_rmse: 0.51147 |  0:00:12s\n",
      "epoch 52 | loss: 0.30423 | val_0_rmse: 0.44361 |  0:00:12s\n",
      "epoch 53 | loss: 0.33095 | val_0_rmse: 0.85649 |  0:00:12s\n",
      "epoch 54 | loss: 0.47699 | val_0_rmse: 0.46292 |  0:00:13s\n",
      "epoch 55 | loss: 0.92869 | val_0_rmse: 0.50123 |  0:00:13s\n",
      "epoch 56 | loss: 0.46912 | val_0_rmse: 0.81714 |  0:00:13s\n",
      "epoch 57 | loss: 0.58349 | val_0_rmse: 0.54807 |  0:00:13s\n",
      "epoch 58 | loss: 0.28818 | val_0_rmse: 0.43266 |  0:00:14s\n",
      "epoch 59 | loss: 0.37861 | val_0_rmse: 0.37087 |  0:00:14s\n",
      "epoch 60 | loss: 0.19131 | val_0_rmse: 0.45855 |  0:00:14s\n",
      "epoch 61 | loss: 0.17284 | val_0_rmse: 0.39543 |  0:00:14s\n",
      "epoch 62 | loss: 0.19156 | val_0_rmse: 0.46237 |  0:00:15s\n",
      "epoch 63 | loss: 0.17214 | val_0_rmse: 0.37692 |  0:00:15s\n",
      "epoch 64 | loss: 0.15197 | val_0_rmse: 0.42051 |  0:00:15s\n",
      "epoch 65 | loss: 0.16714 | val_0_rmse: 0.38054 |  0:00:15s\n",
      "epoch 66 | loss: 0.19891 | val_0_rmse: 0.42429 |  0:00:15s\n",
      "epoch 67 | loss: 0.17085 | val_0_rmse: 0.4418  |  0:00:16s\n",
      "epoch 68 | loss: 0.15161 | val_0_rmse: 0.43229 |  0:00:16s\n",
      "epoch 69 | loss: 0.18003 | val_0_rmse: 0.34722 |  0:00:16s\n",
      "epoch 70 | loss: 0.157   | val_0_rmse: 0.3847  |  0:00:16s\n",
      "epoch 71 | loss: 0.17021 | val_0_rmse: 0.43789 |  0:00:17s\n",
      "epoch 72 | loss: 0.16964 | val_0_rmse: 0.35734 |  0:00:17s\n",
      "epoch 73 | loss: 0.15469 | val_0_rmse: 0.40064 |  0:00:17s\n",
      "epoch 74 | loss: 0.12885 | val_0_rmse: 0.37402 |  0:00:17s\n",
      "epoch 75 | loss: 0.12189 | val_0_rmse: 0.29601 |  0:00:17s\n",
      "epoch 76 | loss: 0.13282 | val_0_rmse: 0.36603 |  0:00:18s\n",
      "epoch 77 | loss: 0.16393 | val_0_rmse: 0.33143 |  0:00:18s\n",
      "epoch 78 | loss: 0.11707 | val_0_rmse: 0.30301 |  0:00:18s\n",
      "epoch 79 | loss: 0.13233 | val_0_rmse: 0.30745 |  0:00:18s\n",
      "epoch 80 | loss: 0.10859 | val_0_rmse: 0.29279 |  0:00:19s\n",
      "epoch 81 | loss: 0.14388 | val_0_rmse: 0.3134  |  0:00:19s\n",
      "epoch 82 | loss: 0.10789 | val_0_rmse: 0.29875 |  0:00:19s\n",
      "epoch 83 | loss: 0.09082 | val_0_rmse: 0.34689 |  0:00:19s\n",
      "epoch 84 | loss: 0.10856 | val_0_rmse: 0.31203 |  0:00:19s\n",
      "epoch 85 | loss: 0.10551 | val_0_rmse: 0.31227 |  0:00:20s\n",
      "epoch 86 | loss: 0.11777 | val_0_rmse: 0.41672 |  0:00:20s\n",
      "epoch 87 | loss: 0.13609 | val_0_rmse: 0.31214 |  0:00:20s\n",
      "epoch 88 | loss: 0.12894 | val_0_rmse: 0.34982 |  0:00:20s\n",
      "epoch 89 | loss: 0.12003 | val_0_rmse: 0.30706 |  0:00:21s\n",
      "epoch 90 | loss: 0.09512 | val_0_rmse: 0.29646 |  0:00:21s\n",
      "epoch 91 | loss: 0.10171 | val_0_rmse: 0.30419 |  0:00:21s\n",
      "epoch 92 | loss: 0.09877 | val_0_rmse: 0.33814 |  0:00:21s\n",
      "epoch 93 | loss: 0.11004 | val_0_rmse: 0.33586 |  0:00:21s\n",
      "epoch 94 | loss: 0.09342 | val_0_rmse: 0.30976 |  0:00:22s\n",
      "epoch 95 | loss: 0.0989  | val_0_rmse: 0.30935 |  0:00:22s\n",
      "epoch 96 | loss: 0.08518 | val_0_rmse: 0.32963 |  0:00:22s\n",
      "epoch 97 | loss: 0.08687 | val_0_rmse: 0.34537 |  0:00:22s\n",
      "epoch 98 | loss: 0.09052 | val_0_rmse: 0.41868 |  0:00:23s\n",
      "epoch 99 | loss: 0.11678 | val_0_rmse: 0.33936 |  0:00:23s\n",
      "epoch 100| loss: 0.19029 | val_0_rmse: 0.35566 |  0:00:23s\n",
      "epoch 101| loss: 0.11734 | val_0_rmse: 0.30991 |  0:00:23s\n",
      "epoch 102| loss: 0.09662 | val_0_rmse: 0.33237 |  0:00:23s\n",
      "epoch 103| loss: 0.11459 | val_0_rmse: 0.32488 |  0:00:24s\n",
      "epoch 104| loss: 0.1383  | val_0_rmse: 0.48183 |  0:00:24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:20:23,513] Trial 34 finished with value: 0.292793582100813 and parameters: {'n_d': 32, 'n_a': 24, 'n_steps': 6, 'gamma': 1.2477320916634067, 'n_independent': 1, 'n_shared': 2, 'lambda_sparse': 1e-05, 'mask_type': 'sparsemax', 'lr': 0.01104454201888091, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 105| loss: 0.14846 | val_0_rmse: 0.46379 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 80 and best_val_0_rmse = 0.29279\n",
      "Trial 034 | rmse_log=0.29279 | RMSE$=59,840 | MAE$=39,667 | MAPE=21.83% | n_d/n_a=32/24 steps=6 lr=0.01104 batch=512 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 224.23793| val_0_rmse: 12.0249 |  0:00:00s\n",
      "epoch 1  | loss: 187.3613| val_0_rmse: 11.60386|  0:00:00s\n",
      "epoch 2  | loss: 161.46965| val_0_rmse: 11.23242|  0:00:00s\n",
      "epoch 3  | loss: 137.10281| val_0_rmse: 10.90377|  0:00:00s\n",
      "epoch 4  | loss: 115.81619| val_0_rmse: 10.61172|  0:00:00s\n",
      "epoch 5  | loss: 98.21264| val_0_rmse: 10.28609|  0:00:00s\n",
      "epoch 6  | loss: 81.96613| val_0_rmse: 9.91287 |  0:00:00s\n",
      "epoch 7  | loss: 70.84543| val_0_rmse: 9.52314 |  0:00:01s\n",
      "epoch 8  | loss: 57.05082| val_0_rmse: 9.11863 |  0:00:01s\n",
      "epoch 9  | loss: 56.1542 | val_0_rmse: 8.69771 |  0:00:01s\n",
      "epoch 10 | loss: 43.63632| val_0_rmse: 8.28497 |  0:00:01s\n",
      "epoch 11 | loss: 38.72512| val_0_rmse: 7.8533  |  0:00:01s\n",
      "epoch 12 | loss: 33.48854| val_0_rmse: 7.3898  |  0:00:01s\n",
      "epoch 13 | loss: 27.46255| val_0_rmse: 6.92446 |  0:00:01s\n",
      "epoch 14 | loss: 25.50464| val_0_rmse: 6.40578 |  0:00:02s\n",
      "epoch 15 | loss: 22.85452| val_0_rmse: 5.8507  |  0:00:02s\n",
      "epoch 16 | loss: 19.12425| val_0_rmse: 5.33977 |  0:00:02s\n",
      "epoch 17 | loss: 17.277  | val_0_rmse: 4.86552 |  0:00:02s\n",
      "epoch 18 | loss: 18.36554| val_0_rmse: 4.41703 |  0:00:02s\n",
      "epoch 19 | loss: 15.38184| val_0_rmse: 4.0177  |  0:00:02s\n",
      "epoch 20 | loss: 13.75049| val_0_rmse: 3.57836 |  0:00:03s\n",
      "epoch 21 | loss: 11.86843| val_0_rmse: 3.33743 |  0:00:03s\n",
      "epoch 22 | loss: 10.42359| val_0_rmse: 3.18906 |  0:00:03s\n",
      "epoch 23 | loss: 10.11764| val_0_rmse: 3.18506 |  0:00:03s\n",
      "epoch 24 | loss: 6.5088  | val_0_rmse: 3.15987 |  0:00:03s\n",
      "epoch 25 | loss: 5.25417 | val_0_rmse: 3.17219 |  0:00:03s\n",
      "epoch 26 | loss: 4.15007 | val_0_rmse: 3.1439  |  0:00:03s\n",
      "epoch 27 | loss: 3.93138 | val_0_rmse: 2.99524 |  0:00:03s\n",
      "epoch 28 | loss: 4.61326 | val_0_rmse: 2.81776 |  0:00:04s\n",
      "epoch 29 | loss: 3.24097 | val_0_rmse: 2.50795 |  0:00:04s\n",
      "epoch 30 | loss: 2.65082 | val_0_rmse: 2.13698 |  0:00:04s\n",
      "epoch 31 | loss: 2.06695 | val_0_rmse: 1.72873 |  0:00:04s\n",
      "epoch 32 | loss: 1.84085 | val_0_rmse: 1.44232 |  0:00:04s\n",
      "epoch 33 | loss: 1.46675 | val_0_rmse: 1.28434 |  0:00:04s\n",
      "epoch 34 | loss: 1.63768 | val_0_rmse: 1.2444  |  0:00:04s\n",
      "epoch 35 | loss: 1.53842 | val_0_rmse: 1.26417 |  0:00:05s\n",
      "epoch 36 | loss: 1.11809 | val_0_rmse: 1.20951 |  0:00:05s\n",
      "epoch 37 | loss: 0.98525 | val_0_rmse: 1.05838 |  0:00:05s\n",
      "epoch 38 | loss: 0.88465 | val_0_rmse: 0.9807  |  0:00:05s\n",
      "epoch 39 | loss: 1.0344  | val_0_rmse: 0.9826  |  0:00:05s\n",
      "epoch 40 | loss: 0.71784 | val_0_rmse: 0.86435 |  0:00:05s\n",
      "epoch 41 | loss: 0.8409  | val_0_rmse: 0.80142 |  0:00:05s\n",
      "epoch 42 | loss: 0.7065  | val_0_rmse: 0.89009 |  0:00:06s\n",
      "epoch 43 | loss: 0.61124 | val_0_rmse: 0.90649 |  0:00:06s\n",
      "epoch 44 | loss: 0.65611 | val_0_rmse: 0.82781 |  0:00:06s\n",
      "epoch 45 | loss: 0.61838 | val_0_rmse: 0.62135 |  0:00:06s\n",
      "epoch 46 | loss: 0.73644 | val_0_rmse: 0.59711 |  0:00:06s\n",
      "epoch 47 | loss: 0.63313 | val_0_rmse: 0.75922 |  0:00:06s\n",
      "epoch 48 | loss: 0.53433 | val_0_rmse: 0.76623 |  0:00:06s\n",
      "epoch 49 | loss: 0.43876 | val_0_rmse: 0.58036 |  0:00:07s\n",
      "epoch 50 | loss: 0.57174 | val_0_rmse: 0.49895 |  0:00:07s\n",
      "epoch 51 | loss: 0.44468 | val_0_rmse: 0.61539 |  0:00:07s\n",
      "epoch 52 | loss: 0.37846 | val_0_rmse: 0.69826 |  0:00:07s\n",
      "epoch 53 | loss: 0.45912 | val_0_rmse: 0.57836 |  0:00:07s\n",
      "epoch 54 | loss: 0.37043 | val_0_rmse: 0.48624 |  0:00:07s\n",
      "epoch 55 | loss: 0.35061 | val_0_rmse: 0.56584 |  0:00:07s\n",
      "epoch 56 | loss: 0.35159 | val_0_rmse: 0.65319 |  0:00:07s\n",
      "epoch 57 | loss: 0.31899 | val_0_rmse: 0.56462 |  0:00:07s\n",
      "epoch 58 | loss: 0.30063 | val_0_rmse: 0.40768 |  0:00:08s\n",
      "epoch 59 | loss: 0.31731 | val_0_rmse: 0.38513 |  0:00:08s\n",
      "epoch 60 | loss: 0.35089 | val_0_rmse: 0.46001 |  0:00:08s\n",
      "epoch 61 | loss: 0.3372  | val_0_rmse: 0.43258 |  0:00:08s\n",
      "epoch 62 | loss: 0.31088 | val_0_rmse: 0.40023 |  0:00:08s\n",
      "epoch 63 | loss: 0.29013 | val_0_rmse: 0.42981 |  0:00:08s\n",
      "epoch 64 | loss: 0.21793 | val_0_rmse: 0.38375 |  0:00:08s\n",
      "epoch 65 | loss: 0.27385 | val_0_rmse: 0.37345 |  0:00:09s\n",
      "epoch 66 | loss: 0.26871 | val_0_rmse: 0.42725 |  0:00:09s\n",
      "epoch 67 | loss: 0.24937 | val_0_rmse: 0.36147 |  0:00:09s\n",
      "epoch 68 | loss: 0.21402 | val_0_rmse: 0.39638 |  0:00:09s\n",
      "epoch 69 | loss: 0.23041 | val_0_rmse: 0.38205 |  0:00:09s\n",
      "epoch 70 | loss: 0.24792 | val_0_rmse: 0.39198 |  0:00:09s\n",
      "epoch 71 | loss: 0.22338 | val_0_rmse: 0.42464 |  0:00:09s\n",
      "epoch 72 | loss: 0.23834 | val_0_rmse: 0.38877 |  0:00:10s\n",
      "epoch 73 | loss: 0.25123 | val_0_rmse: 0.36989 |  0:00:10s\n",
      "epoch 74 | loss: 0.23274 | val_0_rmse: 0.47671 |  0:00:10s\n",
      "epoch 75 | loss: 0.27687 | val_0_rmse: 0.52532 |  0:00:10s\n",
      "epoch 76 | loss: 0.27257 | val_0_rmse: 0.41684 |  0:00:10s\n",
      "epoch 77 | loss: 0.18301 | val_0_rmse: 0.32763 |  0:00:10s\n",
      "epoch 78 | loss: 0.18728 | val_0_rmse: 0.36724 |  0:00:10s\n",
      "epoch 79 | loss: 0.19915 | val_0_rmse: 0.40936 |  0:00:10s\n",
      "epoch 80 | loss: 0.20745 | val_0_rmse: 0.3904  |  0:00:11s\n",
      "epoch 81 | loss: 0.23219 | val_0_rmse: 0.35343 |  0:00:11s\n",
      "epoch 82 | loss: 0.21013 | val_0_rmse: 0.34908 |  0:00:11s\n",
      "epoch 83 | loss: 0.19845 | val_0_rmse: 0.45668 |  0:00:11s\n",
      "epoch 84 | loss: 0.21434 | val_0_rmse: 0.4846  |  0:00:11s\n",
      "epoch 85 | loss: 0.24191 | val_0_rmse: 0.37163 |  0:00:11s\n",
      "epoch 86 | loss: 0.17391 | val_0_rmse: 0.31959 |  0:00:11s\n",
      "epoch 87 | loss: 0.15619 | val_0_rmse: 0.39948 |  0:00:11s\n",
      "epoch 88 | loss: 0.15746 | val_0_rmse: 0.36095 |  0:00:12s\n",
      "epoch 89 | loss: 0.15626 | val_0_rmse: 0.30329 |  0:00:12s\n",
      "epoch 90 | loss: 0.13005 | val_0_rmse: 0.33124 |  0:00:12s\n",
      "epoch 91 | loss: 0.15381 | val_0_rmse: 0.3001  |  0:00:12s\n",
      "epoch 92 | loss: 0.1268  | val_0_rmse: 0.28983 |  0:00:12s\n",
      "epoch 93 | loss: 0.13309 | val_0_rmse: 0.35482 |  0:00:12s\n",
      "epoch 94 | loss: 0.15232 | val_0_rmse: 0.33604 |  0:00:12s\n",
      "epoch 95 | loss: 0.12378 | val_0_rmse: 0.32465 |  0:00:13s\n",
      "epoch 96 | loss: 0.11625 | val_0_rmse: 0.28032 |  0:00:13s\n",
      "epoch 97 | loss: 0.1037  | val_0_rmse: 0.31041 |  0:00:13s\n",
      "epoch 98 | loss: 0.12378 | val_0_rmse: 0.27535 |  0:00:13s\n",
      "epoch 99 | loss: 0.10787 | val_0_rmse: 0.30672 |  0:00:13s\n",
      "epoch 100| loss: 0.10655 | val_0_rmse: 0.28596 |  0:00:13s\n",
      "epoch 101| loss: 0.10906 | val_0_rmse: 0.29446 |  0:00:13s\n",
      "epoch 102| loss: 0.09253 | val_0_rmse: 0.296   |  0:00:13s\n",
      "epoch 103| loss: 0.09482 | val_0_rmse: 0.24991 |  0:00:14s\n",
      "epoch 104| loss: 0.08669 | val_0_rmse: 0.26775 |  0:00:14s\n",
      "epoch 105| loss: 0.09842 | val_0_rmse: 0.31729 |  0:00:14s\n",
      "epoch 106| loss: 0.09861 | val_0_rmse: 0.30293 |  0:00:14s\n",
      "epoch 107| loss: 0.10559 | val_0_rmse: 0.26957 |  0:00:14s\n",
      "epoch 108| loss: 0.13925 | val_0_rmse: 0.26227 |  0:00:14s\n",
      "epoch 109| loss: 0.13746 | val_0_rmse: 0.22967 |  0:00:14s\n",
      "epoch 110| loss: 0.08447 | val_0_rmse: 0.33573 |  0:00:15s\n",
      "epoch 111| loss: 0.1753  | val_0_rmse: 0.40053 |  0:00:15s\n",
      "epoch 112| loss: 0.24657 | val_0_rmse: 0.35522 |  0:00:15s\n",
      "epoch 113| loss: 0.18323 | val_0_rmse: 0.27103 |  0:00:15s\n",
      "epoch 114| loss: 0.10456 | val_0_rmse: 0.284   |  0:00:15s\n",
      "epoch 115| loss: 0.14044 | val_0_rmse: 0.30701 |  0:00:15s\n",
      "epoch 116| loss: 0.20519 | val_0_rmse: 0.26438 |  0:00:15s\n",
      "epoch 117| loss: 0.15319 | val_0_rmse: 0.26026 |  0:00:15s\n",
      "epoch 118| loss: 0.13713 | val_0_rmse: 0.29003 |  0:00:15s\n",
      "epoch 119| loss: 0.14162 | val_0_rmse: 0.25794 |  0:00:16s\n",
      "epoch 120| loss: 0.08906 | val_0_rmse: 0.26584 |  0:00:16s\n",
      "epoch 121| loss: 0.10103 | val_0_rmse: 0.29979 |  0:00:16s\n",
      "epoch 122| loss: 0.11836 | val_0_rmse: 0.26676 |  0:00:16s\n",
      "epoch 123| loss: 0.08958 | val_0_rmse: 0.28527 |  0:00:16s\n",
      "epoch 124| loss: 0.11695 | val_0_rmse: 0.31903 |  0:00:16s\n",
      "epoch 125| loss: 0.16289 | val_0_rmse: 0.27995 |  0:00:16s\n",
      "epoch 126| loss: 0.11656 | val_0_rmse: 0.23681 |  0:00:16s\n",
      "epoch 127| loss: 0.0977  | val_0_rmse: 0.2528  |  0:00:17s\n",
      "epoch 128| loss: 0.11151 | val_0_rmse: 0.23342 |  0:00:17s\n",
      "epoch 129| loss: 0.06324 | val_0_rmse: 0.27167 |  0:00:17s\n",
      "epoch 130| loss: 0.09038 | val_0_rmse: 0.30558 |  0:00:17s\n",
      "epoch 131| loss: 0.12764 | val_0_rmse: 0.27552 |  0:00:17s\n",
      "epoch 132| loss: 0.09264 | val_0_rmse: 0.28727 |  0:00:17s\n",
      "epoch 133| loss: 0.09198 | val_0_rmse: 0.31217 |  0:00:17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:20:41,816] Trial 35 finished with value: 0.22966573743466623 and parameters: {'n_d': 48, 'n_a': 32, 'n_steps': 4, 'gamma': 1.3612092262389233, 'n_independent': 3, 'n_shared': 1, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.012722360335948698, 'batch_size': 2048, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 134| loss: 0.12912 | val_0_rmse: 0.26748 |  0:00:17s\n",
      "\n",
      "Early stopping occurred at epoch 134 with best_epoch = 109 and best_val_0_rmse = 0.22967\n",
      "Trial 035 | rmse_log=0.22967 | RMSE$=47,995 | MAE$=30,392 | MAPE=17.43% | n_d/n_a=48/32 steps=4 lr=0.01272 batch=2048 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 233.98066| val_0_rmse: 11.83914|  0:00:00s\n",
      "epoch 1  | loss: 206.3713| val_0_rmse: 11.50998|  0:00:00s\n",
      "epoch 2  | loss: 172.15336| val_0_rmse: 11.19269|  0:00:00s\n",
      "epoch 3  | loss: 146.52645| val_0_rmse: 10.85204|  0:00:01s\n",
      "epoch 4  | loss: 125.74032| val_0_rmse: 10.48169|  0:00:01s\n",
      "epoch 5  | loss: 104.94406| val_0_rmse: 10.08473|  0:00:01s\n",
      "epoch 6  | loss: 82.62857| val_0_rmse: 9.64434 |  0:00:02s\n",
      "epoch 7  | loss: 68.58389| val_0_rmse: 9.2508  |  0:00:02s\n",
      "epoch 8  | loss: 54.72614| val_0_rmse: 8.88977 |  0:00:02s\n",
      "epoch 9  | loss: 43.50056| val_0_rmse: 8.40553 |  0:00:03s\n",
      "epoch 10 | loss: 35.12825| val_0_rmse: 7.94328 |  0:00:03s\n",
      "epoch 11 | loss: 29.54267| val_0_rmse: 7.54772 |  0:00:03s\n",
      "epoch 12 | loss: 27.09661| val_0_rmse: 7.13792 |  0:00:03s\n",
      "epoch 13 | loss: 25.10324| val_0_rmse: 6.8019  |  0:00:04s\n",
      "epoch 14 | loss: 23.87886| val_0_rmse: 6.52219 |  0:00:04s\n",
      "epoch 15 | loss: 22.77802| val_0_rmse: 6.24286 |  0:00:04s\n",
      "epoch 16 | loss: 21.59903| val_0_rmse: 5.93281 |  0:00:05s\n",
      "epoch 17 | loss: 18.53604| val_0_rmse: 5.6567  |  0:00:05s\n",
      "epoch 18 | loss: 20.78145| val_0_rmse: 5.46546 |  0:00:05s\n",
      "epoch 19 | loss: 18.28311| val_0_rmse: 5.25356 |  0:00:05s\n",
      "epoch 20 | loss: 19.91435| val_0_rmse: 5.10112 |  0:00:06s\n",
      "epoch 21 | loss: 17.20203| val_0_rmse: 5.04371 |  0:00:06s\n",
      "epoch 22 | loss: 15.41323| val_0_rmse: 4.86107 |  0:00:06s\n",
      "epoch 23 | loss: 14.61736| val_0_rmse: 4.78692 |  0:00:07s\n",
      "epoch 24 | loss: 13.6986 | val_0_rmse: 4.63603 |  0:00:07s\n",
      "epoch 25 | loss: 13.09088| val_0_rmse: 4.55247 |  0:00:07s\n",
      "epoch 26 | loss: 13.25262| val_0_rmse: 4.4082  |  0:00:08s\n",
      "epoch 27 | loss: 10.51883| val_0_rmse: 4.20871 |  0:00:08s\n",
      "epoch 28 | loss: 10.48703| val_0_rmse: 3.99775 |  0:00:08s\n",
      "epoch 29 | loss: 9.46995 | val_0_rmse: 3.80712 |  0:00:09s\n",
      "epoch 30 | loss: 9.18728 | val_0_rmse: 3.51162 |  0:00:09s\n",
      "epoch 31 | loss: 7.7139  | val_0_rmse: 3.21731 |  0:00:09s\n",
      "epoch 32 | loss: 7.40106 | val_0_rmse: 2.95506 |  0:00:09s\n",
      "epoch 33 | loss: 7.18631 | val_0_rmse: 2.76805 |  0:00:10s\n",
      "epoch 34 | loss: 7.28387 | val_0_rmse: 2.56996 |  0:00:10s\n",
      "epoch 35 | loss: 5.69161 | val_0_rmse: 2.36424 |  0:00:10s\n",
      "epoch 36 | loss: 5.25231 | val_0_rmse: 2.13943 |  0:00:11s\n",
      "epoch 37 | loss: 5.12087 | val_0_rmse: 1.94791 |  0:00:11s\n",
      "epoch 38 | loss: 4.03987 | val_0_rmse: 1.73803 |  0:00:11s\n",
      "epoch 39 | loss: 4.60073 | val_0_rmse: 1.53699 |  0:00:11s\n",
      "epoch 40 | loss: 3.8182  | val_0_rmse: 1.31732 |  0:00:12s\n",
      "epoch 41 | loss: 3.70375 | val_0_rmse: 1.12369 |  0:00:12s\n",
      "epoch 42 | loss: 3.81251 | val_0_rmse: 1.04605 |  0:00:12s\n",
      "epoch 43 | loss: 3.2745  | val_0_rmse: 0.96965 |  0:00:13s\n",
      "epoch 44 | loss: 3.21212 | val_0_rmse: 0.93772 |  0:00:13s\n",
      "epoch 45 | loss: 2.78223 | val_0_rmse: 0.91152 |  0:00:13s\n",
      "epoch 46 | loss: 2.65949 | val_0_rmse: 0.80918 |  0:00:13s\n",
      "epoch 47 | loss: 2.4094  | val_0_rmse: 0.63409 |  0:00:14s\n",
      "epoch 48 | loss: 2.58101 | val_0_rmse: 0.56297 |  0:00:14s\n",
      "epoch 49 | loss: 2.19786 | val_0_rmse: 0.54653 |  0:00:14s\n",
      "epoch 50 | loss: 2.33355 | val_0_rmse: 0.6104  |  0:00:15s\n",
      "epoch 51 | loss: 1.89645 | val_0_rmse: 0.63994 |  0:00:15s\n",
      "epoch 52 | loss: 2.19914 | val_0_rmse: 0.57229 |  0:00:15s\n",
      "epoch 53 | loss: 2.04223 | val_0_rmse: 0.49395 |  0:00:15s\n",
      "epoch 54 | loss: 2.25347 | val_0_rmse: 0.48041 |  0:00:16s\n",
      "epoch 55 | loss: 1.74101 | val_0_rmse: 0.49183 |  0:00:16s\n",
      "epoch 56 | loss: 1.78544 | val_0_rmse: 0.60138 |  0:00:16s\n",
      "epoch 57 | loss: 1.67466 | val_0_rmse: 0.54267 |  0:00:17s\n",
      "epoch 58 | loss: 1.61706 | val_0_rmse: 0.49285 |  0:00:17s\n",
      "epoch 59 | loss: 1.86136 | val_0_rmse: 0.50967 |  0:00:17s\n",
      "epoch 60 | loss: 1.51176 | val_0_rmse: 0.4775  |  0:00:17s\n",
      "epoch 61 | loss: 1.52406 | val_0_rmse: 0.50792 |  0:00:18s\n",
      "epoch 62 | loss: 1.70566 | val_0_rmse: 0.52917 |  0:00:18s\n",
      "epoch 63 | loss: 1.26818 | val_0_rmse: 0.51267 |  0:00:18s\n",
      "epoch 64 | loss: 1.57302 | val_0_rmse: 0.4722  |  0:00:18s\n",
      "epoch 65 | loss: 1.37568 | val_0_rmse: 0.4765  |  0:00:19s\n",
      "epoch 66 | loss: 1.44578 | val_0_rmse: 0.49363 |  0:00:19s\n",
      "epoch 67 | loss: 1.25652 | val_0_rmse: 0.49991 |  0:00:19s\n",
      "epoch 68 | loss: 1.154   | val_0_rmse: 0.47569 |  0:00:20s\n",
      "epoch 69 | loss: 1.21989 | val_0_rmse: 0.45593 |  0:00:20s\n",
      "epoch 70 | loss: 1.26088 | val_0_rmse: 0.4248  |  0:00:20s\n",
      "epoch 71 | loss: 1.1476  | val_0_rmse: 0.45904 |  0:00:20s\n",
      "epoch 72 | loss: 1.18731 | val_0_rmse: 0.4502  |  0:00:21s\n",
      "epoch 73 | loss: 1.16066 | val_0_rmse: 0.42513 |  0:00:21s\n",
      "epoch 74 | loss: 1.14621 | val_0_rmse: 0.42892 |  0:00:21s\n",
      "epoch 75 | loss: 1.17018 | val_0_rmse: 0.48478 |  0:00:22s\n",
      "epoch 76 | loss: 1.27339 | val_0_rmse: 0.51021 |  0:00:22s\n",
      "epoch 77 | loss: 1.23708 | val_0_rmse: 0.43933 |  0:00:22s\n",
      "epoch 78 | loss: 1.1961  | val_0_rmse: 0.45247 |  0:00:22s\n",
      "epoch 79 | loss: 1.02541 | val_0_rmse: 0.46935 |  0:00:23s\n",
      "epoch 80 | loss: 0.93917 | val_0_rmse: 0.44541 |  0:00:23s\n",
      "epoch 81 | loss: 0.93108 | val_0_rmse: 0.44301 |  0:00:23s\n",
      "epoch 82 | loss: 1.00017 | val_0_rmse: 0.44768 |  0:00:24s\n",
      "epoch 83 | loss: 1.07103 | val_0_rmse: 0.50923 |  0:00:24s\n",
      "epoch 84 | loss: 0.95325 | val_0_rmse: 0.44006 |  0:00:24s\n",
      "epoch 85 | loss: 0.92483 | val_0_rmse: 0.42871 |  0:00:24s\n",
      "epoch 86 | loss: 0.8421  | val_0_rmse: 0.47312 |  0:00:25s\n",
      "epoch 87 | loss: 0.82403 | val_0_rmse: 0.45383 |  0:00:25s\n",
      "epoch 88 | loss: 0.83074 | val_0_rmse: 0.41261 |  0:00:25s\n",
      "epoch 89 | loss: 0.7839  | val_0_rmse: 0.42156 |  0:00:26s\n",
      "epoch 90 | loss: 0.8639  | val_0_rmse: 0.42859 |  0:00:26s\n",
      "epoch 91 | loss: 0.76037 | val_0_rmse: 0.43437 |  0:00:26s\n",
      "epoch 92 | loss: 0.6919  | val_0_rmse: 0.40852 |  0:00:26s\n",
      "epoch 93 | loss: 0.65528 | val_0_rmse: 0.40715 |  0:00:27s\n",
      "epoch 94 | loss: 0.68951 | val_0_rmse: 0.43816 |  0:00:27s\n",
      "epoch 95 | loss: 0.63553 | val_0_rmse: 0.42195 |  0:00:27s\n",
      "epoch 96 | loss: 0.67121 | val_0_rmse: 0.40822 |  0:00:28s\n",
      "epoch 97 | loss: 0.66464 | val_0_rmse: 0.37677 |  0:00:28s\n",
      "epoch 98 | loss: 0.56787 | val_0_rmse: 0.4036  |  0:00:28s\n",
      "epoch 99 | loss: 0.68548 | val_0_rmse: 0.42022 |  0:00:28s\n",
      "epoch 100| loss: 0.64116 | val_0_rmse: 0.39453 |  0:00:29s\n",
      "epoch 101| loss: 0.80822 | val_0_rmse: 0.42535 |  0:00:29s\n",
      "epoch 102| loss: 0.64645 | val_0_rmse: 0.40534 |  0:00:29s\n",
      "epoch 103| loss: 0.63244 | val_0_rmse: 0.40135 |  0:00:30s\n",
      "epoch 104| loss: 0.57594 | val_0_rmse: 0.41467 |  0:00:30s\n",
      "epoch 105| loss: 0.66062 | val_0_rmse: 0.37654 |  0:00:30s\n",
      "epoch 106| loss: 0.67168 | val_0_rmse: 0.38743 |  0:00:30s\n",
      "epoch 107| loss: 0.58929 | val_0_rmse: 0.43056 |  0:00:31s\n",
      "epoch 108| loss: 0.58171 | val_0_rmse: 0.51735 |  0:00:31s\n",
      "epoch 109| loss: 0.68918 | val_0_rmse: 0.42966 |  0:00:31s\n",
      "epoch 110| loss: 0.61151 | val_0_rmse: 0.39912 |  0:00:31s\n",
      "epoch 111| loss: 0.59432 | val_0_rmse: 0.46561 |  0:00:32s\n",
      "epoch 112| loss: 0.60458 | val_0_rmse: 0.37249 |  0:00:32s\n",
      "epoch 113| loss: 0.59023 | val_0_rmse: 0.3596  |  0:00:32s\n",
      "epoch 114| loss: 0.55091 | val_0_rmse: 0.39926 |  0:00:33s\n",
      "epoch 115| loss: 0.5808  | val_0_rmse: 0.40036 |  0:00:33s\n",
      "epoch 116| loss: 0.43251 | val_0_rmse: 0.35156 |  0:00:33s\n",
      "epoch 117| loss: 0.58108 | val_0_rmse: 0.39702 |  0:00:34s\n",
      "epoch 118| loss: 0.4519  | val_0_rmse: 0.42532 |  0:00:34s\n",
      "epoch 119| loss: 0.46343 | val_0_rmse: 0.39111 |  0:00:34s\n",
      "epoch 120| loss: 0.60362 | val_0_rmse: 0.38844 |  0:00:34s\n",
      "epoch 121| loss: 0.46988 | val_0_rmse: 0.42821 |  0:00:35s\n",
      "epoch 122| loss: 0.5609  | val_0_rmse: 0.37748 |  0:00:35s\n",
      "epoch 123| loss: 0.49243 | val_0_rmse: 0.3981  |  0:00:35s\n",
      "epoch 124| loss: 0.47444 | val_0_rmse: 0.38609 |  0:00:35s\n",
      "epoch 125| loss: 0.46021 | val_0_rmse: 0.38224 |  0:00:36s\n",
      "epoch 126| loss: 0.42679 | val_0_rmse: 0.3885  |  0:00:36s\n",
      "epoch 127| loss: 0.48336 | val_0_rmse: 0.41079 |  0:00:36s\n",
      "epoch 128| loss: 0.43653 | val_0_rmse: 0.36338 |  0:00:36s\n",
      "epoch 129| loss: 0.50853 | val_0_rmse: 0.3427  |  0:00:37s\n",
      "epoch 130| loss: 0.3824  | val_0_rmse: 0.35304 |  0:00:37s\n",
      "epoch 131| loss: 0.39497 | val_0_rmse: 0.34558 |  0:00:37s\n",
      "epoch 132| loss: 0.36921 | val_0_rmse: 0.32154 |  0:00:38s\n",
      "epoch 133| loss: 0.37225 | val_0_rmse: 0.30522 |  0:00:38s\n",
      "epoch 134| loss: 0.40256 | val_0_rmse: 0.31688 |  0:00:38s\n",
      "epoch 135| loss: 0.41386 | val_0_rmse: 0.31242 |  0:00:39s\n",
      "epoch 136| loss: 0.37816 | val_0_rmse: 0.33358 |  0:00:39s\n",
      "epoch 137| loss: 0.37333 | val_0_rmse: 0.35821 |  0:00:39s\n",
      "epoch 138| loss: 0.33236 | val_0_rmse: 0.32894 |  0:00:39s\n",
      "epoch 139| loss: 0.34788 | val_0_rmse: 0.34872 |  0:00:40s\n",
      "epoch 140| loss: 0.34284 | val_0_rmse: 0.36076 |  0:00:40s\n",
      "epoch 141| loss: 0.32696 | val_0_rmse: 0.34349 |  0:00:40s\n",
      "epoch 142| loss: 0.31209 | val_0_rmse: 0.347   |  0:00:41s\n",
      "epoch 143| loss: 0.35921 | val_0_rmse: 0.41532 |  0:00:41s\n",
      "epoch 144| loss: 0.32441 | val_0_rmse: 0.40046 |  0:00:41s\n",
      "epoch 145| loss: 0.44125 | val_0_rmse: 0.37035 |  0:00:41s\n",
      "epoch 146| loss: 0.36758 | val_0_rmse: 0.42175 |  0:00:42s\n",
      "epoch 147| loss: 0.30837 | val_0_rmse: 0.47622 |  0:00:42s\n",
      "epoch 148| loss: 0.36266 | val_0_rmse: 0.41577 |  0:00:42s\n",
      "epoch 149| loss: 0.33971 | val_0_rmse: 0.38118 |  0:00:43s\n",
      "epoch 150| loss: 0.30158 | val_0_rmse: 0.38876 |  0:00:43s\n",
      "epoch 151| loss: 0.29181 | val_0_rmse: 0.4728  |  0:00:43s\n",
      "epoch 152| loss: 0.44004 | val_0_rmse: 0.4267  |  0:00:43s\n",
      "epoch 153| loss: 0.29464 | val_0_rmse: 0.3928  |  0:00:44s\n",
      "epoch 154| loss: 0.28729 | val_0_rmse: 0.45016 |  0:00:44s\n",
      "epoch 155| loss: 0.31351 | val_0_rmse: 0.3784  |  0:00:44s\n",
      "epoch 156| loss: 0.26382 | val_0_rmse: 0.39054 |  0:00:44s\n",
      "epoch 157| loss: 0.24181 | val_0_rmse: 0.38936 |  0:00:45s\n",
      "epoch 158| loss: 0.3066  | val_0_rmse: 0.39455 |  0:00:45s\n",
      "\n",
      "Early stopping occurred at epoch 158 with best_epoch = 133 and best_val_0_rmse = 0.30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:21:27,786] Trial 36 finished with value: 0.30521615918712686 and parameters: {'n_d': 64, 'n_a': 64, 'n_steps': 6, 'gamma': 1.3172262186416401, 'n_independent': 1, 'n_shared': 2, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.003001601042429095, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 036 | rmse_log=0.30522 | RMSE$=60,518 | MAE$=40,607 | MAPE=23.57% | n_d/n_a=64/64 steps=6 lr=0.00300 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 179.8657| val_0_rmse: 12.1019 |  0:00:00s\n",
      "epoch 1  | loss: 164.91827| val_0_rmse: 11.82528|  0:00:00s\n",
      "epoch 2  | loss: 153.77636| val_0_rmse: 11.6735 |  0:00:00s\n",
      "epoch 3  | loss: 142.50624| val_0_rmse: 11.48224|  0:00:01s\n",
      "epoch 4  | loss: 132.68711| val_0_rmse: 11.32643|  0:00:01s\n",
      "epoch 5  | loss: 123.81878| val_0_rmse: 11.12257|  0:00:01s\n",
      "epoch 6  | loss: 114.52036| val_0_rmse: 10.90193|  0:00:02s\n",
      "epoch 7  | loss: 105.33621| val_0_rmse: 10.66285|  0:00:02s\n",
      "epoch 8  | loss: 96.30831| val_0_rmse: 10.35761|  0:00:02s\n",
      "epoch 9  | loss: 86.83022| val_0_rmse: 9.98978 |  0:00:02s\n",
      "epoch 10 | loss: 77.88454| val_0_rmse: 9.6119  |  0:00:03s\n",
      "epoch 11 | loss: 70.39733| val_0_rmse: 9.16173 |  0:00:03s\n",
      "epoch 12 | loss: 61.24147| val_0_rmse: 8.69874 |  0:00:03s\n",
      "epoch 13 | loss: 54.9694 | val_0_rmse: 8.24033 |  0:00:03s\n",
      "epoch 14 | loss: 45.25244| val_0_rmse: 7.79508 |  0:00:04s\n",
      "epoch 15 | loss: 38.58088| val_0_rmse: 7.31087 |  0:00:04s\n",
      "epoch 16 | loss: 32.77803| val_0_rmse: 6.85709 |  0:00:04s\n",
      "epoch 17 | loss: 26.76192| val_0_rmse: 6.32473 |  0:00:05s\n",
      "epoch 18 | loss: 21.4326 | val_0_rmse: 5.66938 |  0:00:05s\n",
      "epoch 19 | loss: 18.4882 | val_0_rmse: 5.1696  |  0:00:05s\n",
      "epoch 20 | loss: 14.39993| val_0_rmse: 4.65717 |  0:00:05s\n",
      "epoch 21 | loss: 13.17584| val_0_rmse: 4.21574 |  0:00:06s\n",
      "epoch 22 | loss: 10.73334| val_0_rmse: 3.92953 |  0:00:06s\n",
      "epoch 23 | loss: 9.51449 | val_0_rmse: 3.64632 |  0:00:06s\n",
      "epoch 24 | loss: 9.39703 | val_0_rmse: 3.44082 |  0:00:07s\n",
      "epoch 25 | loss: 7.91204 | val_0_rmse: 3.22932 |  0:00:07s\n",
      "epoch 26 | loss: 5.81359 | val_0_rmse: 3.08358 |  0:00:07s\n",
      "epoch 27 | loss: 4.67717 | val_0_rmse: 3.06434 |  0:00:07s\n",
      "epoch 28 | loss: 4.72295 | val_0_rmse: 2.99267 |  0:00:08s\n",
      "epoch 29 | loss: 4.17871 | val_0_rmse: 2.81466 |  0:00:08s\n",
      "epoch 30 | loss: 3.32116 | val_0_rmse: 2.56569 |  0:00:08s\n",
      "epoch 31 | loss: 2.49467 | val_0_rmse: 2.22812 |  0:00:09s\n",
      "epoch 32 | loss: 2.19397 | val_0_rmse: 1.88494 |  0:00:09s\n",
      "epoch 33 | loss: 1.99878 | val_0_rmse: 1.7523  |  0:00:09s\n",
      "epoch 34 | loss: 2.3666  | val_0_rmse: 1.70902 |  0:00:09s\n",
      "epoch 35 | loss: 1.6121  | val_0_rmse: 1.54629 |  0:00:10s\n",
      "epoch 36 | loss: 1.56506 | val_0_rmse: 1.38401 |  0:00:10s\n",
      "epoch 37 | loss: 1.18838 | val_0_rmse: 1.33184 |  0:00:10s\n",
      "epoch 38 | loss: 1.25562 | val_0_rmse: 1.19572 |  0:00:11s\n",
      "epoch 39 | loss: 1.19454 | val_0_rmse: 1.02772 |  0:00:11s\n",
      "epoch 40 | loss: 0.98317 | val_0_rmse: 1.03252 |  0:00:11s\n",
      "epoch 41 | loss: 1.19549 | val_0_rmse: 0.97079 |  0:00:11s\n",
      "epoch 42 | loss: 1.02221 | val_0_rmse: 0.90693 |  0:00:12s\n",
      "epoch 43 | loss: 0.89126 | val_0_rmse: 0.8469  |  0:00:12s\n",
      "epoch 44 | loss: 1.05349 | val_0_rmse: 0.79065 |  0:00:12s\n",
      "epoch 45 | loss: 0.98492 | val_0_rmse: 0.78724 |  0:00:13s\n",
      "epoch 46 | loss: 0.87445 | val_0_rmse: 0.71544 |  0:00:13s\n",
      "epoch 47 | loss: 0.73608 | val_0_rmse: 0.73832 |  0:00:13s\n",
      "epoch 48 | loss: 0.86307 | val_0_rmse: 0.67586 |  0:00:13s\n",
      "epoch 49 | loss: 0.70535 | val_0_rmse: 0.61396 |  0:00:14s\n",
      "epoch 50 | loss: 0.56164 | val_0_rmse: 0.63848 |  0:00:14s\n",
      "epoch 51 | loss: 0.77808 | val_0_rmse: 0.59835 |  0:00:14s\n",
      "epoch 52 | loss: 0.60882 | val_0_rmse: 0.58196 |  0:00:14s\n",
      "epoch 53 | loss: 0.62731 | val_0_rmse: 0.61424 |  0:00:15s\n",
      "epoch 54 | loss: 0.58345 | val_0_rmse: 0.68413 |  0:00:15s\n",
      "epoch 55 | loss: 0.5401  | val_0_rmse: 0.5662  |  0:00:15s\n",
      "epoch 56 | loss: 0.53538 | val_0_rmse: 0.49126 |  0:00:15s\n",
      "epoch 57 | loss: 0.50799 | val_0_rmse: 0.58065 |  0:00:16s\n",
      "epoch 58 | loss: 0.44335 | val_0_rmse: 0.5783  |  0:00:16s\n",
      "epoch 59 | loss: 0.50439 | val_0_rmse: 0.44235 |  0:00:16s\n",
      "epoch 60 | loss: 0.43128 | val_0_rmse: 0.47503 |  0:00:17s\n",
      "epoch 61 | loss: 0.48196 | val_0_rmse: 0.44571 |  0:00:17s\n",
      "epoch 62 | loss: 0.37911 | val_0_rmse: 0.48824 |  0:00:17s\n",
      "epoch 63 | loss: 0.37585 | val_0_rmse: 0.50863 |  0:00:17s\n",
      "epoch 64 | loss: 0.37775 | val_0_rmse: 0.45268 |  0:00:18s\n",
      "epoch 65 | loss: 0.46757 | val_0_rmse: 0.37986 |  0:00:18s\n",
      "epoch 66 | loss: 0.42931 | val_0_rmse: 0.43871 |  0:00:18s\n",
      "epoch 67 | loss: 0.53116 | val_0_rmse: 0.46775 |  0:00:19s\n",
      "epoch 68 | loss: 0.36795 | val_0_rmse: 0.40547 |  0:00:19s\n",
      "epoch 69 | loss: 0.33978 | val_0_rmse: 0.47482 |  0:00:19s\n",
      "epoch 70 | loss: 0.44635 | val_0_rmse: 0.48129 |  0:00:19s\n",
      "epoch 71 | loss: 0.3     | val_0_rmse: 0.40693 |  0:00:20s\n",
      "epoch 72 | loss: 0.33476 | val_0_rmse: 0.44315 |  0:00:20s\n",
      "epoch 73 | loss: 0.33228 | val_0_rmse: 0.41816 |  0:00:20s\n",
      "epoch 74 | loss: 0.42706 | val_0_rmse: 0.39007 |  0:00:20s\n",
      "epoch 75 | loss: 0.32075 | val_0_rmse: 0.37328 |  0:00:21s\n",
      "epoch 76 | loss: 0.33846 | val_0_rmse: 0.38535 |  0:00:21s\n",
      "epoch 77 | loss: 0.3519  | val_0_rmse: 0.38847 |  0:00:21s\n",
      "epoch 78 | loss: 0.29098 | val_0_rmse: 0.39752 |  0:00:21s\n",
      "epoch 79 | loss: 0.26406 | val_0_rmse: 0.48144 |  0:00:22s\n",
      "epoch 80 | loss: 0.36661 | val_0_rmse: 0.45573 |  0:00:22s\n",
      "epoch 81 | loss: 0.27017 | val_0_rmse: 0.46501 |  0:00:22s\n",
      "epoch 82 | loss: 0.2775  | val_0_rmse: 0.43114 |  0:00:23s\n",
      "epoch 83 | loss: 0.29766 | val_0_rmse: 0.42146 |  0:00:23s\n",
      "epoch 84 | loss: 0.29302 | val_0_rmse: 0.50271 |  0:00:23s\n",
      "epoch 85 | loss: 0.2792  | val_0_rmse: 0.41941 |  0:00:23s\n",
      "epoch 86 | loss: 0.33086 | val_0_rmse: 0.46613 |  0:00:24s\n",
      "epoch 87 | loss: 0.25577 | val_0_rmse: 0.46775 |  0:00:24s\n",
      "epoch 88 | loss: 0.26178 | val_0_rmse: 0.39671 |  0:00:24s\n",
      "epoch 89 | loss: 0.24724 | val_0_rmse: 0.57829 |  0:00:24s\n",
      "epoch 90 | loss: 0.302   | val_0_rmse: 0.43102 |  0:00:25s\n",
      "epoch 91 | loss: 0.26277 | val_0_rmse: 0.5005  |  0:00:25s\n",
      "epoch 92 | loss: 0.26109 | val_0_rmse: 0.4118  |  0:00:25s\n",
      "epoch 93 | loss: 0.20913 | val_0_rmse: 0.35347 |  0:00:25s\n",
      "epoch 94 | loss: 0.22656 | val_0_rmse: 0.52217 |  0:00:26s\n",
      "epoch 95 | loss: 0.34041 | val_0_rmse: 0.37599 |  0:00:26s\n",
      "epoch 96 | loss: 0.23653 | val_0_rmse: 0.33437 |  0:00:26s\n",
      "epoch 97 | loss: 0.21492 | val_0_rmse: 0.55304 |  0:00:26s\n",
      "epoch 98 | loss: 0.31349 | val_0_rmse: 0.45586 |  0:00:27s\n",
      "epoch 99 | loss: 0.24277 | val_0_rmse: 0.3452  |  0:00:27s\n",
      "epoch 100| loss: 0.30687 | val_0_rmse: 0.38507 |  0:00:27s\n",
      "epoch 101| loss: 0.22744 | val_0_rmse: 0.42962 |  0:00:27s\n",
      "epoch 102| loss: 0.22621 | val_0_rmse: 0.3262  |  0:00:28s\n",
      "epoch 103| loss: 0.20561 | val_0_rmse: 0.35032 |  0:00:28s\n",
      "epoch 104| loss: 0.2329  | val_0_rmse: 0.38093 |  0:00:28s\n",
      "epoch 105| loss: 0.19916 | val_0_rmse: 0.34675 |  0:00:29s\n",
      "epoch 106| loss: 0.20647 | val_0_rmse: 0.33818 |  0:00:29s\n",
      "epoch 107| loss: 0.21532 | val_0_rmse: 0.36964 |  0:00:29s\n",
      "epoch 108| loss: 0.20695 | val_0_rmse: 0.33127 |  0:00:29s\n",
      "epoch 109| loss: 0.22595 | val_0_rmse: 0.33363 |  0:00:30s\n",
      "epoch 110| loss: 0.19714 | val_0_rmse: 0.43723 |  0:00:30s\n",
      "epoch 111| loss: 0.20529 | val_0_rmse: 0.3426  |  0:00:30s\n",
      "epoch 112| loss: 0.23532 | val_0_rmse: 0.36411 |  0:00:30s\n",
      "epoch 113| loss: 0.17722 | val_0_rmse: 0.3779  |  0:00:31s\n",
      "epoch 114| loss: 0.19587 | val_0_rmse: 0.33196 |  0:00:31s\n",
      "epoch 115| loss: 0.17574 | val_0_rmse: 0.33759 |  0:00:31s\n",
      "epoch 116| loss: 0.21298 | val_0_rmse: 0.34228 |  0:00:31s\n",
      "epoch 117| loss: 0.16559 | val_0_rmse: 0.33491 |  0:00:32s\n",
      "epoch 118| loss: 0.15943 | val_0_rmse: 0.37396 |  0:00:32s\n",
      "epoch 119| loss: 0.1805  | val_0_rmse: 0.36603 |  0:00:32s\n",
      "epoch 120| loss: 0.1603  | val_0_rmse: 0.33623 |  0:00:32s\n",
      "epoch 121| loss: 0.15752 | val_0_rmse: 0.33966 |  0:00:33s\n",
      "epoch 122| loss: 0.15715 | val_0_rmse: 0.33238 |  0:00:33s\n",
      "epoch 123| loss: 0.17778 | val_0_rmse: 0.3354  |  0:00:33s\n",
      "epoch 124| loss: 0.18772 | val_0_rmse: 0.32405 |  0:00:34s\n",
      "epoch 125| loss: 0.16464 | val_0_rmse: 0.31652 |  0:00:34s\n",
      "epoch 126| loss: 0.1509  | val_0_rmse: 0.29592 |  0:00:34s\n",
      "epoch 127| loss: 0.15912 | val_0_rmse: 0.31334 |  0:00:34s\n",
      "epoch 128| loss: 0.17179 | val_0_rmse: 0.33008 |  0:00:35s\n",
      "epoch 129| loss: 0.16589 | val_0_rmse: 0.29861 |  0:00:35s\n",
      "epoch 130| loss: 0.13667 | val_0_rmse: 0.30055 |  0:00:35s\n",
      "epoch 131| loss: 0.1384  | val_0_rmse: 0.40711 |  0:00:35s\n",
      "epoch 132| loss: 0.19549 | val_0_rmse: 0.30923 |  0:00:36s\n",
      "epoch 133| loss: 0.15217 | val_0_rmse: 0.33891 |  0:00:36s\n",
      "epoch 134| loss: 0.17363 | val_0_rmse: 0.30219 |  0:00:36s\n",
      "epoch 135| loss: 0.19623 | val_0_rmse: 0.30764 |  0:00:36s\n",
      "epoch 136| loss: 0.16385 | val_0_rmse: 0.39554 |  0:00:37s\n",
      "epoch 137| loss: 0.15181 | val_0_rmse: 0.35023 |  0:00:37s\n",
      "epoch 138| loss: 0.17002 | val_0_rmse: 0.32056 |  0:00:37s\n",
      "epoch 139| loss: 0.17158 | val_0_rmse: 0.42447 |  0:00:38s\n",
      "epoch 140| loss: 0.20499 | val_0_rmse: 0.31721 |  0:00:38s\n",
      "epoch 141| loss: 0.15535 | val_0_rmse: 0.34972 |  0:00:38s\n",
      "epoch 142| loss: 0.16551 | val_0_rmse: 0.29445 |  0:00:38s\n",
      "epoch 143| loss: 0.14248 | val_0_rmse: 0.31558 |  0:00:39s\n",
      "epoch 144| loss: 0.15621 | val_0_rmse: 0.36535 |  0:00:39s\n",
      "epoch 145| loss: 0.14829 | val_0_rmse: 0.32709 |  0:00:39s\n",
      "epoch 146| loss: 0.1313  | val_0_rmse: 0.29703 |  0:00:39s\n",
      "epoch 147| loss: 0.11001 | val_0_rmse: 0.32177 |  0:00:40s\n",
      "epoch 148| loss: 0.12611 | val_0_rmse: 0.29434 |  0:00:40s\n",
      "epoch 149| loss: 0.13111 | val_0_rmse: 0.31173 |  0:00:40s\n",
      "epoch 150| loss: 0.12327 | val_0_rmse: 0.33203 |  0:00:40s\n",
      "epoch 151| loss: 0.12585 | val_0_rmse: 0.28302 |  0:00:41s\n",
      "epoch 152| loss: 0.14049 | val_0_rmse: 0.30369 |  0:00:41s\n",
      "epoch 153| loss: 0.11305 | val_0_rmse: 0.31347 |  0:00:41s\n",
      "epoch 154| loss: 0.1125  | val_0_rmse: 0.28567 |  0:00:41s\n",
      "epoch 155| loss: 0.1156  | val_0_rmse: 0.31951 |  0:00:42s\n",
      "epoch 156| loss: 0.12188 | val_0_rmse: 0.29657 |  0:00:42s\n",
      "epoch 157| loss: 0.12853 | val_0_rmse: 0.27939 |  0:00:42s\n",
      "epoch 158| loss: 0.11633 | val_0_rmse: 0.29811 |  0:00:43s\n",
      "epoch 159| loss: 0.12576 | val_0_rmse: 0.30235 |  0:00:43s\n",
      "epoch 160| loss: 0.11164 | val_0_rmse: 0.33986 |  0:00:43s\n",
      "epoch 161| loss: 0.12394 | val_0_rmse: 0.27451 |  0:00:43s\n",
      "epoch 162| loss: 0.1406  | val_0_rmse: 0.31563 |  0:00:44s\n",
      "epoch 163| loss: 0.14163 | val_0_rmse: 0.30682 |  0:00:44s\n",
      "epoch 164| loss: 0.1227  | val_0_rmse: 0.32803 |  0:00:44s\n",
      "epoch 165| loss: 0.12324 | val_0_rmse: 0.37564 |  0:00:44s\n",
      "epoch 166| loss: 0.12063 | val_0_rmse: 0.3156  |  0:00:45s\n",
      "epoch 167| loss: 0.1055  | val_0_rmse: 0.3327  |  0:00:45s\n",
      "epoch 168| loss: 0.11435 | val_0_rmse: 0.29361 |  0:00:45s\n",
      "epoch 169| loss: 0.1067  | val_0_rmse: 0.29487 |  0:00:45s\n",
      "epoch 170| loss: 0.11679 | val_0_rmse: 0.30832 |  0:00:46s\n",
      "epoch 171| loss: 0.10298 | val_0_rmse: 0.33075 |  0:00:46s\n",
      "epoch 172| loss: 0.10296 | val_0_rmse: 0.27041 |  0:00:46s\n",
      "epoch 173| loss: 0.10842 | val_0_rmse: 0.31911 |  0:00:47s\n",
      "epoch 174| loss: 0.10939 | val_0_rmse: 0.24387 |  0:00:47s\n",
      "epoch 175| loss: 0.11214 | val_0_rmse: 0.27684 |  0:00:47s\n",
      "epoch 176| loss: 0.09831 | val_0_rmse: 0.26036 |  0:00:47s\n",
      "epoch 177| loss: 0.10296 | val_0_rmse: 0.27383 |  0:00:48s\n",
      "epoch 178| loss: 0.10039 | val_0_rmse: 0.33629 |  0:00:48s\n",
      "epoch 179| loss: 0.12331 | val_0_rmse: 0.2791  |  0:00:48s\n",
      "epoch 180| loss: 0.08625 | val_0_rmse: 0.26726 |  0:00:48s\n",
      "epoch 181| loss: 0.09529 | val_0_rmse: 0.24201 |  0:00:49s\n",
      "epoch 182| loss: 0.09686 | val_0_rmse: 0.23348 |  0:00:49s\n",
      "epoch 183| loss: 0.10329 | val_0_rmse: 0.25179 |  0:00:49s\n",
      "epoch 184| loss: 0.08875 | val_0_rmse: 0.28018 |  0:00:49s\n",
      "epoch 185| loss: 0.0954  | val_0_rmse: 0.26842 |  0:00:50s\n",
      "epoch 186| loss: 0.08964 | val_0_rmse: 0.28374 |  0:00:50s\n",
      "epoch 187| loss: 0.0946  | val_0_rmse: 0.27153 |  0:00:50s\n",
      "epoch 188| loss: 0.08378 | val_0_rmse: 0.30632 |  0:00:51s\n",
      "epoch 189| loss: 0.09118 | val_0_rmse: 0.27755 |  0:00:51s\n",
      "epoch 190| loss: 0.09686 | val_0_rmse: 0.32565 |  0:00:51s\n",
      "epoch 191| loss: 0.09303 | val_0_rmse: 0.30725 |  0:00:51s\n",
      "epoch 192| loss: 0.09207 | val_0_rmse: 0.27221 |  0:00:52s\n",
      "epoch 193| loss: 0.07831 | val_0_rmse: 0.40772 |  0:00:52s\n",
      "epoch 194| loss: 0.12994 | val_0_rmse: 0.28613 |  0:00:52s\n",
      "epoch 195| loss: 0.09728 | val_0_rmse: 0.27053 |  0:00:52s\n",
      "epoch 196| loss: 0.09249 | val_0_rmse: 0.3579  |  0:00:53s\n",
      "epoch 197| loss: 0.08663 | val_0_rmse: 0.31477 |  0:00:53s\n",
      "epoch 198| loss: 0.09486 | val_0_rmse: 0.3416  |  0:00:53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:22:22,093] Trial 37 finished with value: 0.23347839073500995 and parameters: {'n_d': 16, 'n_a': 24, 'n_steps': 5, 'gamma': 1.2531358604053695, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-06, 'mask_type': 'sparsemax', 'lr': 0.005535731534811681, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199| loss: 0.08597 | val_0_rmse: 0.33426 |  0:00:53s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 182 and best_val_0_rmse = 0.23348\n",
      "Trial 037 | rmse_log=0.23348 | RMSE$=50,366 | MAE$=31,422 | MAPE=17.86% | n_d/n_a=16/24 steps=5 lr=0.00554 batch=512 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 123.48901| val_0_rmse: 11.41956|  0:00:00s\n",
      "epoch 1  | loss: 100.70711| val_0_rmse: 10.96111|  0:00:00s\n",
      "epoch 2  | loss: 83.68935| val_0_rmse: 10.466  |  0:00:00s\n",
      "epoch 3  | loss: 70.17699| val_0_rmse: 9.92813 |  0:00:00s\n",
      "epoch 4  | loss: 61.99934| val_0_rmse: 9.39352 |  0:00:00s\n",
      "epoch 5  | loss: 51.474  | val_0_rmse: 8.80917 |  0:00:00s\n",
      "epoch 6  | loss: 44.79676| val_0_rmse: 8.20673 |  0:00:00s\n",
      "epoch 7  | loss: 33.65446| val_0_rmse: 7.55804 |  0:00:01s\n",
      "epoch 8  | loss: 30.92839| val_0_rmse: 6.87233 |  0:00:01s\n",
      "epoch 9  | loss: 28.78675| val_0_rmse: 6.19215 |  0:00:01s\n",
      "epoch 10 | loss: 25.11988| val_0_rmse: 5.54631 |  0:00:01s\n",
      "epoch 11 | loss: 16.99329| val_0_rmse: 4.88278 |  0:00:01s\n",
      "epoch 12 | loss: 18.30247| val_0_rmse: 4.37167 |  0:00:01s\n",
      "epoch 13 | loss: 14.35993| val_0_rmse: 3.88871 |  0:00:01s\n",
      "epoch 14 | loss: 12.2245 | val_0_rmse: 3.53949 |  0:00:01s\n",
      "epoch 15 | loss: 9.36319 | val_0_rmse: 3.26293 |  0:00:02s\n",
      "epoch 16 | loss: 8.29095 | val_0_rmse: 3.05296 |  0:00:02s\n",
      "epoch 17 | loss: 4.77231 | val_0_rmse: 2.85664 |  0:00:02s\n",
      "epoch 18 | loss: 5.09508 | val_0_rmse: 2.73455 |  0:00:02s\n",
      "epoch 19 | loss: 3.34238 | val_0_rmse: 2.50583 |  0:00:02s\n",
      "epoch 20 | loss: 2.9785  | val_0_rmse: 2.07463 |  0:00:02s\n",
      "epoch 21 | loss: 2.44049 | val_0_rmse: 1.59455 |  0:00:02s\n",
      "epoch 22 | loss: 1.98157 | val_0_rmse: 1.19712 |  0:00:03s\n",
      "epoch 23 | loss: 1.62718 | val_0_rmse: 0.99267 |  0:00:03s\n",
      "epoch 24 | loss: 1.89436 | val_0_rmse: 0.96345 |  0:00:03s\n",
      "epoch 25 | loss: 1.40209 | val_0_rmse: 1.12447 |  0:00:03s\n",
      "epoch 26 | loss: 1.09913 | val_0_rmse: 1.38423 |  0:00:03s\n",
      "epoch 27 | loss: 1.11705 | val_0_rmse: 1.3612  |  0:00:03s\n",
      "epoch 28 | loss: 1.23135 | val_0_rmse: 1.17684 |  0:00:03s\n",
      "epoch 29 | loss: 1.08223 | val_0_rmse: 0.92858 |  0:00:03s\n",
      "epoch 30 | loss: 0.91213 | val_0_rmse: 0.8217  |  0:00:04s\n",
      "epoch 31 | loss: 0.83989 | val_0_rmse: 0.9781  |  0:00:04s\n",
      "epoch 32 | loss: 0.64307 | val_0_rmse: 1.09958 |  0:00:04s\n",
      "epoch 33 | loss: 0.75035 | val_0_rmse: 0.93696 |  0:00:04s\n",
      "epoch 34 | loss: 0.71896 | val_0_rmse: 0.73147 |  0:00:04s\n",
      "epoch 35 | loss: 0.69537 | val_0_rmse: 0.71227 |  0:00:04s\n",
      "epoch 36 | loss: 0.60062 | val_0_rmse: 0.8076  |  0:00:04s\n",
      "epoch 37 | loss: 0.56688 | val_0_rmse: 0.75509 |  0:00:04s\n",
      "epoch 38 | loss: 0.47693 | val_0_rmse: 0.53961 |  0:00:04s\n",
      "epoch 39 | loss: 0.42574 | val_0_rmse: 0.50295 |  0:00:05s\n",
      "epoch 40 | loss: 0.48326 | val_0_rmse: 0.65215 |  0:00:05s\n",
      "epoch 41 | loss: 0.36504 | val_0_rmse: 0.6401  |  0:00:05s\n",
      "epoch 42 | loss: 0.63221 | val_0_rmse: 0.56299 |  0:00:05s\n",
      "epoch 43 | loss: 0.34674 | val_0_rmse: 0.4208  |  0:00:05s\n",
      "epoch 44 | loss: 0.29928 | val_0_rmse: 0.46391 |  0:00:05s\n",
      "epoch 45 | loss: 0.35621 | val_0_rmse: 0.55497 |  0:00:05s\n",
      "epoch 46 | loss: 0.25441 | val_0_rmse: 0.45398 |  0:00:05s\n",
      "epoch 47 | loss: 0.3085  | val_0_rmse: 0.46943 |  0:00:05s\n",
      "epoch 48 | loss: 0.21955 | val_0_rmse: 0.39986 |  0:00:05s\n",
      "epoch 49 | loss: 0.20396 | val_0_rmse: 0.44526 |  0:00:06s\n",
      "epoch 50 | loss: 0.2265  | val_0_rmse: 0.41531 |  0:00:06s\n",
      "epoch 51 | loss: 0.23366 | val_0_rmse: 0.4471  |  0:00:06s\n",
      "epoch 52 | loss: 0.17911 | val_0_rmse: 0.42407 |  0:00:06s\n",
      "epoch 53 | loss: 0.21401 | val_0_rmse: 0.44271 |  0:00:06s\n",
      "epoch 54 | loss: 0.24183 | val_0_rmse: 0.48426 |  0:00:06s\n",
      "epoch 55 | loss: 0.24103 | val_0_rmse: 0.42736 |  0:00:06s\n",
      "epoch 56 | loss: 0.17564 | val_0_rmse: 0.43779 |  0:00:06s\n",
      "epoch 57 | loss: 0.20159 | val_0_rmse: 0.38795 |  0:00:06s\n",
      "epoch 58 | loss: 0.20073 | val_0_rmse: 0.40297 |  0:00:06s\n",
      "epoch 59 | loss: 0.17025 | val_0_rmse: 0.40802 |  0:00:07s\n",
      "epoch 60 | loss: 0.15314 | val_0_rmse: 0.36614 |  0:00:07s\n",
      "epoch 61 | loss: 0.19763 | val_0_rmse: 0.39296 |  0:00:07s\n",
      "epoch 62 | loss: 0.1695  | val_0_rmse: 0.37209 |  0:00:07s\n",
      "epoch 63 | loss: 0.16063 | val_0_rmse: 0.35475 |  0:00:07s\n",
      "epoch 64 | loss: 0.12157 | val_0_rmse: 0.31364 |  0:00:07s\n",
      "epoch 65 | loss: 0.15598 | val_0_rmse: 0.31861 |  0:00:07s\n",
      "epoch 66 | loss: 0.13558 | val_0_rmse: 0.38668 |  0:00:07s\n",
      "epoch 67 | loss: 0.14214 | val_0_rmse: 0.35927 |  0:00:07s\n",
      "epoch 68 | loss: 0.12956 | val_0_rmse: 0.31466 |  0:00:08s\n",
      "epoch 69 | loss: 0.11827 | val_0_rmse: 0.3083  |  0:00:08s\n",
      "epoch 70 | loss: 0.10282 | val_0_rmse: 0.37137 |  0:00:08s\n",
      "epoch 71 | loss: 0.09784 | val_0_rmse: 0.36383 |  0:00:08s\n",
      "epoch 72 | loss: 0.12529 | val_0_rmse: 0.30311 |  0:00:08s\n",
      "epoch 73 | loss: 0.14072 | val_0_rmse: 0.31806 |  0:00:08s\n",
      "epoch 74 | loss: 0.14055 | val_0_rmse: 0.30862 |  0:00:08s\n",
      "epoch 75 | loss: 0.09777 | val_0_rmse: 0.35849 |  0:00:08s\n",
      "epoch 76 | loss: 0.13713 | val_0_rmse: 0.3472  |  0:00:08s\n",
      "epoch 77 | loss: 0.09907 | val_0_rmse: 0.29952 |  0:00:09s\n",
      "epoch 78 | loss: 0.11364 | val_0_rmse: 0.30381 |  0:00:09s\n",
      "epoch 79 | loss: 0.12354 | val_0_rmse: 0.29479 |  0:00:09s\n",
      "epoch 80 | loss: 0.07919 | val_0_rmse: 0.31259 |  0:00:09s\n",
      "epoch 81 | loss: 0.1     | val_0_rmse: 0.28173 |  0:00:09s\n",
      "epoch 82 | loss: 0.06858 | val_0_rmse: 0.30463 |  0:00:09s\n",
      "epoch 83 | loss: 0.12754 | val_0_rmse: 0.32146 |  0:00:09s\n",
      "epoch 84 | loss: 0.14369 | val_0_rmse: 0.29061 |  0:00:09s\n",
      "epoch 85 | loss: 0.07368 | val_0_rmse: 0.38477 |  0:00:09s\n",
      "epoch 86 | loss: 0.15001 | val_0_rmse: 0.44068 |  0:00:10s\n",
      "epoch 87 | loss: 0.24097 | val_0_rmse: 0.37542 |  0:00:10s\n",
      "epoch 88 | loss: 0.17345 | val_0_rmse: 0.27978 |  0:00:10s\n",
      "epoch 89 | loss: 0.06184 | val_0_rmse: 0.32876 |  0:00:10s\n",
      "epoch 90 | loss: 0.14628 | val_0_rmse: 0.33758 |  0:00:10s\n",
      "epoch 91 | loss: 0.18742 | val_0_rmse: 0.27772 |  0:00:10s\n",
      "epoch 92 | loss: 0.09581 | val_0_rmse: 0.34069 |  0:00:10s\n",
      "epoch 93 | loss: 0.10135 | val_0_rmse: 0.38988 |  0:00:10s\n",
      "epoch 94 | loss: 0.16143 | val_0_rmse: 0.33074 |  0:00:10s\n",
      "epoch 95 | loss: 0.11811 | val_0_rmse: 0.28421 |  0:00:10s\n",
      "epoch 96 | loss: 0.07581 | val_0_rmse: 0.30364 |  0:00:11s\n",
      "epoch 97 | loss: 0.09353 | val_0_rmse: 0.26958 |  0:00:11s\n",
      "epoch 98 | loss: 0.06747 | val_0_rmse: 0.29521 |  0:00:11s\n",
      "epoch 99 | loss: 0.09742 | val_0_rmse: 0.30928 |  0:00:11s\n",
      "epoch 100| loss: 0.0982  | val_0_rmse: 0.26695 |  0:00:11s\n",
      "epoch 101| loss: 0.06821 | val_0_rmse: 0.27215 |  0:00:11s\n",
      "epoch 102| loss: 0.08891 | val_0_rmse: 0.27206 |  0:00:11s\n",
      "epoch 103| loss: 0.08137 | val_0_rmse: 0.30654 |  0:00:11s\n",
      "epoch 104| loss: 0.07931 | val_0_rmse: 0.31209 |  0:00:11s\n",
      "epoch 105| loss: 0.08441 | val_0_rmse: 0.27147 |  0:00:12s\n",
      "epoch 106| loss: 0.06342 | val_0_rmse: 0.27957 |  0:00:12s\n",
      "epoch 107| loss: 0.08093 | val_0_rmse: 0.26991 |  0:00:12s\n",
      "epoch 108| loss: 0.0697  | val_0_rmse: 0.29669 |  0:00:12s\n",
      "epoch 109| loss: 0.08196 | val_0_rmse: 0.29577 |  0:00:12s\n",
      "epoch 110| loss: 0.08573 | val_0_rmse: 0.27311 |  0:00:12s\n",
      "epoch 111| loss: 0.05715 | val_0_rmse: 0.29799 |  0:00:12s\n",
      "epoch 112| loss: 0.07179 | val_0_rmse: 0.27055 |  0:00:12s\n",
      "epoch 113| loss: 0.05236 | val_0_rmse: 0.30184 |  0:00:12s\n",
      "epoch 114| loss: 0.08564 | val_0_rmse: 0.32306 |  0:00:13s\n",
      "epoch 115| loss: 0.09377 | val_0_rmse: 0.28077 |  0:00:13s\n",
      "epoch 116| loss: 0.06813 | val_0_rmse: 0.30532 |  0:00:13s\n",
      "epoch 117| loss: 0.08938 | val_0_rmse: 0.31092 |  0:00:13s\n",
      "epoch 118| loss: 0.08574 | val_0_rmse: 0.27893 |  0:00:13s\n",
      "epoch 119| loss: 0.05405 | val_0_rmse: 0.28858 |  0:00:13s\n",
      "epoch 120| loss: 0.06755 | val_0_rmse: 0.26811 |  0:00:13s\n",
      "epoch 121| loss: 0.05651 | val_0_rmse: 0.30262 |  0:00:13s\n",
      "epoch 122| loss: 0.08721 | val_0_rmse: 0.32044 |  0:00:13s\n",
      "epoch 123| loss: 0.11847 | val_0_rmse: 0.2866  |  0:00:14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:22:36,653] Trial 38 finished with value: 0.2669486713718865 and parameters: {'n_d': 48, 'n_a': 16, 'n_steps': 3, 'gamma': 1.7378432670945259, 'n_independent': 3, 'n_shared': 1, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.015307465632765853, 'batch_size': 2048, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 124| loss: 0.06034 | val_0_rmse: 0.33343 |  0:00:14s\n",
      "epoch 125| loss: 0.09638 | val_0_rmse: 0.36995 |  0:00:14s\n",
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 100 and best_val_0_rmse = 0.26695\n",
      "Trial 038 | rmse_log=0.26695 | RMSE$=57,507 | MAE$=35,230 | MAPE=20.03% | n_d/n_a=48/16 steps=3 lr=0.01531 batch=2048 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 108.86054| val_0_rmse: 10.96041|  0:00:00s\n",
      "epoch 1  | loss: 69.14661| val_0_rmse: 10.12973|  0:00:00s\n",
      "epoch 2  | loss: 42.59304| val_0_rmse: 9.1515  |  0:00:00s\n",
      "epoch 3  | loss: 31.46399| val_0_rmse: 8.16808 |  0:00:01s\n",
      "epoch 4  | loss: 28.7543 | val_0_rmse: 7.32889 |  0:00:01s\n",
      "epoch 5  | loss: 28.06107| val_0_rmse: 6.4868  |  0:00:01s\n",
      "epoch 6  | loss: 24.82366| val_0_rmse: 5.82821 |  0:00:01s\n",
      "epoch 7  | loss: 20.68794| val_0_rmse: 5.35855 |  0:00:02s\n",
      "epoch 8  | loss: 18.56781| val_0_rmse: 5.07244 |  0:00:02s\n",
      "epoch 9  | loss: 14.51541| val_0_rmse: 4.91162 |  0:00:02s\n",
      "epoch 10 | loss: 10.81557| val_0_rmse: 4.57075 |  0:00:02s\n",
      "epoch 11 | loss: 10.46145| val_0_rmse: 4.08417 |  0:00:03s\n",
      "epoch 12 | loss: 8.23282 | val_0_rmse: 3.32763 |  0:00:03s\n",
      "epoch 13 | loss: 6.4833  | val_0_rmse: 2.42714 |  0:00:03s\n",
      "epoch 14 | loss: 5.75811 | val_0_rmse: 1.73341 |  0:00:04s\n",
      "epoch 15 | loss: 5.07985 | val_0_rmse: 1.44449 |  0:00:04s\n",
      "epoch 16 | loss: 3.5817  | val_0_rmse: 1.40685 |  0:00:04s\n",
      "epoch 17 | loss: 3.49966 | val_0_rmse: 1.17667 |  0:00:04s\n",
      "epoch 18 | loss: 3.77872 | val_0_rmse: 0.83767 |  0:00:05s\n",
      "epoch 19 | loss: 2.78938 | val_0_rmse: 0.53512 |  0:00:05s\n",
      "epoch 20 | loss: 2.10814 | val_0_rmse: 0.43857 |  0:00:05s\n",
      "epoch 21 | loss: 2.31707 | val_0_rmse: 0.69902 |  0:00:05s\n",
      "epoch 22 | loss: 2.06964 | val_0_rmse: 0.92746 |  0:00:05s\n",
      "epoch 23 | loss: 2.10001 | val_0_rmse: 0.62443 |  0:00:06s\n",
      "epoch 24 | loss: 1.93763 | val_0_rmse: 0.46949 |  0:00:06s\n",
      "epoch 25 | loss: 2.04546 | val_0_rmse: 0.56468 |  0:00:06s\n",
      "epoch 26 | loss: 1.8069  | val_0_rmse: 0.82258 |  0:00:07s\n",
      "epoch 27 | loss: 1.32938 | val_0_rmse: 0.54105 |  0:00:07s\n",
      "epoch 28 | loss: 1.37796 | val_0_rmse: 0.4233  |  0:00:07s\n",
      "epoch 29 | loss: 1.44962 | val_0_rmse: 0.57663 |  0:00:07s\n",
      "epoch 30 | loss: 1.40529 | val_0_rmse: 0.35392 |  0:00:08s\n",
      "epoch 31 | loss: 1.07961 | val_0_rmse: 0.35405 |  0:00:08s\n",
      "epoch 32 | loss: 1.18188 | val_0_rmse: 0.43738 |  0:00:08s\n",
      "epoch 33 | loss: 0.98087 | val_0_rmse: 0.33776 |  0:00:08s\n",
      "epoch 34 | loss: 1.02027 | val_0_rmse: 0.35179 |  0:00:09s\n",
      "epoch 35 | loss: 1.05402 | val_0_rmse: 0.38884 |  0:00:09s\n",
      "epoch 36 | loss: 0.94497 | val_0_rmse: 0.35743 |  0:00:09s\n",
      "epoch 37 | loss: 0.8376  | val_0_rmse: 0.37896 |  0:00:09s\n",
      "epoch 38 | loss: 0.80479 | val_0_rmse: 0.34685 |  0:00:10s\n",
      "epoch 39 | loss: 0.86038 | val_0_rmse: 0.37826 |  0:00:10s\n",
      "epoch 40 | loss: 0.83874 | val_0_rmse: 0.33343 |  0:00:10s\n",
      "epoch 41 | loss: 0.78637 | val_0_rmse: 0.45515 |  0:00:10s\n",
      "epoch 42 | loss: 0.64286 | val_0_rmse: 0.37573 |  0:00:11s\n",
      "epoch 43 | loss: 0.7607  | val_0_rmse: 0.40781 |  0:00:11s\n",
      "epoch 44 | loss: 0.76232 | val_0_rmse: 0.62689 |  0:00:11s\n",
      "epoch 45 | loss: 1.04309 | val_0_rmse: 0.43576 |  0:00:11s\n",
      "epoch 46 | loss: 0.79876 | val_0_rmse: 0.33027 |  0:00:12s\n",
      "epoch 47 | loss: 0.64737 | val_0_rmse: 0.57018 |  0:00:12s\n",
      "epoch 48 | loss: 0.67196 | val_0_rmse: 0.36143 |  0:00:12s\n",
      "epoch 49 | loss: 0.58641 | val_0_rmse: 0.33096 |  0:00:13s\n",
      "epoch 50 | loss: 0.60279 | val_0_rmse: 0.38311 |  0:00:13s\n",
      "epoch 51 | loss: 0.52427 | val_0_rmse: 0.36276 |  0:00:13s\n",
      "epoch 52 | loss: 0.49038 | val_0_rmse: 0.32546 |  0:00:13s\n",
      "epoch 53 | loss: 0.46832 | val_0_rmse: 0.33035 |  0:00:14s\n",
      "epoch 54 | loss: 0.44116 | val_0_rmse: 0.33297 |  0:00:14s\n",
      "epoch 55 | loss: 0.52993 | val_0_rmse: 0.32864 |  0:00:14s\n",
      "epoch 56 | loss: 0.4618  | val_0_rmse: 0.31694 |  0:00:14s\n",
      "epoch 57 | loss: 0.39254 | val_0_rmse: 0.33555 |  0:00:15s\n",
      "epoch 58 | loss: 0.45226 | val_0_rmse: 0.33995 |  0:00:15s\n",
      "epoch 59 | loss: 0.38201 | val_0_rmse: 0.30382 |  0:00:15s\n",
      "epoch 60 | loss: 0.39815 | val_0_rmse: 0.33071 |  0:00:15s\n",
      "epoch 61 | loss: 0.42682 | val_0_rmse: 0.33596 |  0:00:16s\n",
      "epoch 62 | loss: 0.56065 | val_0_rmse: 0.34023 |  0:00:16s\n",
      "epoch 63 | loss: 0.41134 | val_0_rmse: 0.37464 |  0:00:16s\n",
      "epoch 64 | loss: 0.54208 | val_0_rmse: 0.46036 |  0:00:16s\n",
      "epoch 65 | loss: 0.51784 | val_0_rmse: 0.41092 |  0:00:17s\n",
      "epoch 66 | loss: 0.39238 | val_0_rmse: 0.39393 |  0:00:17s\n",
      "epoch 67 | loss: 0.34154 | val_0_rmse: 0.41032 |  0:00:17s\n",
      "epoch 68 | loss: 0.42948 | val_0_rmse: 0.34492 |  0:00:17s\n",
      "epoch 69 | loss: 0.38038 | val_0_rmse: 0.27069 |  0:00:18s\n",
      "epoch 70 | loss: 0.32367 | val_0_rmse: 0.33935 |  0:00:18s\n",
      "epoch 71 | loss: 0.33457 | val_0_rmse: 0.32321 |  0:00:18s\n",
      "epoch 72 | loss: 0.31522 | val_0_rmse: 0.31056 |  0:00:18s\n",
      "epoch 73 | loss: 0.27901 | val_0_rmse: 0.34771 |  0:00:19s\n",
      "epoch 74 | loss: 0.39116 | val_0_rmse: 0.45859 |  0:00:19s\n",
      "epoch 75 | loss: 0.38861 | val_0_rmse: 0.28531 |  0:00:19s\n",
      "epoch 76 | loss: 0.41812 | val_0_rmse: 0.32823 |  0:00:20s\n",
      "epoch 77 | loss: 0.27339 | val_0_rmse: 0.29831 |  0:00:20s\n",
      "epoch 78 | loss: 0.31985 | val_0_rmse: 0.35366 |  0:00:20s\n",
      "epoch 79 | loss: 0.33211 | val_0_rmse: 0.40981 |  0:00:20s\n",
      "epoch 80 | loss: 0.35499 | val_0_rmse: 0.31171 |  0:00:20s\n",
      "epoch 81 | loss: 0.24026 | val_0_rmse: 0.28172 |  0:00:21s\n",
      "epoch 82 | loss: 0.25969 | val_0_rmse: 0.28409 |  0:00:21s\n",
      "epoch 83 | loss: 0.26373 | val_0_rmse: 0.28636 |  0:00:21s\n",
      "epoch 84 | loss: 0.25901 | val_0_rmse: 0.38529 |  0:00:21s\n",
      "epoch 85 | loss: 0.30054 | val_0_rmse: 0.34917 |  0:00:22s\n",
      "epoch 86 | loss: 0.26076 | val_0_rmse: 0.33445 |  0:00:22s\n",
      "epoch 87 | loss: 0.22424 | val_0_rmse: 0.29123 |  0:00:22s\n",
      "epoch 88 | loss: 0.21095 | val_0_rmse: 0.28233 |  0:00:22s\n",
      "epoch 89 | loss: 0.19282 | val_0_rmse: 0.34195 |  0:00:23s\n",
      "epoch 90 | loss: 0.20977 | val_0_rmse: 0.31617 |  0:00:23s\n",
      "epoch 91 | loss: 0.24094 | val_0_rmse: 0.3025  |  0:00:23s\n",
      "epoch 92 | loss: 0.2458  | val_0_rmse: 0.50103 |  0:00:24s\n",
      "epoch 93 | loss: 0.41517 | val_0_rmse: 0.36673 |  0:00:24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:23:01,693] Trial 39 finished with value: 0.2706854577297051 and parameters: {'n_d': 64, 'n_a': 48, 'n_steps': 5, 'gamma': 1.5004221732346814, 'n_independent': 1, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.010103160853344523, 'batch_size': 1024, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 94 | loss: 0.47276 | val_0_rmse: 0.27642 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 94 with best_epoch = 69 and best_val_0_rmse = 0.27069\n",
      "Trial 039 | rmse_log=0.27069 | RMSE$=55,989 | MAE$=36,849 | MAPE=21.32% | n_d/n_a=64/48 steps=5 lr=0.01010 batch=1024 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 126.20713| val_0_rmse: 11.52474|  0:00:00s\n",
      "epoch 1  | loss: 110.8965| val_0_rmse: 11.11072|  0:00:00s\n",
      "epoch 2  | loss: 96.20093| val_0_rmse: 10.76377|  0:00:00s\n",
      "epoch 3  | loss: 82.41292| val_0_rmse: 10.38997|  0:00:00s\n",
      "epoch 4  | loss: 74.86144| val_0_rmse: 10.02591|  0:00:00s\n",
      "epoch 5  | loss: 66.71156| val_0_rmse: 9.64807 |  0:00:00s\n",
      "epoch 6  | loss: 58.58087| val_0_rmse: 9.26107 |  0:00:01s\n",
      "epoch 7  | loss: 47.33758| val_0_rmse: 8.8638  |  0:00:01s\n",
      "epoch 8  | loss: 43.61376| val_0_rmse: 8.46076 |  0:00:01s\n",
      "epoch 9  | loss: 41.27185| val_0_rmse: 8.05139 |  0:00:01s\n",
      "epoch 10 | loss: 33.47075| val_0_rmse: 7.62216 |  0:00:01s\n",
      "epoch 11 | loss: 26.34656| val_0_rmse: 7.20188 |  0:00:01s\n",
      "epoch 12 | loss: 21.90849| val_0_rmse: 6.77494 |  0:00:01s\n",
      "epoch 13 | loss: 21.33333| val_0_rmse: 6.2985  |  0:00:02s\n",
      "epoch 14 | loss: 15.16992| val_0_rmse: 5.8073  |  0:00:02s\n",
      "epoch 15 | loss: 15.18144| val_0_rmse: 5.33912 |  0:00:02s\n",
      "epoch 16 | loss: 8.04622 | val_0_rmse: 4.8722  |  0:00:02s\n",
      "epoch 17 | loss: 6.79238 | val_0_rmse: 4.48501 |  0:00:02s\n",
      "epoch 18 | loss: 4.78048 | val_0_rmse: 4.13123 |  0:00:02s\n",
      "epoch 19 | loss: 3.84155 | val_0_rmse: 3.84024 |  0:00:02s\n",
      "epoch 20 | loss: 2.86645 | val_0_rmse: 3.58009 |  0:00:03s\n",
      "epoch 21 | loss: 1.86341 | val_0_rmse: 3.22042 |  0:00:03s\n",
      "epoch 22 | loss: 1.48522 | val_0_rmse: 2.88309 |  0:00:03s\n",
      "epoch 23 | loss: 2.51296 | val_0_rmse: 2.6469  |  0:00:03s\n",
      "epoch 24 | loss: 1.20007 | val_0_rmse: 2.51162 |  0:00:03s\n",
      "epoch 25 | loss: 0.81144 | val_0_rmse: 2.57498 |  0:00:03s\n",
      "epoch 26 | loss: 0.6955  | val_0_rmse: 2.24955 |  0:00:03s\n",
      "epoch 27 | loss: 0.64458 | val_0_rmse: 1.98178 |  0:00:04s\n",
      "epoch 28 | loss: 0.69923 | val_0_rmse: 1.96795 |  0:00:04s\n",
      "epoch 29 | loss: 0.50712 | val_0_rmse: 1.68862 |  0:00:04s\n",
      "epoch 30 | loss: 0.51729 | val_0_rmse: 1.65809 |  0:00:04s\n",
      "epoch 31 | loss: 0.52769 | val_0_rmse: 1.5948  |  0:00:04s\n",
      "epoch 32 | loss: 0.45439 | val_0_rmse: 1.33645 |  0:00:04s\n",
      "epoch 33 | loss: 0.33483 | val_0_rmse: 1.28867 |  0:00:05s\n",
      "epoch 34 | loss: 0.37616 | val_0_rmse: 1.14203 |  0:00:05s\n",
      "epoch 35 | loss: 0.31087 | val_0_rmse: 1.22876 |  0:00:05s\n",
      "epoch 36 | loss: 0.36673 | val_0_rmse: 0.9616  |  0:00:05s\n",
      "epoch 37 | loss: 0.27431 | val_0_rmse: 1.00992 |  0:00:05s\n",
      "epoch 38 | loss: 0.28631 | val_0_rmse: 1.03479 |  0:00:05s\n",
      "epoch 39 | loss: 0.21558 | val_0_rmse: 0.78158 |  0:00:05s\n",
      "epoch 40 | loss: 0.27301 | val_0_rmse: 0.88338 |  0:00:06s\n",
      "epoch 41 | loss: 0.33087 | val_0_rmse: 0.85416 |  0:00:06s\n",
      "epoch 42 | loss: 0.17364 | val_0_rmse: 0.77817 |  0:00:06s\n",
      "epoch 43 | loss: 0.1372  | val_0_rmse: 0.76852 |  0:00:06s\n",
      "epoch 44 | loss: 0.17637 | val_0_rmse: 0.73687 |  0:00:06s\n",
      "epoch 45 | loss: 0.14869 | val_0_rmse: 0.70716 |  0:00:06s\n",
      "epoch 46 | loss: 0.21041 | val_0_rmse: 0.63025 |  0:00:07s\n",
      "epoch 47 | loss: 0.14905 | val_0_rmse: 0.67941 |  0:00:07s\n",
      "epoch 48 | loss: 0.15664 | val_0_rmse: 0.61474 |  0:00:07s\n",
      "epoch 49 | loss: 0.14981 | val_0_rmse: 0.5704  |  0:00:07s\n",
      "epoch 50 | loss: 0.13744 | val_0_rmse: 0.56502 |  0:00:07s\n",
      "epoch 51 | loss: 0.1171  | val_0_rmse: 0.55684 |  0:00:07s\n",
      "epoch 52 | loss: 0.1093  | val_0_rmse: 0.61274 |  0:00:07s\n",
      "epoch 53 | loss: 0.09821 | val_0_rmse: 0.38096 |  0:00:08s\n",
      "epoch 54 | loss: 0.20432 | val_0_rmse: 0.44229 |  0:00:08s\n",
      "epoch 55 | loss: 0.12664 | val_0_rmse: 0.58204 |  0:00:08s\n",
      "epoch 56 | loss: 0.13045 | val_0_rmse: 0.43701 |  0:00:08s\n",
      "epoch 57 | loss: 0.10525 | val_0_rmse: 0.42615 |  0:00:08s\n",
      "epoch 58 | loss: 0.1622  | val_0_rmse: 0.3892  |  0:00:08s\n",
      "epoch 59 | loss: 0.09955 | val_0_rmse: 0.43224 |  0:00:08s\n",
      "epoch 60 | loss: 0.13374 | val_0_rmse: 0.47129 |  0:00:08s\n",
      "epoch 61 | loss: 0.11356 | val_0_rmse: 0.37046 |  0:00:09s\n",
      "epoch 62 | loss: 0.09338 | val_0_rmse: 0.41932 |  0:00:09s\n",
      "epoch 63 | loss: 0.09256 | val_0_rmse: 0.41132 |  0:00:09s\n",
      "epoch 64 | loss: 0.09148 | val_0_rmse: 0.34995 |  0:00:09s\n",
      "epoch 65 | loss: 0.10627 | val_0_rmse: 0.41922 |  0:00:09s\n",
      "epoch 66 | loss: 0.06992 | val_0_rmse: 0.36795 |  0:00:09s\n",
      "epoch 67 | loss: 0.09449 | val_0_rmse: 0.4002  |  0:00:09s\n",
      "epoch 68 | loss: 0.08429 | val_0_rmse: 0.35168 |  0:00:10s\n",
      "epoch 69 | loss: 0.07488 | val_0_rmse: 0.34142 |  0:00:10s\n",
      "epoch 70 | loss: 0.07448 | val_0_rmse: 0.31554 |  0:00:10s\n",
      "epoch 71 | loss: 0.06041 | val_0_rmse: 0.37727 |  0:00:10s\n",
      "epoch 72 | loss: 0.07195 | val_0_rmse: 0.33012 |  0:00:10s\n",
      "epoch 73 | loss: 0.05725 | val_0_rmse: 0.34426 |  0:00:10s\n",
      "epoch 74 | loss: 0.06865 | val_0_rmse: 0.35613 |  0:00:11s\n",
      "epoch 75 | loss: 0.07219 | val_0_rmse: 0.32422 |  0:00:11s\n",
      "epoch 76 | loss: 0.09602 | val_0_rmse: 0.30155 |  0:00:11s\n",
      "epoch 77 | loss: 0.08905 | val_0_rmse: 0.36635 |  0:00:11s\n",
      "epoch 78 | loss: 0.07106 | val_0_rmse: 0.30268 |  0:00:11s\n",
      "epoch 79 | loss: 0.08659 | val_0_rmse: 0.33202 |  0:00:11s\n",
      "epoch 80 | loss: 0.08313 | val_0_rmse: 0.33904 |  0:00:11s\n",
      "epoch 81 | loss: 0.05785 | val_0_rmse: 0.28486 |  0:00:12s\n",
      "epoch 82 | loss: 0.06528 | val_0_rmse: 0.3413  |  0:00:12s\n",
      "epoch 83 | loss: 0.06282 | val_0_rmse: 0.34353 |  0:00:12s\n",
      "epoch 84 | loss: 0.05827 | val_0_rmse: 0.2883  |  0:00:12s\n",
      "epoch 85 | loss: 0.04736 | val_0_rmse: 0.28444 |  0:00:12s\n",
      "epoch 86 | loss: 0.05445 | val_0_rmse: 0.33772 |  0:00:12s\n",
      "epoch 87 | loss: 0.05747 | val_0_rmse: 0.33272 |  0:00:12s\n",
      "epoch 88 | loss: 0.05023 | val_0_rmse: 0.2682  |  0:00:12s\n",
      "epoch 89 | loss: 0.05812 | val_0_rmse: 0.28835 |  0:00:13s\n",
      "epoch 90 | loss: 0.06391 | val_0_rmse: 0.29408 |  0:00:13s\n",
      "epoch 91 | loss: 0.05883 | val_0_rmse: 0.27957 |  0:00:13s\n",
      "epoch 92 | loss: 0.05117 | val_0_rmse: 0.30313 |  0:00:13s\n",
      "epoch 93 | loss: 0.06266 | val_0_rmse: 0.2778  |  0:00:13s\n",
      "epoch 94 | loss: 0.06886 | val_0_rmse: 0.27016 |  0:00:13s\n",
      "epoch 95 | loss: 0.06122 | val_0_rmse: 0.32972 |  0:00:13s\n",
      "epoch 96 | loss: 0.06564 | val_0_rmse: 0.27434 |  0:00:14s\n",
      "epoch 97 | loss: 0.06957 | val_0_rmse: 0.24984 |  0:00:14s\n",
      "epoch 98 | loss: 0.05098 | val_0_rmse: 0.2907  |  0:00:14s\n",
      "epoch 99 | loss: 0.04173 | val_0_rmse: 0.28572 |  0:00:14s\n",
      "epoch 100| loss: 0.07297 | val_0_rmse: 0.25509 |  0:00:14s\n",
      "epoch 101| loss: 0.05183 | val_0_rmse: 0.25715 |  0:00:14s\n",
      "epoch 102| loss: 0.03519 | val_0_rmse: 0.25382 |  0:00:14s\n",
      "epoch 103| loss: 0.03488 | val_0_rmse: 0.24586 |  0:00:15s\n",
      "epoch 104| loss: 0.03334 | val_0_rmse: 0.25997 |  0:00:15s\n",
      "epoch 105| loss: 0.02917 | val_0_rmse: 0.25581 |  0:00:15s\n",
      "epoch 106| loss: 0.03395 | val_0_rmse: 0.2466  |  0:00:15s\n",
      "epoch 107| loss: 0.04314 | val_0_rmse: 0.2451  |  0:00:15s\n",
      "epoch 108| loss: 0.04933 | val_0_rmse: 0.25666 |  0:00:15s\n",
      "epoch 109| loss: 0.03806 | val_0_rmse: 0.24486 |  0:00:15s\n",
      "epoch 110| loss: 0.02883 | val_0_rmse: 0.24668 |  0:00:16s\n",
      "epoch 111| loss: 0.04111 | val_0_rmse: 0.23312 |  0:00:16s\n",
      "epoch 112| loss: 0.03718 | val_0_rmse: 0.2381  |  0:00:16s\n",
      "epoch 113| loss: 0.03536 | val_0_rmse: 0.2734  |  0:00:16s\n",
      "epoch 114| loss: 0.05079 | val_0_rmse: 0.27657 |  0:00:16s\n",
      "epoch 115| loss: 0.04033 | val_0_rmse: 0.24663 |  0:00:16s\n",
      "epoch 116| loss: 0.04783 | val_0_rmse: 0.23997 |  0:00:16s\n",
      "epoch 117| loss: 0.04848 | val_0_rmse: 0.28095 |  0:00:16s\n",
      "epoch 118| loss: 0.03843 | val_0_rmse: 0.25249 |  0:00:17s\n",
      "epoch 119| loss: 0.03989 | val_0_rmse: 0.26612 |  0:00:17s\n",
      "epoch 120| loss: 0.04856 | val_0_rmse: 0.26876 |  0:00:17s\n",
      "epoch 121| loss: 0.04318 | val_0_rmse: 0.24917 |  0:00:17s\n",
      "epoch 122| loss: 0.04148 | val_0_rmse: 0.26543 |  0:00:17s\n",
      "epoch 123| loss: 0.05361 | val_0_rmse: 0.26611 |  0:00:17s\n",
      "epoch 124| loss: 0.07771 | val_0_rmse: 0.25143 |  0:00:17s\n",
      "epoch 125| loss: 0.04289 | val_0_rmse: 0.25811 |  0:00:18s\n",
      "epoch 126| loss: 0.04539 | val_0_rmse: 0.27741 |  0:00:18s\n",
      "epoch 127| loss: 0.04815 | val_0_rmse: 0.26754 |  0:00:18s\n",
      "epoch 128| loss: 0.03568 | val_0_rmse: 0.26435 |  0:00:18s\n",
      "epoch 129| loss: 0.03704 | val_0_rmse: 0.27945 |  0:00:18s\n",
      "epoch 130| loss: 0.04833 | val_0_rmse: 0.26623 |  0:00:18s\n",
      "epoch 131| loss: 0.03583 | val_0_rmse: 0.26071 |  0:00:18s\n",
      "epoch 132| loss: 0.03248 | val_0_rmse: 0.24929 |  0:00:19s\n",
      "epoch 133| loss: 0.02868 | val_0_rmse: 0.26485 |  0:00:19s\n",
      "epoch 134| loss: 0.03431 | val_0_rmse: 0.26247 |  0:00:19s\n",
      "epoch 135| loss: 0.0388  | val_0_rmse: 0.24435 |  0:00:19s\n",
      "epoch 136| loss: 0.03068 | val_0_rmse: 0.24909 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 136 with best_epoch = 111 and best_val_0_rmse = 0.23312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:23:21,643] Trial 40 finished with value: 0.23312420497043745 and parameters: {'n_d': 48, 'n_a': 32, 'n_steps': 3, 'gamma': 1.4097717675118204, 'n_independent': 2, 'n_shared': 2, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.004883233992139266, 'batch_size': 512, 'virtual_batch_size': 256}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 040 | rmse_log=0.23312 | RMSE$=47,607 | MAE$=28,944 | MAPE=16.98% | n_d/n_a=48/32 steps=3 lr=0.00488 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 117.84112| val_0_rmse: 11.39661|  0:00:00s\n",
      "epoch 1  | loss: 94.21829| val_0_rmse: 10.96488|  0:00:00s\n",
      "epoch 2  | loss: 73.77986| val_0_rmse: 10.46327|  0:00:00s\n",
      "epoch 3  | loss: 62.19889| val_0_rmse: 9.9429  |  0:00:00s\n",
      "epoch 4  | loss: 50.00493| val_0_rmse: 9.38322 |  0:00:00s\n",
      "epoch 5  | loss: 43.84377| val_0_rmse: 8.77469 |  0:00:00s\n",
      "epoch 6  | loss: 33.10231| val_0_rmse: 8.14932 |  0:00:00s\n",
      "epoch 7  | loss: 27.38135| val_0_rmse: 7.55163 |  0:00:01s\n",
      "epoch 8  | loss: 23.71162| val_0_rmse: 6.97202 |  0:00:01s\n",
      "epoch 9  | loss: 21.3233 | val_0_rmse: 6.43366 |  0:00:01s\n",
      "epoch 10 | loss: 19.52394| val_0_rmse: 5.94659 |  0:00:01s\n",
      "epoch 11 | loss: 17.98211| val_0_rmse: 5.53894 |  0:00:01s\n",
      "epoch 12 | loss: 14.35494| val_0_rmse: 5.19875 |  0:00:01s\n",
      "epoch 13 | loss: 13.3991 | val_0_rmse: 4.92755 |  0:00:02s\n",
      "epoch 14 | loss: 12.04678| val_0_rmse: 4.73075 |  0:00:02s\n",
      "epoch 15 | loss: 11.00208| val_0_rmse: 4.50197 |  0:00:02s\n",
      "epoch 16 | loss: 8.361   | val_0_rmse: 4.24531 |  0:00:02s\n",
      "epoch 17 | loss: 7.27102 | val_0_rmse: 3.88796 |  0:00:02s\n",
      "epoch 18 | loss: 6.76141 | val_0_rmse: 3.50206 |  0:00:02s\n",
      "epoch 19 | loss: 5.14213 | val_0_rmse: 3.0353  |  0:00:02s\n",
      "epoch 20 | loss: 4.18255 | val_0_rmse: 2.56093 |  0:00:03s\n",
      "epoch 21 | loss: 3.7975  | val_0_rmse: 2.24456 |  0:00:03s\n",
      "epoch 22 | loss: 3.27566 | val_0_rmse: 1.98407 |  0:00:03s\n",
      "epoch 23 | loss: 2.51778 | val_0_rmse: 1.7017  |  0:00:03s\n",
      "epoch 24 | loss: 2.31015 | val_0_rmse: 1.3903  |  0:00:03s\n",
      "epoch 25 | loss: 1.77534 | val_0_rmse: 1.35649 |  0:00:03s\n",
      "epoch 26 | loss: 1.56533 | val_0_rmse: 1.33111 |  0:00:04s\n",
      "epoch 27 | loss: 1.73939 | val_0_rmse: 1.12288 |  0:00:04s\n",
      "epoch 28 | loss: 1.53258 | val_0_rmse: 0.92874 |  0:00:04s\n",
      "epoch 29 | loss: 1.19268 | val_0_rmse: 0.97185 |  0:00:04s\n",
      "epoch 30 | loss: 1.00095 | val_0_rmse: 0.96085 |  0:00:04s\n",
      "epoch 31 | loss: 0.88575 | val_0_rmse: 0.85216 |  0:00:04s\n",
      "epoch 32 | loss: 0.90699 | val_0_rmse: 0.93563 |  0:00:04s\n",
      "epoch 33 | loss: 0.68284 | val_0_rmse: 0.95732 |  0:00:05s\n",
      "epoch 34 | loss: 0.73287 | val_0_rmse: 0.93948 |  0:00:05s\n",
      "epoch 35 | loss: 0.67594 | val_0_rmse: 0.88305 |  0:00:05s\n",
      "epoch 36 | loss: 0.67389 | val_0_rmse: 0.7992  |  0:00:05s\n",
      "epoch 37 | loss: 0.85142 | val_0_rmse: 0.86931 |  0:00:05s\n",
      "epoch 38 | loss: 0.59356 | val_0_rmse: 0.77537 |  0:00:05s\n",
      "epoch 39 | loss: 0.64167 | val_0_rmse: 0.87666 |  0:00:05s\n",
      "epoch 40 | loss: 0.54168 | val_0_rmse: 0.74861 |  0:00:06s\n",
      "epoch 41 | loss: 0.61855 | val_0_rmse: 0.61986 |  0:00:06s\n",
      "epoch 42 | loss: 0.47929 | val_0_rmse: 0.62078 |  0:00:06s\n",
      "epoch 43 | loss: 0.50934 | val_0_rmse: 0.68262 |  0:00:06s\n",
      "epoch 44 | loss: 0.47762 | val_0_rmse: 0.58698 |  0:00:06s\n",
      "epoch 45 | loss: 0.32429 | val_0_rmse: 0.56945 |  0:00:06s\n",
      "epoch 46 | loss: 0.33036 | val_0_rmse: 0.60141 |  0:00:06s\n",
      "epoch 47 | loss: 0.33997 | val_0_rmse: 0.58358 |  0:00:07s\n",
      "epoch 48 | loss: 0.32639 | val_0_rmse: 0.58993 |  0:00:07s\n",
      "epoch 49 | loss: 0.2739  | val_0_rmse: 0.57548 |  0:00:07s\n",
      "epoch 50 | loss: 0.25739 | val_0_rmse: 0.52377 |  0:00:07s\n",
      "epoch 51 | loss: 0.26608 | val_0_rmse: 0.45525 |  0:00:07s\n",
      "epoch 52 | loss: 0.22305 | val_0_rmse: 0.61843 |  0:00:07s\n",
      "epoch 53 | loss: 0.24717 | val_0_rmse: 0.50348 |  0:00:07s\n",
      "epoch 54 | loss: 0.21481 | val_0_rmse: 0.53644 |  0:00:08s\n",
      "epoch 55 | loss: 0.21278 | val_0_rmse: 0.45759 |  0:00:08s\n",
      "epoch 56 | loss: 0.19407 | val_0_rmse: 0.44907 |  0:00:08s\n",
      "epoch 57 | loss: 0.17221 | val_0_rmse: 0.47371 |  0:00:08s\n",
      "epoch 58 | loss: 0.16672 | val_0_rmse: 0.4047  |  0:00:08s\n",
      "epoch 59 | loss: 0.18568 | val_0_rmse: 0.45567 |  0:00:08s\n",
      "epoch 60 | loss: 0.17126 | val_0_rmse: 0.40949 |  0:00:08s\n",
      "epoch 61 | loss: 0.16834 | val_0_rmse: 0.38507 |  0:00:09s\n",
      "epoch 62 | loss: 0.14066 | val_0_rmse: 0.38916 |  0:00:09s\n",
      "epoch 63 | loss: 0.14105 | val_0_rmse: 0.41089 |  0:00:09s\n",
      "epoch 64 | loss: 0.12444 | val_0_rmse: 0.39049 |  0:00:09s\n",
      "epoch 65 | loss: 0.12291 | val_0_rmse: 0.40361 |  0:00:09s\n",
      "epoch 66 | loss: 0.20343 | val_0_rmse: 0.33505 |  0:00:09s\n",
      "epoch 67 | loss: 0.15287 | val_0_rmse: 0.43882 |  0:00:09s\n",
      "epoch 68 | loss: 0.11878 | val_0_rmse: 0.33409 |  0:00:10s\n",
      "epoch 69 | loss: 0.10595 | val_0_rmse: 0.33565 |  0:00:10s\n",
      "epoch 70 | loss: 0.10744 | val_0_rmse: 0.43599 |  0:00:10s\n",
      "epoch 71 | loss: 0.12015 | val_0_rmse: 0.28904 |  0:00:10s\n",
      "epoch 72 | loss: 0.13609 | val_0_rmse: 0.33489 |  0:00:10s\n",
      "epoch 73 | loss: 0.09816 | val_0_rmse: 0.36434 |  0:00:10s\n",
      "epoch 74 | loss: 0.09933 | val_0_rmse: 0.32008 |  0:00:10s\n",
      "epoch 75 | loss: 0.10039 | val_0_rmse: 0.36011 |  0:00:11s\n",
      "epoch 76 | loss: 0.10654 | val_0_rmse: 0.31208 |  0:00:11s\n",
      "epoch 77 | loss: 0.092   | val_0_rmse: 0.29869 |  0:00:11s\n",
      "epoch 78 | loss: 0.09971 | val_0_rmse: 0.39824 |  0:00:11s\n",
      "epoch 79 | loss: 0.11081 | val_0_rmse: 0.27787 |  0:00:11s\n",
      "epoch 80 | loss: 0.10615 | val_0_rmse: 0.32306 |  0:00:11s\n",
      "epoch 81 | loss: 0.09856 | val_0_rmse: 0.34514 |  0:00:11s\n",
      "epoch 82 | loss: 0.07782 | val_0_rmse: 0.27269 |  0:00:12s\n",
      "epoch 83 | loss: 0.0825  | val_0_rmse: 0.29396 |  0:00:12s\n",
      "epoch 84 | loss: 0.07266 | val_0_rmse: 0.26585 |  0:00:12s\n",
      "epoch 85 | loss: 0.0993  | val_0_rmse: 0.31308 |  0:00:12s\n",
      "epoch 86 | loss: 0.07412 | val_0_rmse: 0.29422 |  0:00:12s\n",
      "epoch 87 | loss: 0.12    | val_0_rmse: 0.27413 |  0:00:12s\n",
      "epoch 88 | loss: 0.08494 | val_0_rmse: 0.33197 |  0:00:12s\n",
      "epoch 89 | loss: 0.1031  | val_0_rmse: 0.3346  |  0:00:13s\n",
      "epoch 90 | loss: 0.14789 | val_0_rmse: 0.26076 |  0:00:13s\n",
      "epoch 91 | loss: 0.0849  | val_0_rmse: 0.34111 |  0:00:13s\n",
      "epoch 92 | loss: 0.10831 | val_0_rmse: 0.26705 |  0:00:13s\n",
      "epoch 93 | loss: 0.08552 | val_0_rmse: 0.26154 |  0:00:13s\n",
      "epoch 94 | loss: 0.06288 | val_0_rmse: 0.259   |  0:00:13s\n",
      "epoch 95 | loss: 0.08055 | val_0_rmse: 0.25533 |  0:00:13s\n",
      "epoch 96 | loss: 0.07448 | val_0_rmse: 0.23937 |  0:00:14s\n",
      "epoch 97 | loss: 0.0761  | val_0_rmse: 0.2359  |  0:00:14s\n",
      "epoch 98 | loss: 0.05922 | val_0_rmse: 0.24644 |  0:00:14s\n",
      "epoch 99 | loss: 0.06221 | val_0_rmse: 0.25539 |  0:00:14s\n",
      "epoch 100| loss: 0.05882 | val_0_rmse: 0.23751 |  0:00:14s\n",
      "epoch 101| loss: 0.05516 | val_0_rmse: 0.24239 |  0:00:14s\n",
      "epoch 102| loss: 0.06876 | val_0_rmse: 0.24027 |  0:00:14s\n",
      "epoch 103| loss: 0.0525  | val_0_rmse: 0.2399  |  0:00:15s\n",
      "epoch 104| loss: 0.05053 | val_0_rmse: 0.24757 |  0:00:15s\n",
      "epoch 105| loss: 0.0915  | val_0_rmse: 0.23755 |  0:00:15s\n",
      "epoch 106| loss: 0.05283 | val_0_rmse: 0.25272 |  0:00:15s\n",
      "epoch 107| loss: 0.0576  | val_0_rmse: 0.24927 |  0:00:15s\n",
      "epoch 108| loss: 0.05723 | val_0_rmse: 0.23234 |  0:00:15s\n",
      "epoch 109| loss: 0.0529  | val_0_rmse: 0.25381 |  0:00:15s\n",
      "epoch 110| loss: 0.05669 | val_0_rmse: 0.23856 |  0:00:16s\n",
      "epoch 111| loss: 0.05811 | val_0_rmse: 0.22252 |  0:00:16s\n",
      "epoch 112| loss: 0.05407 | val_0_rmse: 0.25627 |  0:00:16s\n",
      "epoch 113| loss: 0.06657 | val_0_rmse: 0.22098 |  0:00:16s\n",
      "epoch 114| loss: 0.04856 | val_0_rmse: 0.24652 |  0:00:16s\n",
      "epoch 115| loss: 0.05236 | val_0_rmse: 0.2566  |  0:00:16s\n",
      "epoch 116| loss: 0.05716 | val_0_rmse: 0.21964 |  0:00:16s\n",
      "epoch 117| loss: 0.05023 | val_0_rmse: 0.22241 |  0:00:17s\n",
      "epoch 118| loss: 0.05255 | val_0_rmse: 0.23064 |  0:00:17s\n",
      "epoch 119| loss: 0.04984 | val_0_rmse: 0.22811 |  0:00:17s\n",
      "epoch 120| loss: 0.04453 | val_0_rmse: 0.24094 |  0:00:17s\n",
      "epoch 121| loss: 0.04936 | val_0_rmse: 0.22623 |  0:00:17s\n",
      "epoch 122| loss: 0.04278 | val_0_rmse: 0.21553 |  0:00:17s\n",
      "epoch 123| loss: 0.04352 | val_0_rmse: 0.22697 |  0:00:17s\n",
      "epoch 124| loss: 0.04979 | val_0_rmse: 0.25869 |  0:00:18s\n",
      "epoch 125| loss: 0.05886 | val_0_rmse: 0.23764 |  0:00:18s\n",
      "epoch 126| loss: 0.04975 | val_0_rmse: 0.29375 |  0:00:18s\n",
      "epoch 127| loss: 0.08162 | val_0_rmse: 0.2715  |  0:00:18s\n",
      "epoch 128| loss: 0.07488 | val_0_rmse: 0.27101 |  0:00:18s\n",
      "epoch 129| loss: 0.07084 | val_0_rmse: 0.29861 |  0:00:18s\n",
      "epoch 130| loss: 0.06749 | val_0_rmse: 0.22719 |  0:00:18s\n",
      "epoch 131| loss: 0.04998 | val_0_rmse: 0.24465 |  0:00:18s\n",
      "epoch 132| loss: 0.04606 | val_0_rmse: 0.2275  |  0:00:19s\n",
      "epoch 133| loss: 0.05349 | val_0_rmse: 0.22837 |  0:00:19s\n",
      "epoch 134| loss: 0.04777 | val_0_rmse: 0.22479 |  0:00:19s\n",
      "epoch 135| loss: 0.04761 | val_0_rmse: 0.21376 |  0:00:19s\n",
      "epoch 136| loss: 0.04252 | val_0_rmse: 0.2101  |  0:00:19s\n",
      "epoch 137| loss: 0.04267 | val_0_rmse: 0.23212 |  0:00:19s\n",
      "epoch 138| loss: 0.04098 | val_0_rmse: 0.21572 |  0:00:19s\n",
      "epoch 139| loss: 0.03828 | val_0_rmse: 0.20581 |  0:00:20s\n",
      "epoch 140| loss: 0.04044 | val_0_rmse: 0.22227 |  0:00:20s\n",
      "epoch 141| loss: 0.03678 | val_0_rmse: 0.21304 |  0:00:20s\n",
      "epoch 142| loss: 0.03373 | val_0_rmse: 0.21432 |  0:00:20s\n",
      "epoch 143| loss: 0.04227 | val_0_rmse: 0.21456 |  0:00:20s\n",
      "epoch 144| loss: 0.03425 | val_0_rmse: 0.21811 |  0:00:20s\n",
      "epoch 145| loss: 0.03649 | val_0_rmse: 0.21786 |  0:00:21s\n",
      "epoch 146| loss: 0.03938 | val_0_rmse: 0.21466 |  0:00:21s\n",
      "epoch 147| loss: 0.03578 | val_0_rmse: 0.21269 |  0:00:21s\n",
      "epoch 148| loss: 0.0354  | val_0_rmse: 0.21716 |  0:00:21s\n",
      "epoch 149| loss: 0.03541 | val_0_rmse: 0.20913 |  0:00:21s\n",
      "epoch 150| loss: 0.03137 | val_0_rmse: 0.20816 |  0:00:21s\n",
      "epoch 151| loss: 0.03637 | val_0_rmse: 0.20741 |  0:00:21s\n",
      "epoch 152| loss: 0.03232 | val_0_rmse: 0.22505 |  0:00:21s\n",
      "epoch 153| loss: 0.03685 | val_0_rmse: 0.21552 |  0:00:22s\n",
      "epoch 154| loss: 0.03768 | val_0_rmse: 0.22309 |  0:00:22s\n",
      "epoch 155| loss: 0.0301  | val_0_rmse: 0.22959 |  0:00:22s\n",
      "epoch 156| loss: 0.0333  | val_0_rmse: 0.21189 |  0:00:22s\n",
      "epoch 157| loss: 0.02942 | val_0_rmse: 0.22294 |  0:00:22s\n",
      "epoch 158| loss: 0.03904 | val_0_rmse: 0.22067 |  0:00:22s\n",
      "epoch 159| loss: 0.02761 | val_0_rmse: 0.22119 |  0:00:22s\n",
      "epoch 160| loss: 0.03169 | val_0_rmse: 0.23295 |  0:00:23s\n",
      "epoch 161| loss: 0.03157 | val_0_rmse: 0.23737 |  0:00:23s\n",
      "epoch 162| loss: 0.03647 | val_0_rmse: 0.22112 |  0:00:23s\n",
      "epoch 163| loss: 0.03484 | val_0_rmse: 0.23848 |  0:00:23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:23:45,634] Trial 41 finished with value: 0.20581295917932296 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 4, 'gamma': 1.2804538671299937, 'n_independent': 1, 'n_shared': 1, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.005765713067858712, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 164| loss: 0.03379 | val_0_rmse: 0.23564 |  0:00:23s\n",
      "\n",
      "Early stopping occurred at epoch 164 with best_epoch = 139 and best_val_0_rmse = 0.20581\n",
      "Trial 041 | rmse_log=0.20581 | RMSE$=39,929 | MAE$=26,794 | MAPE=15.55% | n_d/n_a=48/24 steps=4 lr=0.00577 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 85.2732 | val_0_rmse: 10.8062 |  0:00:00s\n",
      "epoch 1  | loss: 52.88741| val_0_rmse: 10.00521|  0:00:00s\n",
      "epoch 2  | loss: 40.5221 | val_0_rmse: 9.2381  |  0:00:00s\n",
      "epoch 3  | loss: 33.54894| val_0_rmse: 8.32211 |  0:00:01s\n",
      "epoch 4  | loss: 32.1179 | val_0_rmse: 7.55911 |  0:00:01s\n",
      "epoch 5  | loss: 29.23579| val_0_rmse: 6.97533 |  0:00:01s\n",
      "epoch 6  | loss: 25.82325| val_0_rmse: 6.61051 |  0:00:01s\n",
      "epoch 7  | loss: 22.72292| val_0_rmse: 6.41407 |  0:00:02s\n",
      "epoch 8  | loss: 19.46084| val_0_rmse: 6.22909 |  0:00:02s\n",
      "epoch 9  | loss: 16.22534| val_0_rmse: 5.90409 |  0:00:02s\n",
      "epoch 10 | loss: 16.14083| val_0_rmse: 5.56889 |  0:00:02s\n",
      "epoch 11 | loss: 13.36429| val_0_rmse: 4.97137 |  0:00:03s\n",
      "epoch 12 | loss: 14.12861| val_0_rmse: 4.1668  |  0:00:03s\n",
      "epoch 13 | loss: 9.07519 | val_0_rmse: 3.31954 |  0:00:03s\n",
      "epoch 14 | loss: 8.4201  | val_0_rmse: 2.70375 |  0:00:04s\n",
      "epoch 15 | loss: 7.4489  | val_0_rmse: 2.37058 |  0:00:04s\n",
      "epoch 16 | loss: 5.81035 | val_0_rmse: 2.09838 |  0:00:04s\n",
      "epoch 17 | loss: 4.562   | val_0_rmse: 1.64329 |  0:00:04s\n",
      "epoch 18 | loss: 3.82303 | val_0_rmse: 1.1428  |  0:00:05s\n",
      "epoch 19 | loss: 3.25865 | val_0_rmse: 1.01843 |  0:00:05s\n",
      "epoch 20 | loss: 3.06945 | val_0_rmse: 1.21258 |  0:00:05s\n",
      "epoch 21 | loss: 3.00332 | val_0_rmse: 0.82655 |  0:00:05s\n",
      "epoch 22 | loss: 2.87528 | val_0_rmse: 0.77082 |  0:00:06s\n",
      "epoch 23 | loss: 2.43325 | val_0_rmse: 1.18437 |  0:00:06s\n",
      "epoch 24 | loss: 2.26967 | val_0_rmse: 0.9644  |  0:00:06s\n",
      "epoch 25 | loss: 1.52592 | val_0_rmse: 0.99451 |  0:00:06s\n",
      "epoch 26 | loss: 1.72656 | val_0_rmse: 0.86114 |  0:00:07s\n",
      "epoch 27 | loss: 1.67869 | val_0_rmse: 0.82207 |  0:00:07s\n",
      "epoch 28 | loss: 1.20798 | val_0_rmse: 0.89199 |  0:00:07s\n",
      "epoch 29 | loss: 1.02521 | val_0_rmse: 0.54616 |  0:00:07s\n",
      "epoch 30 | loss: 1.17338 | val_0_rmse: 0.62231 |  0:00:08s\n",
      "epoch 31 | loss: 1.1017  | val_0_rmse: 0.46299 |  0:00:08s\n",
      "epoch 32 | loss: 1.19052 | val_0_rmse: 0.57125 |  0:00:08s\n",
      "epoch 33 | loss: 0.99479 | val_0_rmse: 0.47279 |  0:00:08s\n",
      "epoch 34 | loss: 1.206   | val_0_rmse: 0.4847  |  0:00:09s\n",
      "epoch 35 | loss: 1.08118 | val_0_rmse: 0.58819 |  0:00:09s\n",
      "epoch 36 | loss: 0.9845  | val_0_rmse: 0.4049  |  0:00:09s\n",
      "epoch 37 | loss: 0.78982 | val_0_rmse: 0.64421 |  0:00:09s\n",
      "epoch 38 | loss: 0.68545 | val_0_rmse: 0.4192  |  0:00:10s\n",
      "epoch 39 | loss: 0.902   | val_0_rmse: 0.73126 |  0:00:10s\n",
      "epoch 40 | loss: 0.71818 | val_0_rmse: 0.40463 |  0:00:10s\n",
      "epoch 41 | loss: 0.75697 | val_0_rmse: 0.59023 |  0:00:10s\n",
      "epoch 42 | loss: 0.69616 | val_0_rmse: 0.44028 |  0:00:11s\n",
      "epoch 43 | loss: 0.59024 | val_0_rmse: 0.40572 |  0:00:11s\n",
      "epoch 44 | loss: 0.54832 | val_0_rmse: 0.61225 |  0:00:11s\n",
      "epoch 45 | loss: 0.63568 | val_0_rmse: 0.4336  |  0:00:11s\n",
      "epoch 46 | loss: 0.68485 | val_0_rmse: 0.53119 |  0:00:11s\n",
      "epoch 47 | loss: 0.59968 | val_0_rmse: 0.43811 |  0:00:12s\n",
      "epoch 48 | loss: 0.63061 | val_0_rmse: 0.39938 |  0:00:12s\n",
      "epoch 49 | loss: 0.58186 | val_0_rmse: 0.62121 |  0:00:12s\n",
      "epoch 50 | loss: 0.56257 | val_0_rmse: 0.37516 |  0:00:12s\n",
      "epoch 51 | loss: 0.51689 | val_0_rmse: 0.73488 |  0:00:13s\n",
      "epoch 52 | loss: 0.63364 | val_0_rmse: 0.37975 |  0:00:13s\n",
      "epoch 53 | loss: 0.59168 | val_0_rmse: 0.58552 |  0:00:13s\n",
      "epoch 54 | loss: 0.46515 | val_0_rmse: 0.44346 |  0:00:14s\n",
      "epoch 55 | loss: 0.40481 | val_0_rmse: 0.41028 |  0:00:14s\n",
      "epoch 56 | loss: 0.34959 | val_0_rmse: 0.79384 |  0:00:14s\n",
      "epoch 57 | loss: 0.69222 | val_0_rmse: 0.53385 |  0:00:14s\n",
      "epoch 58 | loss: 0.39421 | val_0_rmse: 0.36934 |  0:00:15s\n",
      "epoch 59 | loss: 0.39664 | val_0_rmse: 0.62204 |  0:00:15s\n",
      "epoch 60 | loss: 0.41252 | val_0_rmse: 0.40344 |  0:00:15s\n",
      "epoch 61 | loss: 0.34629 | val_0_rmse: 0.70217 |  0:00:15s\n",
      "epoch 62 | loss: 0.77726 | val_0_rmse: 0.62514 |  0:00:16s\n",
      "epoch 63 | loss: 0.43015 | val_0_rmse: 0.5354  |  0:00:16s\n",
      "epoch 64 | loss: 0.60808 | val_0_rmse: 0.70229 |  0:00:16s\n",
      "epoch 65 | loss: 0.76736 | val_0_rmse: 0.36016 |  0:00:17s\n",
      "epoch 66 | loss: 0.33907 | val_0_rmse: 0.35865 |  0:00:17s\n",
      "epoch 67 | loss: 0.41779 | val_0_rmse: 0.88693 |  0:00:17s\n",
      "epoch 68 | loss: 0.81387 | val_0_rmse: 0.73277 |  0:00:17s\n",
      "epoch 69 | loss: 1.51713 | val_0_rmse: 0.73334 |  0:00:18s\n",
      "epoch 70 | loss: 0.71362 | val_0_rmse: 0.91995 |  0:00:18s\n",
      "epoch 71 | loss: 1.44756 | val_0_rmse: 0.74002 |  0:00:18s\n",
      "epoch 72 | loss: 0.57019 | val_0_rmse: 0.78998 |  0:00:18s\n",
      "epoch 73 | loss: 1.13241 | val_0_rmse: 0.4362  |  0:00:19s\n",
      "epoch 74 | loss: 0.43806 | val_0_rmse: 0.91849 |  0:00:19s\n",
      "epoch 75 | loss: 0.75583 | val_0_rmse: 0.33377 |  0:00:19s\n",
      "epoch 76 | loss: 0.28751 | val_0_rmse: 0.35392 |  0:00:19s\n",
      "epoch 77 | loss: 0.28052 | val_0_rmse: 0.50541 |  0:00:20s\n",
      "epoch 78 | loss: 0.29055 | val_0_rmse: 0.36193 |  0:00:20s\n",
      "epoch 79 | loss: 0.32143 | val_0_rmse: 0.4002  |  0:00:20s\n",
      "epoch 80 | loss: 0.3159  | val_0_rmse: 0.50226 |  0:00:21s\n",
      "epoch 81 | loss: 0.29001 | val_0_rmse: 0.33073 |  0:00:21s\n",
      "epoch 82 | loss: 0.39498 | val_0_rmse: 0.30426 |  0:00:22s\n",
      "epoch 83 | loss: 0.30013 | val_0_rmse: 0.69014 |  0:00:22s\n",
      "epoch 84 | loss: 0.52011 | val_0_rmse: 0.45168 |  0:00:22s\n",
      "epoch 85 | loss: 0.31758 | val_0_rmse: 0.5055  |  0:00:23s\n",
      "epoch 86 | loss: 0.5826  | val_0_rmse: 0.37263 |  0:00:23s\n",
      "epoch 87 | loss: 0.32633 | val_0_rmse: 0.84118 |  0:00:23s\n",
      "epoch 88 | loss: 0.46025 | val_0_rmse: 0.51146 |  0:00:24s\n",
      "epoch 89 | loss: 0.64412 | val_0_rmse: 0.40915 |  0:00:24s\n",
      "epoch 90 | loss: 0.29702 | val_0_rmse: 0.81438 |  0:00:24s\n",
      "epoch 91 | loss: 0.59089 | val_0_rmse: 0.38754 |  0:00:24s\n",
      "epoch 92 | loss: 0.19974 | val_0_rmse: 0.33142 |  0:00:25s\n",
      "epoch 93 | loss: 0.18845 | val_0_rmse: 0.33793 |  0:00:25s\n",
      "epoch 94 | loss: 0.21884 | val_0_rmse: 0.36671 |  0:00:25s\n",
      "epoch 95 | loss: 0.20293 | val_0_rmse: 0.32532 |  0:00:25s\n",
      "epoch 96 | loss: 0.23714 | val_0_rmse: 0.58487 |  0:00:26s\n",
      "epoch 97 | loss: 0.26682 | val_0_rmse: 0.32886 |  0:00:26s\n",
      "epoch 98 | loss: 0.22464 | val_0_rmse: 0.5656  |  0:00:27s\n",
      "epoch 99 | loss: 0.22946 | val_0_rmse: 0.41372 |  0:00:27s\n",
      "epoch 100| loss: 0.52162 | val_0_rmse: 0.35959 |  0:00:27s\n",
      "epoch 101| loss: 0.3029  | val_0_rmse: 0.52794 |  0:00:27s\n",
      "epoch 102| loss: 0.31133 | val_0_rmse: 0.66546 |  0:00:28s\n",
      "epoch 103| loss: 1.07008 | val_0_rmse: 0.30278 |  0:00:28s\n",
      "epoch 104| loss: 0.31823 | val_0_rmse: 0.62663 |  0:00:28s\n",
      "epoch 105| loss: 0.33398 | val_0_rmse: 0.44653 |  0:00:29s\n",
      "epoch 106| loss: 0.56886 | val_0_rmse: 0.40699 |  0:00:29s\n",
      "epoch 107| loss: 0.21406 | val_0_rmse: 0.38195 |  0:00:29s\n",
      "epoch 108| loss: 0.2057  | val_0_rmse: 0.41643 |  0:00:29s\n",
      "epoch 109| loss: 0.20577 | val_0_rmse: 0.31969 |  0:00:29s\n",
      "epoch 110| loss: 0.24141 | val_0_rmse: 0.52489 |  0:00:30s\n",
      "epoch 111| loss: 0.38535 | val_0_rmse: 0.32588 |  0:00:30s\n",
      "epoch 112| loss: 0.2032  | val_0_rmse: 0.31043 |  0:00:30s\n",
      "epoch 113| loss: 0.24193 | val_0_rmse: 0.70835 |  0:00:31s\n",
      "epoch 114| loss: 0.30279 | val_0_rmse: 0.52996 |  0:00:31s\n",
      "epoch 115| loss: 0.6161  | val_0_rmse: 0.30366 |  0:00:31s\n",
      "epoch 116| loss: 0.23146 | val_0_rmse: 0.63521 |  0:00:31s\n",
      "epoch 117| loss: 0.25075 | val_0_rmse: 0.40391 |  0:00:32s\n",
      "epoch 118| loss: 0.33767 | val_0_rmse: 0.58068 |  0:00:32s\n",
      "epoch 119| loss: 0.37612 | val_0_rmse: 0.32152 |  0:00:32s\n",
      "epoch 120| loss: 0.1902  | val_0_rmse: 0.31652 |  0:00:32s\n",
      "epoch 121| loss: 0.2415  | val_0_rmse: 0.54519 |  0:00:33s\n",
      "epoch 122| loss: 0.25839 | val_0_rmse: 0.38949 |  0:00:33s\n",
      "epoch 123| loss: 0.24974 | val_0_rmse: 0.5881  |  0:00:33s\n",
      "epoch 124| loss: 0.38783 | val_0_rmse: 0.32758 |  0:00:33s\n",
      "epoch 125| loss: 0.19414 | val_0_rmse: 0.35233 |  0:00:34s\n",
      "epoch 126| loss: 0.22253 | val_0_rmse: 0.67837 |  0:00:34s\n",
      "epoch 127| loss: 0.45331 | val_0_rmse: 0.39027 |  0:00:34s\n",
      "epoch 128| loss: 0.24827 | val_0_rmse: 0.69098 |  0:00:34s\n",
      "\n",
      "Early stopping occurred at epoch 128 with best_epoch = 103 and best_val_0_rmse = 0.30278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:24:21,041] Trial 42 finished with value: 0.3027796449071834 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 8, 'gamma': 1.2922674794128495, 'n_independent': 1, 'n_shared': 1, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.007991753562932557, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 042 | rmse_log=0.30278 | RMSE$=62,013 | MAE$=41,208 | MAPE=24.91% | n_d/n_a=48/24 steps=8 lr=0.00799 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 114.08561| val_0_rmse: 11.25329|  0:00:00s\n",
      "epoch 1  | loss: 88.71873| val_0_rmse: 10.68859|  0:00:00s\n",
      "epoch 2  | loss: 66.80237| val_0_rmse: 10.09544|  0:00:00s\n",
      "epoch 3  | loss: 55.53016| val_0_rmse: 9.46347 |  0:00:00s\n",
      "epoch 4  | loss: 47.31065| val_0_rmse: 8.77626 |  0:00:00s\n",
      "epoch 5  | loss: 35.24541| val_0_rmse: 8.04021 |  0:00:00s\n",
      "epoch 6  | loss: 29.03614| val_0_rmse: 7.30898 |  0:00:01s\n",
      "epoch 7  | loss: 24.96256| val_0_rmse: 6.58883 |  0:00:01s\n",
      "epoch 8  | loss: 22.48262| val_0_rmse: 5.95394 |  0:00:01s\n",
      "epoch 9  | loss: 20.67319| val_0_rmse: 5.42961 |  0:00:01s\n",
      "epoch 10 | loss: 16.73365| val_0_rmse: 5.0169  |  0:00:01s\n",
      "epoch 11 | loss: 13.65542| val_0_rmse: 4.72537 |  0:00:01s\n",
      "epoch 12 | loss: 10.41001| val_0_rmse: 4.4887  |  0:00:01s\n",
      "epoch 13 | loss: 11.06784| val_0_rmse: 4.18202 |  0:00:02s\n",
      "epoch 14 | loss: 6.66201 | val_0_rmse: 3.84694 |  0:00:02s\n",
      "epoch 15 | loss: 6.79705 | val_0_rmse: 3.42145 |  0:00:02s\n",
      "epoch 16 | loss: 5.78987 | val_0_rmse: 2.76374 |  0:00:02s\n",
      "epoch 17 | loss: 3.8568  | val_0_rmse: 2.10951 |  0:00:02s\n",
      "epoch 18 | loss: 3.5603  | val_0_rmse: 1.77379 |  0:00:02s\n",
      "epoch 19 | loss: 2.18066 | val_0_rmse: 1.70393 |  0:00:02s\n",
      "epoch 20 | loss: 1.69378 | val_0_rmse: 1.43202 |  0:00:03s\n",
      "epoch 21 | loss: 1.36535 | val_0_rmse: 1.28524 |  0:00:03s\n",
      "epoch 22 | loss: 1.24578 | val_0_rmse: 1.26473 |  0:00:03s\n",
      "epoch 23 | loss: 1.32623 | val_0_rmse: 1.18501 |  0:00:03s\n",
      "epoch 24 | loss: 0.87724 | val_0_rmse: 1.05922 |  0:00:03s\n",
      "epoch 25 | loss: 0.94049 | val_0_rmse: 1.05495 |  0:00:03s\n",
      "epoch 26 | loss: 0.8443  | val_0_rmse: 1.19386 |  0:00:04s\n",
      "epoch 27 | loss: 0.6539  | val_0_rmse: 0.84899 |  0:00:04s\n",
      "epoch 28 | loss: 0.57722 | val_0_rmse: 1.09978 |  0:00:04s\n",
      "epoch 29 | loss: 0.56124 | val_0_rmse: 0.61868 |  0:00:04s\n",
      "epoch 30 | loss: 0.56227 | val_0_rmse: 0.99752 |  0:00:04s\n",
      "epoch 31 | loss: 0.45833 | val_0_rmse: 0.62541 |  0:00:05s\n",
      "epoch 32 | loss: 0.48617 | val_0_rmse: 0.75694 |  0:00:05s\n",
      "epoch 33 | loss: 0.34282 | val_0_rmse: 0.59234 |  0:00:05s\n",
      "epoch 34 | loss: 0.27105 | val_0_rmse: 0.75191 |  0:00:05s\n",
      "epoch 35 | loss: 0.28562 | val_0_rmse: 0.48928 |  0:00:05s\n",
      "epoch 36 | loss: 0.27717 | val_0_rmse: 0.69005 |  0:00:05s\n",
      "epoch 37 | loss: 0.28175 | val_0_rmse: 0.42773 |  0:00:06s\n",
      "epoch 38 | loss: 0.20734 | val_0_rmse: 0.63052 |  0:00:06s\n",
      "epoch 39 | loss: 0.22905 | val_0_rmse: 0.34777 |  0:00:06s\n",
      "epoch 40 | loss: 0.3345  | val_0_rmse: 0.45462 |  0:00:06s\n",
      "epoch 41 | loss: 0.2634  | val_0_rmse: 0.72409 |  0:00:06s\n",
      "epoch 42 | loss: 0.35858 | val_0_rmse: 0.38822 |  0:00:06s\n",
      "epoch 43 | loss: 0.2795  | val_0_rmse: 0.45242 |  0:00:07s\n",
      "epoch 44 | loss: 0.26561 | val_0_rmse: 0.51526 |  0:00:07s\n",
      "epoch 45 | loss: 0.21485 | val_0_rmse: 0.30822 |  0:00:07s\n",
      "epoch 46 | loss: 0.16482 | val_0_rmse: 0.49302 |  0:00:07s\n",
      "epoch 47 | loss: 0.15662 | val_0_rmse: 0.3004  |  0:00:07s\n",
      "epoch 48 | loss: 0.21691 | val_0_rmse: 0.31611 |  0:00:07s\n",
      "epoch 49 | loss: 0.15569 | val_0_rmse: 0.46624 |  0:00:07s\n",
      "epoch 50 | loss: 0.13638 | val_0_rmse: 0.30732 |  0:00:08s\n",
      "epoch 51 | loss: 0.20597 | val_0_rmse: 0.40662 |  0:00:08s\n",
      "epoch 52 | loss: 0.1084  | val_0_rmse: 0.34683 |  0:00:08s\n",
      "epoch 53 | loss: 0.10474 | val_0_rmse: 0.38093 |  0:00:08s\n",
      "epoch 54 | loss: 0.1286  | val_0_rmse: 0.30275 |  0:00:08s\n",
      "epoch 55 | loss: 0.12637 | val_0_rmse: 0.40975 |  0:00:08s\n",
      "epoch 56 | loss: 0.11927 | val_0_rmse: 0.29999 |  0:00:08s\n",
      "epoch 57 | loss: 0.10094 | val_0_rmse: 0.35043 |  0:00:09s\n",
      "epoch 58 | loss: 0.10386 | val_0_rmse: 0.31716 |  0:00:09s\n",
      "epoch 59 | loss: 0.08785 | val_0_rmse: 0.40247 |  0:00:09s\n",
      "epoch 60 | loss: 0.09773 | val_0_rmse: 0.33156 |  0:00:09s\n",
      "epoch 61 | loss: 0.10315 | val_0_rmse: 0.34806 |  0:00:09s\n",
      "epoch 62 | loss: 0.10057 | val_0_rmse: 0.28098 |  0:00:09s\n",
      "epoch 63 | loss: 0.08403 | val_0_rmse: 0.33492 |  0:00:09s\n",
      "epoch 64 | loss: 0.08281 | val_0_rmse: 0.27155 |  0:00:09s\n",
      "epoch 65 | loss: 0.10017 | val_0_rmse: 0.40358 |  0:00:10s\n",
      "epoch 66 | loss: 0.09701 | val_0_rmse: 0.25909 |  0:00:10s\n",
      "epoch 67 | loss: 0.10093 | val_0_rmse: 0.34098 |  0:00:10s\n",
      "epoch 68 | loss: 0.08035 | val_0_rmse: 0.25809 |  0:00:10s\n",
      "epoch 69 | loss: 0.06469 | val_0_rmse: 0.34792 |  0:00:10s\n",
      "epoch 70 | loss: 0.07036 | val_0_rmse: 0.26634 |  0:00:10s\n",
      "epoch 71 | loss: 0.07493 | val_0_rmse: 0.30317 |  0:00:11s\n",
      "epoch 72 | loss: 0.06135 | val_0_rmse: 0.2806  |  0:00:11s\n",
      "epoch 73 | loss: 0.05475 | val_0_rmse: 0.34485 |  0:00:11s\n",
      "epoch 74 | loss: 0.08877 | val_0_rmse: 0.25562 |  0:00:11s\n",
      "epoch 75 | loss: 0.08858 | val_0_rmse: 0.32955 |  0:00:11s\n",
      "epoch 76 | loss: 0.07789 | val_0_rmse: 0.26236 |  0:00:11s\n",
      "epoch 77 | loss: 0.06098 | val_0_rmse: 0.31838 |  0:00:11s\n",
      "epoch 78 | loss: 0.06471 | val_0_rmse: 0.27302 |  0:00:11s\n",
      "epoch 79 | loss: 0.07323 | val_0_rmse: 0.30686 |  0:00:12s\n",
      "epoch 80 | loss: 0.06861 | val_0_rmse: 0.27071 |  0:00:12s\n",
      "epoch 81 | loss: 0.07318 | val_0_rmse: 0.29502 |  0:00:12s\n",
      "epoch 82 | loss: 0.0835  | val_0_rmse: 0.27665 |  0:00:12s\n",
      "epoch 83 | loss: 0.06878 | val_0_rmse: 0.32372 |  0:00:12s\n",
      "epoch 84 | loss: 0.06541 | val_0_rmse: 0.29615 |  0:00:12s\n",
      "epoch 85 | loss: 0.06442 | val_0_rmse: 0.26338 |  0:00:12s\n",
      "epoch 86 | loss: 0.05403 | val_0_rmse: 0.25936 |  0:00:13s\n",
      "epoch 87 | loss: 0.05576 | val_0_rmse: 0.26585 |  0:00:13s\n",
      "epoch 88 | loss: 0.05091 | val_0_rmse: 0.24951 |  0:00:13s\n",
      "epoch 89 | loss: 0.04254 | val_0_rmse: 0.26108 |  0:00:13s\n",
      "epoch 90 | loss: 0.04402 | val_0_rmse: 0.26773 |  0:00:13s\n",
      "epoch 91 | loss: 0.06347 | val_0_rmse: 0.22569 |  0:00:13s\n",
      "epoch 92 | loss: 0.04669 | val_0_rmse: 0.27638 |  0:00:13s\n",
      "epoch 93 | loss: 0.05998 | val_0_rmse: 0.23931 |  0:00:14s\n",
      "epoch 94 | loss: 0.07388 | val_0_rmse: 0.22712 |  0:00:14s\n",
      "epoch 95 | loss: 0.04494 | val_0_rmse: 0.22391 |  0:00:14s\n",
      "epoch 96 | loss: 0.04668 | val_0_rmse: 0.23446 |  0:00:14s\n",
      "epoch 97 | loss: 0.04149 | val_0_rmse: 0.23444 |  0:00:14s\n",
      "epoch 98 | loss: 0.04216 | val_0_rmse: 0.24205 |  0:00:14s\n",
      "epoch 99 | loss: 0.0396  | val_0_rmse: 0.2396  |  0:00:14s\n",
      "epoch 100| loss: 0.04113 | val_0_rmse: 0.2319  |  0:00:15s\n",
      "epoch 101| loss: 0.05638 | val_0_rmse: 0.25995 |  0:00:15s\n",
      "epoch 102| loss: 0.07555 | val_0_rmse: 0.22551 |  0:00:15s\n",
      "epoch 103| loss: 0.04738 | val_0_rmse: 0.21767 |  0:00:15s\n",
      "epoch 104| loss: 0.04248 | val_0_rmse: 0.21524 |  0:00:15s\n",
      "epoch 105| loss: 0.0469  | val_0_rmse: 0.21887 |  0:00:15s\n",
      "epoch 106| loss: 0.04509 | val_0_rmse: 0.24641 |  0:00:15s\n",
      "epoch 107| loss: 0.04621 | val_0_rmse: 0.21841 |  0:00:16s\n",
      "epoch 108| loss: 0.03892 | val_0_rmse: 0.21085 |  0:00:16s\n",
      "epoch 109| loss: 0.04162 | val_0_rmse: 0.2173  |  0:00:16s\n",
      "epoch 110| loss: 0.03805 | val_0_rmse: 0.21727 |  0:00:16s\n",
      "epoch 111| loss: 0.03165 | val_0_rmse: 0.23478 |  0:00:16s\n",
      "epoch 112| loss: 0.03895 | val_0_rmse: 0.2243  |  0:00:16s\n",
      "epoch 113| loss: 0.04464 | val_0_rmse: 0.21843 |  0:00:17s\n",
      "epoch 114| loss: 0.03174 | val_0_rmse: 0.22191 |  0:00:17s\n",
      "epoch 115| loss: 0.04015 | val_0_rmse: 0.22467 |  0:00:17s\n",
      "epoch 116| loss: 0.03269 | val_0_rmse: 0.21951 |  0:00:17s\n",
      "epoch 117| loss: 0.03696 | val_0_rmse: 0.23092 |  0:00:17s\n",
      "epoch 118| loss: 0.03171 | val_0_rmse: 0.24338 |  0:00:17s\n",
      "epoch 119| loss: 0.03423 | val_0_rmse: 0.23196 |  0:00:17s\n",
      "epoch 120| loss: 0.03469 | val_0_rmse: 0.24823 |  0:00:18s\n",
      "epoch 121| loss: 0.03295 | val_0_rmse: 0.22511 |  0:00:18s\n",
      "epoch 122| loss: 0.03004 | val_0_rmse: 0.22495 |  0:00:18s\n",
      "epoch 123| loss: 0.03404 | val_0_rmse: 0.22659 |  0:00:18s\n",
      "epoch 124| loss: 0.03993 | val_0_rmse: 0.22832 |  0:00:18s\n",
      "epoch 125| loss: 0.03138 | val_0_rmse: 0.22541 |  0:00:18s\n",
      "epoch 126| loss: 0.05004 | val_0_rmse: 0.25314 |  0:00:18s\n",
      "epoch 127| loss: 0.05213 | val_0_rmse: 0.21055 |  0:00:19s\n",
      "epoch 128| loss: 0.03749 | val_0_rmse: 0.20894 |  0:00:19s\n",
      "epoch 129| loss: 0.03889 | val_0_rmse: 0.20921 |  0:00:19s\n",
      "epoch 130| loss: 0.02827 | val_0_rmse: 0.20472 |  0:00:19s\n",
      "epoch 131| loss: 0.03019 | val_0_rmse: 0.21401 |  0:00:19s\n",
      "epoch 132| loss: 0.04306 | val_0_rmse: 0.20664 |  0:00:19s\n",
      "epoch 133| loss: 0.03408 | val_0_rmse: 0.20825 |  0:00:19s\n",
      "epoch 134| loss: 0.03475 | val_0_rmse: 0.20757 |  0:00:20s\n",
      "epoch 135| loss: 0.02748 | val_0_rmse: 0.20977 |  0:00:20s\n",
      "epoch 136| loss: 0.0323  | val_0_rmse: 0.19897 |  0:00:20s\n",
      "epoch 137| loss: 0.02957 | val_0_rmse: 0.1968  |  0:00:20s\n",
      "epoch 138| loss: 0.02386 | val_0_rmse: 0.20501 |  0:00:20s\n",
      "epoch 139| loss: 0.03288 | val_0_rmse: 0.19837 |  0:00:20s\n",
      "epoch 140| loss: 0.02808 | val_0_rmse: 0.20169 |  0:00:20s\n",
      "epoch 141| loss: 0.02624 | val_0_rmse: 0.20046 |  0:00:21s\n",
      "epoch 142| loss: 0.02541 | val_0_rmse: 0.19933 |  0:00:21s\n",
      "epoch 143| loss: 0.026   | val_0_rmse: 0.19782 |  0:00:21s\n",
      "epoch 144| loss: 0.0239  | val_0_rmse: 0.19387 |  0:00:21s\n",
      "epoch 145| loss: 0.02498 | val_0_rmse: 0.19591 |  0:00:21s\n",
      "epoch 146| loss: 0.02347 | val_0_rmse: 0.20195 |  0:00:21s\n",
      "epoch 147| loss: 0.02361 | val_0_rmse: 0.21534 |  0:00:21s\n",
      "epoch 148| loss: 0.0255  | val_0_rmse: 0.2052  |  0:00:22s\n",
      "epoch 149| loss: 0.02443 | val_0_rmse: 0.20566 |  0:00:22s\n",
      "epoch 150| loss: 0.0253  | val_0_rmse: 0.25039 |  0:00:22s\n",
      "epoch 151| loss: 0.04684 | val_0_rmse: 0.21679 |  0:00:22s\n",
      "epoch 152| loss: 0.03591 | val_0_rmse: 0.2037  |  0:00:22s\n",
      "epoch 153| loss: 0.02608 | val_0_rmse: 0.20812 |  0:00:23s\n",
      "epoch 154| loss: 0.0253  | val_0_rmse: 0.22312 |  0:00:23s\n",
      "epoch 155| loss: 0.02366 | val_0_rmse: 0.21718 |  0:00:23s\n",
      "epoch 156| loss: 0.02477 | val_0_rmse: 0.21188 |  0:00:23s\n",
      "epoch 157| loss: 0.02318 | val_0_rmse: 0.21351 |  0:00:23s\n",
      "epoch 158| loss: 0.0225  | val_0_rmse: 0.21896 |  0:00:23s\n",
      "epoch 159| loss: 0.02784 | val_0_rmse: 0.20052 |  0:00:24s\n",
      "epoch 160| loss: 0.02044 | val_0_rmse: 0.21338 |  0:00:24s\n",
      "epoch 161| loss: 0.02087 | val_0_rmse: 0.21738 |  0:00:24s\n",
      "epoch 162| loss: 0.02497 | val_0_rmse: 0.22302 |  0:00:24s\n",
      "epoch 163| loss: 0.03124 | val_0_rmse: 0.22518 |  0:00:24s\n",
      "epoch 164| loss: 0.02867 | val_0_rmse: 0.2279  |  0:00:24s\n",
      "epoch 165| loss: 0.02929 | val_0_rmse: 0.25635 |  0:00:24s\n",
      "epoch 166| loss: 0.04981 | val_0_rmse: 0.20617 |  0:00:25s\n",
      "epoch 167| loss: 0.03728 | val_0_rmse: 0.20039 |  0:00:25s\n",
      "epoch 168| loss: 0.0356  | val_0_rmse: 0.229   |  0:00:25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:24:46,952] Trial 43 finished with value: 0.19387153466464097 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 4, 'gamma': 1.2075005214352985, 'n_independent': 1, 'n_shared': 1, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.007261342244338096, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 169| loss: 0.02801 | val_0_rmse: 0.20747 |  0:00:25s\n",
      "\n",
      "Early stopping occurred at epoch 169 with best_epoch = 144 and best_val_0_rmse = 0.19387\n",
      "Trial 043 | rmse_log=0.19387 | RMSE$=44,105 | MAE$=25,263 | MAPE=14.28% | n_d/n_a=48/24 steps=4 lr=0.00726 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 130.18872| val_0_rmse: 11.21806|  0:00:00s\n",
      "epoch 1  | loss: 88.07421| val_0_rmse: 10.40074|  0:00:00s\n",
      "epoch 2  | loss: 58.5603 | val_0_rmse: 9.56497 |  0:00:00s\n",
      "epoch 3  | loss: 39.8005 | val_0_rmse: 8.71063 |  0:00:00s\n",
      "epoch 4  | loss: 31.47641| val_0_rmse: 7.90286 |  0:00:01s\n",
      "epoch 5  | loss: 34.52248| val_0_rmse: 7.21729 |  0:00:01s\n",
      "epoch 6  | loss: 30.39152| val_0_rmse: 6.69751 |  0:00:01s\n",
      "epoch 7  | loss: 30.61434| val_0_rmse: 6.28319 |  0:00:01s\n",
      "epoch 8  | loss: 28.00845| val_0_rmse: 6.03014 |  0:00:01s\n",
      "epoch 9  | loss: 24.24997| val_0_rmse: 5.90015 |  0:00:02s\n",
      "epoch 10 | loss: 19.14563| val_0_rmse: 5.795   |  0:00:02s\n",
      "epoch 11 | loss: 15.98474| val_0_rmse: 5.65874 |  0:00:02s\n",
      "epoch 12 | loss: 14.38812| val_0_rmse: 5.33833 |  0:00:02s\n",
      "epoch 13 | loss: 13.95877| val_0_rmse: 4.95085 |  0:00:03s\n",
      "epoch 14 | loss: 10.85082| val_0_rmse: 4.37846 |  0:00:03s\n",
      "epoch 15 | loss: 8.47879 | val_0_rmse: 3.70442 |  0:00:03s\n",
      "epoch 16 | loss: 9.11044 | val_0_rmse: 3.0689  |  0:00:03s\n",
      "epoch 17 | loss: 7.55452 | val_0_rmse: 2.65472 |  0:00:04s\n",
      "epoch 18 | loss: 6.15065 | val_0_rmse: 2.46073 |  0:00:04s\n",
      "epoch 19 | loss: 5.98212 | val_0_rmse: 2.32027 |  0:00:04s\n",
      "epoch 20 | loss: 4.60661 | val_0_rmse: 2.07285 |  0:00:04s\n",
      "epoch 21 | loss: 4.10675 | val_0_rmse: 1.67374 |  0:00:05s\n",
      "epoch 22 | loss: 5.12559 | val_0_rmse: 1.45546 |  0:00:05s\n",
      "epoch 23 | loss: 3.66105 | val_0_rmse: 1.4549  |  0:00:05s\n",
      "epoch 24 | loss: 3.40703 | val_0_rmse: 1.48569 |  0:00:05s\n",
      "epoch 25 | loss: 2.89562 | val_0_rmse: 1.30218 |  0:00:06s\n",
      "epoch 26 | loss: 2.56196 | val_0_rmse: 1.12109 |  0:00:06s\n",
      "epoch 27 | loss: 2.74061 | val_0_rmse: 0.9489  |  0:00:06s\n",
      "epoch 28 | loss: 2.73261 | val_0_rmse: 0.94116 |  0:00:06s\n",
      "epoch 29 | loss: 1.84985 | val_0_rmse: 0.86509 |  0:00:07s\n",
      "epoch 30 | loss: 2.00665 | val_0_rmse: 0.8873  |  0:00:07s\n",
      "epoch 31 | loss: 1.78666 | val_0_rmse: 0.91504 |  0:00:07s\n",
      "epoch 32 | loss: 1.6427  | val_0_rmse: 0.74912 |  0:00:07s\n",
      "epoch 33 | loss: 1.54004 | val_0_rmse: 0.9485  |  0:00:08s\n",
      "epoch 34 | loss: 1.44299 | val_0_rmse: 0.88649 |  0:00:08s\n",
      "epoch 35 | loss: 1.85639 | val_0_rmse: 0.86524 |  0:00:08s\n",
      "epoch 36 | loss: 1.64688 | val_0_rmse: 0.8386  |  0:00:08s\n",
      "epoch 37 | loss: 1.33233 | val_0_rmse: 0.53526 |  0:00:08s\n",
      "epoch 38 | loss: 1.29736 | val_0_rmse: 0.84521 |  0:00:09s\n",
      "epoch 39 | loss: 1.36683 | val_0_rmse: 0.63687 |  0:00:09s\n",
      "epoch 40 | loss: 1.11476 | val_0_rmse: 0.63284 |  0:00:09s\n",
      "epoch 41 | loss: 1.14916 | val_0_rmse: 0.83435 |  0:00:09s\n",
      "epoch 42 | loss: 1.12288 | val_0_rmse: 0.53967 |  0:00:10s\n",
      "epoch 43 | loss: 0.98196 | val_0_rmse: 0.50882 |  0:00:10s\n",
      "epoch 44 | loss: 0.85034 | val_0_rmse: 0.45109 |  0:00:10s\n",
      "epoch 45 | loss: 0.87682 | val_0_rmse: 0.44894 |  0:00:10s\n",
      "epoch 46 | loss: 0.77548 | val_0_rmse: 0.56507 |  0:00:11s\n",
      "epoch 47 | loss: 0.76467 | val_0_rmse: 0.41107 |  0:00:11s\n",
      "epoch 48 | loss: 0.89496 | val_0_rmse: 0.72034 |  0:00:11s\n",
      "epoch 49 | loss: 0.7434  | val_0_rmse: 0.46329 |  0:00:11s\n",
      "epoch 50 | loss: 0.92018 | val_0_rmse: 0.53067 |  0:00:12s\n",
      "epoch 51 | loss: 0.72583 | val_0_rmse: 0.44835 |  0:00:12s\n",
      "epoch 52 | loss: 0.60127 | val_0_rmse: 0.43609 |  0:00:12s\n",
      "epoch 53 | loss: 0.62087 | val_0_rmse: 0.49257 |  0:00:12s\n",
      "epoch 54 | loss: 0.64876 | val_0_rmse: 0.39231 |  0:00:13s\n",
      "epoch 55 | loss: 0.65346 | val_0_rmse: 0.54802 |  0:00:13s\n",
      "epoch 56 | loss: 0.64603 | val_0_rmse: 0.35488 |  0:00:13s\n",
      "epoch 57 | loss: 0.58981 | val_0_rmse: 0.37557 |  0:00:13s\n",
      "epoch 58 | loss: 0.5398  | val_0_rmse: 0.53579 |  0:00:13s\n",
      "epoch 59 | loss: 0.63163 | val_0_rmse: 0.37581 |  0:00:14s\n",
      "epoch 60 | loss: 0.60798 | val_0_rmse: 0.37909 |  0:00:14s\n",
      "epoch 61 | loss: 0.55389 | val_0_rmse: 0.45172 |  0:00:14s\n",
      "epoch 62 | loss: 0.51091 | val_0_rmse: 0.61147 |  0:00:14s\n",
      "epoch 63 | loss: 0.81334 | val_0_rmse: 0.67354 |  0:00:15s\n",
      "epoch 64 | loss: 0.86855 | val_0_rmse: 0.32312 |  0:00:15s\n",
      "epoch 65 | loss: 0.4355  | val_0_rmse: 0.38812 |  0:00:15s\n",
      "epoch 66 | loss: 0.43152 | val_0_rmse: 0.56055 |  0:00:15s\n",
      "epoch 67 | loss: 0.60331 | val_0_rmse: 0.38137 |  0:00:16s\n",
      "epoch 68 | loss: 0.50839 | val_0_rmse: 0.36573 |  0:00:16s\n",
      "epoch 69 | loss: 0.33114 | val_0_rmse: 0.40874 |  0:00:16s\n",
      "epoch 70 | loss: 0.42422 | val_0_rmse: 0.37186 |  0:00:16s\n",
      "epoch 71 | loss: 0.38503 | val_0_rmse: 0.33801 |  0:00:16s\n",
      "epoch 72 | loss: 0.40241 | val_0_rmse: 0.37799 |  0:00:17s\n",
      "epoch 73 | loss: 0.38379 | val_0_rmse: 0.34694 |  0:00:17s\n",
      "epoch 74 | loss: 0.54552 | val_0_rmse: 0.39516 |  0:00:17s\n",
      "epoch 75 | loss: 0.33524 | val_0_rmse: 0.34855 |  0:00:17s\n",
      "epoch 76 | loss: 0.35972 | val_0_rmse: 0.34948 |  0:00:18s\n",
      "epoch 77 | loss: 0.27097 | val_0_rmse: 0.45168 |  0:00:18s\n",
      "epoch 78 | loss: 0.33819 | val_0_rmse: 0.41588 |  0:00:18s\n",
      "epoch 79 | loss: 0.3434  | val_0_rmse: 0.33307 |  0:00:18s\n",
      "epoch 80 | loss: 0.30521 | val_0_rmse: 0.51142 |  0:00:18s\n",
      "epoch 81 | loss: 0.28063 | val_0_rmse: 0.32432 |  0:00:19s\n",
      "epoch 82 | loss: 0.33113 | val_0_rmse: 0.54619 |  0:00:19s\n",
      "epoch 83 | loss: 0.43347 | val_0_rmse: 0.34248 |  0:00:19s\n",
      "epoch 84 | loss: 0.37566 | val_0_rmse: 0.34623 |  0:00:20s\n",
      "epoch 85 | loss: 0.33544 | val_0_rmse: 0.35433 |  0:00:20s\n",
      "epoch 86 | loss: 0.23856 | val_0_rmse: 0.32422 |  0:00:20s\n",
      "epoch 87 | loss: 0.28429 | val_0_rmse: 0.48955 |  0:00:21s\n",
      "epoch 88 | loss: 0.28574 | val_0_rmse: 0.40762 |  0:00:21s\n",
      "epoch 89 | loss: 0.37825 | val_0_rmse: 0.46593 |  0:00:21s\n",
      "\n",
      "Early stopping occurred at epoch 89 with best_epoch = 64 and best_val_0_rmse = 0.32312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:25:08,901] Trial 44 finished with value: 0.32312217750574757 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 7, 'gamma': 1.2563144868770966, 'n_independent': 1, 'n_shared': 1, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.00678370144484322, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 044 | rmse_log=0.32312 | RMSE$=64,652 | MAE$=44,398 | MAPE=26.84% | n_d/n_a=48/24 steps=7 lr=0.00678 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 118.76253| val_0_rmse: 11.35343|  0:00:00s\n",
      "epoch 1  | loss: 91.2119 | val_0_rmse: 10.86025|  0:00:00s\n",
      "epoch 2  | loss: 72.67049| val_0_rmse: 10.28744|  0:00:00s\n",
      "epoch 3  | loss: 54.03658| val_0_rmse: 9.6065  |  0:00:01s\n",
      "epoch 4  | loss: 44.27801| val_0_rmse: 8.76333 |  0:00:01s\n",
      "epoch 5  | loss: 33.87918| val_0_rmse: 7.87811 |  0:00:01s\n",
      "epoch 6  | loss: 27.19345| val_0_rmse: 7.03211 |  0:00:01s\n",
      "epoch 7  | loss: 23.50898| val_0_rmse: 6.21609 |  0:00:02s\n",
      "epoch 8  | loss: 24.3985 | val_0_rmse: 5.53948 |  0:00:02s\n",
      "epoch 9  | loss: 18.43077| val_0_rmse: 4.96441 |  0:00:02s\n",
      "epoch 10 | loss: 20.58689| val_0_rmse: 4.51014 |  0:00:03s\n",
      "epoch 11 | loss: 16.94766| val_0_rmse: 4.20553 |  0:00:03s\n",
      "epoch 12 | loss: 14.40121| val_0_rmse: 4.07091 |  0:00:03s\n",
      "epoch 13 | loss: 12.28834| val_0_rmse: 3.86756 |  0:00:03s\n",
      "epoch 14 | loss: 9.4649  | val_0_rmse: 3.71447 |  0:00:04s\n",
      "epoch 15 | loss: 8.46866 | val_0_rmse: 3.36111 |  0:00:04s\n",
      "epoch 16 | loss: 7.85    | val_0_rmse: 2.8153  |  0:00:04s\n",
      "epoch 17 | loss: 5.56207 | val_0_rmse: 2.12485 |  0:00:05s\n",
      "epoch 18 | loss: 4.19513 | val_0_rmse: 1.55868 |  0:00:05s\n",
      "epoch 19 | loss: 3.27437 | val_0_rmse: 1.257   |  0:00:05s\n",
      "epoch 20 | loss: 2.79415 | val_0_rmse: 1.10815 |  0:00:05s\n",
      "epoch 21 | loss: 2.27433 | val_0_rmse: 0.92973 |  0:00:06s\n",
      "epoch 22 | loss: 1.95675 | val_0_rmse: 0.62439 |  0:00:06s\n",
      "epoch 23 | loss: 1.80846 | val_0_rmse: 0.60983 |  0:00:06s\n",
      "epoch 24 | loss: 1.60515 | val_0_rmse: 0.54069 |  0:00:07s\n",
      "epoch 25 | loss: 1.44238 | val_0_rmse: 0.45264 |  0:00:07s\n",
      "epoch 26 | loss: 1.18988 | val_0_rmse: 0.48688 |  0:00:07s\n",
      "epoch 27 | loss: 1.22457 | val_0_rmse: 0.4476  |  0:00:07s\n",
      "epoch 28 | loss: 0.98575 | val_0_rmse: 0.43484 |  0:00:08s\n",
      "epoch 29 | loss: 1.02662 | val_0_rmse: 0.41936 |  0:00:08s\n",
      "epoch 30 | loss: 0.96263 | val_0_rmse: 0.41083 |  0:00:08s\n",
      "epoch 31 | loss: 0.87616 | val_0_rmse: 0.42462 |  0:00:08s\n",
      "epoch 32 | loss: 0.82264 | val_0_rmse: 0.41652 |  0:00:09s\n",
      "epoch 33 | loss: 0.74108 | val_0_rmse: 0.39297 |  0:00:09s\n",
      "epoch 34 | loss: 0.6226  | val_0_rmse: 0.41992 |  0:00:09s\n",
      "epoch 35 | loss: 0.64623 | val_0_rmse: 0.49045 |  0:00:09s\n",
      "epoch 36 | loss: 0.62126 | val_0_rmse: 0.59757 |  0:00:10s\n",
      "epoch 37 | loss: 0.66561 | val_0_rmse: 0.46695 |  0:00:10s\n",
      "epoch 38 | loss: 0.61384 | val_0_rmse: 0.42059 |  0:00:10s\n",
      "epoch 39 | loss: 0.60264 | val_0_rmse: 0.42033 |  0:00:11s\n",
      "epoch 40 | loss: 0.47889 | val_0_rmse: 0.41539 |  0:00:11s\n",
      "epoch 41 | loss: 0.45059 | val_0_rmse: 0.39883 |  0:00:11s\n",
      "epoch 42 | loss: 0.40955 | val_0_rmse: 0.36898 |  0:00:11s\n",
      "epoch 43 | loss: 0.59797 | val_0_rmse: 0.36532 |  0:00:12s\n",
      "epoch 44 | loss: 0.45234 | val_0_rmse: 0.36457 |  0:00:12s\n",
      "epoch 45 | loss: 0.42668 | val_0_rmse: 0.36006 |  0:00:12s\n",
      "epoch 46 | loss: 0.39673 | val_0_rmse: 0.37367 |  0:00:13s\n",
      "epoch 47 | loss: 0.35965 | val_0_rmse: 0.37914 |  0:00:13s\n",
      "epoch 48 | loss: 0.3892  | val_0_rmse: 0.40632 |  0:00:13s\n",
      "epoch 49 | loss: 0.38147 | val_0_rmse: 0.3683  |  0:00:14s\n",
      "epoch 50 | loss: 0.38477 | val_0_rmse: 0.37962 |  0:00:14s\n",
      "epoch 51 | loss: 0.26736 | val_0_rmse: 0.36496 |  0:00:14s\n",
      "epoch 52 | loss: 0.32969 | val_0_rmse: 0.37943 |  0:00:15s\n",
      "epoch 53 | loss: 0.35795 | val_0_rmse: 0.34503 |  0:00:15s\n",
      "epoch 54 | loss: 0.2785  | val_0_rmse: 0.3491  |  0:00:15s\n",
      "epoch 55 | loss: 0.25255 | val_0_rmse: 0.34909 |  0:00:15s\n",
      "epoch 56 | loss: 0.31803 | val_0_rmse: 0.38888 |  0:00:16s\n",
      "epoch 57 | loss: 0.27376 | val_0_rmse: 0.34761 |  0:00:16s\n",
      "epoch 58 | loss: 0.27554 | val_0_rmse: 0.36247 |  0:00:16s\n",
      "epoch 59 | loss: 0.22164 | val_0_rmse: 0.3602  |  0:00:16s\n",
      "epoch 60 | loss: 0.25063 | val_0_rmse: 0.40395 |  0:00:17s\n",
      "epoch 61 | loss: 0.22663 | val_0_rmse: 0.35973 |  0:00:17s\n",
      "epoch 62 | loss: 0.21255 | val_0_rmse: 0.33933 |  0:00:17s\n",
      "epoch 63 | loss: 0.24706 | val_0_rmse: 0.34293 |  0:00:17s\n",
      "epoch 64 | loss: 0.20813 | val_0_rmse: 0.34501 |  0:00:18s\n",
      "epoch 65 | loss: 0.2582  | val_0_rmse: 0.39022 |  0:00:18s\n",
      "epoch 66 | loss: 0.21977 | val_0_rmse: 0.34691 |  0:00:18s\n",
      "epoch 67 | loss: 0.21197 | val_0_rmse: 0.33097 |  0:00:19s\n",
      "epoch 68 | loss: 0.23938 | val_0_rmse: 0.35253 |  0:00:19s\n",
      "epoch 69 | loss: 0.21031 | val_0_rmse: 0.35427 |  0:00:19s\n",
      "epoch 70 | loss: 0.20263 | val_0_rmse: 0.33087 |  0:00:20s\n",
      "epoch 71 | loss: 0.19925 | val_0_rmse: 0.39806 |  0:00:20s\n",
      "epoch 72 | loss: 0.19238 | val_0_rmse: 0.3266  |  0:00:20s\n",
      "epoch 73 | loss: 0.19175 | val_0_rmse: 0.34119 |  0:00:21s\n",
      "epoch 74 | loss: 0.17271 | val_0_rmse: 0.30611 |  0:00:21s\n",
      "epoch 75 | loss: 0.14317 | val_0_rmse: 0.30952 |  0:00:21s\n",
      "epoch 76 | loss: 0.14968 | val_0_rmse: 0.31305 |  0:00:22s\n",
      "epoch 77 | loss: 0.15824 | val_0_rmse: 0.32176 |  0:00:22s\n",
      "epoch 78 | loss: 0.16275 | val_0_rmse: 0.31497 |  0:00:23s\n",
      "epoch 79 | loss: 0.13365 | val_0_rmse: 0.34358 |  0:00:23s\n",
      "epoch 80 | loss: 0.15459 | val_0_rmse: 0.30636 |  0:00:23s\n",
      "epoch 81 | loss: 0.13555 | val_0_rmse: 0.33103 |  0:00:24s\n",
      "epoch 82 | loss: 0.14895 | val_0_rmse: 0.32181 |  0:00:24s\n",
      "epoch 83 | loss: 0.13191 | val_0_rmse: 0.29455 |  0:00:24s\n",
      "epoch 84 | loss: 0.13673 | val_0_rmse: 0.34887 |  0:00:25s\n",
      "epoch 85 | loss: 0.15563 | val_0_rmse: 0.28669 |  0:00:25s\n",
      "epoch 86 | loss: 0.15253 | val_0_rmse: 0.39609 |  0:00:25s\n",
      "epoch 87 | loss: 0.14264 | val_0_rmse: 0.29546 |  0:00:25s\n",
      "epoch 88 | loss: 0.12    | val_0_rmse: 0.31193 |  0:00:26s\n",
      "epoch 89 | loss: 0.13187 | val_0_rmse: 0.30344 |  0:00:26s\n",
      "epoch 90 | loss: 0.12935 | val_0_rmse: 0.42405 |  0:00:26s\n",
      "epoch 91 | loss: 0.21678 | val_0_rmse: 0.30582 |  0:00:26s\n",
      "epoch 92 | loss: 0.18402 | val_0_rmse: 0.29789 |  0:00:27s\n",
      "epoch 93 | loss: 0.11809 | val_0_rmse: 0.31218 |  0:00:27s\n",
      "epoch 94 | loss: 0.12138 | val_0_rmse: 0.3585  |  0:00:27s\n",
      "epoch 95 | loss: 0.13631 | val_0_rmse: 0.27107 |  0:00:27s\n",
      "epoch 96 | loss: 0.0974  | val_0_rmse: 0.25408 |  0:00:28s\n",
      "epoch 97 | loss: 0.09088 | val_0_rmse: 0.27304 |  0:00:28s\n",
      "epoch 98 | loss: 0.10978 | val_0_rmse: 0.28569 |  0:00:28s\n",
      "epoch 99 | loss: 0.1067  | val_0_rmse: 0.24843 |  0:00:28s\n",
      "epoch 100| loss: 0.09509 | val_0_rmse: 0.25261 |  0:00:29s\n",
      "epoch 101| loss: 0.10417 | val_0_rmse: 0.24714 |  0:00:29s\n",
      "epoch 102| loss: 0.09875 | val_0_rmse: 0.24484 |  0:00:29s\n",
      "epoch 103| loss: 0.0968  | val_0_rmse: 0.25814 |  0:00:29s\n",
      "epoch 104| loss: 0.09165 | val_0_rmse: 0.24793 |  0:00:30s\n",
      "epoch 105| loss: 0.11365 | val_0_rmse: 0.26833 |  0:00:30s\n",
      "epoch 106| loss: 0.10004 | val_0_rmse: 0.26261 |  0:00:30s\n",
      "epoch 107| loss: 0.08807 | val_0_rmse: 0.25235 |  0:00:31s\n",
      "epoch 108| loss: 0.09031 | val_0_rmse: 0.24931 |  0:00:31s\n",
      "epoch 109| loss: 0.08826 | val_0_rmse: 0.26618 |  0:00:31s\n",
      "epoch 110| loss: 0.08486 | val_0_rmse: 0.25459 |  0:00:31s\n",
      "epoch 111| loss: 0.07099 | val_0_rmse: 0.25383 |  0:00:32s\n",
      "epoch 112| loss: 0.08274 | val_0_rmse: 0.24792 |  0:00:32s\n",
      "epoch 113| loss: 0.07499 | val_0_rmse: 0.24247 |  0:00:32s\n",
      "epoch 114| loss: 0.07569 | val_0_rmse: 0.28061 |  0:00:32s\n",
      "epoch 115| loss: 0.09511 | val_0_rmse: 0.26657 |  0:00:33s\n",
      "epoch 116| loss: 0.09356 | val_0_rmse: 0.26834 |  0:00:33s\n",
      "epoch 117| loss: 0.08486 | val_0_rmse: 0.2511  |  0:00:33s\n",
      "epoch 118| loss: 0.0741  | val_0_rmse: 0.24685 |  0:00:34s\n",
      "epoch 119| loss: 0.07967 | val_0_rmse: 0.24402 |  0:00:34s\n",
      "epoch 120| loss: 0.07226 | val_0_rmse: 0.25356 |  0:00:34s\n",
      "epoch 121| loss: 0.06822 | val_0_rmse: 0.24233 |  0:00:34s\n",
      "epoch 122| loss: 0.07051 | val_0_rmse: 0.23565 |  0:00:35s\n",
      "epoch 123| loss: 0.07473 | val_0_rmse: 0.24154 |  0:00:35s\n",
      "epoch 124| loss: 0.06799 | val_0_rmse: 0.24426 |  0:00:35s\n",
      "epoch 125| loss: 0.06768 | val_0_rmse: 0.2457  |  0:00:35s\n",
      "epoch 126| loss: 0.06495 | val_0_rmse: 0.23555 |  0:00:36s\n",
      "epoch 127| loss: 0.0611  | val_0_rmse: 0.24314 |  0:00:36s\n",
      "epoch 128| loss: 0.05955 | val_0_rmse: 0.24487 |  0:00:36s\n",
      "epoch 129| loss: 0.06956 | val_0_rmse: 0.25621 |  0:00:37s\n",
      "epoch 130| loss: 0.0709  | val_0_rmse: 0.27598 |  0:00:37s\n",
      "epoch 131| loss: 0.08692 | val_0_rmse: 0.23989 |  0:00:37s\n",
      "epoch 132| loss: 0.06317 | val_0_rmse: 0.24724 |  0:00:38s\n",
      "epoch 133| loss: 0.07021 | val_0_rmse: 0.2326  |  0:00:38s\n",
      "epoch 134| loss: 0.0679  | val_0_rmse: 0.23055 |  0:00:38s\n",
      "epoch 135| loss: 0.05103 | val_0_rmse: 0.22928 |  0:00:38s\n",
      "epoch 136| loss: 0.04974 | val_0_rmse: 0.22378 |  0:00:39s\n",
      "epoch 137| loss: 0.0538  | val_0_rmse: 0.22541 |  0:00:39s\n",
      "epoch 138| loss: 0.05419 | val_0_rmse: 0.22658 |  0:00:39s\n",
      "epoch 139| loss: 0.05404 | val_0_rmse: 0.25216 |  0:00:39s\n",
      "epoch 140| loss: 0.0684  | val_0_rmse: 0.22594 |  0:00:40s\n",
      "epoch 141| loss: 0.05616 | val_0_rmse: 0.2802  |  0:00:40s\n",
      "epoch 142| loss: 0.07449 | val_0_rmse: 0.2237  |  0:00:40s\n",
      "epoch 143| loss: 0.05219 | val_0_rmse: 0.24779 |  0:00:41s\n",
      "epoch 144| loss: 0.0673  | val_0_rmse: 0.2275  |  0:00:41s\n",
      "epoch 145| loss: 0.0574  | val_0_rmse: 0.26921 |  0:00:41s\n",
      "epoch 146| loss: 0.09905 | val_0_rmse: 0.23322 |  0:00:42s\n",
      "epoch 147| loss: 0.06941 | val_0_rmse: 0.29184 |  0:00:42s\n",
      "epoch 148| loss: 0.08779 | val_0_rmse: 0.25101 |  0:00:42s\n",
      "epoch 149| loss: 0.06796 | val_0_rmse: 0.24625 |  0:00:42s\n",
      "epoch 150| loss: 0.0886  | val_0_rmse: 0.22445 |  0:00:43s\n",
      "epoch 151| loss: 0.07468 | val_0_rmse: 0.30878 |  0:00:43s\n",
      "epoch 152| loss: 0.08318 | val_0_rmse: 0.2416  |  0:00:43s\n",
      "epoch 153| loss: 0.07353 | val_0_rmse: 0.25096 |  0:00:43s\n",
      "epoch 154| loss: 0.06547 | val_0_rmse: 0.24003 |  0:00:44s\n",
      "epoch 155| loss: 0.08015 | val_0_rmse: 0.23803 |  0:00:44s\n",
      "epoch 156| loss: 0.06975 | val_0_rmse: 0.27266 |  0:00:44s\n",
      "epoch 157| loss: 0.05829 | val_0_rmse: 0.24291 |  0:00:44s\n",
      "epoch 158| loss: 0.05469 | val_0_rmse: 0.2608  |  0:00:45s\n",
      "epoch 159| loss: 0.05218 | val_0_rmse: 0.23096 |  0:00:45s\n",
      "epoch 160| loss: 0.05125 | val_0_rmse: 0.22476 |  0:00:45s\n",
      "epoch 161| loss: 0.04711 | val_0_rmse: 0.23547 |  0:00:46s\n",
      "epoch 162| loss: 0.04974 | val_0_rmse: 0.23916 |  0:00:46s\n",
      "epoch 163| loss: 0.04823 | val_0_rmse: 0.22828 |  0:00:46s\n",
      "epoch 164| loss: 0.05198 | val_0_rmse: 0.22157 |  0:00:46s\n",
      "epoch 165| loss: 0.05385 | val_0_rmse: 0.25267 |  0:00:47s\n",
      "epoch 166| loss: 0.05682 | val_0_rmse: 0.25676 |  0:00:47s\n",
      "epoch 167| loss: 0.05193 | val_0_rmse: 0.23608 |  0:00:47s\n",
      "epoch 168| loss: 0.04909 | val_0_rmse: 0.25863 |  0:00:47s\n",
      "epoch 169| loss: 0.05923 | val_0_rmse: 0.22323 |  0:00:48s\n",
      "epoch 170| loss: 0.04103 | val_0_rmse: 0.22448 |  0:00:48s\n",
      "epoch 171| loss: 0.04494 | val_0_rmse: 0.22856 |  0:00:48s\n",
      "epoch 172| loss: 0.04864 | val_0_rmse: 0.23357 |  0:00:48s\n",
      "epoch 173| loss: 0.04632 | val_0_rmse: 0.22575 |  0:00:49s\n",
      "epoch 174| loss: 0.04194 | val_0_rmse: 0.22456 |  0:00:49s\n",
      "epoch 175| loss: 0.04616 | val_0_rmse: 0.22442 |  0:00:49s\n",
      "epoch 176| loss: 0.04605 | val_0_rmse: 0.2379  |  0:00:50s\n",
      "epoch 177| loss: 0.04572 | val_0_rmse: 0.22158 |  0:00:50s\n",
      "epoch 178| loss: 0.04225 | val_0_rmse: 0.21687 |  0:00:50s\n",
      "epoch 179| loss: 0.04363 | val_0_rmse: 0.21727 |  0:00:51s\n",
      "epoch 180| loss: 0.04094 | val_0_rmse: 0.22465 |  0:00:51s\n",
      "epoch 181| loss: 0.04168 | val_0_rmse: 0.21974 |  0:00:51s\n",
      "epoch 182| loss: 0.04257 | val_0_rmse: 0.21704 |  0:00:51s\n",
      "epoch 183| loss: 0.03761 | val_0_rmse: 0.23355 |  0:00:52s\n",
      "epoch 184| loss: 0.04331 | val_0_rmse: 0.22619 |  0:00:52s\n",
      "epoch 185| loss: 0.04014 | val_0_rmse: 0.22804 |  0:00:52s\n",
      "epoch 186| loss: 0.04028 | val_0_rmse: 0.24354 |  0:00:52s\n",
      "epoch 187| loss: 0.04366 | val_0_rmse: 0.22337 |  0:00:53s\n",
      "epoch 188| loss: 0.03994 | val_0_rmse: 0.25397 |  0:00:53s\n",
      "epoch 189| loss: 0.0501  | val_0_rmse: 0.2347  |  0:00:53s\n",
      "epoch 190| loss: 0.05881 | val_0_rmse: 0.22851 |  0:00:54s\n",
      "epoch 191| loss: 0.04473 | val_0_rmse: 0.23136 |  0:00:54s\n",
      "epoch 192| loss: 0.05064 | val_0_rmse: 0.24681 |  0:00:54s\n",
      "epoch 193| loss: 0.0507  | val_0_rmse: 0.35243 |  0:00:54s\n",
      "epoch 194| loss: 0.11134 | val_0_rmse: 0.22887 |  0:00:55s\n",
      "epoch 195| loss: 0.04698 | val_0_rmse: 0.22874 |  0:00:55s\n",
      "epoch 196| loss: 0.03839 | val_0_rmse: 0.25173 |  0:00:55s\n",
      "epoch 197| loss: 0.04202 | val_0_rmse: 0.2268  |  0:00:55s\n",
      "epoch 198| loss: 0.04028 | val_0_rmse: 0.24746 |  0:00:56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:26:05,674] Trial 45 finished with value: 0.21686960560651053 and parameters: {'n_d': 48, 'n_a': 64, 'n_steps': 4, 'gamma': 1.3600086487827157, 'n_independent': 1, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.006523492339448779, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199| loss: 0.03748 | val_0_rmse: 0.23187 |  0:00:56s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 178 and best_val_0_rmse = 0.21687\n",
      "Trial 045 | rmse_log=0.21687 | RMSE$=46,932 | MAE$=28,316 | MAPE=16.62% | n_d/n_a=48/64 steps=4 lr=0.00652 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 132.12715| val_0_rmse: 11.88725|  0:00:00s\n",
      "epoch 1  | loss: 128.174 | val_0_rmse: 11.84533|  0:00:00s\n",
      "epoch 2  | loss: 125.99855| val_0_rmse: 11.77853|  0:00:00s\n",
      "epoch 3  | loss: 120.58983| val_0_rmse: 11.71604|  0:00:00s\n",
      "epoch 4  | loss: 117.28667| val_0_rmse: 11.65714|  0:00:00s\n",
      "epoch 5  | loss: 113.99491| val_0_rmse: 11.58013|  0:00:00s\n",
      "epoch 6  | loss: 109.89941| val_0_rmse: 11.50155|  0:00:00s\n",
      "epoch 7  | loss: 108.17593| val_0_rmse: 11.40894|  0:00:01s\n",
      "epoch 8  | loss: 105.91465| val_0_rmse: 11.31659|  0:00:01s\n",
      "epoch 9  | loss: 102.3658| val_0_rmse: 11.23197|  0:00:01s\n",
      "epoch 10 | loss: 98.66711| val_0_rmse: 11.14031|  0:00:01s\n",
      "epoch 11 | loss: 97.0925 | val_0_rmse: 11.0358 |  0:00:01s\n",
      "epoch 12 | loss: 94.89579| val_0_rmse: 10.9372 |  0:00:01s\n",
      "epoch 13 | loss: 92.74011| val_0_rmse: 10.82197|  0:00:02s\n",
      "epoch 14 | loss: 87.83282| val_0_rmse: 10.70939|  0:00:02s\n",
      "epoch 15 | loss: 85.51443| val_0_rmse: 10.58724|  0:00:02s\n",
      "epoch 16 | loss: 81.282  | val_0_rmse: 10.46232|  0:00:02s\n",
      "epoch 17 | loss: 79.65633| val_0_rmse: 10.3455 |  0:00:02s\n",
      "epoch 18 | loss: 77.35043| val_0_rmse: 10.23417|  0:00:02s\n",
      "epoch 19 | loss: 75.17167| val_0_rmse: 10.09704|  0:00:02s\n",
      "epoch 20 | loss: 72.52591| val_0_rmse: 9.96715 |  0:00:02s\n",
      "epoch 21 | loss: 69.00285| val_0_rmse: 9.8265  |  0:00:03s\n",
      "epoch 22 | loss: 66.92971| val_0_rmse: 9.70006 |  0:00:03s\n",
      "epoch 23 | loss: 64.46615| val_0_rmse: 9.55801 |  0:00:03s\n",
      "epoch 24 | loss: 62.682  | val_0_rmse: 9.41243 |  0:00:03s\n",
      "epoch 25 | loss: 58.67516| val_0_rmse: 9.25788 |  0:00:03s\n",
      "epoch 26 | loss: 56.02826| val_0_rmse: 9.1202  |  0:00:03s\n",
      "epoch 27 | loss: 55.44656| val_0_rmse: 8.97145 |  0:00:03s\n",
      "epoch 28 | loss: 51.3982 | val_0_rmse: 8.83605 |  0:00:03s\n",
      "epoch 29 | loss: 53.33657| val_0_rmse: 8.68203 |  0:00:04s\n",
      "epoch 30 | loss: 46.03167| val_0_rmse: 8.50615 |  0:00:04s\n",
      "epoch 31 | loss: 48.45378| val_0_rmse: 8.37641 |  0:00:04s\n",
      "epoch 32 | loss: 44.14008| val_0_rmse: 8.23699 |  0:00:04s\n",
      "epoch 33 | loss: 41.22878| val_0_rmse: 8.07232 |  0:00:04s\n",
      "epoch 34 | loss: 38.81118| val_0_rmse: 7.91097 |  0:00:04s\n",
      "epoch 35 | loss: 39.13828| val_0_rmse: 7.78225 |  0:00:04s\n",
      "epoch 36 | loss: 32.94738| val_0_rmse: 7.58622 |  0:00:04s\n",
      "epoch 37 | loss: 36.69958| val_0_rmse: 7.43362 |  0:00:05s\n",
      "epoch 38 | loss: 31.80294| val_0_rmse: 7.23462 |  0:00:05s\n",
      "epoch 39 | loss: 30.01787| val_0_rmse: 7.02916 |  0:00:05s\n",
      "epoch 40 | loss: 28.10577| val_0_rmse: 6.80415 |  0:00:05s\n",
      "epoch 41 | loss: 28.12173| val_0_rmse: 6.62664 |  0:00:05s\n",
      "epoch 42 | loss: 26.17211| val_0_rmse: 6.42335 |  0:00:05s\n",
      "epoch 43 | loss: 24.96038| val_0_rmse: 6.25325 |  0:00:05s\n",
      "epoch 44 | loss: 22.32566| val_0_rmse: 6.03861 |  0:00:05s\n",
      "epoch 45 | loss: 19.63844| val_0_rmse: 5.83026 |  0:00:06s\n",
      "epoch 46 | loss: 20.35972| val_0_rmse: 5.6742  |  0:00:06s\n",
      "epoch 47 | loss: 21.12557| val_0_rmse: 5.61157 |  0:00:06s\n",
      "epoch 48 | loss: 18.02415| val_0_rmse: 5.41614 |  0:00:06s\n",
      "epoch 49 | loss: 14.56596| val_0_rmse: 5.27554 |  0:00:06s\n",
      "epoch 50 | loss: 15.56736| val_0_rmse: 5.10882 |  0:00:06s\n",
      "epoch 51 | loss: 13.27777| val_0_rmse: 4.9162  |  0:00:06s\n",
      "epoch 52 | loss: 15.13006| val_0_rmse: 4.72705 |  0:00:07s\n",
      "epoch 53 | loss: 13.35925| val_0_rmse: 4.53757 |  0:00:07s\n",
      "epoch 54 | loss: 11.46591| val_0_rmse: 4.34308 |  0:00:07s\n",
      "epoch 55 | loss: 10.10193| val_0_rmse: 4.11213 |  0:00:07s\n",
      "epoch 56 | loss: 11.07995| val_0_rmse: 3.93361 |  0:00:07s\n",
      "epoch 57 | loss: 9.57057 | val_0_rmse: 3.71431 |  0:00:07s\n",
      "epoch 58 | loss: 10.24036| val_0_rmse: 3.55395 |  0:00:08s\n",
      "epoch 59 | loss: 9.52086 | val_0_rmse: 3.40987 |  0:00:08s\n",
      "epoch 60 | loss: 8.06933 | val_0_rmse: 3.24663 |  0:00:08s\n",
      "epoch 61 | loss: 8.52043 | val_0_rmse: 3.14156 |  0:00:08s\n",
      "epoch 62 | loss: 7.23693 | val_0_rmse: 3.04139 |  0:00:08s\n",
      "epoch 63 | loss: 7.63804 | val_0_rmse: 2.9131  |  0:00:08s\n",
      "epoch 64 | loss: 7.55665 | val_0_rmse: 2.84387 |  0:00:09s\n",
      "epoch 65 | loss: 6.19312 | val_0_rmse: 2.77569 |  0:00:09s\n",
      "epoch 66 | loss: 5.43838 | val_0_rmse: 2.67526 |  0:00:09s\n",
      "epoch 67 | loss: 5.50156 | val_0_rmse: 2.63902 |  0:00:09s\n",
      "epoch 68 | loss: 4.47283 | val_0_rmse: 2.53968 |  0:00:09s\n",
      "epoch 69 | loss: 4.68229 | val_0_rmse: 2.50576 |  0:00:09s\n",
      "epoch 70 | loss: 4.50523 | val_0_rmse: 2.45266 |  0:00:09s\n",
      "epoch 71 | loss: 3.75789 | val_0_rmse: 2.35252 |  0:00:10s\n",
      "epoch 72 | loss: 3.89247 | val_0_rmse: 2.27706 |  0:00:10s\n",
      "epoch 73 | loss: 4.56132 | val_0_rmse: 2.25827 |  0:00:10s\n",
      "epoch 74 | loss: 3.19043 | val_0_rmse: 2.17617 |  0:00:10s\n",
      "epoch 75 | loss: 2.91393 | val_0_rmse: 2.10191 |  0:00:10s\n",
      "epoch 76 | loss: 2.92322 | val_0_rmse: 2.03018 |  0:00:10s\n",
      "epoch 77 | loss: 3.45325 | val_0_rmse: 1.98831 |  0:00:11s\n",
      "epoch 78 | loss: 2.19259 | val_0_rmse: 1.87842 |  0:00:11s\n",
      "epoch 79 | loss: 2.75747 | val_0_rmse: 1.81225 |  0:00:11s\n",
      "epoch 80 | loss: 2.31824 | val_0_rmse: 1.75172 |  0:00:11s\n",
      "epoch 81 | loss: 2.60689 | val_0_rmse: 1.77552 |  0:00:11s\n",
      "epoch 82 | loss: 2.03377 | val_0_rmse: 1.68495 |  0:00:11s\n",
      "epoch 83 | loss: 2.25051 | val_0_rmse: 1.55824 |  0:00:11s\n",
      "epoch 84 | loss: 2.22324 | val_0_rmse: 1.46597 |  0:00:11s\n",
      "epoch 85 | loss: 1.87182 | val_0_rmse: 1.35983 |  0:00:12s\n",
      "epoch 86 | loss: 1.60966 | val_0_rmse: 1.26396 |  0:00:12s\n",
      "epoch 87 | loss: 1.61533 | val_0_rmse: 1.19538 |  0:00:12s\n",
      "epoch 88 | loss: 1.74038 | val_0_rmse: 1.15158 |  0:00:12s\n",
      "epoch 89 | loss: 1.40762 | val_0_rmse: 1.12483 |  0:00:12s\n",
      "epoch 90 | loss: 1.26963 | val_0_rmse: 1.09786 |  0:00:12s\n",
      "epoch 91 | loss: 1.14518 | val_0_rmse: 1.07656 |  0:00:12s\n",
      "epoch 92 | loss: 1.13163 | val_0_rmse: 1.0552  |  0:00:13s\n",
      "epoch 93 | loss: 0.93905 | val_0_rmse: 1.00351 |  0:00:13s\n",
      "epoch 94 | loss: 1.1456  | val_0_rmse: 0.95584 |  0:00:13s\n",
      "epoch 95 | loss: 0.92408 | val_0_rmse: 0.89531 |  0:00:13s\n",
      "epoch 96 | loss: 1.06942 | val_0_rmse: 0.86207 |  0:00:13s\n",
      "epoch 97 | loss: 1.04418 | val_0_rmse: 0.80869 |  0:00:13s\n",
      "epoch 98 | loss: 0.96398 | val_0_rmse: 0.77262 |  0:00:14s\n",
      "epoch 99 | loss: 0.78839 | val_0_rmse: 0.75192 |  0:00:14s\n",
      "epoch 100| loss: 0.69036 | val_0_rmse: 0.77714 |  0:00:14s\n",
      "epoch 101| loss: 0.66072 | val_0_rmse: 0.77973 |  0:00:14s\n",
      "epoch 102| loss: 0.70436 | val_0_rmse: 0.75997 |  0:00:14s\n",
      "epoch 103| loss: 0.76018 | val_0_rmse: 0.69977 |  0:00:15s\n",
      "epoch 104| loss: 0.62248 | val_0_rmse: 0.62962 |  0:00:15s\n",
      "epoch 105| loss: 0.55556 | val_0_rmse: 0.56395 |  0:00:15s\n",
      "epoch 106| loss: 0.57803 | val_0_rmse: 0.54713 |  0:00:15s\n",
      "epoch 107| loss: 0.56899 | val_0_rmse: 0.56806 |  0:00:16s\n",
      "epoch 108| loss: 0.45087 | val_0_rmse: 0.61158 |  0:00:16s\n",
      "epoch 109| loss: 0.45191 | val_0_rmse: 0.63748 |  0:00:16s\n",
      "epoch 110| loss: 0.4393  | val_0_rmse: 0.63429 |  0:00:16s\n",
      "epoch 111| loss: 0.51987 | val_0_rmse: 0.59503 |  0:00:16s\n",
      "epoch 112| loss: 0.48204 | val_0_rmse: 0.5436  |  0:00:17s\n",
      "epoch 113| loss: 0.40041 | val_0_rmse: 0.50842 |  0:00:17s\n",
      "epoch 114| loss: 0.35026 | val_0_rmse: 0.48584 |  0:00:17s\n",
      "epoch 115| loss: 0.3993  | val_0_rmse: 0.47499 |  0:00:17s\n",
      "epoch 116| loss: 0.42567 | val_0_rmse: 0.47083 |  0:00:17s\n",
      "epoch 117| loss: 0.43812 | val_0_rmse: 0.46346 |  0:00:17s\n",
      "epoch 118| loss: 0.32552 | val_0_rmse: 0.48107 |  0:00:17s\n",
      "epoch 119| loss: 0.38228 | val_0_rmse: 0.49731 |  0:00:18s\n",
      "epoch 120| loss: 0.39613 | val_0_rmse: 0.50234 |  0:00:18s\n",
      "epoch 121| loss: 0.34391 | val_0_rmse: 0.46099 |  0:00:18s\n",
      "epoch 122| loss: 0.29522 | val_0_rmse: 0.39076 |  0:00:18s\n",
      "epoch 123| loss: 0.2911  | val_0_rmse: 0.37931 |  0:00:18s\n",
      "epoch 124| loss: 0.30321 | val_0_rmse: 0.39634 |  0:00:18s\n",
      "epoch 125| loss: 0.23539 | val_0_rmse: 0.39872 |  0:00:18s\n",
      "epoch 126| loss: 0.31195 | val_0_rmse: 0.40144 |  0:00:18s\n",
      "epoch 127| loss: 0.27633 | val_0_rmse: 0.36995 |  0:00:19s\n",
      "epoch 128| loss: 0.23202 | val_0_rmse: 0.35425 |  0:00:19s\n",
      "epoch 129| loss: 0.21198 | val_0_rmse: 0.37359 |  0:00:19s\n",
      "epoch 130| loss: 0.18443 | val_0_rmse: 0.39044 |  0:00:19s\n",
      "epoch 131| loss: 0.22798 | val_0_rmse: 0.40074 |  0:00:19s\n",
      "epoch 132| loss: 0.22514 | val_0_rmse: 0.39484 |  0:00:19s\n",
      "epoch 133| loss: 0.20223 | val_0_rmse: 0.36042 |  0:00:19s\n",
      "epoch 134| loss: 0.2063  | val_0_rmse: 0.35591 |  0:00:20s\n",
      "epoch 135| loss: 0.21958 | val_0_rmse: 0.3456  |  0:00:20s\n",
      "epoch 136| loss: 0.1957  | val_0_rmse: 0.3467  |  0:00:20s\n",
      "epoch 137| loss: 0.20792 | val_0_rmse: 0.34679 |  0:00:20s\n",
      "epoch 138| loss: 0.17123 | val_0_rmse: 0.33247 |  0:00:20s\n",
      "epoch 139| loss: 0.20698 | val_0_rmse: 0.32872 |  0:00:20s\n",
      "epoch 140| loss: 0.20344 | val_0_rmse: 0.35507 |  0:00:20s\n",
      "epoch 141| loss: 0.18707 | val_0_rmse: 0.35252 |  0:00:20s\n",
      "epoch 142| loss: 0.18092 | val_0_rmse: 0.32798 |  0:00:20s\n",
      "epoch 143| loss: 0.14649 | val_0_rmse: 0.32222 |  0:00:21s\n",
      "epoch 144| loss: 0.16572 | val_0_rmse: 0.32716 |  0:00:21s\n",
      "epoch 145| loss: 0.1702  | val_0_rmse: 0.3403  |  0:00:21s\n",
      "epoch 146| loss: 0.15659 | val_0_rmse: 0.33261 |  0:00:21s\n",
      "epoch 147| loss: 0.16925 | val_0_rmse: 0.3275  |  0:00:21s\n",
      "epoch 148| loss: 0.17194 | val_0_rmse: 0.32553 |  0:00:21s\n",
      "epoch 149| loss: 0.15214 | val_0_rmse: 0.32583 |  0:00:21s\n",
      "epoch 150| loss: 0.15378 | val_0_rmse: 0.33363 |  0:00:21s\n",
      "epoch 151| loss: 0.15002 | val_0_rmse: 0.33125 |  0:00:22s\n",
      "epoch 152| loss: 0.17297 | val_0_rmse: 0.30659 |  0:00:22s\n",
      "epoch 153| loss: 0.1554  | val_0_rmse: 0.29061 |  0:00:22s\n",
      "epoch 154| loss: 0.16282 | val_0_rmse: 0.29375 |  0:00:22s\n",
      "epoch 155| loss: 0.14797 | val_0_rmse: 0.3152  |  0:00:22s\n",
      "epoch 156| loss: 0.14704 | val_0_rmse: 0.32754 |  0:00:22s\n",
      "epoch 157| loss: 0.15621 | val_0_rmse: 0.31945 |  0:00:22s\n",
      "epoch 158| loss: 0.12173 | val_0_rmse: 0.30453 |  0:00:22s\n",
      "epoch 159| loss: 0.12607 | val_0_rmse: 0.30197 |  0:00:23s\n",
      "epoch 160| loss: 0.14029 | val_0_rmse: 0.30498 |  0:00:23s\n",
      "epoch 161| loss: 0.11927 | val_0_rmse: 0.30653 |  0:00:23s\n",
      "epoch 162| loss: 0.12684 | val_0_rmse: 0.30326 |  0:00:23s\n",
      "epoch 163| loss: 0.12671 | val_0_rmse: 0.29801 |  0:00:23s\n",
      "epoch 164| loss: 0.11182 | val_0_rmse: 0.29571 |  0:00:23s\n",
      "epoch 165| loss: 0.11884 | val_0_rmse: 0.29301 |  0:00:23s\n",
      "epoch 166| loss: 0.12802 | val_0_rmse: 0.2931  |  0:00:23s\n",
      "epoch 167| loss: 0.11241 | val_0_rmse: 0.29556 |  0:00:24s\n",
      "epoch 168| loss: 0.11718 | val_0_rmse: 0.28902 |  0:00:24s\n",
      "epoch 169| loss: 0.12374 | val_0_rmse: 0.28692 |  0:00:24s\n",
      "epoch 170| loss: 0.1297  | val_0_rmse: 0.29297 |  0:00:24s\n",
      "epoch 171| loss: 0.11818 | val_0_rmse: 0.30205 |  0:00:24s\n",
      "epoch 172| loss: 0.11093 | val_0_rmse: 0.30675 |  0:00:24s\n",
      "epoch 173| loss: 0.10272 | val_0_rmse: 0.29703 |  0:00:24s\n",
      "epoch 174| loss: 0.11669 | val_0_rmse: 0.28642 |  0:00:24s\n",
      "epoch 175| loss: 0.09932 | val_0_rmse: 0.29129 |  0:00:25s\n",
      "epoch 176| loss: 0.0933  | val_0_rmse: 0.29711 |  0:00:25s\n",
      "epoch 177| loss: 0.11723 | val_0_rmse: 0.29244 |  0:00:25s\n",
      "epoch 178| loss: 0.10549 | val_0_rmse: 0.29771 |  0:00:25s\n",
      "epoch 179| loss: 0.10074 | val_0_rmse: 0.29616 |  0:00:25s\n",
      "epoch 180| loss: 0.09534 | val_0_rmse: 0.29053 |  0:00:25s\n",
      "epoch 181| loss: 0.09353 | val_0_rmse: 0.28484 |  0:00:25s\n",
      "epoch 182| loss: 0.12127 | val_0_rmse: 0.28125 |  0:00:25s\n",
      "epoch 183| loss: 0.08335 | val_0_rmse: 0.2812  |  0:00:26s\n",
      "epoch 184| loss: 0.09214 | val_0_rmse: 0.28613 |  0:00:26s\n",
      "epoch 185| loss: 0.07639 | val_0_rmse: 0.28934 |  0:00:26s\n",
      "epoch 186| loss: 0.09054 | val_0_rmse: 0.29322 |  0:00:26s\n",
      "epoch 187| loss: 0.08593 | val_0_rmse: 0.2979  |  0:00:26s\n",
      "epoch 188| loss: 0.08751 | val_0_rmse: 0.29813 |  0:00:26s\n",
      "epoch 189| loss: 0.08677 | val_0_rmse: 0.28872 |  0:00:26s\n",
      "epoch 190| loss: 0.09113 | val_0_rmse: 0.28554 |  0:00:26s\n",
      "epoch 191| loss: 0.09189 | val_0_rmse: 0.28787 |  0:00:26s\n",
      "epoch 192| loss: 0.07834 | val_0_rmse: 0.28639 |  0:00:27s\n",
      "epoch 193| loss: 0.0811  | val_0_rmse: 0.28453 |  0:00:27s\n",
      "epoch 194| loss: 0.0836  | val_0_rmse: 0.28267 |  0:00:27s\n",
      "epoch 195| loss: 0.08207 | val_0_rmse: 0.27705 |  0:00:27s\n",
      "epoch 196| loss: 0.11162 | val_0_rmse: 0.27295 |  0:00:27s\n",
      "epoch 197| loss: 0.07306 | val_0_rmse: 0.27461 |  0:00:27s\n",
      "epoch 198| loss: 0.08828 | val_0_rmse: 0.2724  |  0:00:27s\n",
      "epoch 199| loss: 0.07816 | val_0_rmse: 0.26954 |  0:00:27s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 199 and best_val_0_rmse = 0.26954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:26:33,810] Trial 46 finished with value: 0.2695366374780138 and parameters: {'n_d': 32, 'n_a': 24, 'n_steps': 3, 'gamma': 1.5783369077136102, 'n_independent': 1, 'n_shared': 3, 'lambda_sparse': 1e-05, 'mask_type': 'entmax', 'lr': 0.004274337903042017, 'batch_size': 2048, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 046 | rmse_log=0.26954 | RMSE$=59,361 | MAE$=36,602 | MAPE=20.52% | n_d/n_a=32/24 steps=3 lr=0.00427 batch=2048 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 149.64925| val_0_rmse: 11.6311 |  0:00:00s\n",
      "epoch 1  | loss: 118.14278| val_0_rmse: 11.19392|  0:00:00s\n",
      "epoch 2  | loss: 93.26453| val_0_rmse: 10.63905|  0:00:00s\n",
      "epoch 3  | loss: 74.21915| val_0_rmse: 10.01607|  0:00:01s\n",
      "epoch 4  | loss: 59.39321| val_0_rmse: 9.38559 |  0:00:01s\n",
      "epoch 5  | loss: 46.73739| val_0_rmse: 8.74281 |  0:00:01s\n",
      "epoch 6  | loss: 39.64669| val_0_rmse: 8.10621 |  0:00:01s\n",
      "epoch 7  | loss: 35.69412| val_0_rmse: 7.43834 |  0:00:01s\n",
      "epoch 8  | loss: 29.5432 | val_0_rmse: 6.76919 |  0:00:02s\n",
      "epoch 9  | loss: 30.99439| val_0_rmse: 6.12138 |  0:00:02s\n",
      "epoch 10 | loss: 24.04984| val_0_rmse: 5.53374 |  0:00:02s\n",
      "epoch 11 | loss: 22.65086| val_0_rmse: 5.03349 |  0:00:02s\n",
      "epoch 12 | loss: 15.40739| val_0_rmse: 4.63972 |  0:00:03s\n",
      "epoch 13 | loss: 18.09913| val_0_rmse: 4.36607 |  0:00:03s\n",
      "epoch 14 | loss: 13.70131| val_0_rmse: 4.10941 |  0:00:03s\n",
      "epoch 15 | loss: 11.67039| val_0_rmse: 3.85651 |  0:00:04s\n",
      "epoch 16 | loss: 8.87994 | val_0_rmse: 3.53192 |  0:00:04s\n",
      "epoch 17 | loss: 8.20468 | val_0_rmse: 3.05481 |  0:00:04s\n",
      "epoch 18 | loss: 5.77727 | val_0_rmse: 2.44363 |  0:00:04s\n",
      "epoch 19 | loss: 4.93358 | val_0_rmse: 1.88697 |  0:00:05s\n",
      "epoch 20 | loss: 3.98629 | val_0_rmse: 1.7288  |  0:00:05s\n",
      "epoch 21 | loss: 3.19896 | val_0_rmse: 1.66113 |  0:00:05s\n",
      "epoch 22 | loss: 3.27888 | val_0_rmse: 1.43317 |  0:00:05s\n",
      "epoch 23 | loss: 2.57481 | val_0_rmse: 1.16372 |  0:00:06s\n",
      "epoch 24 | loss: 2.29045 | val_0_rmse: 0.91206 |  0:00:06s\n",
      "epoch 25 | loss: 2.08705 | val_0_rmse: 0.86382 |  0:00:06s\n",
      "epoch 26 | loss: 1.75725 | val_0_rmse: 0.88571 |  0:00:06s\n",
      "epoch 27 | loss: 1.65024 | val_0_rmse: 0.73833 |  0:00:07s\n",
      "epoch 28 | loss: 1.39367 | val_0_rmse: 0.73625 |  0:00:07s\n",
      "epoch 29 | loss: 1.16955 | val_0_rmse: 0.80965 |  0:00:07s\n",
      "epoch 30 | loss: 1.23348 | val_0_rmse: 0.73394 |  0:00:08s\n",
      "epoch 31 | loss: 0.91412 | val_0_rmse: 0.60468 |  0:00:08s\n",
      "epoch 32 | loss: 1.22843 | val_0_rmse: 0.68143 |  0:00:08s\n",
      "epoch 33 | loss: 0.80368 | val_0_rmse: 0.70235 |  0:00:08s\n",
      "epoch 34 | loss: 1.00967 | val_0_rmse: 0.57049 |  0:00:09s\n",
      "epoch 35 | loss: 0.91597 | val_0_rmse: 0.6497  |  0:00:09s\n",
      "epoch 36 | loss: 1.03605 | val_0_rmse: 0.59258 |  0:00:09s\n",
      "epoch 37 | loss: 0.86312 | val_0_rmse: 0.39699 |  0:00:09s\n",
      "epoch 38 | loss: 0.96532 | val_0_rmse: 0.76234 |  0:00:09s\n",
      "epoch 39 | loss: 1.03043 | val_0_rmse: 0.68031 |  0:00:10s\n",
      "epoch 40 | loss: 0.87106 | val_0_rmse: 0.39539 |  0:00:10s\n",
      "epoch 41 | loss: 0.92949 | val_0_rmse: 0.35399 |  0:00:10s\n",
      "epoch 42 | loss: 0.99085 | val_0_rmse: 0.36363 |  0:00:10s\n",
      "epoch 43 | loss: 0.67056 | val_0_rmse: 0.60265 |  0:00:11s\n",
      "epoch 44 | loss: 0.8205  | val_0_rmse: 0.52024 |  0:00:11s\n",
      "epoch 45 | loss: 0.71151 | val_0_rmse: 0.34672 |  0:00:11s\n",
      "epoch 46 | loss: 0.72044 | val_0_rmse: 0.35748 |  0:00:11s\n",
      "epoch 47 | loss: 0.61276 | val_0_rmse: 0.46617 |  0:00:11s\n",
      "epoch 48 | loss: 0.68879 | val_0_rmse: 0.42231 |  0:00:12s\n",
      "epoch 49 | loss: 0.78098 | val_0_rmse: 0.38805 |  0:00:12s\n",
      "epoch 50 | loss: 0.66518 | val_0_rmse: 0.39957 |  0:00:12s\n",
      "epoch 51 | loss: 0.69486 | val_0_rmse: 0.4703  |  0:00:13s\n",
      "epoch 52 | loss: 0.81704 | val_0_rmse: 0.38189 |  0:00:13s\n",
      "epoch 53 | loss: 0.6719  | val_0_rmse: 0.31023 |  0:00:13s\n",
      "epoch 54 | loss: 0.62401 | val_0_rmse: 0.32174 |  0:00:13s\n",
      "epoch 55 | loss: 0.52623 | val_0_rmse: 0.34577 |  0:00:14s\n",
      "epoch 56 | loss: 0.48572 | val_0_rmse: 0.33254 |  0:00:14s\n",
      "epoch 57 | loss: 0.50604 | val_0_rmse: 0.41613 |  0:00:14s\n",
      "epoch 58 | loss: 0.44411 | val_0_rmse: 0.38781 |  0:00:14s\n",
      "epoch 59 | loss: 0.43492 | val_0_rmse: 0.37495 |  0:00:15s\n",
      "epoch 60 | loss: 0.44298 | val_0_rmse: 0.37779 |  0:00:15s\n",
      "epoch 61 | loss: 0.44359 | val_0_rmse: 0.30671 |  0:00:15s\n",
      "epoch 62 | loss: 0.4132  | val_0_rmse: 0.31508 |  0:00:15s\n",
      "epoch 63 | loss: 0.45677 | val_0_rmse: 0.35762 |  0:00:15s\n",
      "epoch 64 | loss: 0.36642 | val_0_rmse: 0.4689  |  0:00:16s\n",
      "epoch 65 | loss: 0.44709 | val_0_rmse: 0.3393  |  0:00:16s\n",
      "epoch 66 | loss: 0.54868 | val_0_rmse: 0.43808 |  0:00:16s\n",
      "epoch 67 | loss: 0.4603  | val_0_rmse: 0.38366 |  0:00:16s\n",
      "epoch 68 | loss: 0.45947 | val_0_rmse: 0.29577 |  0:00:17s\n",
      "epoch 69 | loss: 0.3567  | val_0_rmse: 0.30114 |  0:00:17s\n",
      "epoch 70 | loss: 0.43392 | val_0_rmse: 0.55134 |  0:00:17s\n",
      "epoch 71 | loss: 0.56493 | val_0_rmse: 0.27451 |  0:00:18s\n",
      "epoch 72 | loss: 0.39526 | val_0_rmse: 0.28423 |  0:00:18s\n",
      "epoch 73 | loss: 0.37855 | val_0_rmse: 0.62089 |  0:00:18s\n",
      "epoch 74 | loss: 0.63071 | val_0_rmse: 0.38273 |  0:00:18s\n",
      "epoch 75 | loss: 0.38005 | val_0_rmse: 0.39424 |  0:00:19s\n",
      "epoch 76 | loss: 0.51582 | val_0_rmse: 0.40232 |  0:00:19s\n",
      "epoch 77 | loss: 0.4595  | val_0_rmse: 0.72468 |  0:00:19s\n",
      "epoch 78 | loss: 0.95487 | val_0_rmse: 0.38052 |  0:00:19s\n",
      "epoch 79 | loss: 0.33664 | val_0_rmse: 0.6538  |  0:00:19s\n",
      "epoch 80 | loss: 1.17499 | val_0_rmse: 0.51838 |  0:00:20s\n",
      "epoch 81 | loss: 0.71737 | val_0_rmse: 0.52101 |  0:00:20s\n",
      "epoch 82 | loss: 0.60732 | val_0_rmse: 0.65473 |  0:00:20s\n",
      "epoch 83 | loss: 0.7549  | val_0_rmse: 0.29992 |  0:00:20s\n",
      "epoch 84 | loss: 0.27508 | val_0_rmse: 0.42438 |  0:00:21s\n",
      "epoch 85 | loss: 0.69575 | val_0_rmse: 0.31552 |  0:00:21s\n",
      "epoch 86 | loss: 0.37867 | val_0_rmse: 0.43978 |  0:00:21s\n",
      "epoch 87 | loss: 0.36499 | val_0_rmse: 0.40076 |  0:00:21s\n",
      "epoch 88 | loss: 0.37365 | val_0_rmse: 0.28571 |  0:00:22s\n",
      "epoch 89 | loss: 0.25469 | val_0_rmse: 0.29987 |  0:00:22s\n",
      "epoch 90 | loss: 0.24807 | val_0_rmse: 0.33724 |  0:00:22s\n",
      "epoch 91 | loss: 0.26529 | val_0_rmse: 0.32008 |  0:00:22s\n",
      "epoch 92 | loss: 0.28011 | val_0_rmse: 0.32177 |  0:00:23s\n",
      "epoch 93 | loss: 0.32256 | val_0_rmse: 0.35562 |  0:00:23s\n",
      "epoch 94 | loss: 0.22544 | val_0_rmse: 0.39895 |  0:00:23s\n",
      "epoch 95 | loss: 0.28397 | val_0_rmse: 0.31032 |  0:00:23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:26:58,348] Trial 47 finished with value: 0.27451458723896605 and parameters: {'n_d': 48, 'n_a': 16, 'n_steps': 5, 'gamma': 1.3154454151604447, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.00865193964688639, 'batch_size': 1024, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96 | loss: 0.28042 | val_0_rmse: 0.30043 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 71 and best_val_0_rmse = 0.27451\n",
      "Trial 047 | rmse_log=0.27451 | RMSE$=58,386 | MAE$=36,467 | MAPE=20.64% | n_d/n_a=48/16 steps=5 lr=0.00865 batch=1024 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 126.15727| val_0_rmse: 11.08367|  0:00:00s\n",
      "epoch 1  | loss: 94.78498| val_0_rmse: 10.43728|  0:00:00s\n",
      "epoch 2  | loss: 73.05153| val_0_rmse: 9.75635 |  0:00:00s\n",
      "epoch 3  | loss: 55.08105| val_0_rmse: 8.99503 |  0:00:00s\n",
      "epoch 4  | loss: 43.92708| val_0_rmse: 8.27689 |  0:00:01s\n",
      "epoch 5  | loss: 34.7415 | val_0_rmse: 7.48133 |  0:00:01s\n",
      "epoch 6  | loss: 27.75068| val_0_rmse: 6.69375 |  0:00:01s\n",
      "epoch 7  | loss: 23.6363 | val_0_rmse: 5.9352  |  0:00:01s\n",
      "epoch 8  | loss: 22.57474| val_0_rmse: 5.33246 |  0:00:02s\n",
      "epoch 9  | loss: 17.26303| val_0_rmse: 4.82214 |  0:00:02s\n",
      "epoch 10 | loss: 13.49982| val_0_rmse: 4.40192 |  0:00:02s\n",
      "epoch 11 | loss: 12.44093| val_0_rmse: 4.07288 |  0:00:02s\n",
      "epoch 12 | loss: 10.33176| val_0_rmse: 3.81984 |  0:00:02s\n",
      "epoch 13 | loss: 8.84634 | val_0_rmse: 3.61161 |  0:00:03s\n",
      "epoch 14 | loss: 6.45274 | val_0_rmse: 3.10801 |  0:00:03s\n",
      "epoch 15 | loss: 4.66701 | val_0_rmse: 2.57343 |  0:00:03s\n",
      "epoch 16 | loss: 4.5358  | val_0_rmse: 2.0767  |  0:00:03s\n",
      "epoch 17 | loss: 3.87225 | val_0_rmse: 1.64026 |  0:00:03s\n",
      "epoch 18 | loss: 2.50996 | val_0_rmse: 1.33174 |  0:00:04s\n",
      "epoch 19 | loss: 2.11318 | val_0_rmse: 1.26112 |  0:00:04s\n",
      "epoch 20 | loss: 2.19094 | val_0_rmse: 1.1398  |  0:00:04s\n",
      "epoch 21 | loss: 1.67729 | val_0_rmse: 0.93338 |  0:00:04s\n",
      "epoch 22 | loss: 1.8604  | val_0_rmse: 0.88815 |  0:00:05s\n",
      "epoch 23 | loss: 1.32541 | val_0_rmse: 0.79377 |  0:00:05s\n",
      "epoch 24 | loss: 1.38027 | val_0_rmse: 0.73073 |  0:00:05s\n",
      "epoch 25 | loss: 1.16832 | val_0_rmse: 0.87023 |  0:00:05s\n",
      "epoch 26 | loss: 1.09888 | val_0_rmse: 0.77309 |  0:00:06s\n",
      "epoch 27 | loss: 0.87881 | val_0_rmse: 0.74984 |  0:00:06s\n",
      "epoch 28 | loss: 0.94662 | val_0_rmse: 0.88369 |  0:00:06s\n",
      "epoch 29 | loss: 0.88538 | val_0_rmse: 0.55976 |  0:00:06s\n",
      "epoch 30 | loss: 0.97129 | val_0_rmse: 0.5363  |  0:00:07s\n",
      "epoch 31 | loss: 0.77356 | val_0_rmse: 0.89387 |  0:00:07s\n",
      "epoch 32 | loss: 0.74987 | val_0_rmse: 0.57013 |  0:00:07s\n",
      "epoch 33 | loss: 0.66779 | val_0_rmse: 0.64942 |  0:00:07s\n",
      "epoch 34 | loss: 0.79669 | val_0_rmse: 0.50494 |  0:00:08s\n",
      "epoch 35 | loss: 0.5868  | val_0_rmse: 0.91714 |  0:00:08s\n",
      "epoch 36 | loss: 0.76608 | val_0_rmse: 0.46893 |  0:00:08s\n",
      "epoch 37 | loss: 0.67794 | val_0_rmse: 0.68752 |  0:00:08s\n",
      "epoch 38 | loss: 0.83295 | val_0_rmse: 0.65035 |  0:00:09s\n",
      "epoch 39 | loss: 0.56136 | val_0_rmse: 0.39781 |  0:00:09s\n",
      "epoch 40 | loss: 0.59312 | val_0_rmse: 0.79929 |  0:00:09s\n",
      "epoch 41 | loss: 0.61884 | val_0_rmse: 0.44615 |  0:00:09s\n",
      "epoch 42 | loss: 0.62153 | val_0_rmse: 0.48165 |  0:00:09s\n",
      "epoch 43 | loss: 0.51131 | val_0_rmse: 0.75529 |  0:00:10s\n",
      "epoch 44 | loss: 0.5479  | val_0_rmse: 0.36833 |  0:00:10s\n",
      "epoch 45 | loss: 0.58736 | val_0_rmse: 0.48654 |  0:00:10s\n",
      "epoch 46 | loss: 0.39818 | val_0_rmse: 0.53964 |  0:00:10s\n",
      "epoch 47 | loss: 0.44644 | val_0_rmse: 0.35471 |  0:00:10s\n",
      "epoch 48 | loss: 0.42707 | val_0_rmse: 0.44355 |  0:00:11s\n",
      "epoch 49 | loss: 0.37528 | val_0_rmse: 0.57306 |  0:00:11s\n",
      "epoch 50 | loss: 0.37858 | val_0_rmse: 0.36611 |  0:00:11s\n",
      "epoch 51 | loss: 0.44481 | val_0_rmse: 0.51978 |  0:00:11s\n",
      "epoch 52 | loss: 0.28278 | val_0_rmse: 0.46138 |  0:00:12s\n",
      "epoch 53 | loss: 0.406   | val_0_rmse: 0.46904 |  0:00:12s\n",
      "epoch 54 | loss: 0.34009 | val_0_rmse: 0.44944 |  0:00:12s\n",
      "epoch 55 | loss: 0.35028 | val_0_rmse: 0.4286  |  0:00:12s\n",
      "epoch 56 | loss: 0.36866 | val_0_rmse: 0.46695 |  0:00:12s\n",
      "epoch 57 | loss: 0.29481 | val_0_rmse: 0.46115 |  0:00:12s\n",
      "epoch 58 | loss: 0.26966 | val_0_rmse: 0.46755 |  0:00:13s\n",
      "epoch 59 | loss: 0.34716 | val_0_rmse: 0.53282 |  0:00:13s\n",
      "epoch 60 | loss: 0.31393 | val_0_rmse: 0.39772 |  0:00:13s\n",
      "epoch 61 | loss: 0.299   | val_0_rmse: 0.51845 |  0:00:13s\n",
      "epoch 62 | loss: 0.40151 | val_0_rmse: 0.41382 |  0:00:14s\n",
      "epoch 63 | loss: 0.27341 | val_0_rmse: 0.4384  |  0:00:14s\n",
      "epoch 64 | loss: 0.28984 | val_0_rmse: 0.50446 |  0:00:14s\n",
      "epoch 65 | loss: 0.28669 | val_0_rmse: 0.48615 |  0:00:14s\n",
      "epoch 66 | loss: 0.30003 | val_0_rmse: 0.40684 |  0:00:15s\n",
      "epoch 67 | loss: 0.22903 | val_0_rmse: 0.48103 |  0:00:15s\n",
      "epoch 68 | loss: 0.24713 | val_0_rmse: 0.36295 |  0:00:15s\n",
      "epoch 69 | loss: 0.27223 | val_0_rmse: 0.58526 |  0:00:15s\n",
      "epoch 70 | loss: 0.31674 | val_0_rmse: 0.32568 |  0:00:16s\n",
      "epoch 71 | loss: 0.31788 | val_0_rmse: 0.4491  |  0:00:16s\n",
      "epoch 72 | loss: 0.29788 | val_0_rmse: 0.41841 |  0:00:16s\n",
      "epoch 73 | loss: 0.18702 | val_0_rmse: 0.38298 |  0:00:16s\n",
      "epoch 74 | loss: 0.17553 | val_0_rmse: 0.40973 |  0:00:17s\n",
      "epoch 75 | loss: 0.24849 | val_0_rmse: 0.43084 |  0:00:17s\n",
      "epoch 76 | loss: 0.27562 | val_0_rmse: 0.38265 |  0:00:17s\n",
      "epoch 77 | loss: 0.20833 | val_0_rmse: 0.4488  |  0:00:17s\n",
      "epoch 78 | loss: 0.18746 | val_0_rmse: 0.35956 |  0:00:17s\n",
      "epoch 79 | loss: 0.18758 | val_0_rmse: 0.44867 |  0:00:18s\n",
      "epoch 80 | loss: 0.24638 | val_0_rmse: 0.37655 |  0:00:18s\n",
      "epoch 81 | loss: 0.20064 | val_0_rmse: 0.54838 |  0:00:18s\n",
      "epoch 82 | loss: 0.28776 | val_0_rmse: 0.35716 |  0:00:18s\n",
      "epoch 83 | loss: 0.21054 | val_0_rmse: 0.47981 |  0:00:18s\n",
      "epoch 84 | loss: 0.18756 | val_0_rmse: 0.35629 |  0:00:19s\n",
      "epoch 85 | loss: 0.19693 | val_0_rmse: 0.41042 |  0:00:19s\n",
      "epoch 86 | loss: 0.17679 | val_0_rmse: 0.33254 |  0:00:19s\n",
      "epoch 87 | loss: 0.19707 | val_0_rmse: 0.44315 |  0:00:19s\n",
      "epoch 88 | loss: 0.15385 | val_0_rmse: 0.3351  |  0:00:19s\n",
      "epoch 89 | loss: 0.23602 | val_0_rmse: 0.42474 |  0:00:20s\n",
      "epoch 90 | loss: 0.17702 | val_0_rmse: 0.32396 |  0:00:20s\n",
      "epoch 91 | loss: 0.13945 | val_0_rmse: 0.38771 |  0:00:20s\n",
      "epoch 92 | loss: 0.11966 | val_0_rmse: 0.35686 |  0:00:20s\n",
      "epoch 93 | loss: 0.13535 | val_0_rmse: 0.3222  |  0:00:20s\n",
      "epoch 94 | loss: 0.16337 | val_0_rmse: 0.40455 |  0:00:21s\n",
      "epoch 95 | loss: 0.16997 | val_0_rmse: 0.37478 |  0:00:21s\n",
      "epoch 96 | loss: 0.21489 | val_0_rmse: 0.37374 |  0:00:21s\n",
      "epoch 97 | loss: 0.20019 | val_0_rmse: 0.29756 |  0:00:21s\n",
      "epoch 98 | loss: 0.17209 | val_0_rmse: 0.44641 |  0:00:21s\n",
      "epoch 99 | loss: 0.20332 | val_0_rmse: 0.332   |  0:00:22s\n",
      "epoch 100| loss: 0.29269 | val_0_rmse: 0.32567 |  0:00:22s\n",
      "epoch 101| loss: 0.17098 | val_0_rmse: 0.42432 |  0:00:22s\n",
      "epoch 102| loss: 0.16352 | val_0_rmse: 0.36047 |  0:00:22s\n",
      "epoch 103| loss: 0.28782 | val_0_rmse: 0.34871 |  0:00:23s\n",
      "epoch 104| loss: 0.21965 | val_0_rmse: 0.54821 |  0:00:23s\n",
      "epoch 105| loss: 0.25408 | val_0_rmse: 0.30658 |  0:00:23s\n",
      "epoch 106| loss: 0.21858 | val_0_rmse: 0.2861  |  0:00:23s\n",
      "epoch 107| loss: 0.16812 | val_0_rmse: 0.44005 |  0:00:23s\n",
      "epoch 108| loss: 0.17326 | val_0_rmse: 0.34148 |  0:00:24s\n",
      "epoch 109| loss: 0.1917  | val_0_rmse: 0.32672 |  0:00:24s\n",
      "epoch 110| loss: 0.09906 | val_0_rmse: 0.33618 |  0:00:24s\n",
      "epoch 111| loss: 0.13113 | val_0_rmse: 0.29934 |  0:00:24s\n",
      "epoch 112| loss: 0.1284  | val_0_rmse: 0.41228 |  0:00:24s\n",
      "epoch 113| loss: 0.2108  | val_0_rmse: 0.33962 |  0:00:25s\n",
      "epoch 114| loss: 0.15473 | val_0_rmse: 0.32072 |  0:00:25s\n",
      "epoch 115| loss: 0.12933 | val_0_rmse: 0.31604 |  0:00:25s\n",
      "epoch 116| loss: 0.12508 | val_0_rmse: 0.29329 |  0:00:25s\n",
      "epoch 117| loss: 0.13295 | val_0_rmse: 0.35383 |  0:00:25s\n",
      "epoch 118| loss: 0.12833 | val_0_rmse: 0.29242 |  0:00:26s\n",
      "epoch 119| loss: 0.13349 | val_0_rmse: 0.32823 |  0:00:26s\n",
      "epoch 120| loss: 0.14561 | val_0_rmse: 0.30211 |  0:00:26s\n",
      "epoch 121| loss: 0.1532  | val_0_rmse: 0.3811  |  0:00:26s\n",
      "epoch 122| loss: 0.1427  | val_0_rmse: 0.29425 |  0:00:26s\n",
      "epoch 123| loss: 0.09428 | val_0_rmse: 0.36849 |  0:00:27s\n",
      "epoch 124| loss: 0.13364 | val_0_rmse: 0.33071 |  0:00:27s\n",
      "epoch 125| loss: 0.13786 | val_0_rmse: 0.27626 |  0:00:27s\n",
      "epoch 126| loss: 0.11901 | val_0_rmse: 0.28518 |  0:00:27s\n",
      "epoch 127| loss: 0.10229 | val_0_rmse: 0.28406 |  0:00:27s\n",
      "epoch 128| loss: 0.09023 | val_0_rmse: 0.29184 |  0:00:28s\n",
      "epoch 129| loss: 0.09242 | val_0_rmse: 0.27847 |  0:00:28s\n",
      "epoch 130| loss: 0.09517 | val_0_rmse: 0.34348 |  0:00:28s\n",
      "epoch 131| loss: 0.09555 | val_0_rmse: 0.32495 |  0:00:28s\n",
      "epoch 132| loss: 0.11113 | val_0_rmse: 0.2927  |  0:00:28s\n",
      "epoch 133| loss: 0.08605 | val_0_rmse: 0.29137 |  0:00:29s\n",
      "epoch 134| loss: 0.09142 | val_0_rmse: 0.31366 |  0:00:29s\n",
      "epoch 135| loss: 0.09862 | val_0_rmse: 0.28227 |  0:00:29s\n",
      "epoch 136| loss: 0.11879 | val_0_rmse: 0.28044 |  0:00:29s\n",
      "epoch 137| loss: 0.08736 | val_0_rmse: 0.29213 |  0:00:30s\n",
      "epoch 138| loss: 0.08281 | val_0_rmse: 0.31429 |  0:00:30s\n",
      "epoch 139| loss: 0.09569 | val_0_rmse: 0.27933 |  0:00:30s\n",
      "epoch 140| loss: 0.08131 | val_0_rmse: 0.34425 |  0:00:30s\n",
      "epoch 141| loss: 0.10966 | val_0_rmse: 0.28731 |  0:00:30s\n",
      "epoch 142| loss: 0.09665 | val_0_rmse: 0.294   |  0:00:31s\n",
      "epoch 143| loss: 0.08311 | val_0_rmse: 0.27628 |  0:00:31s\n",
      "epoch 144| loss: 0.08145 | val_0_rmse: 0.29014 |  0:00:31s\n",
      "epoch 145| loss: 0.08285 | val_0_rmse: 0.28829 |  0:00:31s\n",
      "epoch 146| loss: 0.0806  | val_0_rmse: 0.27867 |  0:00:31s\n",
      "epoch 147| loss: 0.0745  | val_0_rmse: 0.28986 |  0:00:32s\n",
      "epoch 148| loss: 0.07262 | val_0_rmse: 0.29908 |  0:00:32s\n",
      "epoch 149| loss: 0.0806  | val_0_rmse: 0.29169 |  0:00:32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:27:31,529] Trial 48 finished with value: 0.2762573988519106 and parameters: {'n_d': 64, 'n_a': 24, 'n_steps': 4, 'gamma': 1.4301417644024426, 'n_independent': 2, 'n_shared': 2, 'lambda_sparse': 1e-05, 'mask_type': 'sparsemax', 'lr': 0.006205353240268698, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 150| loss: 0.08617 | val_0_rmse: 0.3267  |  0:00:32s\n",
      "\n",
      "Early stopping occurred at epoch 150 with best_epoch = 125 and best_val_0_rmse = 0.27626\n",
      "Trial 048 | rmse_log=0.27626 | RMSE$=62,594 | MAE$=36,045 | MAPE=20.30% | n_d/n_a=64/24 steps=4 lr=0.00621 batch=512 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 128.58192| val_0_rmse: 10.51903|  0:00:00s\n",
      "epoch 1  | loss: 58.08847| val_0_rmse: 9.02139 |  0:00:00s\n",
      "epoch 2  | loss: 35.31827| val_0_rmse: 7.4079  |  0:00:00s\n",
      "epoch 3  | loss: 32.2367 | val_0_rmse: 5.8849  |  0:00:00s\n",
      "epoch 4  | loss: 27.3271 | val_0_rmse: 5.1761  |  0:00:01s\n",
      "epoch 5  | loss: 17.05733| val_0_rmse: 4.99517 |  0:00:01s\n",
      "epoch 6  | loss: 10.74957| val_0_rmse: 4.43395 |  0:00:01s\n",
      "epoch 7  | loss: 7.63219 | val_0_rmse: 3.10435 |  0:00:01s\n",
      "epoch 8  | loss: 4.24224 | val_0_rmse: 1.69761 |  0:00:01s\n",
      "epoch 9  | loss: 2.96836 | val_0_rmse: 1.55521 |  0:00:02s\n",
      "epoch 10 | loss: 1.77889 | val_0_rmse: 1.09397 |  0:00:02s\n",
      "epoch 11 | loss: 1.62606 | val_0_rmse: 1.5253  |  0:00:02s\n",
      "epoch 12 | loss: 1.3297  | val_0_rmse: 0.7133  |  0:00:02s\n",
      "epoch 13 | loss: 1.744   | val_0_rmse: 1.58883 |  0:00:02s\n",
      "epoch 14 | loss: 1.49773 | val_0_rmse: 0.53947 |  0:00:03s\n",
      "epoch 15 | loss: 1.94317 | val_0_rmse: 1.7748  |  0:00:03s\n",
      "epoch 16 | loss: 2.34095 | val_0_rmse: 1.08697 |  0:00:03s\n",
      "epoch 17 | loss: 0.81561 | val_0_rmse: 0.55281 |  0:00:03s\n",
      "epoch 18 | loss: 1.40013 | val_0_rmse: 0.73146 |  0:00:04s\n",
      "epoch 19 | loss: 0.79607 | val_0_rmse: 1.40399 |  0:00:04s\n",
      "epoch 20 | loss: 0.98318 | val_0_rmse: 0.64486 |  0:00:04s\n",
      "epoch 21 | loss: 0.8519  | val_0_rmse: 0.77008 |  0:00:04s\n",
      "epoch 22 | loss: 0.4821  | val_0_rmse: 0.7407  |  0:00:04s\n",
      "epoch 23 | loss: 0.62772 | val_0_rmse: 0.53135 |  0:00:04s\n",
      "epoch 24 | loss: 0.49776 | val_0_rmse: 0.6714  |  0:00:05s\n",
      "epoch 25 | loss: 0.35647 | val_0_rmse: 0.64404 |  0:00:05s\n",
      "epoch 26 | loss: 0.36942 | val_0_rmse: 0.51054 |  0:00:05s\n",
      "epoch 27 | loss: 0.3792  | val_0_rmse: 0.80926 |  0:00:05s\n",
      "epoch 28 | loss: 0.45257 | val_0_rmse: 0.47415 |  0:00:05s\n",
      "epoch 29 | loss: 0.30736 | val_0_rmse: 0.96643 |  0:00:06s\n",
      "epoch 30 | loss: 0.38918 | val_0_rmse: 0.53158 |  0:00:06s\n",
      "epoch 31 | loss: 0.28725 | val_0_rmse: 0.48581 |  0:00:06s\n",
      "epoch 32 | loss: 0.27071 | val_0_rmse: 0.64641 |  0:00:06s\n",
      "epoch 33 | loss: 0.28307 | val_0_rmse: 0.76597 |  0:00:07s\n",
      "epoch 34 | loss: 0.28135 | val_0_rmse: 0.82734 |  0:00:07s\n",
      "epoch 35 | loss: 0.4707  | val_0_rmse: 0.37503 |  0:00:07s\n",
      "epoch 36 | loss: 0.33847 | val_0_rmse: 0.73073 |  0:00:07s\n",
      "epoch 37 | loss: 0.41897 | val_0_rmse: 0.57775 |  0:00:07s\n",
      "epoch 38 | loss: 0.64089 | val_0_rmse: 0.85507 |  0:00:08s\n",
      "epoch 39 | loss: 0.4534  | val_0_rmse: 0.33721 |  0:00:08s\n",
      "epoch 40 | loss: 0.30935 | val_0_rmse: 0.73705 |  0:00:08s\n",
      "epoch 41 | loss: 0.43593 | val_0_rmse: 0.41653 |  0:00:08s\n",
      "epoch 42 | loss: 0.27896 | val_0_rmse: 0.36269 |  0:00:08s\n",
      "epoch 43 | loss: 0.25959 | val_0_rmse: 0.58381 |  0:00:09s\n",
      "epoch 44 | loss: 0.22072 | val_0_rmse: 0.52807 |  0:00:09s\n",
      "epoch 45 | loss: 0.23093 | val_0_rmse: 0.31791 |  0:00:09s\n",
      "epoch 46 | loss: 0.22648 | val_0_rmse: 0.72363 |  0:00:09s\n",
      "epoch 47 | loss: 0.26861 | val_0_rmse: 0.3561  |  0:00:09s\n",
      "epoch 48 | loss: 0.22514 | val_0_rmse: 0.59541 |  0:00:10s\n",
      "epoch 49 | loss: 0.19978 | val_0_rmse: 0.40197 |  0:00:10s\n",
      "epoch 50 | loss: 0.21246 | val_0_rmse: 0.69301 |  0:00:10s\n",
      "epoch 51 | loss: 0.32014 | val_0_rmse: 0.31373 |  0:00:10s\n",
      "epoch 52 | loss: 0.19221 | val_0_rmse: 0.46868 |  0:00:10s\n",
      "epoch 53 | loss: 0.1846  | val_0_rmse: 0.36729 |  0:00:11s\n",
      "epoch 54 | loss: 0.28125 | val_0_rmse: 0.64249 |  0:00:11s\n",
      "epoch 55 | loss: 0.28571 | val_0_rmse: 0.28901 |  0:00:11s\n",
      "epoch 56 | loss: 0.15246 | val_0_rmse: 0.33725 |  0:00:11s\n",
      "epoch 57 | loss: 0.15664 | val_0_rmse: 0.46936 |  0:00:12s\n",
      "epoch 58 | loss: 0.14151 | val_0_rmse: 0.28007 |  0:00:12s\n",
      "epoch 59 | loss: 0.14582 | val_0_rmse: 0.41107 |  0:00:12s\n",
      "epoch 60 | loss: 0.11915 | val_0_rmse: 0.26739 |  0:00:13s\n",
      "epoch 61 | loss: 0.14288 | val_0_rmse: 0.37609 |  0:00:13s\n",
      "epoch 62 | loss: 0.11567 | val_0_rmse: 0.34878 |  0:00:13s\n",
      "epoch 63 | loss: 0.08535 | val_0_rmse: 0.33797 |  0:00:13s\n",
      "epoch 64 | loss: 0.14834 | val_0_rmse: 0.4819  |  0:00:13s\n",
      "epoch 65 | loss: 0.18649 | val_0_rmse: 0.32133 |  0:00:13s\n",
      "epoch 66 | loss: 0.12813 | val_0_rmse: 0.44692 |  0:00:14s\n",
      "epoch 67 | loss: 0.1806  | val_0_rmse: 0.3696  |  0:00:14s\n",
      "epoch 68 | loss: 0.13189 | val_0_rmse: 0.36767 |  0:00:14s\n",
      "epoch 69 | loss: 0.11775 | val_0_rmse: 0.40887 |  0:00:14s\n",
      "epoch 70 | loss: 0.19272 | val_0_rmse: 0.34455 |  0:00:15s\n",
      "epoch 71 | loss: 0.10093 | val_0_rmse: 0.30546 |  0:00:15s\n",
      "epoch 72 | loss: 0.09651 | val_0_rmse: 0.22482 |  0:00:15s\n",
      "epoch 73 | loss: 0.07641 | val_0_rmse: 0.21617 |  0:00:15s\n",
      "epoch 74 | loss: 0.10021 | val_0_rmse: 0.28049 |  0:00:15s\n",
      "epoch 75 | loss: 0.08264 | val_0_rmse: 0.25125 |  0:00:16s\n",
      "epoch 76 | loss: 0.07084 | val_0_rmse: 0.33731 |  0:00:16s\n",
      "epoch 77 | loss: 0.12137 | val_0_rmse: 0.26474 |  0:00:16s\n",
      "epoch 78 | loss: 0.10922 | val_0_rmse: 0.43838 |  0:00:16s\n",
      "epoch 79 | loss: 0.13394 | val_0_rmse: 0.39073 |  0:00:16s\n",
      "epoch 80 | loss: 0.16364 | val_0_rmse: 0.32566 |  0:00:17s\n",
      "epoch 81 | loss: 0.10778 | val_0_rmse: 0.30195 |  0:00:17s\n",
      "epoch 82 | loss: 0.10204 | val_0_rmse: 0.46519 |  0:00:17s\n",
      "epoch 83 | loss: 0.18244 | val_0_rmse: 0.31593 |  0:00:17s\n",
      "epoch 84 | loss: 0.09082 | val_0_rmse: 0.41805 |  0:00:18s\n",
      "epoch 85 | loss: 0.15927 | val_0_rmse: 0.36312 |  0:00:18s\n",
      "epoch 86 | loss: 0.15004 | val_0_rmse: 0.31905 |  0:00:18s\n",
      "epoch 87 | loss: 0.18797 | val_0_rmse: 0.2485  |  0:00:18s\n",
      "epoch 88 | loss: 0.18152 | val_0_rmse: 0.53961 |  0:00:19s\n",
      "epoch 89 | loss: 0.24753 | val_0_rmse: 0.45943 |  0:00:19s\n",
      "epoch 90 | loss: 0.2232  | val_0_rmse: 0.28237 |  0:00:19s\n",
      "epoch 91 | loss: 0.1366  | val_0_rmse: 0.23495 |  0:00:19s\n",
      "epoch 92 | loss: 0.06908 | val_0_rmse: 0.23594 |  0:00:19s\n",
      "epoch 93 | loss: 0.06121 | val_0_rmse: 0.23511 |  0:00:20s\n",
      "epoch 94 | loss: 0.09791 | val_0_rmse: 0.23941 |  0:00:20s\n",
      "epoch 95 | loss: 0.10991 | val_0_rmse: 0.33149 |  0:00:20s\n",
      "epoch 96 | loss: 0.08957 | val_0_rmse: 0.23898 |  0:00:20s\n",
      "epoch 97 | loss: 0.06892 | val_0_rmse: 0.24065 |  0:00:20s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:27:53,063] Trial 49 finished with value: 0.21617235090708367 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 6, 'gamma': 1.3800224684849423, 'n_independent': 1, 'n_shared': 1, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.018393436745230408, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 98 | loss: 0.12609 | val_0_rmse: 0.33535 |  0:00:21s\n",
      "\n",
      "Early stopping occurred at epoch 98 with best_epoch = 73 and best_val_0_rmse = 0.21617\n",
      "Trial 049 | rmse_log=0.21617 | RMSE$=40,240 | MAE$=27,187 | MAPE=16.38% | n_d/n_a=48/24 steps=6 lr=0.01839 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 126.12041| val_0_rmse: 11.20991|  0:00:00s\n",
      "epoch 1  | loss: 93.53067| val_0_rmse: 10.29789|  0:00:00s\n",
      "epoch 2  | loss: 66.72991| val_0_rmse: 9.25494 |  0:00:00s\n",
      "epoch 3  | loss: 43.93418| val_0_rmse: 8.16348 |  0:00:01s\n",
      "epoch 4  | loss: 28.48852| val_0_rmse: 6.98085 |  0:00:01s\n",
      "epoch 5  | loss: 19.25215| val_0_rmse: 5.65989 |  0:00:01s\n",
      "epoch 6  | loss: 14.30064| val_0_rmse: 4.30189 |  0:00:01s\n",
      "epoch 7  | loss: 11.59393| val_0_rmse: 3.38392 |  0:00:02s\n",
      "epoch 8  | loss: 9.80154 | val_0_rmse: 3.12267 |  0:00:02s\n",
      "epoch 9  | loss: 6.45763 | val_0_rmse: 3.21009 |  0:00:02s\n",
      "epoch 10 | loss: 3.95372 | val_0_rmse: 3.14277 |  0:00:02s\n",
      "epoch 11 | loss: 2.88455 | val_0_rmse: 2.4995  |  0:00:03s\n",
      "epoch 12 | loss: 1.88223 | val_0_rmse: 1.87486 |  0:00:03s\n",
      "epoch 13 | loss: 1.48254 | val_0_rmse: 2.10705 |  0:00:03s\n",
      "epoch 14 | loss: 1.22424 | val_0_rmse: 1.82488 |  0:00:03s\n",
      "epoch 15 | loss: 0.95893 | val_0_rmse: 1.1928  |  0:00:03s\n",
      "epoch 16 | loss: 0.83901 | val_0_rmse: 1.61208 |  0:00:04s\n",
      "epoch 17 | loss: 0.87617 | val_0_rmse: 1.27156 |  0:00:04s\n",
      "epoch 18 | loss: 0.58159 | val_0_rmse: 0.95025 |  0:00:04s\n",
      "epoch 19 | loss: 0.52387 | val_0_rmse: 0.87458 |  0:00:04s\n",
      "epoch 20 | loss: 0.3859  | val_0_rmse: 0.78607 |  0:00:05s\n",
      "epoch 21 | loss: 0.36749 | val_0_rmse: 0.65455 |  0:00:05s\n",
      "epoch 22 | loss: 0.33529 | val_0_rmse: 0.71408 |  0:00:05s\n",
      "epoch 23 | loss: 0.2583  | val_0_rmse: 0.52392 |  0:00:06s\n",
      "epoch 24 | loss: 0.22231 | val_0_rmse: 0.90005 |  0:00:06s\n",
      "epoch 25 | loss: 0.36598 | val_0_rmse: 0.57459 |  0:00:07s\n",
      "epoch 26 | loss: 0.29397 | val_0_rmse: 0.6106  |  0:00:07s\n",
      "epoch 27 | loss: 0.25079 | val_0_rmse: 0.45497 |  0:00:07s\n",
      "epoch 28 | loss: 0.2852  | val_0_rmse: 0.57997 |  0:00:07s\n",
      "epoch 29 | loss: 0.21977 | val_0_rmse: 0.49463 |  0:00:07s\n",
      "epoch 30 | loss: 0.17913 | val_0_rmse: 0.49487 |  0:00:08s\n",
      "epoch 31 | loss: 0.14481 | val_0_rmse: 0.4747  |  0:00:08s\n",
      "epoch 32 | loss: 0.14169 | val_0_rmse: 0.42356 |  0:00:08s\n",
      "epoch 33 | loss: 0.15848 | val_0_rmse: 0.38241 |  0:00:08s\n",
      "epoch 34 | loss: 0.13939 | val_0_rmse: 0.39998 |  0:00:09s\n",
      "epoch 35 | loss: 0.11825 | val_0_rmse: 0.3714  |  0:00:09s\n",
      "epoch 36 | loss: 0.12597 | val_0_rmse: 0.39156 |  0:00:09s\n",
      "epoch 37 | loss: 0.18946 | val_0_rmse: 0.37212 |  0:00:09s\n",
      "epoch 38 | loss: 0.13607 | val_0_rmse: 0.36607 |  0:00:10s\n",
      "epoch 39 | loss: 0.11266 | val_0_rmse: 0.3082  |  0:00:10s\n",
      "epoch 40 | loss: 0.11168 | val_0_rmse: 0.29574 |  0:00:10s\n",
      "epoch 41 | loss: 0.10841 | val_0_rmse: 0.34959 |  0:00:10s\n",
      "epoch 42 | loss: 0.09595 | val_0_rmse: 0.37141 |  0:00:11s\n",
      "epoch 43 | loss: 0.0991  | val_0_rmse: 0.26288 |  0:00:11s\n",
      "epoch 44 | loss: 0.09362 | val_0_rmse: 0.32823 |  0:00:11s\n",
      "epoch 45 | loss: 0.0838  | val_0_rmse: 0.29325 |  0:00:12s\n",
      "epoch 46 | loss: 0.07205 | val_0_rmse: 0.29105 |  0:00:12s\n",
      "epoch 47 | loss: 0.08602 | val_0_rmse: 0.27443 |  0:00:12s\n",
      "epoch 48 | loss: 0.08007 | val_0_rmse: 0.33115 |  0:00:12s\n",
      "epoch 49 | loss: 0.07823 | val_0_rmse: 0.24522 |  0:00:13s\n",
      "epoch 50 | loss: 0.09028 | val_0_rmse: 0.28989 |  0:00:13s\n",
      "epoch 51 | loss: 0.08598 | val_0_rmse: 0.23843 |  0:00:13s\n",
      "epoch 52 | loss: 0.10244 | val_0_rmse: 0.2752  |  0:00:13s\n",
      "epoch 53 | loss: 0.0804  | val_0_rmse: 0.25011 |  0:00:13s\n",
      "epoch 54 | loss: 0.09547 | val_0_rmse: 0.25063 |  0:00:14s\n",
      "epoch 55 | loss: 0.07073 | val_0_rmse: 0.24099 |  0:00:14s\n",
      "epoch 56 | loss: 0.07961 | val_0_rmse: 0.26568 |  0:00:14s\n",
      "epoch 57 | loss: 0.0747  | val_0_rmse: 0.22812 |  0:00:14s\n",
      "epoch 58 | loss: 0.07355 | val_0_rmse: 0.2358  |  0:00:15s\n",
      "epoch 59 | loss: 0.06153 | val_0_rmse: 0.23332 |  0:00:15s\n",
      "epoch 60 | loss: 0.06369 | val_0_rmse: 0.23678 |  0:00:15s\n",
      "epoch 61 | loss: 0.06578 | val_0_rmse: 0.22721 |  0:00:15s\n",
      "epoch 62 | loss: 0.07095 | val_0_rmse: 0.23502 |  0:00:16s\n",
      "epoch 63 | loss: 0.06964 | val_0_rmse: 0.21825 |  0:00:16s\n",
      "epoch 64 | loss: 0.05525 | val_0_rmse: 0.21651 |  0:00:16s\n",
      "epoch 65 | loss: 0.05166 | val_0_rmse: 0.22562 |  0:00:16s\n",
      "epoch 66 | loss: 0.05606 | val_0_rmse: 0.21812 |  0:00:17s\n",
      "epoch 67 | loss: 0.05504 | val_0_rmse: 0.26814 |  0:00:17s\n",
      "epoch 68 | loss: 0.06715 | val_0_rmse: 0.22052 |  0:00:17s\n",
      "epoch 69 | loss: 0.06024 | val_0_rmse: 0.24019 |  0:00:17s\n",
      "epoch 70 | loss: 0.05536 | val_0_rmse: 0.21543 |  0:00:18s\n",
      "epoch 71 | loss: 0.05528 | val_0_rmse: 0.21387 |  0:00:18s\n",
      "epoch 72 | loss: 0.04905 | val_0_rmse: 0.23634 |  0:00:18s\n",
      "epoch 73 | loss: 0.05068 | val_0_rmse: 0.20819 |  0:00:19s\n",
      "epoch 74 | loss: 0.05099 | val_0_rmse: 0.20339 |  0:00:19s\n",
      "epoch 75 | loss: 0.0447  | val_0_rmse: 0.25436 |  0:00:19s\n",
      "epoch 76 | loss: 0.05387 | val_0_rmse: 0.20304 |  0:00:20s\n",
      "epoch 77 | loss: 0.04943 | val_0_rmse: 0.20283 |  0:00:20s\n",
      "epoch 78 | loss: 0.03651 | val_0_rmse: 0.21749 |  0:00:20s\n",
      "epoch 79 | loss: 0.04325 | val_0_rmse: 0.20577 |  0:00:20s\n",
      "epoch 80 | loss: 0.04143 | val_0_rmse: 0.22214 |  0:00:21s\n",
      "epoch 81 | loss: 0.04588 | val_0_rmse: 0.21753 |  0:00:21s\n",
      "epoch 82 | loss: 0.04333 | val_0_rmse: 0.21716 |  0:00:21s\n",
      "epoch 83 | loss: 0.0481  | val_0_rmse: 0.20782 |  0:00:21s\n",
      "epoch 84 | loss: 0.04059 | val_0_rmse: 0.22157 |  0:00:21s\n",
      "epoch 85 | loss: 0.04437 | val_0_rmse: 0.21309 |  0:00:22s\n",
      "epoch 86 | loss: 0.04423 | val_0_rmse: 0.20432 |  0:00:22s\n",
      "epoch 87 | loss: 0.04251 | val_0_rmse: 0.21216 |  0:00:22s\n",
      "epoch 88 | loss: 0.041   | val_0_rmse: 0.20225 |  0:00:22s\n",
      "epoch 89 | loss: 0.03669 | val_0_rmse: 0.20244 |  0:00:23s\n",
      "epoch 90 | loss: 0.03259 | val_0_rmse: 0.21026 |  0:00:23s\n",
      "epoch 91 | loss: 0.03709 | val_0_rmse: 0.20409 |  0:00:24s\n",
      "epoch 92 | loss: 0.036   | val_0_rmse: 0.21989 |  0:00:24s\n",
      "epoch 93 | loss: 0.03717 | val_0_rmse: 0.21667 |  0:00:24s\n",
      "epoch 94 | loss: 0.0374  | val_0_rmse: 0.19839 |  0:00:24s\n",
      "epoch 95 | loss: 0.03546 | val_0_rmse: 0.21593 |  0:00:25s\n",
      "epoch 96 | loss: 0.0369  | val_0_rmse: 0.21254 |  0:00:25s\n",
      "epoch 97 | loss: 0.04243 | val_0_rmse: 0.23177 |  0:00:25s\n",
      "epoch 98 | loss: 0.03898 | val_0_rmse: 0.20575 |  0:00:25s\n",
      "epoch 99 | loss: 0.03699 | val_0_rmse: 0.23751 |  0:00:26s\n",
      "epoch 100| loss: 0.04339 | val_0_rmse: 0.19353 |  0:00:26s\n",
      "epoch 101| loss: 0.03158 | val_0_rmse: 0.20333 |  0:00:26s\n",
      "epoch 102| loss: 0.03132 | val_0_rmse: 0.20588 |  0:00:26s\n",
      "epoch 103| loss: 0.0319  | val_0_rmse: 0.1984  |  0:00:27s\n",
      "epoch 104| loss: 0.03523 | val_0_rmse: 0.20192 |  0:00:27s\n",
      "epoch 105| loss: 0.03621 | val_0_rmse: 0.20399 |  0:00:27s\n",
      "epoch 106| loss: 0.03419 | val_0_rmse: 0.1979  |  0:00:27s\n",
      "epoch 107| loss: 0.0321  | val_0_rmse: 0.20765 |  0:00:28s\n",
      "epoch 108| loss: 0.03198 | val_0_rmse: 0.20587 |  0:00:28s\n",
      "epoch 109| loss: 0.03203 | val_0_rmse: 0.20376 |  0:00:28s\n",
      "epoch 110| loss: 0.03175 | val_0_rmse: 0.22263 |  0:00:28s\n",
      "epoch 111| loss: 0.03754 | val_0_rmse: 0.20356 |  0:00:29s\n",
      "epoch 112| loss: 0.03858 | val_0_rmse: 0.19997 |  0:00:29s\n",
      "epoch 113| loss: 0.02783 | val_0_rmse: 0.19861 |  0:00:29s\n",
      "epoch 114| loss: 0.03224 | val_0_rmse: 0.21122 |  0:00:29s\n",
      "epoch 115| loss: 0.02992 | val_0_rmse: 0.20613 |  0:00:30s\n",
      "epoch 116| loss: 0.03552 | val_0_rmse: 0.20708 |  0:00:30s\n",
      "epoch 117| loss: 0.03654 | val_0_rmse: 0.20153 |  0:00:30s\n",
      "epoch 118| loss: 0.03179 | val_0_rmse: 0.20253 |  0:00:30s\n",
      "epoch 119| loss: 0.03724 | val_0_rmse: 0.19473 |  0:00:30s\n",
      "epoch 120| loss: 0.0331  | val_0_rmse: 0.1955  |  0:00:31s\n",
      "epoch 121| loss: 0.03199 | val_0_rmse: 0.22429 |  0:00:31s\n",
      "epoch 122| loss: 0.03678 | val_0_rmse: 0.19766 |  0:00:31s\n",
      "epoch 123| loss: 0.0283  | val_0_rmse: 0.20788 |  0:00:31s\n",
      "epoch 124| loss: 0.03167 | val_0_rmse: 0.21869 |  0:00:32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:28:25,943] Trial 50 finished with value: 0.19352736116669098 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2354455221431144, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.01432665197989217, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 125| loss: 0.03317 | val_0_rmse: 0.19862 |  0:00:32s\n",
      "\n",
      "Early stopping occurred at epoch 125 with best_epoch = 100 and best_val_0_rmse = 0.19353\n",
      "Trial 050 | rmse_log=0.19353 | RMSE$=34,901 | MAE$=24,055 | MAPE=14.81% | n_d/n_a=24/16 steps=3 lr=0.01433 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.64271| val_0_rmse: 11.05787|  0:00:00s\n",
      "epoch 1  | loss: 87.40164| val_0_rmse: 9.90031 |  0:00:00s\n",
      "epoch 2  | loss: 60.34417| val_0_rmse: 8.7787  |  0:00:00s\n",
      "epoch 3  | loss: 39.38041| val_0_rmse: 7.56965 |  0:00:00s\n",
      "epoch 4  | loss: 27.48695| val_0_rmse: 6.24189 |  0:00:01s\n",
      "epoch 5  | loss: 18.11583| val_0_rmse: 5.10766 |  0:00:01s\n",
      "epoch 6  | loss: 15.21665| val_0_rmse: 4.35795 |  0:00:01s\n",
      "epoch 7  | loss: 11.56482| val_0_rmse: 4.00725 |  0:00:01s\n",
      "epoch 8  | loss: 6.21627 | val_0_rmse: 3.96741 |  0:00:02s\n",
      "epoch 9  | loss: 4.06318 | val_0_rmse: 3.67583 |  0:00:02s\n",
      "epoch 10 | loss: 3.50253 | val_0_rmse: 2.78205 |  0:00:02s\n",
      "epoch 11 | loss: 2.26129 | val_0_rmse: 1.79127 |  0:00:02s\n",
      "epoch 12 | loss: 1.80209 | val_0_rmse: 1.91357 |  0:00:03s\n",
      "epoch 13 | loss: 1.31736 | val_0_rmse: 1.80596 |  0:00:03s\n",
      "epoch 14 | loss: 0.94035 | val_0_rmse: 1.31085 |  0:00:03s\n",
      "epoch 15 | loss: 0.80917 | val_0_rmse: 1.51214 |  0:00:04s\n",
      "epoch 16 | loss: 0.63242 | val_0_rmse: 1.1116  |  0:00:04s\n",
      "epoch 17 | loss: 0.51587 | val_0_rmse: 1.21941 |  0:00:04s\n",
      "epoch 18 | loss: 0.41291 | val_0_rmse: 0.86385 |  0:00:04s\n",
      "epoch 19 | loss: 0.2981  | val_0_rmse: 0.74061 |  0:00:05s\n",
      "epoch 20 | loss: 0.26351 | val_0_rmse: 0.79492 |  0:00:05s\n",
      "epoch 21 | loss: 0.29988 | val_0_rmse: 0.71157 |  0:00:05s\n",
      "epoch 22 | loss: 0.26365 | val_0_rmse: 0.52443 |  0:00:05s\n",
      "epoch 23 | loss: 0.27311 | val_0_rmse: 0.79551 |  0:00:06s\n",
      "epoch 24 | loss: 0.26908 | val_0_rmse: 0.51402 |  0:00:06s\n",
      "epoch 25 | loss: 0.17541 | val_0_rmse: 0.62718 |  0:00:06s\n",
      "epoch 26 | loss: 0.20532 | val_0_rmse: 0.52912 |  0:00:06s\n",
      "epoch 27 | loss: 0.15979 | val_0_rmse: 0.39517 |  0:00:06s\n",
      "epoch 28 | loss: 0.1617  | val_0_rmse: 0.46083 |  0:00:07s\n",
      "epoch 29 | loss: 0.15391 | val_0_rmse: 0.3401  |  0:00:07s\n",
      "epoch 30 | loss: 0.13054 | val_0_rmse: 0.41955 |  0:00:07s\n",
      "epoch 31 | loss: 0.17244 | val_0_rmse: 0.36394 |  0:00:07s\n",
      "epoch 32 | loss: 0.13236 | val_0_rmse: 0.34085 |  0:00:08s\n",
      "epoch 33 | loss: 0.13136 | val_0_rmse: 0.41225 |  0:00:08s\n",
      "epoch 34 | loss: 0.10341 | val_0_rmse: 0.39915 |  0:00:08s\n",
      "epoch 35 | loss: 0.10348 | val_0_rmse: 0.3202  |  0:00:08s\n",
      "epoch 36 | loss: 0.10666 | val_0_rmse: 0.3494  |  0:00:09s\n",
      "epoch 37 | loss: 0.12461 | val_0_rmse: 0.27919 |  0:00:09s\n",
      "epoch 38 | loss: 0.0988  | val_0_rmse: 0.3365  |  0:00:09s\n",
      "epoch 39 | loss: 0.09874 | val_0_rmse: 0.30318 |  0:00:09s\n",
      "epoch 40 | loss: 0.09177 | val_0_rmse: 0.28239 |  0:00:10s\n",
      "epoch 41 | loss: 0.0923  | val_0_rmse: 0.26103 |  0:00:10s\n",
      "epoch 42 | loss: 0.0803  | val_0_rmse: 0.33769 |  0:00:10s\n",
      "epoch 43 | loss: 0.08509 | val_0_rmse: 0.30727 |  0:00:10s\n",
      "epoch 44 | loss: 0.07787 | val_0_rmse: 0.27312 |  0:00:11s\n",
      "epoch 45 | loss: 0.06405 | val_0_rmse: 0.2664  |  0:00:11s\n",
      "epoch 46 | loss: 0.07752 | val_0_rmse: 0.26662 |  0:00:11s\n",
      "epoch 47 | loss: 0.0693  | val_0_rmse: 0.27111 |  0:00:11s\n",
      "epoch 48 | loss: 0.06241 | val_0_rmse: 0.25939 |  0:00:11s\n",
      "epoch 49 | loss: 0.06382 | val_0_rmse: 0.24805 |  0:00:12s\n",
      "epoch 50 | loss: 0.06545 | val_0_rmse: 0.24764 |  0:00:12s\n",
      "epoch 51 | loss: 0.05918 | val_0_rmse: 0.27152 |  0:00:12s\n",
      "epoch 52 | loss: 0.06149 | val_0_rmse: 0.24758 |  0:00:13s\n",
      "epoch 53 | loss: 0.0862  | val_0_rmse: 0.31875 |  0:00:13s\n",
      "epoch 54 | loss: 0.08199 | val_0_rmse: 0.29267 |  0:00:13s\n",
      "epoch 55 | loss: 0.0814  | val_0_rmse: 0.32087 |  0:00:13s\n",
      "epoch 56 | loss: 0.10369 | val_0_rmse: 0.29125 |  0:00:13s\n",
      "epoch 57 | loss: 0.12939 | val_0_rmse: 0.25734 |  0:00:14s\n",
      "epoch 58 | loss: 0.1022  | val_0_rmse: 0.36969 |  0:00:14s\n",
      "epoch 59 | loss: 0.14391 | val_0_rmse: 0.27698 |  0:00:14s\n",
      "epoch 60 | loss: 0.11051 | val_0_rmse: 0.26683 |  0:00:14s\n",
      "epoch 61 | loss: 0.11401 | val_0_rmse: 0.24589 |  0:00:15s\n",
      "epoch 62 | loss: 0.06621 | val_0_rmse: 0.23773 |  0:00:15s\n",
      "epoch 63 | loss: 0.08708 | val_0_rmse: 0.27574 |  0:00:15s\n",
      "epoch 64 | loss: 0.06767 | val_0_rmse: 0.24102 |  0:00:15s\n",
      "epoch 65 | loss: 0.06828 | val_0_rmse: 0.2537  |  0:00:16s\n",
      "epoch 66 | loss: 0.0738  | val_0_rmse: 0.22962 |  0:00:16s\n",
      "epoch 67 | loss: 0.0622  | val_0_rmse: 0.24218 |  0:00:16s\n",
      "epoch 68 | loss: 0.05934 | val_0_rmse: 0.23538 |  0:00:16s\n",
      "epoch 69 | loss: 0.04618 | val_0_rmse: 0.22767 |  0:00:17s\n",
      "epoch 70 | loss: 0.05703 | val_0_rmse: 0.22418 |  0:00:17s\n",
      "epoch 71 | loss: 0.04635 | val_0_rmse: 0.24289 |  0:00:17s\n",
      "epoch 72 | loss: 0.04858 | val_0_rmse: 0.22665 |  0:00:17s\n",
      "epoch 73 | loss: 0.05248 | val_0_rmse: 0.21663 |  0:00:18s\n",
      "epoch 74 | loss: 0.04982 | val_0_rmse: 0.23833 |  0:00:18s\n",
      "epoch 75 | loss: 0.05249 | val_0_rmse: 0.24013 |  0:00:18s\n",
      "epoch 76 | loss: 0.0502  | val_0_rmse: 0.24263 |  0:00:18s\n",
      "epoch 77 | loss: 0.07173 | val_0_rmse: 0.22947 |  0:00:19s\n",
      "epoch 78 | loss: 0.06806 | val_0_rmse: 0.23685 |  0:00:19s\n",
      "epoch 79 | loss: 0.05359 | val_0_rmse: 0.22619 |  0:00:19s\n",
      "epoch 80 | loss: 0.04903 | val_0_rmse: 0.23354 |  0:00:19s\n",
      "epoch 81 | loss: 0.0508  | val_0_rmse: 0.2101  |  0:00:19s\n",
      "epoch 82 | loss: 0.04568 | val_0_rmse: 0.22611 |  0:00:20s\n",
      "epoch 83 | loss: 0.04756 | val_0_rmse: 0.23503 |  0:00:20s\n",
      "epoch 84 | loss: 0.05065 | val_0_rmse: 0.24352 |  0:00:20s\n",
      "epoch 85 | loss: 0.04773 | val_0_rmse: 0.21669 |  0:00:21s\n",
      "epoch 86 | loss: 0.03791 | val_0_rmse: 0.22202 |  0:00:21s\n",
      "epoch 87 | loss: 0.04536 | val_0_rmse: 0.22673 |  0:00:21s\n",
      "epoch 88 | loss: 0.04139 | val_0_rmse: 0.22401 |  0:00:21s\n",
      "epoch 89 | loss: 0.0432  | val_0_rmse: 0.23636 |  0:00:21s\n",
      "epoch 90 | loss: 0.04736 | val_0_rmse: 0.2253  |  0:00:22s\n",
      "epoch 91 | loss: 0.04601 | val_0_rmse: 0.23369 |  0:00:22s\n",
      "epoch 92 | loss: 0.0428  | val_0_rmse: 0.21522 |  0:00:22s\n",
      "epoch 93 | loss: 0.03972 | val_0_rmse: 0.24293 |  0:00:22s\n",
      "epoch 94 | loss: 0.05083 | val_0_rmse: 0.22196 |  0:00:23s\n",
      "epoch 95 | loss: 0.03796 | val_0_rmse: 0.2132  |  0:00:23s\n",
      "epoch 96 | loss: 0.04043 | val_0_rmse: 0.22106 |  0:00:23s\n",
      "epoch 97 | loss: 0.04178 | val_0_rmse: 0.21068 |  0:00:23s\n",
      "epoch 98 | loss: 0.04217 | val_0_rmse: 0.21202 |  0:00:24s\n",
      "epoch 99 | loss: 0.03926 | val_0_rmse: 0.22311 |  0:00:24s\n",
      "epoch 100| loss: 0.04401 | val_0_rmse: 0.21754 |  0:00:24s\n",
      "epoch 101| loss: 0.03991 | val_0_rmse: 0.20552 |  0:00:24s\n",
      "epoch 102| loss: 0.04277 | val_0_rmse: 0.21387 |  0:00:25s\n",
      "epoch 103| loss: 0.03795 | val_0_rmse: 0.21121 |  0:00:25s\n",
      "epoch 104| loss: 0.03372 | val_0_rmse: 0.22489 |  0:00:25s\n",
      "epoch 105| loss: 0.03086 | val_0_rmse: 0.2054  |  0:00:25s\n",
      "epoch 106| loss: 0.03656 | val_0_rmse: 0.22866 |  0:00:25s\n",
      "epoch 107| loss: 0.04464 | val_0_rmse: 0.21451 |  0:00:26s\n",
      "epoch 108| loss: 0.03618 | val_0_rmse: 0.21304 |  0:00:26s\n",
      "epoch 109| loss: 0.03675 | val_0_rmse: 0.23725 |  0:00:26s\n",
      "epoch 110| loss: 0.04555 | val_0_rmse: 0.24116 |  0:00:26s\n",
      "epoch 111| loss: 0.06245 | val_0_rmse: 0.23685 |  0:00:27s\n",
      "epoch 112| loss: 0.05    | val_0_rmse: 0.20635 |  0:00:27s\n",
      "epoch 113| loss: 0.05678 | val_0_rmse: 0.20795 |  0:00:27s\n",
      "epoch 114| loss: 0.0344  | val_0_rmse: 0.21742 |  0:00:27s\n",
      "epoch 115| loss: 0.03596 | val_0_rmse: 0.19917 |  0:00:28s\n",
      "epoch 116| loss: 0.02867 | val_0_rmse: 0.20197 |  0:00:28s\n",
      "epoch 117| loss: 0.03521 | val_0_rmse: 0.20285 |  0:00:28s\n",
      "epoch 118| loss: 0.0374  | val_0_rmse: 0.20785 |  0:00:28s\n",
      "epoch 119| loss: 0.03379 | val_0_rmse: 0.19626 |  0:00:29s\n",
      "epoch 120| loss: 0.03169 | val_0_rmse: 0.20236 |  0:00:29s\n",
      "epoch 121| loss: 0.03457 | val_0_rmse: 0.19657 |  0:00:29s\n",
      "epoch 122| loss: 0.03112 | val_0_rmse: 0.19962 |  0:00:29s\n",
      "epoch 123| loss: 0.03389 | val_0_rmse: 0.22798 |  0:00:30s\n",
      "epoch 124| loss: 0.03949 | val_0_rmse: 0.20256 |  0:00:30s\n",
      "epoch 125| loss: 0.03208 | val_0_rmse: 0.21269 |  0:00:30s\n",
      "epoch 126| loss: 0.03199 | val_0_rmse: 0.20231 |  0:00:30s\n",
      "epoch 127| loss: 0.03028 | val_0_rmse: 0.20437 |  0:00:30s\n",
      "epoch 128| loss: 0.03418 | val_0_rmse: 0.20195 |  0:00:31s\n",
      "epoch 129| loss: 0.02855 | val_0_rmse: 0.21126 |  0:00:31s\n",
      "epoch 130| loss: 0.03199 | val_0_rmse: 0.23741 |  0:00:31s\n",
      "epoch 131| loss: 0.04182 | val_0_rmse: 0.22117 |  0:00:31s\n",
      "epoch 132| loss: 0.03754 | val_0_rmse: 0.202   |  0:00:32s\n",
      "epoch 133| loss: 0.03257 | val_0_rmse: 0.20838 |  0:00:32s\n",
      "epoch 134| loss: 0.03104 | val_0_rmse: 0.24019 |  0:00:32s\n",
      "epoch 135| loss: 0.04515 | val_0_rmse: 0.19512 |  0:00:32s\n",
      "epoch 136| loss: 0.03575 | val_0_rmse: 0.20009 |  0:00:33s\n",
      "epoch 137| loss: 0.02707 | val_0_rmse: 0.20054 |  0:00:33s\n",
      "epoch 138| loss: 0.02464 | val_0_rmse: 0.20188 |  0:00:33s\n",
      "epoch 139| loss: 0.02651 | val_0_rmse: 0.20401 |  0:00:33s\n",
      "epoch 140| loss: 0.02622 | val_0_rmse: 0.2022  |  0:00:34s\n",
      "epoch 141| loss: 0.02372 | val_0_rmse: 0.19431 |  0:00:34s\n",
      "epoch 142| loss: 0.02485 | val_0_rmse: 0.19385 |  0:00:34s\n",
      "epoch 143| loss: 0.02293 | val_0_rmse: 0.20046 |  0:00:34s\n",
      "epoch 144| loss: 0.03196 | val_0_rmse: 0.19122 |  0:00:35s\n",
      "epoch 145| loss: 0.02378 | val_0_rmse: 0.19131 |  0:00:35s\n",
      "epoch 146| loss: 0.02224 | val_0_rmse: 0.19782 |  0:00:35s\n",
      "epoch 147| loss: 0.028   | val_0_rmse: 0.1927  |  0:00:35s\n",
      "epoch 148| loss: 0.02906 | val_0_rmse: 0.20026 |  0:00:35s\n",
      "epoch 149| loss: 0.03253 | val_0_rmse: 0.19431 |  0:00:36s\n",
      "epoch 150| loss: 0.02907 | val_0_rmse: 0.18957 |  0:00:36s\n",
      "epoch 151| loss: 0.02582 | val_0_rmse: 0.19455 |  0:00:36s\n",
      "epoch 152| loss: 0.03537 | val_0_rmse: 0.18625 |  0:00:36s\n",
      "epoch 153| loss: 0.02647 | val_0_rmse: 0.19643 |  0:00:37s\n",
      "epoch 154| loss: 0.02496 | val_0_rmse: 0.21132 |  0:00:37s\n",
      "epoch 155| loss: 0.03035 | val_0_rmse: 0.18743 |  0:00:37s\n",
      "epoch 156| loss: 0.02693 | val_0_rmse: 0.19688 |  0:00:37s\n",
      "epoch 157| loss: 0.02522 | val_0_rmse: 0.1875  |  0:00:38s\n",
      "epoch 158| loss: 0.02185 | val_0_rmse: 0.18762 |  0:00:38s\n",
      "epoch 159| loss: 0.02038 | val_0_rmse: 0.213   |  0:00:38s\n",
      "epoch 160| loss: 0.03224 | val_0_rmse: 0.20874 |  0:00:38s\n",
      "epoch 161| loss: 0.03747 | val_0_rmse: 0.22109 |  0:00:39s\n",
      "epoch 162| loss: 0.02966 | val_0_rmse: 0.21978 |  0:00:39s\n",
      "epoch 163| loss: 0.0403  | val_0_rmse: 0.24902 |  0:00:39s\n",
      "epoch 164| loss: 0.05051 | val_0_rmse: 0.20743 |  0:00:39s\n",
      "epoch 165| loss: 0.03334 | val_0_rmse: 0.20267 |  0:00:40s\n",
      "epoch 166| loss: 0.02717 | val_0_rmse: 0.19576 |  0:00:40s\n",
      "epoch 167| loss: 0.02361 | val_0_rmse: 0.19973 |  0:00:40s\n",
      "epoch 168| loss: 0.02182 | val_0_rmse: 0.20192 |  0:00:40s\n",
      "epoch 169| loss: 0.03251 | val_0_rmse: 0.19263 |  0:00:40s\n",
      "epoch 170| loss: 0.0249  | val_0_rmse: 0.18941 |  0:00:41s\n",
      "epoch 171| loss: 0.02123 | val_0_rmse: 0.19639 |  0:00:41s\n",
      "epoch 172| loss: 0.03231 | val_0_rmse: 0.21699 |  0:00:41s\n",
      "epoch 173| loss: 0.03389 | val_0_rmse: 0.22318 |  0:00:41s\n",
      "epoch 174| loss: 0.04403 | val_0_rmse: 0.26067 |  0:00:42s\n",
      "epoch 175| loss: 0.05222 | val_0_rmse: 0.21207 |  0:00:42s\n",
      "epoch 176| loss: 0.03347 | val_0_rmse: 0.20456 |  0:00:42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:29:09,273] Trial 51 finished with value: 0.18624742170979972 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2326232224289788, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.01696814645956467, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 177| loss: 0.02302 | val_0_rmse: 0.18652 |  0:00:42s\n",
      "\n",
      "Early stopping occurred at epoch 177 with best_epoch = 152 and best_val_0_rmse = 0.18625\n",
      "Trial 051 | rmse_log=0.18625 | RMSE$=34,332 | MAE$=22,953 | MAPE=13.91% | n_d/n_a=24/16 steps=3 lr=0.01697 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.79811| val_0_rmse: 10.94675|  0:00:00s\n",
      "epoch 1  | loss: 86.7898 | val_0_rmse: 9.85745 |  0:00:00s\n",
      "epoch 2  | loss: 58.03582| val_0_rmse: 8.57961 |  0:00:00s\n",
      "epoch 3  | loss: 38.1684 | val_0_rmse: 7.21801 |  0:00:00s\n",
      "epoch 4  | loss: 24.23116| val_0_rmse: 5.98206 |  0:00:01s\n",
      "epoch 5  | loss: 14.36469| val_0_rmse: 4.86965 |  0:00:01s\n",
      "epoch 6  | loss: 14.13134| val_0_rmse: 4.11576 |  0:00:01s\n",
      "epoch 7  | loss: 11.68701| val_0_rmse: 3.81115 |  0:00:01s\n",
      "epoch 8  | loss: 5.75957 | val_0_rmse: 3.80792 |  0:00:02s\n",
      "epoch 9  | loss: 4.79139 | val_0_rmse: 3.73822 |  0:00:02s\n",
      "epoch 10 | loss: 2.95034 | val_0_rmse: 2.7466  |  0:00:02s\n",
      "epoch 11 | loss: 2.08614 | val_0_rmse: 2.08205 |  0:00:03s\n",
      "epoch 12 | loss: 1.2229  | val_0_rmse: 2.17542 |  0:00:03s\n",
      "epoch 13 | loss: 0.85455 | val_0_rmse: 1.86098 |  0:00:03s\n",
      "epoch 14 | loss: 0.67062 | val_0_rmse: 1.37871 |  0:00:03s\n",
      "epoch 15 | loss: 0.68127 | val_0_rmse: 1.12003 |  0:00:04s\n",
      "epoch 16 | loss: 0.45893 | val_0_rmse: 1.06372 |  0:00:04s\n",
      "epoch 17 | loss: 0.36468 | val_0_rmse: 0.89861 |  0:00:04s\n",
      "epoch 18 | loss: 0.39259 | val_0_rmse: 0.82588 |  0:00:04s\n",
      "epoch 19 | loss: 0.29743 | val_0_rmse: 0.61504 |  0:00:04s\n",
      "epoch 20 | loss: 0.33357 | val_0_rmse: 0.65348 |  0:00:05s\n",
      "epoch 21 | loss: 0.2275  | val_0_rmse: 0.60027 |  0:00:05s\n",
      "epoch 22 | loss: 0.20372 | val_0_rmse: 0.53575 |  0:00:05s\n",
      "epoch 23 | loss: 0.21451 | val_0_rmse: 0.45019 |  0:00:05s\n",
      "epoch 24 | loss: 0.16772 | val_0_rmse: 0.53599 |  0:00:06s\n",
      "epoch 25 | loss: 0.23067 | val_0_rmse: 0.31875 |  0:00:06s\n",
      "epoch 26 | loss: 0.16182 | val_0_rmse: 0.45303 |  0:00:06s\n",
      "epoch 27 | loss: 0.17823 | val_0_rmse: 0.32681 |  0:00:07s\n",
      "epoch 28 | loss: 0.12206 | val_0_rmse: 0.37661 |  0:00:07s\n",
      "epoch 29 | loss: 0.13822 | val_0_rmse: 0.28159 |  0:00:07s\n",
      "epoch 30 | loss: 0.14977 | val_0_rmse: 0.38185 |  0:00:07s\n",
      "epoch 31 | loss: 0.12421 | val_0_rmse: 0.33476 |  0:00:08s\n",
      "epoch 32 | loss: 0.10637 | val_0_rmse: 0.29086 |  0:00:08s\n",
      "epoch 33 | loss: 0.11771 | val_0_rmse: 0.30614 |  0:00:08s\n",
      "epoch 34 | loss: 0.10262 | val_0_rmse: 0.31792 |  0:00:08s\n",
      "epoch 35 | loss: 0.08395 | val_0_rmse: 0.32729 |  0:00:08s\n",
      "epoch 36 | loss: 0.1201  | val_0_rmse: 0.29097 |  0:00:09s\n",
      "epoch 37 | loss: 0.10242 | val_0_rmse: 0.32156 |  0:00:09s\n",
      "epoch 38 | loss: 0.10145 | val_0_rmse: 0.28833 |  0:00:09s\n",
      "epoch 39 | loss: 0.09442 | val_0_rmse: 0.28133 |  0:00:09s\n",
      "epoch 40 | loss: 0.084   | val_0_rmse: 0.26894 |  0:00:10s\n",
      "epoch 41 | loss: 0.07791 | val_0_rmse: 0.27587 |  0:00:10s\n",
      "epoch 42 | loss: 0.0692  | val_0_rmse: 0.26735 |  0:00:10s\n",
      "epoch 43 | loss: 0.08416 | val_0_rmse: 0.32012 |  0:00:10s\n",
      "epoch 44 | loss: 0.09387 | val_0_rmse: 0.24743 |  0:00:11s\n",
      "epoch 45 | loss: 0.10584 | val_0_rmse: 0.29446 |  0:00:11s\n",
      "epoch 46 | loss: 0.0989  | val_0_rmse: 0.24702 |  0:00:11s\n",
      "epoch 47 | loss: 0.08702 | val_0_rmse: 0.26097 |  0:00:11s\n",
      "epoch 48 | loss: 0.08242 | val_0_rmse: 0.25309 |  0:00:11s\n",
      "epoch 49 | loss: 0.06522 | val_0_rmse: 0.25486 |  0:00:12s\n",
      "epoch 50 | loss: 0.07922 | val_0_rmse: 0.23275 |  0:00:12s\n",
      "epoch 51 | loss: 0.07471 | val_0_rmse: 0.25308 |  0:00:12s\n",
      "epoch 52 | loss: 0.06801 | val_0_rmse: 0.26159 |  0:00:12s\n",
      "epoch 53 | loss: 0.0805  | val_0_rmse: 0.2687  |  0:00:13s\n",
      "epoch 54 | loss: 0.07945 | val_0_rmse: 0.26626 |  0:00:13s\n",
      "epoch 55 | loss: 0.09892 | val_0_rmse: 0.28055 |  0:00:13s\n",
      "epoch 56 | loss: 0.10471 | val_0_rmse: 0.24763 |  0:00:13s\n",
      "epoch 57 | loss: 0.0663  | val_0_rmse: 0.25077 |  0:00:13s\n",
      "epoch 58 | loss: 0.06054 | val_0_rmse: 0.24525 |  0:00:14s\n",
      "epoch 59 | loss: 0.06473 | val_0_rmse: 0.24834 |  0:00:14s\n",
      "epoch 60 | loss: 0.05986 | val_0_rmse: 0.22589 |  0:00:14s\n",
      "epoch 61 | loss: 0.05003 | val_0_rmse: 0.247   |  0:00:15s\n",
      "epoch 62 | loss: 0.05249 | val_0_rmse: 0.2306  |  0:00:15s\n",
      "epoch 63 | loss: 0.05029 | val_0_rmse: 0.23401 |  0:00:15s\n",
      "epoch 64 | loss: 0.05259 | val_0_rmse: 0.22826 |  0:00:15s\n",
      "epoch 65 | loss: 0.04665 | val_0_rmse: 0.23524 |  0:00:16s\n",
      "epoch 66 | loss: 0.04795 | val_0_rmse: 0.22545 |  0:00:16s\n",
      "epoch 67 | loss: 0.04867 | val_0_rmse: 0.22338 |  0:00:16s\n",
      "epoch 68 | loss: 0.04145 | val_0_rmse: 0.23506 |  0:00:16s\n",
      "epoch 69 | loss: 0.04858 | val_0_rmse: 0.2239  |  0:00:16s\n",
      "epoch 70 | loss: 0.04788 | val_0_rmse: 0.22799 |  0:00:17s\n",
      "epoch 71 | loss: 0.05356 | val_0_rmse: 0.23841 |  0:00:17s\n",
      "epoch 72 | loss: 0.04746 | val_0_rmse: 0.2267  |  0:00:17s\n",
      "epoch 73 | loss: 0.04096 | val_0_rmse: 0.23477 |  0:00:17s\n",
      "epoch 74 | loss: 0.05224 | val_0_rmse: 0.2392  |  0:00:18s\n",
      "epoch 75 | loss: 0.04803 | val_0_rmse: 0.2289  |  0:00:18s\n",
      "epoch 76 | loss: 0.04669 | val_0_rmse: 0.22152 |  0:00:18s\n",
      "epoch 77 | loss: 0.04054 | val_0_rmse: 0.21695 |  0:00:18s\n",
      "epoch 78 | loss: 0.04182 | val_0_rmse: 0.22441 |  0:00:19s\n",
      "epoch 79 | loss: 0.04316 | val_0_rmse: 0.21908 |  0:00:19s\n",
      "epoch 80 | loss: 0.04524 | val_0_rmse: 0.23809 |  0:00:19s\n",
      "epoch 81 | loss: 0.04233 | val_0_rmse: 0.21397 |  0:00:20s\n",
      "epoch 82 | loss: 0.04188 | val_0_rmse: 0.22571 |  0:00:20s\n",
      "epoch 83 | loss: 0.04232 | val_0_rmse: 0.25663 |  0:00:20s\n",
      "epoch 84 | loss: 0.057   | val_0_rmse: 0.25401 |  0:00:20s\n",
      "epoch 85 | loss: 0.05375 | val_0_rmse: 0.26571 |  0:00:20s\n",
      "epoch 86 | loss: 0.07707 | val_0_rmse: 0.24875 |  0:00:21s\n",
      "epoch 87 | loss: 0.05305 | val_0_rmse: 0.23866 |  0:00:21s\n",
      "epoch 88 | loss: 0.04514 | val_0_rmse: 0.21223 |  0:00:21s\n",
      "epoch 89 | loss: 0.03968 | val_0_rmse: 0.22653 |  0:00:21s\n",
      "epoch 90 | loss: 0.04368 | val_0_rmse: 0.21464 |  0:00:22s\n",
      "epoch 91 | loss: 0.03929 | val_0_rmse: 0.21838 |  0:00:22s\n",
      "epoch 92 | loss: 0.03804 | val_0_rmse: 0.21329 |  0:00:22s\n",
      "epoch 93 | loss: 0.03624 | val_0_rmse: 0.21364 |  0:00:22s\n",
      "epoch 94 | loss: 0.04131 | val_0_rmse: 0.21526 |  0:00:23s\n",
      "epoch 95 | loss: 0.03998 | val_0_rmse: 0.22216 |  0:00:23s\n",
      "epoch 96 | loss: 0.0384  | val_0_rmse: 0.21851 |  0:00:23s\n",
      "epoch 97 | loss: 0.05225 | val_0_rmse: 0.23946 |  0:00:23s\n",
      "epoch 98 | loss: 0.04893 | val_0_rmse: 0.22585 |  0:00:24s\n",
      "epoch 99 | loss: 0.0477  | val_0_rmse: 0.23543 |  0:00:24s\n",
      "epoch 100| loss: 0.04648 | val_0_rmse: 0.26022 |  0:00:24s\n",
      "epoch 101| loss: 0.0507  | val_0_rmse: 0.2459  |  0:00:24s\n",
      "epoch 102| loss: 0.05538 | val_0_rmse: 0.28637 |  0:00:25s\n",
      "epoch 103| loss: 0.0579  | val_0_rmse: 0.2331  |  0:00:25s\n",
      "epoch 104| loss: 0.05339 | val_0_rmse: 0.24005 |  0:00:25s\n",
      "epoch 105| loss: 0.04386 | val_0_rmse: 0.26128 |  0:00:25s\n",
      "epoch 106| loss: 0.06388 | val_0_rmse: 0.2385  |  0:00:26s\n",
      "epoch 107| loss: 0.05371 | val_0_rmse: 0.24426 |  0:00:26s\n",
      "epoch 108| loss: 0.04929 | val_0_rmse: 0.2714  |  0:00:26s\n",
      "epoch 109| loss: 0.06425 | val_0_rmse: 0.24064 |  0:00:26s\n",
      "epoch 110| loss: 0.05277 | val_0_rmse: 0.24872 |  0:00:27s\n",
      "epoch 111| loss: 0.0557  | val_0_rmse: 0.24671 |  0:00:27s\n",
      "epoch 112| loss: 0.05353 | val_0_rmse: 0.25237 |  0:00:27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:29:37,617] Trial 52 finished with value: 0.21223258982338888 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.3325775559711508, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.017097300772875135, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 113| loss: 0.04437 | val_0_rmse: 0.25254 |  0:00:27s\n",
      "\n",
      "Early stopping occurred at epoch 113 with best_epoch = 88 and best_val_0_rmse = 0.21223\n",
      "Trial 052 | rmse_log=0.21223 | RMSE$=43,625 | MAE$=26,295 | MAPE=15.76% | n_d/n_a=24/16 steps=3 lr=0.01710 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 123.63772| val_0_rmse: 10.75888|  0:00:00s\n",
      "epoch 1  | loss: 84.21214| val_0_rmse: 9.483   |  0:00:00s\n",
      "epoch 2  | loss: 54.61929| val_0_rmse: 7.98221 |  0:00:00s\n",
      "epoch 3  | loss: 34.01668| val_0_rmse: 6.51008 |  0:00:00s\n",
      "epoch 4  | loss: 20.24779| val_0_rmse: 5.18849 |  0:00:01s\n",
      "epoch 5  | loss: 17.21913| val_0_rmse: 4.29248 |  0:00:01s\n",
      "epoch 6  | loss: 12.05623| val_0_rmse: 3.93574 |  0:00:01s\n",
      "epoch 7  | loss: 6.77927 | val_0_rmse: 3.86585 |  0:00:01s\n",
      "epoch 8  | loss: 4.59642 | val_0_rmse: 3.62964 |  0:00:02s\n",
      "epoch 9  | loss: 2.85099 | val_0_rmse: 2.55602 |  0:00:02s\n",
      "epoch 10 | loss: 1.88863 | val_0_rmse: 2.28627 |  0:00:02s\n",
      "epoch 11 | loss: 1.42772 | val_0_rmse: 2.12641 |  0:00:03s\n",
      "epoch 12 | loss: 0.9711  | val_0_rmse: 1.38043 |  0:00:03s\n",
      "epoch 13 | loss: 0.75464 | val_0_rmse: 1.42512 |  0:00:03s\n",
      "epoch 14 | loss: 0.49006 | val_0_rmse: 1.13488 |  0:00:04s\n",
      "epoch 15 | loss: 0.44806 | val_0_rmse: 1.00299 |  0:00:04s\n",
      "epoch 16 | loss: 0.35602 | val_0_rmse: 0.92489 |  0:00:05s\n",
      "epoch 17 | loss: 0.36735 | val_0_rmse: 0.88198 |  0:00:05s\n",
      "epoch 18 | loss: 0.27744 | val_0_rmse: 0.75467 |  0:00:05s\n",
      "epoch 19 | loss: 0.23489 | val_0_rmse: 0.80239 |  0:00:05s\n",
      "epoch 20 | loss: 0.27806 | val_0_rmse: 0.76694 |  0:00:06s\n",
      "epoch 21 | loss: 0.26703 | val_0_rmse: 0.88631 |  0:00:06s\n",
      "epoch 22 | loss: 0.2166  | val_0_rmse: 0.66147 |  0:00:06s\n",
      "epoch 23 | loss: 0.16158 | val_0_rmse: 0.57725 |  0:00:06s\n",
      "epoch 24 | loss: 0.15501 | val_0_rmse: 0.55476 |  0:00:07s\n",
      "epoch 25 | loss: 0.12857 | val_0_rmse: 0.35296 |  0:00:07s\n",
      "epoch 26 | loss: 0.13801 | val_0_rmse: 0.54032 |  0:00:07s\n",
      "epoch 27 | loss: 0.11598 | val_0_rmse: 0.40659 |  0:00:07s\n",
      "epoch 28 | loss: 0.10217 | val_0_rmse: 0.29103 |  0:00:08s\n",
      "epoch 29 | loss: 0.11579 | val_0_rmse: 0.41169 |  0:00:08s\n",
      "epoch 30 | loss: 0.10893 | val_0_rmse: 0.37146 |  0:00:08s\n",
      "epoch 31 | loss: 0.09814 | val_0_rmse: 0.30769 |  0:00:08s\n",
      "epoch 32 | loss: 0.09004 | val_0_rmse: 0.29007 |  0:00:09s\n",
      "epoch 33 | loss: 0.09793 | val_0_rmse: 0.2679  |  0:00:09s\n",
      "epoch 34 | loss: 0.10705 | val_0_rmse: 0.39297 |  0:00:09s\n",
      "epoch 35 | loss: 0.13459 | val_0_rmse: 0.26389 |  0:00:09s\n",
      "epoch 36 | loss: 0.09223 | val_0_rmse: 0.31953 |  0:00:10s\n",
      "epoch 37 | loss: 0.06901 | val_0_rmse: 0.27864 |  0:00:10s\n",
      "epoch 38 | loss: 0.07125 | val_0_rmse: 0.25179 |  0:00:10s\n",
      "epoch 39 | loss: 0.06056 | val_0_rmse: 0.29487 |  0:00:10s\n",
      "epoch 40 | loss: 0.07145 | val_0_rmse: 0.26422 |  0:00:11s\n",
      "epoch 41 | loss: 0.0731  | val_0_rmse: 0.26484 |  0:00:11s\n",
      "epoch 42 | loss: 0.06184 | val_0_rmse: 0.26151 |  0:00:11s\n",
      "epoch 43 | loss: 0.07527 | val_0_rmse: 0.23106 |  0:00:11s\n",
      "epoch 44 | loss: 0.0637  | val_0_rmse: 0.21847 |  0:00:12s\n",
      "epoch 45 | loss: 0.06163 | val_0_rmse: 0.2221  |  0:00:12s\n",
      "epoch 46 | loss: 0.05944 | val_0_rmse: 0.22112 |  0:00:12s\n",
      "epoch 47 | loss: 0.05558 | val_0_rmse: 0.21208 |  0:00:12s\n",
      "epoch 48 | loss: 0.058   | val_0_rmse: 0.21242 |  0:00:13s\n",
      "epoch 49 | loss: 0.04975 | val_0_rmse: 0.25822 |  0:00:13s\n",
      "epoch 50 | loss: 0.06162 | val_0_rmse: 0.21686 |  0:00:13s\n",
      "epoch 51 | loss: 0.05619 | val_0_rmse: 0.22623 |  0:00:13s\n",
      "epoch 52 | loss: 0.05764 | val_0_rmse: 0.25666 |  0:00:14s\n",
      "epoch 53 | loss: 0.1234  | val_0_rmse: 0.2579  |  0:00:14s\n",
      "epoch 54 | loss: 0.06721 | val_0_rmse: 0.29707 |  0:00:14s\n",
      "epoch 55 | loss: 0.08899 | val_0_rmse: 0.26691 |  0:00:14s\n",
      "epoch 56 | loss: 0.09883 | val_0_rmse: 0.23966 |  0:00:15s\n",
      "epoch 57 | loss: 0.06953 | val_0_rmse: 0.28696 |  0:00:15s\n",
      "epoch 58 | loss: 0.1138  | val_0_rmse: 0.25302 |  0:00:15s\n",
      "epoch 59 | loss: 0.06251 | val_0_rmse: 0.21688 |  0:00:15s\n",
      "epoch 60 | loss: 0.04966 | val_0_rmse: 0.22312 |  0:00:15s\n",
      "epoch 61 | loss: 0.05051 | val_0_rmse: 0.23626 |  0:00:16s\n",
      "epoch 62 | loss: 0.05236 | val_0_rmse: 0.22977 |  0:00:16s\n",
      "epoch 63 | loss: 0.05601 | val_0_rmse: 0.21348 |  0:00:16s\n",
      "epoch 64 | loss: 0.06057 | val_0_rmse: 0.20637 |  0:00:17s\n",
      "epoch 65 | loss: 0.04999 | val_0_rmse: 0.22072 |  0:00:17s\n",
      "epoch 66 | loss: 0.04546 | val_0_rmse: 0.20384 |  0:00:17s\n",
      "epoch 67 | loss: 0.04902 | val_0_rmse: 0.23684 |  0:00:17s\n",
      "epoch 68 | loss: 0.05559 | val_0_rmse: 0.21219 |  0:00:17s\n",
      "epoch 69 | loss: 0.05039 | val_0_rmse: 0.20619 |  0:00:18s\n",
      "epoch 70 | loss: 0.04338 | val_0_rmse: 0.19683 |  0:00:18s\n",
      "epoch 71 | loss: 0.04088 | val_0_rmse: 0.21663 |  0:00:18s\n",
      "epoch 72 | loss: 0.05648 | val_0_rmse: 0.19627 |  0:00:18s\n",
      "epoch 73 | loss: 0.04454 | val_0_rmse: 0.20527 |  0:00:19s\n",
      "epoch 74 | loss: 0.0426  | val_0_rmse: 0.20134 |  0:00:19s\n",
      "epoch 75 | loss: 0.03773 | val_0_rmse: 0.21133 |  0:00:19s\n",
      "epoch 76 | loss: 0.04596 | val_0_rmse: 0.20843 |  0:00:19s\n",
      "epoch 77 | loss: 0.04075 | val_0_rmse: 0.23847 |  0:00:20s\n",
      "epoch 78 | loss: 0.04917 | val_0_rmse: 0.20034 |  0:00:20s\n",
      "epoch 79 | loss: 0.04925 | val_0_rmse: 0.20519 |  0:00:20s\n",
      "epoch 80 | loss: 0.04616 | val_0_rmse: 0.19945 |  0:00:21s\n",
      "epoch 81 | loss: 0.05549 | val_0_rmse: 0.22167 |  0:00:21s\n",
      "epoch 82 | loss: 0.04878 | val_0_rmse: 0.21468 |  0:00:21s\n",
      "epoch 83 | loss: 0.04661 | val_0_rmse: 0.19187 |  0:00:22s\n",
      "epoch 84 | loss: 0.04818 | val_0_rmse: 0.19575 |  0:00:22s\n",
      "epoch 85 | loss: 0.04588 | val_0_rmse: 0.19538 |  0:00:22s\n",
      "epoch 86 | loss: 0.05834 | val_0_rmse: 0.21583 |  0:00:22s\n",
      "epoch 87 | loss: 0.04829 | val_0_rmse: 0.21633 |  0:00:23s\n",
      "epoch 88 | loss: 0.04031 | val_0_rmse: 0.19201 |  0:00:23s\n",
      "epoch 89 | loss: 0.04871 | val_0_rmse: 0.20989 |  0:00:23s\n",
      "epoch 90 | loss: 0.0533  | val_0_rmse: 0.22101 |  0:00:23s\n",
      "epoch 91 | loss: 0.04608 | val_0_rmse: 0.19062 |  0:00:24s\n",
      "epoch 92 | loss: 0.03613 | val_0_rmse: 0.20335 |  0:00:24s\n",
      "epoch 93 | loss: 0.03837 | val_0_rmse: 0.19152 |  0:00:24s\n",
      "epoch 94 | loss: 0.03005 | val_0_rmse: 0.19052 |  0:00:24s\n",
      "epoch 95 | loss: 0.03064 | val_0_rmse: 0.19918 |  0:00:25s\n",
      "epoch 96 | loss: 0.03127 | val_0_rmse: 0.24928 |  0:00:25s\n",
      "epoch 97 | loss: 0.06262 | val_0_rmse: 0.21458 |  0:00:25s\n",
      "epoch 98 | loss: 0.05931 | val_0_rmse: 0.2493  |  0:00:26s\n",
      "epoch 99 | loss: 0.06812 | val_0_rmse: 0.19759 |  0:00:26s\n",
      "epoch 100| loss: 0.03894 | val_0_rmse: 0.25523 |  0:00:26s\n",
      "epoch 101| loss: 0.07793 | val_0_rmse: 0.21462 |  0:00:26s\n",
      "epoch 102| loss: 0.05312 | val_0_rmse: 0.21318 |  0:00:27s\n",
      "epoch 103| loss: 0.05328 | val_0_rmse: 0.21503 |  0:00:27s\n",
      "epoch 104| loss: 0.06069 | val_0_rmse: 0.22676 |  0:00:27s\n",
      "epoch 105| loss: 0.05195 | val_0_rmse: 0.21482 |  0:00:27s\n",
      "epoch 106| loss: 0.06083 | val_0_rmse: 0.21321 |  0:00:28s\n",
      "epoch 107| loss: 0.05345 | val_0_rmse: 0.2292  |  0:00:28s\n",
      "epoch 108| loss: 0.05647 | val_0_rmse: 0.2155  |  0:00:28s\n",
      "epoch 109| loss: 0.04952 | val_0_rmse: 0.20618 |  0:00:28s\n",
      "epoch 110| loss: 0.05388 | val_0_rmse: 0.22052 |  0:00:29s\n",
      "epoch 111| loss: 0.05186 | val_0_rmse: 0.21022 |  0:00:29s\n",
      "epoch 112| loss: 0.05269 | val_0_rmse: 0.21309 |  0:00:29s\n",
      "epoch 113| loss: 0.04862 | val_0_rmse: 0.21839 |  0:00:29s\n",
      "epoch 114| loss: 0.05605 | val_0_rmse: 0.22235 |  0:00:30s\n",
      "epoch 115| loss: 0.05041 | val_0_rmse: 0.20887 |  0:00:30s\n",
      "epoch 116| loss: 0.04751 | val_0_rmse: 0.25863 |  0:00:30s\n",
      "epoch 117| loss: 0.06388 | val_0_rmse: 0.20386 |  0:00:30s\n",
      "epoch 118| loss: 0.04197 | val_0_rmse: 0.21163 |  0:00:31s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:30:09,370] Trial 53 finished with value: 0.19052135482208263 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2678747673007342, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.019767082327903345, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 119| loss: 0.03207 | val_0_rmse: 0.21503 |  0:00:31s\n",
      "\n",
      "Early stopping occurred at epoch 119 with best_epoch = 94 and best_val_0_rmse = 0.19052\n",
      "Trial 053 | rmse_log=0.19052 | RMSE$=35,442 | MAE$=24,121 | MAPE=14.44% | n_d/n_a=24/16 steps=3 lr=0.01977 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 123.96453| val_0_rmse: 10.93964|  0:00:00s\n",
      "epoch 1  | loss: 83.66888| val_0_rmse: 9.61822 |  0:00:00s\n",
      "epoch 2  | loss: 53.39551| val_0_rmse: 7.96322 |  0:00:00s\n",
      "epoch 3  | loss: 33.07106| val_0_rmse: 6.25627 |  0:00:01s\n",
      "epoch 4  | loss: 17.60545| val_0_rmse: 4.7025  |  0:00:01s\n",
      "epoch 5  | loss: 13.5581 | val_0_rmse: 3.6337  |  0:00:01s\n",
      "epoch 6  | loss: 12.43873| val_0_rmse: 3.50325 |  0:00:01s\n",
      "epoch 7  | loss: 7.4485  | val_0_rmse: 3.62824 |  0:00:02s\n",
      "epoch 8  | loss: 3.95837 | val_0_rmse: 3.45422 |  0:00:02s\n",
      "epoch 9  | loss: 3.00757 | val_0_rmse: 2.53054 |  0:00:02s\n",
      "epoch 10 | loss: 1.91832 | val_0_rmse: 2.06463 |  0:00:02s\n",
      "epoch 11 | loss: 1.05056 | val_0_rmse: 2.01441 |  0:00:03s\n",
      "epoch 12 | loss: 0.81816 | val_0_rmse: 1.39293 |  0:00:03s\n",
      "epoch 13 | loss: 0.78639 | val_0_rmse: 1.74694 |  0:00:03s\n",
      "epoch 14 | loss: 0.56629 | val_0_rmse: 1.06619 |  0:00:03s\n",
      "epoch 15 | loss: 0.50489 | val_0_rmse: 1.28138 |  0:00:04s\n",
      "epoch 16 | loss: 0.44716 | val_0_rmse: 0.86933 |  0:00:04s\n",
      "epoch 17 | loss: 0.29729 | val_0_rmse: 1.06405 |  0:00:04s\n",
      "epoch 18 | loss: 0.31859 | val_0_rmse: 0.84586 |  0:00:04s\n",
      "epoch 19 | loss: 0.25547 | val_0_rmse: 0.62152 |  0:00:05s\n",
      "epoch 20 | loss: 0.30802 | val_0_rmse: 0.74972 |  0:00:05s\n",
      "epoch 21 | loss: 0.21903 | val_0_rmse: 0.66286 |  0:00:05s\n",
      "epoch 22 | loss: 0.17198 | val_0_rmse: 0.59392 |  0:00:05s\n",
      "epoch 23 | loss: 0.1818  | val_0_rmse: 0.64974 |  0:00:06s\n",
      "epoch 24 | loss: 0.15529 | val_0_rmse: 0.70254 |  0:00:06s\n",
      "epoch 25 | loss: 0.16028 | val_0_rmse: 0.60358 |  0:00:06s\n",
      "epoch 26 | loss: 0.1415  | val_0_rmse: 0.69832 |  0:00:07s\n",
      "epoch 27 | loss: 0.14716 | val_0_rmse: 0.39434 |  0:00:07s\n",
      "epoch 28 | loss: 0.15958 | val_0_rmse: 0.56854 |  0:00:07s\n",
      "epoch 29 | loss: 0.14361 | val_0_rmse: 0.29074 |  0:00:07s\n",
      "epoch 30 | loss: 0.21329 | val_0_rmse: 0.4989  |  0:00:08s\n",
      "epoch 31 | loss: 0.18978 | val_0_rmse: 0.32034 |  0:00:08s\n",
      "epoch 32 | loss: 0.11601 | val_0_rmse: 0.33732 |  0:00:08s\n",
      "epoch 33 | loss: 0.09176 | val_0_rmse: 0.2784  |  0:00:08s\n",
      "epoch 34 | loss: 0.18857 | val_0_rmse: 0.32604 |  0:00:09s\n",
      "epoch 35 | loss: 0.12339 | val_0_rmse: 0.28766 |  0:00:09s\n",
      "epoch 36 | loss: 0.11131 | val_0_rmse: 0.3491  |  0:00:09s\n",
      "epoch 37 | loss: 0.12203 | val_0_rmse: 0.25358 |  0:00:09s\n",
      "epoch 38 | loss: 0.11321 | val_0_rmse: 0.29705 |  0:00:09s\n",
      "epoch 39 | loss: 0.1024  | val_0_rmse: 0.27062 |  0:00:10s\n",
      "epoch 40 | loss: 0.11949 | val_0_rmse: 0.32137 |  0:00:10s\n",
      "epoch 41 | loss: 0.12987 | val_0_rmse: 0.27384 |  0:00:10s\n",
      "epoch 42 | loss: 0.09427 | val_0_rmse: 0.31779 |  0:00:10s\n",
      "epoch 43 | loss: 0.08773 | val_0_rmse: 0.26472 |  0:00:11s\n",
      "epoch 44 | loss: 0.08128 | val_0_rmse: 0.28819 |  0:00:11s\n",
      "epoch 45 | loss: 0.0697  | val_0_rmse: 0.26859 |  0:00:11s\n",
      "epoch 46 | loss: 0.07908 | val_0_rmse: 0.25747 |  0:00:11s\n",
      "epoch 47 | loss: 0.09476 | val_0_rmse: 0.29706 |  0:00:12s\n",
      "epoch 48 | loss: 0.07784 | val_0_rmse: 0.24553 |  0:00:12s\n",
      "epoch 49 | loss: 0.08606 | val_0_rmse: 0.2844  |  0:00:12s\n",
      "epoch 50 | loss: 0.0786  | val_0_rmse: 0.24325 |  0:00:12s\n",
      "epoch 51 | loss: 0.08421 | val_0_rmse: 0.22885 |  0:00:13s\n",
      "epoch 52 | loss: 0.07737 | val_0_rmse: 0.31192 |  0:00:13s\n",
      "epoch 53 | loss: 0.09181 | val_0_rmse: 0.2492  |  0:00:13s\n",
      "epoch 54 | loss: 0.06274 | val_0_rmse: 0.23306 |  0:00:14s\n",
      "epoch 55 | loss: 0.07078 | val_0_rmse: 0.23231 |  0:00:14s\n",
      "epoch 56 | loss: 0.05997 | val_0_rmse: 0.27054 |  0:00:14s\n",
      "epoch 57 | loss: 0.05417 | val_0_rmse: 0.22847 |  0:00:14s\n",
      "epoch 58 | loss: 0.06218 | val_0_rmse: 0.23865 |  0:00:15s\n",
      "epoch 59 | loss: 0.06322 | val_0_rmse: 0.21673 |  0:00:15s\n",
      "epoch 60 | loss: 0.05854 | val_0_rmse: 0.23171 |  0:00:15s\n",
      "epoch 61 | loss: 0.06295 | val_0_rmse: 0.23699 |  0:00:15s\n",
      "epoch 62 | loss: 0.05467 | val_0_rmse: 0.21682 |  0:00:16s\n",
      "epoch 63 | loss: 0.0549  | val_0_rmse: 0.21719 |  0:00:16s\n",
      "epoch 64 | loss: 0.05147 | val_0_rmse: 0.23424 |  0:00:16s\n",
      "epoch 65 | loss: 0.04672 | val_0_rmse: 0.22818 |  0:00:16s\n",
      "epoch 66 | loss: 0.05453 | val_0_rmse: 0.22438 |  0:00:16s\n",
      "epoch 67 | loss: 0.05529 | val_0_rmse: 0.21386 |  0:00:17s\n",
      "epoch 68 | loss: 0.05097 | val_0_rmse: 0.21036 |  0:00:17s\n",
      "epoch 69 | loss: 0.04409 | val_0_rmse: 0.22879 |  0:00:17s\n",
      "epoch 70 | loss: 0.06319 | val_0_rmse: 0.21991 |  0:00:17s\n",
      "epoch 71 | loss: 0.06658 | val_0_rmse: 0.25285 |  0:00:18s\n",
      "epoch 72 | loss: 0.0785  | val_0_rmse: 0.20672 |  0:00:18s\n",
      "epoch 73 | loss: 0.06055 | val_0_rmse: 0.20609 |  0:00:18s\n",
      "epoch 74 | loss: 0.04425 | val_0_rmse: 0.21218 |  0:00:18s\n",
      "epoch 75 | loss: 0.05051 | val_0_rmse: 0.21364 |  0:00:19s\n",
      "epoch 76 | loss: 0.03692 | val_0_rmse: 0.2011  |  0:00:19s\n",
      "epoch 77 | loss: 0.04106 | val_0_rmse: 0.20696 |  0:00:19s\n",
      "epoch 78 | loss: 0.04992 | val_0_rmse: 0.2039  |  0:00:19s\n",
      "epoch 79 | loss: 0.04051 | val_0_rmse: 0.20553 |  0:00:20s\n",
      "epoch 80 | loss: 0.03571 | val_0_rmse: 0.20159 |  0:00:20s\n",
      "epoch 81 | loss: 0.04017 | val_0_rmse: 0.22231 |  0:00:20s\n",
      "epoch 82 | loss: 0.0454  | val_0_rmse: 0.21322 |  0:00:21s\n",
      "epoch 83 | loss: 0.04167 | val_0_rmse: 0.2091  |  0:00:21s\n",
      "epoch 84 | loss: 0.0425  | val_0_rmse: 0.19423 |  0:00:21s\n",
      "epoch 85 | loss: 0.04052 | val_0_rmse: 0.22063 |  0:00:21s\n",
      "epoch 86 | loss: 0.06591 | val_0_rmse: 0.21563 |  0:00:22s\n",
      "epoch 87 | loss: 0.05082 | val_0_rmse: 0.20637 |  0:00:22s\n",
      "epoch 88 | loss: 0.04013 | val_0_rmse: 0.19924 |  0:00:22s\n",
      "epoch 89 | loss: 0.0384  | val_0_rmse: 0.2068  |  0:00:22s\n",
      "epoch 90 | loss: 0.03783 | val_0_rmse: 0.19132 |  0:00:22s\n",
      "epoch 91 | loss: 0.03678 | val_0_rmse: 0.19853 |  0:00:23s\n",
      "epoch 92 | loss: 0.03928 | val_0_rmse: 0.1962  |  0:00:23s\n",
      "epoch 93 | loss: 0.03889 | val_0_rmse: 0.20747 |  0:00:23s\n",
      "epoch 94 | loss: 0.04204 | val_0_rmse: 0.20779 |  0:00:23s\n",
      "epoch 95 | loss: 0.04378 | val_0_rmse: 0.18649 |  0:00:24s\n",
      "epoch 96 | loss: 0.04059 | val_0_rmse: 0.20173 |  0:00:24s\n",
      "epoch 97 | loss: 0.04539 | val_0_rmse: 0.19515 |  0:00:24s\n",
      "epoch 98 | loss: 0.03724 | val_0_rmse: 0.1923  |  0:00:25s\n",
      "epoch 99 | loss: 0.03542 | val_0_rmse: 0.20694 |  0:00:25s\n",
      "epoch 100| loss: 0.04943 | val_0_rmse: 0.2121  |  0:00:25s\n",
      "epoch 101| loss: 0.03949 | val_0_rmse: 0.18871 |  0:00:25s\n",
      "epoch 102| loss: 0.03419 | val_0_rmse: 0.22232 |  0:00:26s\n",
      "epoch 103| loss: 0.05471 | val_0_rmse: 0.18898 |  0:00:26s\n",
      "epoch 104| loss: 0.03712 | val_0_rmse: 0.20339 |  0:00:26s\n",
      "epoch 105| loss: 0.03938 | val_0_rmse: 0.19013 |  0:00:26s\n",
      "epoch 106| loss: 0.03479 | val_0_rmse: 0.20595 |  0:00:26s\n",
      "epoch 107| loss: 0.0347  | val_0_rmse: 0.19801 |  0:00:27s\n",
      "epoch 108| loss: 0.04194 | val_0_rmse: 0.2096  |  0:00:27s\n",
      "epoch 109| loss: 0.05013 | val_0_rmse: 0.1992  |  0:00:27s\n",
      "epoch 110| loss: 0.03627 | val_0_rmse: 0.19638 |  0:00:27s\n",
      "epoch 111| loss: 0.03383 | val_0_rmse: 0.19941 |  0:00:28s\n",
      "epoch 112| loss: 0.0346  | val_0_rmse: 0.20714 |  0:00:28s\n",
      "epoch 113| loss: 0.03695 | val_0_rmse: 0.23086 |  0:00:28s\n",
      "epoch 114| loss: 0.0474  | val_0_rmse: 0.21753 |  0:00:28s\n",
      "epoch 115| loss: 0.04714 | val_0_rmse: 0.24504 |  0:00:29s\n",
      "epoch 116| loss: 0.03845 | val_0_rmse: 0.21736 |  0:00:29s\n",
      "epoch 117| loss: 0.03666 | val_0_rmse: 0.20512 |  0:00:29s\n",
      "epoch 118| loss: 0.03109 | val_0_rmse: 0.20338 |  0:00:29s\n",
      "epoch 119| loss: 0.03657 | val_0_rmse: 0.22055 |  0:00:30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:30:40,037] Trial 54 finished with value: 0.18648712453528626 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2050017248227476, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.018864658769769435, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 120| loss: 0.03618 | val_0_rmse: 0.21261 |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 120 with best_epoch = 95 and best_val_0_rmse = 0.18649\n",
      "Trial 054 | rmse_log=0.18649 | RMSE$=44,841 | MAE$=24,090 | MAPE=13.32% | n_d/n_a=24/16 steps=3 lr=0.01886 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.9337| val_0_rmse: 11.00771|  0:00:00s\n",
      "epoch 1  | loss: 88.10677| val_0_rmse: 9.7408  |  0:00:00s\n",
      "epoch 2  | loss: 60.35424| val_0_rmse: 8.43876 |  0:00:00s\n",
      "epoch 3  | loss: 37.11478| val_0_rmse: 7.13504 |  0:00:01s\n",
      "epoch 4  | loss: 24.59676| val_0_rmse: 5.86286 |  0:00:01s\n",
      "epoch 5  | loss: 14.32837| val_0_rmse: 4.69211 |  0:00:01s\n",
      "epoch 6  | loss: 11.56495| val_0_rmse: 3.77826 |  0:00:02s\n",
      "epoch 7  | loss: 10.07132| val_0_rmse: 3.46684 |  0:00:02s\n",
      "epoch 8  | loss: 5.4371  | val_0_rmse: 3.62989 |  0:00:02s\n",
      "epoch 9  | loss: 4.17323 | val_0_rmse: 3.50843 |  0:00:02s\n",
      "epoch 10 | loss: 3.19201 | val_0_rmse: 2.67582 |  0:00:03s\n",
      "epoch 11 | loss: 1.68735 | val_0_rmse: 2.09786 |  0:00:03s\n",
      "epoch 12 | loss: 1.34702 | val_0_rmse: 2.19576 |  0:00:03s\n",
      "epoch 13 | loss: 0.98394 | val_0_rmse: 1.94618 |  0:00:04s\n",
      "epoch 14 | loss: 0.74536 | val_0_rmse: 1.60483 |  0:00:04s\n",
      "epoch 15 | loss: 0.51846 | val_0_rmse: 1.45496 |  0:00:04s\n",
      "epoch 16 | loss: 0.39197 | val_0_rmse: 1.30135 |  0:00:04s\n",
      "epoch 17 | loss: 0.49188 | val_0_rmse: 0.84398 |  0:00:05s\n",
      "epoch 18 | loss: 0.40116 | val_0_rmse: 0.93979 |  0:00:05s\n",
      "epoch 19 | loss: 0.33674 | val_0_rmse: 0.76313 |  0:00:05s\n",
      "epoch 20 | loss: 0.28186 | val_0_rmse: 0.78689 |  0:00:06s\n",
      "epoch 21 | loss: 0.2568  | val_0_rmse: 0.50509 |  0:00:06s\n",
      "epoch 22 | loss: 0.24555 | val_0_rmse: 0.61887 |  0:00:06s\n",
      "epoch 23 | loss: 0.2307  | val_0_rmse: 0.57576 |  0:00:06s\n",
      "epoch 24 | loss: 0.19429 | val_0_rmse: 0.62977 |  0:00:07s\n",
      "epoch 25 | loss: 0.18958 | val_0_rmse: 0.46423 |  0:00:07s\n",
      "epoch 26 | loss: 0.21905 | val_0_rmse: 0.70707 |  0:00:07s\n",
      "epoch 27 | loss: 0.21886 | val_0_rmse: 0.61941 |  0:00:07s\n",
      "epoch 28 | loss: 0.14512 | val_0_rmse: 0.42619 |  0:00:08s\n",
      "epoch 29 | loss: 0.14147 | val_0_rmse: 0.48616 |  0:00:08s\n",
      "epoch 30 | loss: 0.14515 | val_0_rmse: 0.40483 |  0:00:08s\n",
      "epoch 31 | loss: 0.11918 | val_0_rmse: 0.55626 |  0:00:09s\n",
      "epoch 32 | loss: 0.12199 | val_0_rmse: 0.35532 |  0:00:09s\n",
      "epoch 33 | loss: 0.12043 | val_0_rmse: 0.31253 |  0:00:09s\n",
      "epoch 34 | loss: 0.12164 | val_0_rmse: 0.37568 |  0:00:10s\n",
      "epoch 35 | loss: 0.13078 | val_0_rmse: 0.28118 |  0:00:10s\n",
      "epoch 36 | loss: 0.1225  | val_0_rmse: 0.30794 |  0:00:10s\n",
      "epoch 37 | loss: 0.10265 | val_0_rmse: 0.38191 |  0:00:11s\n",
      "epoch 38 | loss: 0.09731 | val_0_rmse: 0.32152 |  0:00:11s\n",
      "epoch 39 | loss: 0.09779 | val_0_rmse: 0.36139 |  0:00:11s\n",
      "epoch 40 | loss: 0.084   | val_0_rmse: 0.365   |  0:00:11s\n",
      "epoch 41 | loss: 0.08841 | val_0_rmse: 0.24543 |  0:00:12s\n",
      "epoch 42 | loss: 0.08609 | val_0_rmse: 0.422   |  0:00:12s\n",
      "epoch 43 | loss: 0.10075 | val_0_rmse: 0.24232 |  0:00:12s\n",
      "epoch 44 | loss: 0.07748 | val_0_rmse: 0.31108 |  0:00:13s\n",
      "epoch 45 | loss: 0.06472 | val_0_rmse: 0.21549 |  0:00:13s\n",
      "epoch 46 | loss: 0.07487 | val_0_rmse: 0.28082 |  0:00:13s\n",
      "epoch 47 | loss: 0.0609  | val_0_rmse: 0.23035 |  0:00:13s\n",
      "epoch 48 | loss: 0.06818 | val_0_rmse: 0.21857 |  0:00:13s\n",
      "epoch 49 | loss: 0.052   | val_0_rmse: 0.26361 |  0:00:14s\n",
      "epoch 50 | loss: 0.06112 | val_0_rmse: 0.20161 |  0:00:14s\n",
      "epoch 51 | loss: 0.05171 | val_0_rmse: 0.22205 |  0:00:14s\n",
      "epoch 52 | loss: 0.06279 | val_0_rmse: 0.24362 |  0:00:14s\n",
      "epoch 53 | loss: 0.16224 | val_0_rmse: 0.23215 |  0:00:15s\n",
      "epoch 54 | loss: 0.12244 | val_0_rmse: 0.37925 |  0:00:15s\n",
      "epoch 55 | loss: 0.14508 | val_0_rmse: 0.21948 |  0:00:15s\n",
      "epoch 56 | loss: 0.07135 | val_0_rmse: 0.25165 |  0:00:15s\n",
      "epoch 57 | loss: 0.07846 | val_0_rmse: 0.23127 |  0:00:16s\n",
      "epoch 58 | loss: 0.06655 | val_0_rmse: 0.23198 |  0:00:16s\n",
      "epoch 59 | loss: 0.06993 | val_0_rmse: 0.27994 |  0:00:16s\n",
      "epoch 60 | loss: 0.12558 | val_0_rmse: 0.24637 |  0:00:16s\n",
      "epoch 61 | loss: 0.09728 | val_0_rmse: 0.33025 |  0:00:17s\n",
      "epoch 62 | loss: 0.10563 | val_0_rmse: 0.26501 |  0:00:17s\n",
      "epoch 63 | loss: 0.09649 | val_0_rmse: 0.24683 |  0:00:17s\n",
      "epoch 64 | loss: 0.11375 | val_0_rmse: 0.22941 |  0:00:17s\n",
      "epoch 65 | loss: 0.08693 | val_0_rmse: 0.27602 |  0:00:18s\n",
      "epoch 66 | loss: 0.09404 | val_0_rmse: 0.25397 |  0:00:18s\n",
      "epoch 67 | loss: 0.07961 | val_0_rmse: 0.22253 |  0:00:18s\n",
      "epoch 68 | loss: 0.06211 | val_0_rmse: 0.23857 |  0:00:18s\n",
      "epoch 69 | loss: 0.09718 | val_0_rmse: 0.22202 |  0:00:19s\n",
      "epoch 70 | loss: 0.07055 | val_0_rmse: 0.21359 |  0:00:19s\n",
      "epoch 71 | loss: 0.05721 | val_0_rmse: 0.23088 |  0:00:19s\n",
      "epoch 72 | loss: 0.05494 | val_0_rmse: 0.2137  |  0:00:20s\n",
      "epoch 73 | loss: 0.04529 | val_0_rmse: 0.20173 |  0:00:20s\n",
      "epoch 74 | loss: 0.04594 | val_0_rmse: 0.20363 |  0:00:20s\n",
      "epoch 75 | loss: 0.0454  | val_0_rmse: 0.19899 |  0:00:20s\n",
      "epoch 76 | loss: 0.04179 | val_0_rmse: 0.21444 |  0:00:21s\n",
      "epoch 77 | loss: 0.04788 | val_0_rmse: 0.21077 |  0:00:21s\n",
      "epoch 78 | loss: 0.0498  | val_0_rmse: 0.21103 |  0:00:21s\n",
      "epoch 79 | loss: 0.0449  | val_0_rmse: 0.20605 |  0:00:21s\n",
      "epoch 80 | loss: 0.04121 | val_0_rmse: 0.19479 |  0:00:22s\n",
      "epoch 81 | loss: 0.03909 | val_0_rmse: 0.19898 |  0:00:22s\n",
      "epoch 82 | loss: 0.04314 | val_0_rmse: 0.20449 |  0:00:22s\n",
      "epoch 83 | loss: 0.04022 | val_0_rmse: 0.21047 |  0:00:22s\n",
      "epoch 84 | loss: 0.03762 | val_0_rmse: 0.19235 |  0:00:22s\n",
      "epoch 85 | loss: 0.0391  | val_0_rmse: 0.19694 |  0:00:23s\n",
      "epoch 86 | loss: 0.03595 | val_0_rmse: 0.19699 |  0:00:23s\n",
      "epoch 87 | loss: 0.04036 | val_0_rmse: 0.19563 |  0:00:23s\n",
      "epoch 88 | loss: 0.03857 | val_0_rmse: 0.20892 |  0:00:23s\n",
      "epoch 89 | loss: 0.03721 | val_0_rmse: 0.2025  |  0:00:24s\n",
      "epoch 90 | loss: 0.03758 | val_0_rmse: 0.20458 |  0:00:24s\n",
      "epoch 91 | loss: 0.04259 | val_0_rmse: 0.20439 |  0:00:24s\n",
      "epoch 92 | loss: 0.03828 | val_0_rmse: 0.20876 |  0:00:24s\n",
      "epoch 93 | loss: 0.04021 | val_0_rmse: 0.2106  |  0:00:25s\n",
      "epoch 94 | loss: 0.03838 | val_0_rmse: 0.2081  |  0:00:25s\n",
      "epoch 95 | loss: 0.03933 | val_0_rmse: 0.19285 |  0:00:25s\n",
      "epoch 96 | loss: 0.03651 | val_0_rmse: 0.23547 |  0:00:26s\n",
      "epoch 97 | loss: 0.05193 | val_0_rmse: 0.20457 |  0:00:26s\n",
      "epoch 98 | loss: 0.03427 | val_0_rmse: 0.19009 |  0:00:26s\n",
      "epoch 99 | loss: 0.03716 | val_0_rmse: 0.20795 |  0:00:26s\n",
      "epoch 100| loss: 0.04161 | val_0_rmse: 0.19859 |  0:00:27s\n",
      "epoch 101| loss: 0.04522 | val_0_rmse: 0.20961 |  0:00:27s\n",
      "epoch 102| loss: 0.04014 | val_0_rmse: 0.21568 |  0:00:27s\n",
      "epoch 103| loss: 0.04518 | val_0_rmse: 0.20323 |  0:00:28s\n",
      "epoch 104| loss: 0.03879 | val_0_rmse: 0.20055 |  0:00:28s\n",
      "epoch 105| loss: 0.04384 | val_0_rmse: 0.19985 |  0:00:28s\n",
      "epoch 106| loss: 0.0392  | val_0_rmse: 0.20856 |  0:00:28s\n",
      "epoch 107| loss: 0.03279 | val_0_rmse: 0.19352 |  0:00:29s\n",
      "epoch 108| loss: 0.03269 | val_0_rmse: 0.19315 |  0:00:29s\n",
      "epoch 109| loss: 0.03211 | val_0_rmse: 0.20138 |  0:00:29s\n",
      "epoch 110| loss: 0.0297  | val_0_rmse: 0.21169 |  0:00:30s\n",
      "epoch 111| loss: 0.03343 | val_0_rmse: 0.22128 |  0:00:30s\n",
      "epoch 112| loss: 0.03241 | val_0_rmse: 0.21981 |  0:00:30s\n",
      "epoch 113| loss: 0.03873 | val_0_rmse: 0.22214 |  0:00:30s\n",
      "epoch 114| loss: 0.03511 | val_0_rmse: 0.22613 |  0:00:31s\n",
      "epoch 115| loss: 0.03214 | val_0_rmse: 0.24452 |  0:00:31s\n",
      "epoch 116| loss: 0.03792 | val_0_rmse: 0.2404  |  0:00:31s\n",
      "epoch 117| loss: 0.03698 | val_0_rmse: 0.22857 |  0:00:32s\n",
      "epoch 118| loss: 0.03189 | val_0_rmse: 0.21539 |  0:00:32s\n",
      "epoch 119| loss: 0.03003 | val_0_rmse: 0.23277 |  0:00:32s\n",
      "epoch 120| loss: 0.0439  | val_0_rmse: 0.24024 |  0:00:32s\n",
      "epoch 121| loss: 0.07789 | val_0_rmse: 0.20841 |  0:00:33s\n",
      "epoch 122| loss: 0.04115 | val_0_rmse: 0.23128 |  0:00:33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:31:14,063] Trial 55 finished with value: 0.19009301061907127 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2021491293328077, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.01643795731506787, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 123| loss: 0.04154 | val_0_rmse: 0.22935 |  0:00:33s\n",
      "\n",
      "Early stopping occurred at epoch 123 with best_epoch = 98 and best_val_0_rmse = 0.19009\n",
      "Trial 055 | rmse_log=0.19009 | RMSE$=37,149 | MAE$=23,891 | MAPE=14.88% | n_d/n_a=24/16 steps=3 lr=0.01644 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.95883| val_0_rmse: 11.06178|  0:00:00s\n",
      "epoch 1  | loss: 90.375  | val_0_rmse: 9.8805  |  0:00:00s\n",
      "epoch 2  | loss: 58.70526| val_0_rmse: 8.55207 |  0:00:00s\n",
      "epoch 3  | loss: 38.73307| val_0_rmse: 7.1573  |  0:00:01s\n",
      "epoch 4  | loss: 24.58479| val_0_rmse: 5.81254 |  0:00:01s\n",
      "epoch 5  | loss: 16.59662| val_0_rmse: 4.54347 |  0:00:01s\n",
      "epoch 6  | loss: 12.73768| val_0_rmse: 3.52293 |  0:00:01s\n",
      "epoch 7  | loss: 11.5278 | val_0_rmse: 3.01995 |  0:00:02s\n",
      "epoch 8  | loss: 5.85426 | val_0_rmse: 3.19778 |  0:00:02s\n",
      "epoch 9  | loss: 3.35144 | val_0_rmse: 3.29621 |  0:00:02s\n",
      "epoch 10 | loss: 2.46183 | val_0_rmse: 2.49389 |  0:00:02s\n",
      "epoch 11 | loss: 1.89053 | val_0_rmse: 2.00467 |  0:00:03s\n",
      "epoch 12 | loss: 1.1992  | val_0_rmse: 2.1333  |  0:00:03s\n",
      "epoch 13 | loss: 0.91451 | val_0_rmse: 1.53629 |  0:00:03s\n",
      "epoch 14 | loss: 0.71096 | val_0_rmse: 1.52936 |  0:00:03s\n",
      "epoch 15 | loss: 0.60956 | val_0_rmse: 1.28473 |  0:00:04s\n",
      "epoch 16 | loss: 0.49483 | val_0_rmse: 1.16268 |  0:00:04s\n",
      "epoch 17 | loss: 0.49454 | val_0_rmse: 0.82416 |  0:00:04s\n",
      "epoch 18 | loss: 0.40621 | val_0_rmse: 0.7043  |  0:00:04s\n",
      "epoch 19 | loss: 0.28654 | val_0_rmse: 0.82331 |  0:00:05s\n",
      "epoch 20 | loss: 0.30621 | val_0_rmse: 0.57456 |  0:00:05s\n",
      "epoch 21 | loss: 0.24625 | val_0_rmse: 0.58032 |  0:00:05s\n",
      "epoch 22 | loss: 0.25252 | val_0_rmse: 0.59835 |  0:00:05s\n",
      "epoch 23 | loss: 0.18759 | val_0_rmse: 0.61728 |  0:00:06s\n",
      "epoch 24 | loss: 0.18948 | val_0_rmse: 0.55843 |  0:00:06s\n",
      "epoch 25 | loss: 0.19354 | val_0_rmse: 0.58746 |  0:00:06s\n",
      "epoch 26 | loss: 0.17973 | val_0_rmse: 0.62381 |  0:00:06s\n",
      "epoch 27 | loss: 0.14989 | val_0_rmse: 0.38027 |  0:00:07s\n",
      "epoch 28 | loss: 0.14972 | val_0_rmse: 0.49367 |  0:00:07s\n",
      "epoch 29 | loss: 0.15201 | val_0_rmse: 0.39802 |  0:00:07s\n",
      "epoch 30 | loss: 0.1519  | val_0_rmse: 0.34171 |  0:00:07s\n",
      "epoch 31 | loss: 0.12823 | val_0_rmse: 0.46266 |  0:00:08s\n",
      "epoch 32 | loss: 0.17643 | val_0_rmse: 0.35154 |  0:00:08s\n",
      "epoch 33 | loss: 0.1827  | val_0_rmse: 0.3974  |  0:00:08s\n",
      "epoch 34 | loss: 0.12196 | val_0_rmse: 0.30112 |  0:00:09s\n",
      "epoch 35 | loss: 0.12047 | val_0_rmse: 0.42331 |  0:00:09s\n",
      "epoch 36 | loss: 0.16289 | val_0_rmse: 0.31314 |  0:00:09s\n",
      "epoch 37 | loss: 0.14027 | val_0_rmse: 0.37375 |  0:00:09s\n",
      "epoch 38 | loss: 0.11163 | val_0_rmse: 0.29304 |  0:00:09s\n",
      "epoch 39 | loss: 0.10043 | val_0_rmse: 0.36998 |  0:00:10s\n",
      "epoch 40 | loss: 0.09469 | val_0_rmse: 0.35555 |  0:00:10s\n",
      "epoch 41 | loss: 0.10126 | val_0_rmse: 0.28839 |  0:00:10s\n",
      "epoch 42 | loss: 0.08975 | val_0_rmse: 0.32824 |  0:00:11s\n",
      "epoch 43 | loss: 0.0938  | val_0_rmse: 0.35837 |  0:00:11s\n",
      "epoch 44 | loss: 0.08278 | val_0_rmse: 0.25041 |  0:00:11s\n",
      "epoch 45 | loss: 0.07128 | val_0_rmse: 0.2572  |  0:00:11s\n",
      "epoch 46 | loss: 0.06917 | val_0_rmse: 0.29851 |  0:00:12s\n",
      "epoch 47 | loss: 0.06179 | val_0_rmse: 0.26083 |  0:00:12s\n",
      "epoch 48 | loss: 0.06887 | val_0_rmse: 0.27405 |  0:00:12s\n",
      "epoch 49 | loss: 0.06033 | val_0_rmse: 0.27538 |  0:00:12s\n",
      "epoch 50 | loss: 0.06032 | val_0_rmse: 0.26921 |  0:00:13s\n",
      "epoch 51 | loss: 0.08224 | val_0_rmse: 0.31213 |  0:00:13s\n",
      "epoch 52 | loss: 0.1071  | val_0_rmse: 0.28393 |  0:00:13s\n",
      "epoch 53 | loss: 0.10164 | val_0_rmse: 0.25131 |  0:00:13s\n",
      "epoch 54 | loss: 0.06872 | val_0_rmse: 0.25132 |  0:00:13s\n",
      "epoch 55 | loss: 0.07669 | val_0_rmse: 0.32588 |  0:00:14s\n",
      "epoch 56 | loss: 0.09535 | val_0_rmse: 0.2511  |  0:00:14s\n",
      "epoch 57 | loss: 0.08044 | val_0_rmse: 0.27067 |  0:00:14s\n",
      "epoch 58 | loss: 0.06486 | val_0_rmse: 0.24212 |  0:00:14s\n",
      "epoch 59 | loss: 0.06218 | val_0_rmse: 0.2531  |  0:00:15s\n",
      "epoch 60 | loss: 0.04589 | val_0_rmse: 0.23741 |  0:00:15s\n",
      "epoch 61 | loss: 0.05101 | val_0_rmse: 0.24932 |  0:00:15s\n",
      "epoch 62 | loss: 0.06424 | val_0_rmse: 0.25392 |  0:00:15s\n",
      "epoch 63 | loss: 0.05109 | val_0_rmse: 0.24779 |  0:00:16s\n",
      "epoch 64 | loss: 0.0516  | val_0_rmse: 0.24752 |  0:00:16s\n",
      "epoch 65 | loss: 0.04958 | val_0_rmse: 0.23872 |  0:00:16s\n",
      "epoch 66 | loss: 0.04754 | val_0_rmse: 0.23997 |  0:00:16s\n",
      "epoch 67 | loss: 0.04283 | val_0_rmse: 0.27111 |  0:00:17s\n",
      "epoch 68 | loss: 0.05153 | val_0_rmse: 0.23489 |  0:00:17s\n",
      "epoch 69 | loss: 0.04224 | val_0_rmse: 0.25435 |  0:00:17s\n",
      "epoch 70 | loss: 0.05329 | val_0_rmse: 0.24327 |  0:00:17s\n",
      "epoch 71 | loss: 0.04511 | val_0_rmse: 0.23593 |  0:00:17s\n",
      "epoch 72 | loss: 0.04537 | val_0_rmse: 0.2333  |  0:00:18s\n",
      "epoch 73 | loss: 0.04158 | val_0_rmse: 0.23199 |  0:00:18s\n",
      "epoch 74 | loss: 0.03656 | val_0_rmse: 0.22146 |  0:00:18s\n",
      "epoch 75 | loss: 0.03822 | val_0_rmse: 0.25594 |  0:00:19s\n",
      "epoch 76 | loss: 0.04235 | val_0_rmse: 0.2356  |  0:00:19s\n",
      "epoch 77 | loss: 0.03919 | val_0_rmse: 0.22302 |  0:00:19s\n",
      "epoch 78 | loss: 0.04477 | val_0_rmse: 0.26097 |  0:00:20s\n",
      "epoch 79 | loss: 0.04634 | val_0_rmse: 0.22049 |  0:00:20s\n",
      "epoch 80 | loss: 0.0387  | val_0_rmse: 0.21648 |  0:00:20s\n",
      "epoch 81 | loss: 0.04283 | val_0_rmse: 0.23478 |  0:00:20s\n",
      "epoch 82 | loss: 0.04214 | val_0_rmse: 0.21925 |  0:00:21s\n",
      "epoch 83 | loss: 0.03329 | val_0_rmse: 0.21388 |  0:00:21s\n",
      "epoch 84 | loss: 0.04101 | val_0_rmse: 0.22753 |  0:00:21s\n",
      "epoch 85 | loss: 0.03588 | val_0_rmse: 0.21662 |  0:00:22s\n",
      "epoch 86 | loss: 0.03734 | val_0_rmse: 0.23438 |  0:00:22s\n",
      "epoch 87 | loss: 0.04526 | val_0_rmse: 0.23161 |  0:00:22s\n",
      "epoch 88 | loss: 0.04617 | val_0_rmse: 0.21291 |  0:00:22s\n",
      "epoch 89 | loss: 0.03173 | val_0_rmse: 0.21964 |  0:00:23s\n",
      "epoch 90 | loss: 0.03662 | val_0_rmse: 0.22087 |  0:00:23s\n",
      "epoch 91 | loss: 0.03196 | val_0_rmse: 0.21185 |  0:00:23s\n",
      "epoch 92 | loss: 0.02845 | val_0_rmse: 0.22831 |  0:00:23s\n",
      "epoch 93 | loss: 0.03286 | val_0_rmse: 0.21726 |  0:00:24s\n",
      "epoch 94 | loss: 0.03171 | val_0_rmse: 0.21432 |  0:00:24s\n",
      "epoch 95 | loss: 0.02992 | val_0_rmse: 0.21702 |  0:00:24s\n",
      "epoch 96 | loss: 0.03186 | val_0_rmse: 0.23808 |  0:00:24s\n",
      "epoch 97 | loss: 0.05249 | val_0_rmse: 0.22854 |  0:00:25s\n",
      "epoch 98 | loss: 0.0353  | val_0_rmse: 0.21719 |  0:00:25s\n",
      "epoch 99 | loss: 0.02996 | val_0_rmse: 0.21983 |  0:00:25s\n",
      "epoch 100| loss: 0.03191 | val_0_rmse: 0.22434 |  0:00:25s\n",
      "epoch 101| loss: 0.02802 | val_0_rmse: 0.22001 |  0:00:26s\n",
      "epoch 102| loss: 0.03172 | val_0_rmse: 0.21945 |  0:00:26s\n",
      "epoch 103| loss: 0.03772 | val_0_rmse: 0.25946 |  0:00:26s\n",
      "epoch 104| loss: 0.03622 | val_0_rmse: 0.25676 |  0:00:27s\n",
      "epoch 105| loss: 0.05346 | val_0_rmse: 0.24883 |  0:00:27s\n",
      "epoch 106| loss: 0.04392 | val_0_rmse: 0.24789 |  0:00:27s\n",
      "epoch 107| loss: 0.0403  | val_0_rmse: 0.27087 |  0:00:28s\n",
      "epoch 108| loss: 0.04978 | val_0_rmse: 0.24273 |  0:00:28s\n",
      "epoch 109| loss: 0.04896 | val_0_rmse: 0.22347 |  0:00:29s\n",
      "epoch 110| loss: 0.03415 | val_0_rmse: 0.21953 |  0:00:29s\n",
      "epoch 111| loss: 0.0334  | val_0_rmse: 0.21207 |  0:00:30s\n",
      "epoch 112| loss: 0.0305  | val_0_rmse: 0.222   |  0:00:30s\n",
      "epoch 113| loss: 0.02926 | val_0_rmse: 0.21153 |  0:00:31s\n",
      "epoch 114| loss: 0.03223 | val_0_rmse: 0.20197 |  0:00:31s\n",
      "epoch 115| loss: 0.02524 | val_0_rmse: 0.20543 |  0:00:32s\n",
      "epoch 116| loss: 0.02745 | val_0_rmse: 0.21153 |  0:00:32s\n",
      "epoch 117| loss: 0.031   | val_0_rmse: 0.20902 |  0:00:33s\n",
      "epoch 118| loss: 0.03055 | val_0_rmse: 0.20555 |  0:00:33s\n",
      "epoch 119| loss: 0.02868 | val_0_rmse: 0.20987 |  0:00:34s\n",
      "epoch 120| loss: 0.02711 | val_0_rmse: 0.21326 |  0:00:34s\n",
      "epoch 121| loss: 0.0265  | val_0_rmse: 0.21793 |  0:00:35s\n",
      "epoch 122| loss: 0.0281  | val_0_rmse: 0.21197 |  0:00:35s\n",
      "epoch 123| loss: 0.02652 | val_0_rmse: 0.20965 |  0:00:36s\n",
      "epoch 124| loss: 0.02732 | val_0_rmse: 0.21515 |  0:00:36s\n",
      "epoch 125| loss: 0.02909 | val_0_rmse: 0.20317 |  0:00:36s\n",
      "epoch 126| loss: 0.02541 | val_0_rmse: 0.21448 |  0:00:37s\n",
      "epoch 127| loss: 0.02759 | val_0_rmse: 0.22072 |  0:00:37s\n",
      "epoch 128| loss: 0.02938 | val_0_rmse: 0.21081 |  0:00:38s\n",
      "epoch 129| loss: 0.02636 | val_0_rmse: 0.22464 |  0:00:38s\n",
      "epoch 130| loss: 0.02477 | val_0_rmse: 0.19951 |  0:00:39s\n",
      "epoch 131| loss: 0.02801 | val_0_rmse: 0.20227 |  0:00:39s\n",
      "epoch 132| loss: 0.02528 | val_0_rmse: 0.20976 |  0:00:39s\n",
      "epoch 133| loss: 0.02573 | val_0_rmse: 0.20072 |  0:00:40s\n",
      "epoch 134| loss: 0.02274 | val_0_rmse: 0.20419 |  0:00:40s\n",
      "epoch 135| loss: 0.02393 | val_0_rmse: 0.20282 |  0:00:41s\n",
      "epoch 136| loss: 0.02722 | val_0_rmse: 0.20948 |  0:00:41s\n",
      "epoch 137| loss: 0.02856 | val_0_rmse: 0.21144 |  0:00:41s\n",
      "epoch 138| loss: 0.02627 | val_0_rmse: 0.19644 |  0:00:41s\n",
      "epoch 139| loss: 0.02328 | val_0_rmse: 0.19849 |  0:00:42s\n",
      "epoch 140| loss: 0.02622 | val_0_rmse: 0.19926 |  0:00:42s\n",
      "epoch 141| loss: 0.02617 | val_0_rmse: 0.21829 |  0:00:42s\n",
      "epoch 142| loss: 0.02862 | val_0_rmse: 0.21177 |  0:00:43s\n",
      "epoch 143| loss: 0.03025 | val_0_rmse: 0.2094  |  0:00:43s\n",
      "epoch 144| loss: 0.02607 | val_0_rmse: 0.20747 |  0:00:43s\n",
      "epoch 145| loss: 0.02931 | val_0_rmse: 0.2139  |  0:00:44s\n",
      "epoch 146| loss: 0.03214 | val_0_rmse: 0.22034 |  0:00:44s\n",
      "epoch 147| loss: 0.02483 | val_0_rmse: 0.21868 |  0:00:44s\n",
      "epoch 148| loss: 0.03286 | val_0_rmse: 0.23157 |  0:00:44s\n",
      "epoch 149| loss: 0.03036 | val_0_rmse: 0.22532 |  0:00:45s\n",
      "epoch 150| loss: 0.04143 | val_0_rmse: 0.24441 |  0:00:45s\n",
      "epoch 151| loss: 0.04985 | val_0_rmse: 0.22448 |  0:00:45s\n",
      "epoch 152| loss: 0.05713 | val_0_rmse: 0.27217 |  0:00:45s\n",
      "epoch 153| loss: 0.09099 | val_0_rmse: 0.21361 |  0:00:45s\n",
      "epoch 154| loss: 0.06574 | val_0_rmse: 0.30192 |  0:00:46s\n",
      "epoch 155| loss: 0.07651 | val_0_rmse: 0.24186 |  0:00:46s\n",
      "epoch 156| loss: 0.04995 | val_0_rmse: 0.23487 |  0:00:46s\n",
      "epoch 157| loss: 0.0669  | val_0_rmse: 0.19077 |  0:00:46s\n",
      "epoch 158| loss: 0.03282 | val_0_rmse: 0.19083 |  0:00:47s\n",
      "epoch 159| loss: 0.02856 | val_0_rmse: 0.19037 |  0:00:47s\n",
      "epoch 160| loss: 0.0242  | val_0_rmse: 0.19641 |  0:00:47s\n",
      "epoch 161| loss: 0.02628 | val_0_rmse: 0.19381 |  0:00:47s\n",
      "epoch 162| loss: 0.02343 | val_0_rmse: 0.20334 |  0:00:47s\n",
      "epoch 163| loss: 0.0264  | val_0_rmse: 0.20111 |  0:00:48s\n",
      "epoch 164| loss: 0.02387 | val_0_rmse: 0.19755 |  0:00:48s\n",
      "epoch 165| loss: 0.0209  | val_0_rmse: 0.1824  |  0:00:48s\n",
      "epoch 166| loss: 0.02593 | val_0_rmse: 0.19596 |  0:00:48s\n",
      "epoch 167| loss: 0.02344 | val_0_rmse: 0.19897 |  0:00:49s\n",
      "epoch 168| loss: 0.02392 | val_0_rmse: 0.19269 |  0:00:49s\n",
      "epoch 169| loss: 0.0254  | val_0_rmse: 0.21585 |  0:00:49s\n",
      "epoch 170| loss: 0.0236  | val_0_rmse: 0.19431 |  0:00:49s\n",
      "epoch 171| loss: 0.02482 | val_0_rmse: 0.19353 |  0:00:50s\n",
      "epoch 172| loss: 0.02372 | val_0_rmse: 0.21082 |  0:00:50s\n",
      "epoch 173| loss: 0.02362 | val_0_rmse: 0.20008 |  0:00:50s\n",
      "epoch 174| loss: 0.02896 | val_0_rmse: 0.20454 |  0:00:50s\n",
      "epoch 175| loss: 0.02541 | val_0_rmse: 0.2066  |  0:00:51s\n",
      "epoch 176| loss: 0.02089 | val_0_rmse: 0.20466 |  0:00:51s\n",
      "epoch 177| loss: 0.02687 | val_0_rmse: 0.20199 |  0:00:51s\n",
      "epoch 178| loss: 0.02611 | val_0_rmse: 0.20379 |  0:00:51s\n",
      "epoch 179| loss: 0.02677 | val_0_rmse: 0.19835 |  0:00:51s\n",
      "epoch 180| loss: 0.01985 | val_0_rmse: 0.19831 |  0:00:52s\n",
      "epoch 181| loss: 0.02696 | val_0_rmse: 0.19928 |  0:00:52s\n",
      "epoch 182| loss: 0.02459 | val_0_rmse: 0.20634 |  0:00:52s\n",
      "epoch 183| loss: 0.02207 | val_0_rmse: 0.18413 |  0:00:52s\n",
      "epoch 184| loss: 0.01856 | val_0_rmse: 0.19917 |  0:00:53s\n",
      "epoch 185| loss: 0.02132 | val_0_rmse: 0.19348 |  0:00:53s\n",
      "epoch 186| loss: 0.0187  | val_0_rmse: 0.18775 |  0:00:53s\n",
      "epoch 187| loss: 0.02197 | val_0_rmse: 0.19505 |  0:00:53s\n",
      "epoch 188| loss: 0.01872 | val_0_rmse: 0.18855 |  0:00:54s\n",
      "epoch 189| loss: 0.02171 | val_0_rmse: 0.21264 |  0:00:54s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:32:08,938] Trial 56 finished with value: 0.18240421294744488 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2134801974380751, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.016568589209497286, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 190| loss: 0.03067 | val_0_rmse: 0.19272 |  0:00:54s\n",
      "\n",
      "Early stopping occurred at epoch 190 with best_epoch = 165 and best_val_0_rmse = 0.1824\n",
      "Trial 056 | rmse_log=0.18240 | RMSE$=34,798 | MAE$=22,032 | MAPE=13.29% | n_d/n_a=24/16 steps=3 lr=0.01657 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.06891| val_0_rmse: 10.95127|  0:00:00s\n",
      "epoch 1  | loss: 83.59657| val_0_rmse: 9.60655 |  0:00:00s\n",
      "epoch 2  | loss: 54.51024| val_0_rmse: 7.99876 |  0:00:00s\n",
      "epoch 3  | loss: 32.66063| val_0_rmse: 6.28382 |  0:00:00s\n",
      "epoch 4  | loss: 17.32724| val_0_rmse: 4.80874 |  0:00:01s\n",
      "epoch 5  | loss: 14.35131| val_0_rmse: 3.70502 |  0:00:01s\n",
      "epoch 6  | loss: 12.75582| val_0_rmse: 3.51626 |  0:00:01s\n",
      "epoch 7  | loss: 7.37107 | val_0_rmse: 3.61828 |  0:00:01s\n",
      "epoch 8  | loss: 3.6423  | val_0_rmse: 3.60366 |  0:00:02s\n",
      "epoch 9  | loss: 2.72023 | val_0_rmse: 2.9513  |  0:00:02s\n",
      "epoch 10 | loss: 1.74231 | val_0_rmse: 2.12297 |  0:00:02s\n",
      "epoch 11 | loss: 1.3441  | val_0_rmse: 2.18798 |  0:00:02s\n",
      "epoch 12 | loss: 1.05551 | val_0_rmse: 1.8923  |  0:00:03s\n",
      "epoch 13 | loss: 0.68009 | val_0_rmse: 1.71674 |  0:00:03s\n",
      "epoch 14 | loss: 0.57772 | val_0_rmse: 1.26635 |  0:00:03s\n",
      "epoch 15 | loss: 0.40599 | val_0_rmse: 1.5127  |  0:00:03s\n",
      "epoch 16 | loss: 0.46692 | val_0_rmse: 1.04253 |  0:00:04s\n",
      "epoch 17 | loss: 0.35142 | val_0_rmse: 0.94229 |  0:00:04s\n",
      "epoch 18 | loss: 0.23438 | val_0_rmse: 0.78016 |  0:00:04s\n",
      "epoch 19 | loss: 0.22204 | val_0_rmse: 0.80718 |  0:00:05s\n",
      "epoch 20 | loss: 0.20562 | val_0_rmse: 0.64443 |  0:00:05s\n",
      "epoch 21 | loss: 0.20895 | val_0_rmse: 0.583   |  0:00:05s\n",
      "epoch 22 | loss: 0.18685 | val_0_rmse: 0.49792 |  0:00:05s\n",
      "epoch 23 | loss: 0.2084  | val_0_rmse: 0.33303 |  0:00:06s\n",
      "epoch 24 | loss: 0.16588 | val_0_rmse: 0.41823 |  0:00:06s\n",
      "epoch 25 | loss: 0.16723 | val_0_rmse: 0.33421 |  0:00:06s\n",
      "epoch 26 | loss: 0.1115  | val_0_rmse: 0.30416 |  0:00:06s\n",
      "epoch 27 | loss: 0.16186 | val_0_rmse: 0.38627 |  0:00:07s\n",
      "epoch 28 | loss: 0.11157 | val_0_rmse: 0.32557 |  0:00:07s\n",
      "epoch 29 | loss: 0.10158 | val_0_rmse: 0.26056 |  0:00:07s\n",
      "epoch 30 | loss: 0.11897 | val_0_rmse: 0.35381 |  0:00:07s\n",
      "epoch 31 | loss: 0.1093  | val_0_rmse: 0.28478 |  0:00:07s\n",
      "epoch 32 | loss: 0.09468 | val_0_rmse: 0.27659 |  0:00:08s\n",
      "epoch 33 | loss: 0.09262 | val_0_rmse: 0.26997 |  0:00:08s\n",
      "epoch 34 | loss: 0.10551 | val_0_rmse: 0.30396 |  0:00:08s\n",
      "epoch 35 | loss: 0.10258 | val_0_rmse: 0.41649 |  0:00:08s\n",
      "epoch 36 | loss: 0.0956  | val_0_rmse: 0.25159 |  0:00:09s\n",
      "epoch 37 | loss: 0.13324 | val_0_rmse: 0.31224 |  0:00:09s\n",
      "epoch 38 | loss: 0.09119 | val_0_rmse: 0.24211 |  0:00:09s\n",
      "epoch 39 | loss: 0.08259 | val_0_rmse: 0.37585 |  0:00:09s\n",
      "epoch 40 | loss: 0.10434 | val_0_rmse: 0.23599 |  0:00:10s\n",
      "epoch 41 | loss: 0.06598 | val_0_rmse: 0.27482 |  0:00:10s\n",
      "epoch 42 | loss: 0.07682 | val_0_rmse: 0.25677 |  0:00:10s\n",
      "epoch 43 | loss: 0.08607 | val_0_rmse: 0.27854 |  0:00:10s\n",
      "epoch 44 | loss: 0.08964 | val_0_rmse: 0.2613  |  0:00:11s\n",
      "epoch 45 | loss: 0.09259 | val_0_rmse: 0.23804 |  0:00:11s\n",
      "epoch 46 | loss: 0.09552 | val_0_rmse: 0.22297 |  0:00:11s\n",
      "epoch 47 | loss: 0.06783 | val_0_rmse: 0.22293 |  0:00:11s\n",
      "epoch 48 | loss: 0.07264 | val_0_rmse: 0.21354 |  0:00:12s\n",
      "epoch 49 | loss: 0.08031 | val_0_rmse: 0.24022 |  0:00:12s\n",
      "epoch 50 | loss: 0.06726 | val_0_rmse: 0.2763  |  0:00:12s\n",
      "epoch 51 | loss: 0.07611 | val_0_rmse: 0.32125 |  0:00:12s\n",
      "epoch 52 | loss: 0.13439 | val_0_rmse: 0.21711 |  0:00:13s\n",
      "epoch 53 | loss: 0.05598 | val_0_rmse: 0.25168 |  0:00:13s\n",
      "epoch 54 | loss: 0.06005 | val_0_rmse: 0.21773 |  0:00:13s\n",
      "epoch 55 | loss: 0.06    | val_0_rmse: 0.23386 |  0:00:13s\n",
      "epoch 56 | loss: 0.05019 | val_0_rmse: 0.2515  |  0:00:14s\n",
      "epoch 57 | loss: 0.0437  | val_0_rmse: 0.22371 |  0:00:14s\n",
      "epoch 58 | loss: 0.04343 | val_0_rmse: 0.21663 |  0:00:14s\n",
      "epoch 59 | loss: 0.05134 | val_0_rmse: 0.21921 |  0:00:14s\n",
      "epoch 60 | loss: 0.04656 | val_0_rmse: 0.22083 |  0:00:15s\n",
      "epoch 61 | loss: 0.04338 | val_0_rmse: 0.22137 |  0:00:15s\n",
      "epoch 62 | loss: 0.0422  | val_0_rmse: 0.21853 |  0:00:15s\n",
      "epoch 63 | loss: 0.05077 | val_0_rmse: 0.20753 |  0:00:15s\n",
      "epoch 64 | loss: 0.05131 | val_0_rmse: 0.22973 |  0:00:16s\n",
      "epoch 65 | loss: 0.04794 | val_0_rmse: 0.20442 |  0:00:16s\n",
      "epoch 66 | loss: 0.0483  | val_0_rmse: 0.2241  |  0:00:16s\n",
      "epoch 67 | loss: 0.04449 | val_0_rmse: 0.24447 |  0:00:16s\n",
      "epoch 68 | loss: 0.06506 | val_0_rmse: 0.19874 |  0:00:16s\n",
      "epoch 69 | loss: 0.05228 | val_0_rmse: 0.23872 |  0:00:17s\n",
      "epoch 70 | loss: 0.04836 | val_0_rmse: 0.23877 |  0:00:17s\n",
      "epoch 71 | loss: 0.05902 | val_0_rmse: 0.26113 |  0:00:17s\n",
      "epoch 72 | loss: 0.07995 | val_0_rmse: 0.23031 |  0:00:18s\n",
      "epoch 73 | loss: 0.04779 | val_0_rmse: 0.27787 |  0:00:18s\n",
      "epoch 74 | loss: 0.06344 | val_0_rmse: 0.25397 |  0:00:18s\n",
      "epoch 75 | loss: 0.05318 | val_0_rmse: 0.22128 |  0:00:18s\n",
      "epoch 76 | loss: 0.05321 | val_0_rmse: 0.26264 |  0:00:19s\n",
      "epoch 77 | loss: 0.06469 | val_0_rmse: 0.23346 |  0:00:19s\n",
      "epoch 78 | loss: 0.05451 | val_0_rmse: 0.23738 |  0:00:19s\n",
      "epoch 79 | loss: 0.04593 | val_0_rmse: 0.26591 |  0:00:19s\n",
      "epoch 80 | loss: 0.07085 | val_0_rmse: 0.22764 |  0:00:20s\n",
      "epoch 81 | loss: 0.03877 | val_0_rmse: 0.22885 |  0:00:20s\n",
      "epoch 82 | loss: 0.05447 | val_0_rmse: 0.2497  |  0:00:20s\n",
      "epoch 83 | loss: 0.05709 | val_0_rmse: 0.23415 |  0:00:20s\n",
      "epoch 84 | loss: 0.0523  | val_0_rmse: 0.23175 |  0:00:20s\n",
      "epoch 85 | loss: 0.05137 | val_0_rmse: 0.1976  |  0:00:21s\n",
      "epoch 86 | loss: 0.04068 | val_0_rmse: 0.21227 |  0:00:21s\n",
      "epoch 87 | loss: 0.04192 | val_0_rmse: 0.21988 |  0:00:21s\n",
      "epoch 88 | loss: 0.03721 | val_0_rmse: 0.22888 |  0:00:21s\n",
      "epoch 89 | loss: 0.05015 | val_0_rmse: 0.21604 |  0:00:22s\n",
      "epoch 90 | loss: 0.0385  | val_0_rmse: 0.20349 |  0:00:22s\n",
      "epoch 91 | loss: 0.03749 | val_0_rmse: 0.19074 |  0:00:22s\n",
      "epoch 92 | loss: 0.03447 | val_0_rmse: 0.25256 |  0:00:23s\n",
      "epoch 93 | loss: 0.04328 | val_0_rmse: 0.20295 |  0:00:23s\n",
      "epoch 94 | loss: 0.04039 | val_0_rmse: 0.19097 |  0:00:23s\n",
      "epoch 95 | loss: 0.03258 | val_0_rmse: 0.20788 |  0:00:23s\n",
      "epoch 96 | loss: 0.0307  | val_0_rmse: 0.19072 |  0:00:24s\n",
      "epoch 97 | loss: 0.03162 | val_0_rmse: 0.18251 |  0:00:24s\n",
      "epoch 98 | loss: 0.03027 | val_0_rmse: 0.18398 |  0:00:24s\n",
      "epoch 99 | loss: 0.02805 | val_0_rmse: 0.20014 |  0:00:24s\n",
      "epoch 100| loss: 0.0381  | val_0_rmse: 0.19861 |  0:00:24s\n",
      "epoch 101| loss: 0.03921 | val_0_rmse: 0.19485 |  0:00:25s\n",
      "epoch 102| loss: 0.03203 | val_0_rmse: 0.20255 |  0:00:25s\n",
      "epoch 103| loss: 0.03657 | val_0_rmse: 0.19419 |  0:00:25s\n",
      "epoch 104| loss: 0.02736 | val_0_rmse: 0.1891  |  0:00:25s\n",
      "epoch 105| loss: 0.02872 | val_0_rmse: 0.18277 |  0:00:26s\n",
      "epoch 106| loss: 0.02911 | val_0_rmse: 0.19426 |  0:00:26s\n",
      "epoch 107| loss: 0.02617 | val_0_rmse: 0.19092 |  0:00:26s\n",
      "epoch 108| loss: 0.03175 | val_0_rmse: 0.21536 |  0:00:26s\n",
      "epoch 109| loss: 0.03421 | val_0_rmse: 0.19237 |  0:00:27s\n",
      "epoch 110| loss: 0.03276 | val_0_rmse: 0.21329 |  0:00:27s\n",
      "epoch 111| loss: 0.04276 | val_0_rmse: 0.20954 |  0:00:27s\n",
      "epoch 112| loss: 0.03916 | val_0_rmse: 0.20126 |  0:00:27s\n",
      "epoch 113| loss: 0.03236 | val_0_rmse: 0.21101 |  0:00:28s\n",
      "epoch 114| loss: 0.0296  | val_0_rmse: 0.19686 |  0:00:28s\n",
      "epoch 115| loss: 0.02937 | val_0_rmse: 0.18408 |  0:00:28s\n",
      "epoch 116| loss: 0.02607 | val_0_rmse: 0.18885 |  0:00:28s\n",
      "epoch 117| loss: 0.02786 | val_0_rmse: 0.18297 |  0:00:28s\n",
      "epoch 118| loss: 0.03252 | val_0_rmse: 0.19355 |  0:00:29s\n",
      "epoch 119| loss: 0.0365  | val_0_rmse: 0.22285 |  0:00:29s\n",
      "epoch 120| loss: 0.03228 | val_0_rmse: 0.1832  |  0:00:29s\n",
      "epoch 121| loss: 0.0307  | val_0_rmse: 0.18167 |  0:00:29s\n",
      "epoch 122| loss: 0.02584 | val_0_rmse: 0.18844 |  0:00:30s\n",
      "epoch 123| loss: 0.0294  | val_0_rmse: 0.20397 |  0:00:30s\n",
      "epoch 124| loss: 0.02503 | val_0_rmse: 0.18834 |  0:00:30s\n",
      "epoch 125| loss: 0.03144 | val_0_rmse: 0.17527 |  0:00:30s\n",
      "epoch 126| loss: 0.02461 | val_0_rmse: 0.19304 |  0:00:31s\n",
      "epoch 127| loss: 0.02444 | val_0_rmse: 0.18028 |  0:00:31s\n",
      "epoch 128| loss: 0.02661 | val_0_rmse: 0.20848 |  0:00:31s\n",
      "epoch 129| loss: 0.03609 | val_0_rmse: 0.18173 |  0:00:31s\n",
      "epoch 130| loss: 0.02741 | val_0_rmse: 0.18409 |  0:00:32s\n",
      "epoch 131| loss: 0.02607 | val_0_rmse: 0.18755 |  0:00:32s\n",
      "epoch 132| loss: 0.02333 | val_0_rmse: 0.18206 |  0:00:32s\n",
      "epoch 133| loss: 0.02565 | val_0_rmse: 0.18392 |  0:00:32s\n",
      "epoch 134| loss: 0.02405 | val_0_rmse: 0.18956 |  0:00:32s\n",
      "epoch 135| loss: 0.02495 | val_0_rmse: 0.18137 |  0:00:33s\n",
      "epoch 136| loss: 0.02051 | val_0_rmse: 0.18768 |  0:00:33s\n",
      "epoch 137| loss: 0.02165 | val_0_rmse: 0.18118 |  0:00:33s\n",
      "epoch 138| loss: 0.02182 | val_0_rmse: 0.19351 |  0:00:33s\n",
      "epoch 139| loss: 0.02457 | val_0_rmse: 0.183   |  0:00:33s\n",
      "epoch 140| loss: 0.02272 | val_0_rmse: 0.19448 |  0:00:34s\n",
      "epoch 141| loss: 0.02931 | val_0_rmse: 0.19212 |  0:00:34s\n",
      "epoch 142| loss: 0.02528 | val_0_rmse: 0.18918 |  0:00:34s\n",
      "epoch 143| loss: 0.02815 | val_0_rmse: 0.19211 |  0:00:34s\n",
      "epoch 144| loss: 0.02224 | val_0_rmse: 0.18654 |  0:00:34s\n",
      "epoch 145| loss: 0.02083 | val_0_rmse: 0.19933 |  0:00:35s\n",
      "epoch 146| loss: 0.02337 | val_0_rmse: 0.19826 |  0:00:35s\n",
      "epoch 147| loss: 0.03138 | val_0_rmse: 0.18985 |  0:00:35s\n",
      "epoch 148| loss: 0.02835 | val_0_rmse: 0.19635 |  0:00:35s\n",
      "epoch 149| loss: 0.02348 | val_0_rmse: 0.18959 |  0:00:36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:32:45,723] Trial 57 finished with value: 0.17527130438519692 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.232331506646917, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.01823761229899789, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 150| loss: 0.02474 | val_0_rmse: 0.18791 |  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 150 with best_epoch = 125 and best_val_0_rmse = 0.17527\n",
      "Trial 057 | rmse_log=0.17527 | RMSE$=32,794 | MAE$=22,151 | MAPE=13.15% | n_d/n_a=24/16 steps=3 lr=0.01824 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 132.09183| val_0_rmse: 11.39344|  0:00:00s\n",
      "epoch 1  | loss: 108.2617| val_0_rmse: 10.90451|  0:00:00s\n",
      "epoch 2  | loss: 90.22478| val_0_rmse: 10.36245|  0:00:00s\n",
      "epoch 3  | loss: 72.4495 | val_0_rmse: 9.77181 |  0:00:00s\n",
      "epoch 4  | loss: 56.54852| val_0_rmse: 9.14707 |  0:00:01s\n",
      "epoch 5  | loss: 44.99265| val_0_rmse: 8.40933 |  0:00:01s\n",
      "epoch 6  | loss: 33.85454| val_0_rmse: 7.44797 |  0:00:01s\n",
      "epoch 7  | loss: 22.20258| val_0_rmse: 6.34409 |  0:00:01s\n",
      "epoch 8  | loss: 16.66563| val_0_rmse: 5.09167 |  0:00:01s\n",
      "epoch 9  | loss: 13.84965| val_0_rmse: 3.99566 |  0:00:02s\n",
      "epoch 10 | loss: 11.50141| val_0_rmse: 3.17032 |  0:00:02s\n",
      "epoch 11 | loss: 11.15751| val_0_rmse: 2.60688 |  0:00:02s\n",
      "epoch 12 | loss: 8.43268 | val_0_rmse: 2.44635 |  0:00:02s\n",
      "epoch 13 | loss: 6.60475 | val_0_rmse: 2.5403  |  0:00:02s\n",
      "epoch 14 | loss: 4.59937 | val_0_rmse: 2.80857 |  0:00:03s\n",
      "epoch 15 | loss: 4.56129 | val_0_rmse: 2.83186 |  0:00:03s\n",
      "epoch 16 | loss: 3.42612 | val_0_rmse: 2.55211 |  0:00:03s\n",
      "epoch 17 | loss: 2.71973 | val_0_rmse: 2.01222 |  0:00:03s\n",
      "epoch 18 | loss: 1.78126 | val_0_rmse: 1.52021 |  0:00:03s\n",
      "epoch 19 | loss: 1.63609 | val_0_rmse: 1.51182 |  0:00:04s\n",
      "epoch 20 | loss: 1.22959 | val_0_rmse: 1.65802 |  0:00:04s\n",
      "epoch 21 | loss: 1.13833 | val_0_rmse: 1.48974 |  0:00:04s\n",
      "epoch 22 | loss: 0.80754 | val_0_rmse: 1.03335 |  0:00:04s\n",
      "epoch 23 | loss: 0.77053 | val_0_rmse: 0.96244 |  0:00:04s\n",
      "epoch 24 | loss: 0.6305  | val_0_rmse: 1.05428 |  0:00:05s\n",
      "epoch 25 | loss: 0.81033 | val_0_rmse: 1.02151 |  0:00:05s\n",
      "epoch 26 | loss: 0.6208  | val_0_rmse: 0.73249 |  0:00:05s\n",
      "epoch 27 | loss: 0.55075 | val_0_rmse: 0.78547 |  0:00:05s\n",
      "epoch 28 | loss: 0.48207 | val_0_rmse: 0.74939 |  0:00:05s\n",
      "epoch 29 | loss: 0.55341 | val_0_rmse: 0.42167 |  0:00:06s\n",
      "epoch 30 | loss: 0.39199 | val_0_rmse: 0.44531 |  0:00:06s\n",
      "epoch 31 | loss: 0.36796 | val_0_rmse: 0.67168 |  0:00:06s\n",
      "epoch 32 | loss: 0.34893 | val_0_rmse: 0.46818 |  0:00:06s\n",
      "epoch 33 | loss: 0.36465 | val_0_rmse: 0.48395 |  0:00:06s\n",
      "epoch 34 | loss: 0.32547 | val_0_rmse: 0.55398 |  0:00:06s\n",
      "epoch 35 | loss: 0.29448 | val_0_rmse: 0.46417 |  0:00:07s\n",
      "epoch 36 | loss: 0.34264 | val_0_rmse: 0.48015 |  0:00:07s\n",
      "epoch 37 | loss: 0.2654  | val_0_rmse: 0.50352 |  0:00:07s\n",
      "epoch 38 | loss: 0.29038 | val_0_rmse: 0.37816 |  0:00:07s\n",
      "epoch 39 | loss: 0.22284 | val_0_rmse: 0.50997 |  0:00:08s\n",
      "epoch 40 | loss: 0.26037 | val_0_rmse: 0.57694 |  0:00:08s\n",
      "epoch 41 | loss: 0.23736 | val_0_rmse: 0.37622 |  0:00:08s\n",
      "epoch 42 | loss: 0.20443 | val_0_rmse: 0.4074  |  0:00:08s\n",
      "epoch 43 | loss: 0.18037 | val_0_rmse: 0.54122 |  0:00:08s\n",
      "epoch 44 | loss: 0.21856 | val_0_rmse: 0.47694 |  0:00:09s\n",
      "epoch 45 | loss: 0.15882 | val_0_rmse: 0.41673 |  0:00:09s\n",
      "epoch 46 | loss: 0.1874  | val_0_rmse: 0.48818 |  0:00:09s\n",
      "epoch 47 | loss: 0.22337 | val_0_rmse: 0.42803 |  0:00:09s\n",
      "epoch 48 | loss: 0.19008 | val_0_rmse: 0.38511 |  0:00:09s\n",
      "epoch 49 | loss: 0.2007  | val_0_rmse: 0.49237 |  0:00:10s\n",
      "epoch 50 | loss: 0.15732 | val_0_rmse: 0.35074 |  0:00:10s\n",
      "epoch 51 | loss: 0.13252 | val_0_rmse: 0.36318 |  0:00:10s\n",
      "epoch 52 | loss: 0.13477 | val_0_rmse: 0.33828 |  0:00:10s\n",
      "epoch 53 | loss: 0.13392 | val_0_rmse: 0.34608 |  0:00:10s\n",
      "epoch 54 | loss: 0.11696 | val_0_rmse: 0.34226 |  0:00:10s\n",
      "epoch 55 | loss: 0.1284  | val_0_rmse: 0.35457 |  0:00:11s\n",
      "epoch 56 | loss: 0.12639 | val_0_rmse: 0.31417 |  0:00:11s\n",
      "epoch 57 | loss: 0.10126 | val_0_rmse: 0.3117  |  0:00:11s\n",
      "epoch 58 | loss: 0.09471 | val_0_rmse: 0.29518 |  0:00:11s\n",
      "epoch 59 | loss: 0.09903 | val_0_rmse: 0.29457 |  0:00:12s\n",
      "epoch 60 | loss: 0.11792 | val_0_rmse: 0.34352 |  0:00:12s\n",
      "epoch 61 | loss: 0.13636 | val_0_rmse: 0.30996 |  0:00:12s\n",
      "epoch 62 | loss: 0.10379 | val_0_rmse: 0.35193 |  0:00:12s\n",
      "epoch 63 | loss: 0.15595 | val_0_rmse: 0.33848 |  0:00:12s\n",
      "epoch 64 | loss: 0.11888 | val_0_rmse: 0.3664  |  0:00:13s\n",
      "epoch 65 | loss: 0.15578 | val_0_rmse: 0.31478 |  0:00:13s\n",
      "epoch 66 | loss: 0.09804 | val_0_rmse: 0.36019 |  0:00:13s\n",
      "epoch 67 | loss: 0.16397 | val_0_rmse: 0.32985 |  0:00:13s\n",
      "epoch 68 | loss: 0.10416 | val_0_rmse: 0.36436 |  0:00:14s\n",
      "epoch 69 | loss: 0.1819  | val_0_rmse: 0.29458 |  0:00:14s\n",
      "epoch 70 | loss: 0.0903  | val_0_rmse: 0.3977  |  0:00:14s\n",
      "epoch 71 | loss: 0.1757  | val_0_rmse: 0.3498  |  0:00:14s\n",
      "epoch 72 | loss: 0.10465 | val_0_rmse: 0.31871 |  0:00:14s\n",
      "epoch 73 | loss: 0.13022 | val_0_rmse: 0.28901 |  0:00:15s\n",
      "epoch 74 | loss: 0.07386 | val_0_rmse: 0.35924 |  0:00:15s\n",
      "epoch 75 | loss: 0.12919 | val_0_rmse: 0.28008 |  0:00:15s\n",
      "epoch 76 | loss: 0.07238 | val_0_rmse: 0.32018 |  0:00:15s\n",
      "epoch 77 | loss: 0.11116 | val_0_rmse: 0.33809 |  0:00:15s\n",
      "epoch 78 | loss: 0.08593 | val_0_rmse: 0.30292 |  0:00:16s\n",
      "epoch 79 | loss: 0.07617 | val_0_rmse: 0.27844 |  0:00:16s\n",
      "epoch 80 | loss: 0.08305 | val_0_rmse: 0.28484 |  0:00:16s\n",
      "epoch 81 | loss: 0.06812 | val_0_rmse: 0.2867  |  0:00:16s\n",
      "epoch 82 | loss: 0.06691 | val_0_rmse: 0.26705 |  0:00:17s\n",
      "epoch 83 | loss: 0.06992 | val_0_rmse: 0.28182 |  0:00:17s\n",
      "epoch 84 | loss: 0.05798 | val_0_rmse: 0.29986 |  0:00:17s\n",
      "epoch 85 | loss: 0.07033 | val_0_rmse: 0.26016 |  0:00:17s\n",
      "epoch 86 | loss: 0.06631 | val_0_rmse: 0.27653 |  0:00:18s\n",
      "epoch 87 | loss: 0.06802 | val_0_rmse: 0.27966 |  0:00:18s\n",
      "epoch 88 | loss: 0.06505 | val_0_rmse: 0.25781 |  0:00:18s\n",
      "epoch 89 | loss: 0.05991 | val_0_rmse: 0.30066 |  0:00:18s\n",
      "epoch 90 | loss: 0.09636 | val_0_rmse: 0.27807 |  0:00:19s\n",
      "epoch 91 | loss: 0.06438 | val_0_rmse: 0.28715 |  0:00:19s\n",
      "epoch 92 | loss: 0.09613 | val_0_rmse: 0.29282 |  0:00:19s\n",
      "epoch 93 | loss: 0.07066 | val_0_rmse: 0.27233 |  0:00:19s\n",
      "epoch 94 | loss: 0.04981 | val_0_rmse: 0.27868 |  0:00:20s\n",
      "epoch 95 | loss: 0.07115 | val_0_rmse: 0.27423 |  0:00:20s\n",
      "epoch 96 | loss: 0.05289 | val_0_rmse: 0.31517 |  0:00:20s\n",
      "epoch 97 | loss: 0.06188 | val_0_rmse: 0.25895 |  0:00:20s\n",
      "epoch 98 | loss: 0.05963 | val_0_rmse: 0.25067 |  0:00:20s\n",
      "epoch 99 | loss: 0.0598  | val_0_rmse: 0.2508  |  0:00:21s\n",
      "epoch 100| loss: 0.06932 | val_0_rmse: 0.25633 |  0:00:21s\n",
      "epoch 101| loss: 0.05807 | val_0_rmse: 0.24846 |  0:00:21s\n",
      "epoch 102| loss: 0.0624  | val_0_rmse: 0.31914 |  0:00:21s\n",
      "epoch 103| loss: 0.09976 | val_0_rmse: 0.34772 |  0:00:22s\n",
      "epoch 104| loss: 0.11422 | val_0_rmse: 0.25212 |  0:00:22s\n",
      "epoch 105| loss: 0.08962 | val_0_rmse: 0.29943 |  0:00:22s\n",
      "epoch 106| loss: 0.13585 | val_0_rmse: 0.26851 |  0:00:22s\n",
      "epoch 107| loss: 0.05611 | val_0_rmse: 0.28619 |  0:00:22s\n",
      "epoch 108| loss: 0.06779 | val_0_rmse: 0.24486 |  0:00:22s\n",
      "epoch 109| loss: 0.07069 | val_0_rmse: 0.24503 |  0:00:23s\n",
      "epoch 110| loss: 0.06508 | val_0_rmse: 0.26768 |  0:00:23s\n",
      "epoch 111| loss: 0.05932 | val_0_rmse: 0.23347 |  0:00:23s\n",
      "epoch 112| loss: 0.04311 | val_0_rmse: 0.23421 |  0:00:23s\n",
      "epoch 113| loss: 0.04529 | val_0_rmse: 0.24612 |  0:00:23s\n",
      "epoch 114| loss: 0.04105 | val_0_rmse: 0.23436 |  0:00:24s\n",
      "epoch 115| loss: 0.04207 | val_0_rmse: 0.23821 |  0:00:24s\n",
      "epoch 116| loss: 0.04232 | val_0_rmse: 0.22831 |  0:00:24s\n",
      "epoch 117| loss: 0.04154 | val_0_rmse: 0.22406 |  0:00:24s\n",
      "epoch 118| loss: 0.03821 | val_0_rmse: 0.22861 |  0:00:24s\n",
      "epoch 119| loss: 0.04172 | val_0_rmse: 0.22193 |  0:00:25s\n",
      "epoch 120| loss: 0.04377 | val_0_rmse: 0.2378  |  0:00:25s\n",
      "epoch 121| loss: 0.04027 | val_0_rmse: 0.22099 |  0:00:25s\n",
      "epoch 122| loss: 0.0362  | val_0_rmse: 0.22063 |  0:00:25s\n",
      "epoch 123| loss: 0.03893 | val_0_rmse: 0.22444 |  0:00:25s\n",
      "epoch 124| loss: 0.04302 | val_0_rmse: 0.22941 |  0:00:26s\n",
      "epoch 125| loss: 0.04685 | val_0_rmse: 0.26042 |  0:00:26s\n",
      "epoch 126| loss: 0.04472 | val_0_rmse: 0.24746 |  0:00:26s\n",
      "epoch 127| loss: 0.04232 | val_0_rmse: 0.24642 |  0:00:26s\n",
      "epoch 128| loss: 0.03952 | val_0_rmse: 0.22333 |  0:00:26s\n",
      "epoch 129| loss: 0.03624 | val_0_rmse: 0.21636 |  0:00:27s\n",
      "epoch 130| loss: 0.04053 | val_0_rmse: 0.2204  |  0:00:27s\n",
      "epoch 131| loss: 0.03535 | val_0_rmse: 0.20973 |  0:00:27s\n",
      "epoch 132| loss: 0.03967 | val_0_rmse: 0.20945 |  0:00:27s\n",
      "epoch 133| loss: 0.03472 | val_0_rmse: 0.22609 |  0:00:27s\n",
      "epoch 134| loss: 0.03785 | val_0_rmse: 0.22496 |  0:00:28s\n",
      "epoch 135| loss: 0.04513 | val_0_rmse: 0.23676 |  0:00:28s\n",
      "epoch 136| loss: 0.0407  | val_0_rmse: 0.21158 |  0:00:28s\n",
      "epoch 137| loss: 0.03373 | val_0_rmse: 0.20746 |  0:00:28s\n",
      "epoch 138| loss: 0.0346  | val_0_rmse: 0.19857 |  0:00:28s\n",
      "epoch 139| loss: 0.03448 | val_0_rmse: 0.2054  |  0:00:29s\n",
      "epoch 140| loss: 0.03655 | val_0_rmse: 0.22767 |  0:00:29s\n",
      "epoch 141| loss: 0.03878 | val_0_rmse: 0.19961 |  0:00:29s\n",
      "epoch 142| loss: 0.03023 | val_0_rmse: 0.2012  |  0:00:29s\n",
      "epoch 143| loss: 0.03306 | val_0_rmse: 0.21408 |  0:00:29s\n",
      "epoch 144| loss: 0.04257 | val_0_rmse: 0.19506 |  0:00:30s\n",
      "epoch 145| loss: 0.02935 | val_0_rmse: 0.20707 |  0:00:30s\n",
      "epoch 146| loss: 0.03349 | val_0_rmse: 0.21017 |  0:00:30s\n",
      "epoch 147| loss: 0.03371 | val_0_rmse: 0.19756 |  0:00:30s\n",
      "epoch 148| loss: 0.03173 | val_0_rmse: 0.19449 |  0:00:30s\n",
      "epoch 149| loss: 0.03176 | val_0_rmse: 0.20566 |  0:00:31s\n",
      "epoch 150| loss: 0.03071 | val_0_rmse: 0.20434 |  0:00:31s\n",
      "epoch 151| loss: 0.03765 | val_0_rmse: 0.21162 |  0:00:31s\n",
      "epoch 152| loss: 0.03786 | val_0_rmse: 0.20299 |  0:00:31s\n",
      "epoch 153| loss: 0.04064 | val_0_rmse: 0.20313 |  0:00:31s\n",
      "epoch 154| loss: 0.0338  | val_0_rmse: 0.22243 |  0:00:32s\n",
      "epoch 155| loss: 0.03475 | val_0_rmse: 0.20814 |  0:00:32s\n",
      "epoch 156| loss: 0.04092 | val_0_rmse: 0.22662 |  0:00:32s\n",
      "epoch 157| loss: 0.03991 | val_0_rmse: 0.20611 |  0:00:32s\n",
      "epoch 158| loss: 0.03176 | val_0_rmse: 0.21069 |  0:00:33s\n",
      "epoch 159| loss: 0.03265 | val_0_rmse: 0.25379 |  0:00:33s\n",
      "epoch 160| loss: 0.06555 | val_0_rmse: 0.21806 |  0:00:33s\n",
      "epoch 161| loss: 0.0383  | val_0_rmse: 0.26175 |  0:00:33s\n",
      "epoch 162| loss: 0.07321 | val_0_rmse: 0.20728 |  0:00:33s\n",
      "epoch 163| loss: 0.03301 | val_0_rmse: 0.22302 |  0:00:34s\n",
      "epoch 164| loss: 0.04008 | val_0_rmse: 0.20315 |  0:00:34s\n",
      "epoch 165| loss: 0.03709 | val_0_rmse: 0.21588 |  0:00:34s\n",
      "epoch 166| loss: 0.03872 | val_0_rmse: 0.196   |  0:00:34s\n",
      "epoch 167| loss: 0.02976 | val_0_rmse: 0.2088  |  0:00:34s\n",
      "epoch 168| loss: 0.02997 | val_0_rmse: 0.2158  |  0:00:34s\n",
      "epoch 169| loss: 0.03384 | val_0_rmse: 0.20684 |  0:00:35s\n",
      "epoch 170| loss: 0.03279 | val_0_rmse: 0.22726 |  0:00:35s\n",
      "epoch 171| loss: 0.05581 | val_0_rmse: 0.21676 |  0:00:35s\n",
      "epoch 172| loss: 0.04512 | val_0_rmse: 0.24751 |  0:00:35s\n",
      "epoch 173| loss: 0.06748 | val_0_rmse: 0.20941 |  0:00:35s\n",
      "\n",
      "Early stopping occurred at epoch 173 with best_epoch = 148 and best_val_0_rmse = 0.19449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:33:22,066] Trial 58 finished with value: 0.1944908404575237 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2336154971813704, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.014582728761281414, 'batch_size': 1024, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 058 | rmse_log=0.19449 | RMSE$=38,629 | MAE$=24,641 | MAPE=14.36% | n_d/n_a=24/16 steps=3 lr=0.01458 batch=1024 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 126.83216| val_0_rmse: 11.28772|  0:00:00s\n",
      "epoch 1  | loss: 99.40835| val_0_rmse: 10.55261|  0:00:00s\n",
      "epoch 2  | loss: 73.47162| val_0_rmse: 9.69252 |  0:00:00s\n",
      "epoch 3  | loss: 54.02677| val_0_rmse: 8.65142 |  0:00:01s\n",
      "epoch 4  | loss: 36.09616| val_0_rmse: 7.45273 |  0:00:01s\n",
      "epoch 5  | loss: 22.46779| val_0_rmse: 6.1483  |  0:00:01s\n",
      "epoch 6  | loss: 16.49123| val_0_rmse: 4.95024 |  0:00:01s\n",
      "epoch 7  | loss: 13.5139 | val_0_rmse: 3.95824 |  0:00:02s\n",
      "epoch 8  | loss: 12.09025| val_0_rmse: 3.49259 |  0:00:02s\n",
      "epoch 9  | loss: 8.74999 | val_0_rmse: 3.3551  |  0:00:02s\n",
      "epoch 10 | loss: 6.26856 | val_0_rmse: 3.46932 |  0:00:02s\n",
      "epoch 11 | loss: 3.32419 | val_0_rmse: 3.25185 |  0:00:03s\n",
      "epoch 12 | loss: 2.14436 | val_0_rmse: 2.53473 |  0:00:03s\n",
      "epoch 13 | loss: 1.47036 | val_0_rmse: 2.09477 |  0:00:03s\n",
      "epoch 14 | loss: 0.99511 | val_0_rmse: 2.1979  |  0:00:03s\n",
      "epoch 15 | loss: 0.94504 | val_0_rmse: 1.68485 |  0:00:03s\n",
      "epoch 16 | loss: 0.61203 | val_0_rmse: 1.41811 |  0:00:04s\n",
      "epoch 17 | loss: 0.46449 | val_0_rmse: 1.06567 |  0:00:04s\n",
      "epoch 18 | loss: 0.38793 | val_0_rmse: 1.05186 |  0:00:04s\n",
      "epoch 19 | loss: 0.34403 | val_0_rmse: 0.82893 |  0:00:04s\n",
      "epoch 20 | loss: 0.2963  | val_0_rmse: 0.95112 |  0:00:05s\n",
      "epoch 21 | loss: 0.28712 | val_0_rmse: 0.64304 |  0:00:05s\n",
      "epoch 22 | loss: 0.31543 | val_0_rmse: 0.677   |  0:00:05s\n",
      "epoch 23 | loss: 0.23631 | val_0_rmse: 0.4673  |  0:00:05s\n",
      "epoch 24 | loss: 0.21539 | val_0_rmse: 0.649   |  0:00:06s\n",
      "epoch 25 | loss: 0.18599 | val_0_rmse: 0.48793 |  0:00:06s\n",
      "epoch 26 | loss: 0.16693 | val_0_rmse: 0.58036 |  0:00:06s\n",
      "epoch 27 | loss: 0.21151 | val_0_rmse: 0.3421  |  0:00:06s\n",
      "epoch 28 | loss: 0.21137 | val_0_rmse: 0.33331 |  0:00:06s\n",
      "epoch 29 | loss: 0.13846 | val_0_rmse: 0.39022 |  0:00:07s\n",
      "epoch 30 | loss: 0.13526 | val_0_rmse: 0.28971 |  0:00:07s\n",
      "epoch 31 | loss: 0.12597 | val_0_rmse: 0.34349 |  0:00:07s\n",
      "epoch 32 | loss: 0.1314  | val_0_rmse: 0.30954 |  0:00:07s\n",
      "epoch 33 | loss: 0.11457 | val_0_rmse: 0.28806 |  0:00:08s\n",
      "epoch 34 | loss: 0.1597  | val_0_rmse: 0.33682 |  0:00:08s\n",
      "epoch 35 | loss: 0.12645 | val_0_rmse: 0.27956 |  0:00:08s\n",
      "epoch 36 | loss: 0.09949 | val_0_rmse: 0.33545 |  0:00:08s\n",
      "epoch 37 | loss: 0.09875 | val_0_rmse: 0.256   |  0:00:09s\n",
      "epoch 38 | loss: 0.16527 | val_0_rmse: 0.34137 |  0:00:09s\n",
      "epoch 39 | loss: 0.13876 | val_0_rmse: 0.3147  |  0:00:09s\n",
      "epoch 40 | loss: 0.1336  | val_0_rmse: 0.30517 |  0:00:09s\n",
      "epoch 41 | loss: 0.10565 | val_0_rmse: 0.28791 |  0:00:10s\n",
      "epoch 42 | loss: 0.10361 | val_0_rmse: 0.28142 |  0:00:10s\n",
      "epoch 43 | loss: 0.12557 | val_0_rmse: 0.3362  |  0:00:10s\n",
      "epoch 44 | loss: 0.13978 | val_0_rmse: 0.27287 |  0:00:10s\n",
      "epoch 45 | loss: 0.11778 | val_0_rmse: 0.36494 |  0:00:10s\n",
      "epoch 46 | loss: 0.14606 | val_0_rmse: 0.31955 |  0:00:11s\n",
      "epoch 47 | loss: 0.13542 | val_0_rmse: 0.30151 |  0:00:11s\n",
      "epoch 48 | loss: 0.08458 | val_0_rmse: 0.26835 |  0:00:11s\n",
      "epoch 49 | loss: 0.07331 | val_0_rmse: 0.2617  |  0:00:11s\n",
      "epoch 50 | loss: 0.09009 | val_0_rmse: 0.27247 |  0:00:11s\n",
      "epoch 51 | loss: 0.0793  | val_0_rmse: 0.28373 |  0:00:12s\n",
      "epoch 52 | loss: 0.07731 | val_0_rmse: 0.25102 |  0:00:12s\n",
      "epoch 53 | loss: 0.11062 | val_0_rmse: 0.27325 |  0:00:12s\n",
      "epoch 54 | loss: 0.0796  | val_0_rmse: 0.31491 |  0:00:12s\n",
      "epoch 55 | loss: 0.08423 | val_0_rmse: 0.249   |  0:00:13s\n",
      "epoch 56 | loss: 0.07393 | val_0_rmse: 0.26318 |  0:00:13s\n",
      "epoch 57 | loss: 0.07128 | val_0_rmse: 0.25475 |  0:00:13s\n",
      "epoch 58 | loss: 0.05812 | val_0_rmse: 0.23061 |  0:00:13s\n",
      "epoch 59 | loss: 0.05285 | val_0_rmse: 0.26747 |  0:00:14s\n",
      "epoch 60 | loss: 0.05416 | val_0_rmse: 0.2417  |  0:00:14s\n",
      "epoch 61 | loss: 0.06371 | val_0_rmse: 0.25392 |  0:00:14s\n",
      "epoch 62 | loss: 0.05691 | val_0_rmse: 0.24003 |  0:00:14s\n",
      "epoch 63 | loss: 0.05812 | val_0_rmse: 0.23424 |  0:00:14s\n",
      "epoch 64 | loss: 0.04971 | val_0_rmse: 0.26416 |  0:00:15s\n",
      "epoch 65 | loss: 0.06603 | val_0_rmse: 0.24347 |  0:00:15s\n",
      "epoch 66 | loss: 0.07715 | val_0_rmse: 0.27362 |  0:00:15s\n",
      "epoch 67 | loss: 0.0749  | val_0_rmse: 0.24673 |  0:00:15s\n",
      "epoch 68 | loss: 0.05596 | val_0_rmse: 0.24912 |  0:00:16s\n",
      "epoch 69 | loss: 0.05494 | val_0_rmse: 0.23785 |  0:00:16s\n",
      "epoch 70 | loss: 0.05424 | val_0_rmse: 0.25924 |  0:00:16s\n",
      "epoch 71 | loss: 0.04772 | val_0_rmse: 0.22883 |  0:00:16s\n",
      "epoch 72 | loss: 0.04423 | val_0_rmse: 0.23365 |  0:00:16s\n",
      "epoch 73 | loss: 0.0516  | val_0_rmse: 0.24314 |  0:00:17s\n",
      "epoch 74 | loss: 0.04497 | val_0_rmse: 0.24388 |  0:00:17s\n",
      "epoch 75 | loss: 0.06327 | val_0_rmse: 0.25857 |  0:00:17s\n",
      "epoch 76 | loss: 0.07457 | val_0_rmse: 0.23482 |  0:00:17s\n",
      "epoch 77 | loss: 0.05441 | val_0_rmse: 0.23031 |  0:00:18s\n",
      "epoch 78 | loss: 0.05347 | val_0_rmse: 0.23364 |  0:00:18s\n",
      "epoch 79 | loss: 0.05687 | val_0_rmse: 0.24247 |  0:00:18s\n",
      "epoch 80 | loss: 0.048   | val_0_rmse: 0.25084 |  0:00:18s\n",
      "epoch 81 | loss: 0.05559 | val_0_rmse: 0.26418 |  0:00:18s\n",
      "epoch 82 | loss: 0.06362 | val_0_rmse: 0.23931 |  0:00:19s\n",
      "epoch 83 | loss: 0.06366 | val_0_rmse: 0.25152 |  0:00:19s\n",
      "epoch 84 | loss: 0.05704 | val_0_rmse: 0.22088 |  0:00:19s\n",
      "epoch 85 | loss: 0.04644 | val_0_rmse: 0.23419 |  0:00:19s\n",
      "epoch 86 | loss: 0.04269 | val_0_rmse: 0.23194 |  0:00:20s\n",
      "epoch 87 | loss: 0.04314 | val_0_rmse: 0.23088 |  0:00:20s\n",
      "epoch 88 | loss: 0.03677 | val_0_rmse: 0.22402 |  0:00:20s\n",
      "epoch 89 | loss: 0.04042 | val_0_rmse: 0.2216  |  0:00:20s\n",
      "epoch 90 | loss: 0.03705 | val_0_rmse: 0.22135 |  0:00:21s\n",
      "epoch 91 | loss: 0.0365  | val_0_rmse: 0.22468 |  0:00:21s\n",
      "epoch 92 | loss: 0.03538 | val_0_rmse: 0.2206  |  0:00:21s\n",
      "epoch 93 | loss: 0.03629 | val_0_rmse: 0.22986 |  0:00:21s\n",
      "epoch 94 | loss: 0.03534 | val_0_rmse: 0.22657 |  0:00:21s\n",
      "epoch 95 | loss: 0.03602 | val_0_rmse: 0.21825 |  0:00:22s\n",
      "epoch 96 | loss: 0.03511 | val_0_rmse: 0.22462 |  0:00:22s\n",
      "epoch 97 | loss: 0.044   | val_0_rmse: 0.21489 |  0:00:22s\n",
      "epoch 98 | loss: 0.0373  | val_0_rmse: 0.21765 |  0:00:22s\n",
      "epoch 99 | loss: 0.03539 | val_0_rmse: 0.21307 |  0:00:23s\n",
      "epoch 100| loss: 0.03944 | val_0_rmse: 0.22751 |  0:00:23s\n",
      "epoch 101| loss: 0.03953 | val_0_rmse: 0.2259  |  0:00:23s\n",
      "epoch 102| loss: 0.04161 | val_0_rmse: 0.21285 |  0:00:23s\n",
      "epoch 103| loss: 0.03874 | val_0_rmse: 0.2272  |  0:00:23s\n",
      "epoch 104| loss: 0.03551 | val_0_rmse: 0.21138 |  0:00:24s\n",
      "epoch 105| loss: 0.04051 | val_0_rmse: 0.20569 |  0:00:24s\n",
      "epoch 106| loss: 0.03539 | val_0_rmse: 0.21146 |  0:00:24s\n",
      "epoch 107| loss: 0.03153 | val_0_rmse: 0.21669 |  0:00:24s\n",
      "epoch 108| loss: 0.0324  | val_0_rmse: 0.21735 |  0:00:25s\n",
      "epoch 109| loss: 0.03328 | val_0_rmse: 0.20866 |  0:00:25s\n",
      "epoch 110| loss: 0.03015 | val_0_rmse: 0.21349 |  0:00:25s\n",
      "epoch 111| loss: 0.03044 | val_0_rmse: 0.20669 |  0:00:25s\n",
      "epoch 112| loss: 0.02857 | val_0_rmse: 0.2078  |  0:00:26s\n",
      "epoch 113| loss: 0.02751 | val_0_rmse: 0.2124  |  0:00:26s\n",
      "epoch 114| loss: 0.02855 | val_0_rmse: 0.20218 |  0:00:26s\n",
      "epoch 115| loss: 0.02812 | val_0_rmse: 0.20754 |  0:00:26s\n",
      "epoch 116| loss: 0.02543 | val_0_rmse: 0.20152 |  0:00:27s\n",
      "epoch 117| loss: 0.02858 | val_0_rmse: 0.20298 |  0:00:27s\n",
      "epoch 118| loss: 0.03194 | val_0_rmse: 0.1984  |  0:00:27s\n",
      "epoch 119| loss: 0.02942 | val_0_rmse: 0.20901 |  0:00:27s\n",
      "epoch 120| loss: 0.02954 | val_0_rmse: 0.21494 |  0:00:27s\n",
      "epoch 121| loss: 0.03096 | val_0_rmse: 0.22424 |  0:00:28s\n",
      "epoch 122| loss: 0.04004 | val_0_rmse: 0.20547 |  0:00:28s\n",
      "epoch 123| loss: 0.03023 | val_0_rmse: 0.22194 |  0:00:28s\n",
      "epoch 124| loss: 0.03425 | val_0_rmse: 0.20431 |  0:00:28s\n",
      "epoch 125| loss: 0.02959 | val_0_rmse: 0.20921 |  0:00:28s\n",
      "epoch 126| loss: 0.03127 | val_0_rmse: 0.22193 |  0:00:29s\n",
      "epoch 127| loss: 0.05159 | val_0_rmse: 0.22218 |  0:00:29s\n",
      "epoch 128| loss: 0.06072 | val_0_rmse: 0.20542 |  0:00:29s\n",
      "epoch 129| loss: 0.03341 | val_0_rmse: 0.19788 |  0:00:29s\n",
      "epoch 130| loss: 0.02661 | val_0_rmse: 0.19587 |  0:00:30s\n",
      "epoch 131| loss: 0.02697 | val_0_rmse: 0.19989 |  0:00:30s\n",
      "epoch 132| loss: 0.0331  | val_0_rmse: 0.20961 |  0:00:30s\n",
      "epoch 133| loss: 0.02822 | val_0_rmse: 0.20714 |  0:00:30s\n",
      "epoch 134| loss: 0.02903 | val_0_rmse: 0.20687 |  0:00:31s\n",
      "epoch 135| loss: 0.03021 | val_0_rmse: 0.20593 |  0:00:31s\n",
      "epoch 136| loss: 0.02971 | val_0_rmse: 0.20987 |  0:00:31s\n",
      "epoch 137| loss: 0.03641 | val_0_rmse: 0.22113 |  0:00:31s\n",
      "epoch 138| loss: 0.03009 | val_0_rmse: 0.20766 |  0:00:31s\n",
      "epoch 139| loss: 0.02853 | val_0_rmse: 0.20578 |  0:00:32s\n",
      "epoch 140| loss: 0.02609 | val_0_rmse: 0.2121  |  0:00:32s\n",
      "epoch 141| loss: 0.02697 | val_0_rmse: 0.20379 |  0:00:32s\n",
      "epoch 142| loss: 0.02354 | val_0_rmse: 0.20857 |  0:00:32s\n",
      "epoch 143| loss: 0.02692 | val_0_rmse: 0.20848 |  0:00:33s\n",
      "epoch 144| loss: 0.03057 | val_0_rmse: 0.2137  |  0:00:33s\n",
      "epoch 145| loss: 0.02825 | val_0_rmse: 0.21116 |  0:00:33s\n",
      "epoch 146| loss: 0.02449 | val_0_rmse: 0.21246 |  0:00:33s\n",
      "epoch 147| loss: 0.02767 | val_0_rmse: 0.20217 |  0:00:33s\n",
      "epoch 148| loss: 0.0257  | val_0_rmse: 0.20578 |  0:00:34s\n",
      "epoch 149| loss: 0.02542 | val_0_rmse: 0.20172 |  0:00:34s\n",
      "epoch 150| loss: 0.0266  | val_0_rmse: 0.20774 |  0:00:34s\n",
      "epoch 151| loss: 0.03082 | val_0_rmse: 0.21738 |  0:00:34s\n",
      "epoch 152| loss: 0.02451 | val_0_rmse: 0.24397 |  0:00:35s\n",
      "epoch 153| loss: 0.04881 | val_0_rmse: 0.21607 |  0:00:35s\n",
      "epoch 154| loss: 0.04587 | val_0_rmse: 0.21351 |  0:00:35s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:33:58,221] Trial 59 finished with value: 0.1958650603456934 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2003427596107628, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.01324364537029663, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 155| loss: 0.02814 | val_0_rmse: 0.21156 |  0:00:35s\n",
      "\n",
      "Early stopping occurred at epoch 155 with best_epoch = 130 and best_val_0_rmse = 0.19587\n",
      "Trial 059 | rmse_log=0.19587 | RMSE$=36,678 | MAE$=24,256 | MAPE=14.34% | n_d/n_a=24/16 steps=3 lr=0.01324 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.76964| val_0_rmse: 10.8955 |  0:00:00s\n",
      "epoch 1  | loss: 87.37944| val_0_rmse: 9.55157 |  0:00:00s\n",
      "epoch 2  | loss: 58.50286| val_0_rmse: 8.19875 |  0:00:00s\n",
      "epoch 3  | loss: 34.75148| val_0_rmse: 6.72539 |  0:00:00s\n",
      "epoch 4  | loss: 21.23934| val_0_rmse: 5.21541 |  0:00:01s\n",
      "epoch 5  | loss: 13.60175| val_0_rmse: 3.9252  |  0:00:01s\n",
      "epoch 6  | loss: 13.13116| val_0_rmse: 3.20591 |  0:00:01s\n",
      "epoch 7  | loss: 11.50077| val_0_rmse: 3.3037  |  0:00:01s\n",
      "epoch 8  | loss: 6.53629 | val_0_rmse: 3.54516 |  0:00:02s\n",
      "epoch 9  | loss: 4.1836  | val_0_rmse: 3.03791 |  0:00:02s\n",
      "epoch 10 | loss: 2.80014 | val_0_rmse: 1.89388 |  0:00:02s\n",
      "epoch 11 | loss: 2.05972 | val_0_rmse: 1.73512 |  0:00:02s\n",
      "epoch 12 | loss: 1.25979 | val_0_rmse: 1.71873 |  0:00:03s\n",
      "epoch 13 | loss: 0.83707 | val_0_rmse: 1.12302 |  0:00:03s\n",
      "epoch 14 | loss: 0.86482 | val_0_rmse: 1.27718 |  0:00:03s\n",
      "epoch 15 | loss: 0.69016 | val_0_rmse: 0.84231 |  0:00:03s\n",
      "epoch 16 | loss: 0.64467 | val_0_rmse: 1.16719 |  0:00:04s\n",
      "epoch 17 | loss: 0.7103  | val_0_rmse: 0.7265  |  0:00:04s\n",
      "epoch 18 | loss: 0.55439 | val_0_rmse: 0.91897 |  0:00:04s\n",
      "epoch 19 | loss: 0.4189  | val_0_rmse: 0.6134  |  0:00:04s\n",
      "epoch 20 | loss: 0.51176 | val_0_rmse: 0.9723  |  0:00:04s\n",
      "epoch 21 | loss: 0.36526 | val_0_rmse: 0.65605 |  0:00:05s\n",
      "epoch 22 | loss: 0.34965 | val_0_rmse: 0.9198  |  0:00:05s\n",
      "epoch 23 | loss: 0.47692 | val_0_rmse: 0.43922 |  0:00:05s\n",
      "epoch 24 | loss: 0.42494 | val_0_rmse: 0.71198 |  0:00:05s\n",
      "epoch 25 | loss: 0.33721 | val_0_rmse: 0.4441  |  0:00:05s\n",
      "epoch 26 | loss: 0.27362 | val_0_rmse: 0.81514 |  0:00:06s\n",
      "epoch 27 | loss: 0.29576 | val_0_rmse: 0.4288  |  0:00:06s\n",
      "epoch 28 | loss: 0.40532 | val_0_rmse: 0.54939 |  0:00:06s\n",
      "epoch 29 | loss: 0.22887 | val_0_rmse: 0.48658 |  0:00:06s\n",
      "epoch 30 | loss: 0.18196 | val_0_rmse: 0.43231 |  0:00:07s\n",
      "epoch 31 | loss: 0.23044 | val_0_rmse: 0.83318 |  0:00:07s\n",
      "epoch 32 | loss: 0.30863 | val_0_rmse: 0.43309 |  0:00:07s\n",
      "epoch 33 | loss: 0.31327 | val_0_rmse: 0.5093  |  0:00:07s\n",
      "epoch 34 | loss: 0.21372 | val_0_rmse: 0.49406 |  0:00:08s\n",
      "epoch 35 | loss: 0.19181 | val_0_rmse: 0.4035  |  0:00:08s\n",
      "epoch 36 | loss: 0.16479 | val_0_rmse: 0.3699  |  0:00:08s\n",
      "epoch 37 | loss: 0.16861 | val_0_rmse: 0.46765 |  0:00:08s\n",
      "epoch 38 | loss: 0.16555 | val_0_rmse: 0.41531 |  0:00:08s\n",
      "epoch 39 | loss: 0.14506 | val_0_rmse: 0.40648 |  0:00:09s\n",
      "epoch 40 | loss: 0.11892 | val_0_rmse: 0.43041 |  0:00:09s\n",
      "epoch 41 | loss: 0.12335 | val_0_rmse: 0.38568 |  0:00:09s\n",
      "epoch 42 | loss: 0.1129  | val_0_rmse: 0.41686 |  0:00:09s\n",
      "epoch 43 | loss: 0.11433 | val_0_rmse: 0.37115 |  0:00:09s\n",
      "epoch 44 | loss: 0.12467 | val_0_rmse: 0.3852  |  0:00:10s\n",
      "epoch 45 | loss: 0.09646 | val_0_rmse: 0.35028 |  0:00:10s\n",
      "epoch 46 | loss: 0.11068 | val_0_rmse: 0.33081 |  0:00:10s\n",
      "epoch 47 | loss: 0.10165 | val_0_rmse: 0.42227 |  0:00:10s\n",
      "epoch 48 | loss: 0.12283 | val_0_rmse: 0.29466 |  0:00:11s\n",
      "epoch 49 | loss: 0.10243 | val_0_rmse: 0.34556 |  0:00:11s\n",
      "epoch 50 | loss: 0.11269 | val_0_rmse: 0.27162 |  0:00:11s\n",
      "epoch 51 | loss: 0.11444 | val_0_rmse: 0.35495 |  0:00:11s\n",
      "epoch 52 | loss: 0.12154 | val_0_rmse: 0.30412 |  0:00:12s\n",
      "epoch 53 | loss: 0.15557 | val_0_rmse: 0.36016 |  0:00:12s\n",
      "epoch 54 | loss: 0.12168 | val_0_rmse: 0.27027 |  0:00:12s\n",
      "epoch 55 | loss: 0.09608 | val_0_rmse: 0.37003 |  0:00:12s\n",
      "epoch 56 | loss: 0.12768 | val_0_rmse: 0.27563 |  0:00:13s\n",
      "epoch 57 | loss: 0.08693 | val_0_rmse: 0.3205  |  0:00:13s\n",
      "epoch 58 | loss: 0.08614 | val_0_rmse: 0.2726  |  0:00:13s\n",
      "epoch 59 | loss: 0.08461 | val_0_rmse: 0.31112 |  0:00:13s\n",
      "epoch 60 | loss: 0.07389 | val_0_rmse: 0.30528 |  0:00:13s\n",
      "epoch 61 | loss: 0.07569 | val_0_rmse: 0.24652 |  0:00:14s\n",
      "epoch 62 | loss: 0.06595 | val_0_rmse: 0.24828 |  0:00:14s\n",
      "epoch 63 | loss: 0.06532 | val_0_rmse: 0.24765 |  0:00:14s\n",
      "epoch 64 | loss: 0.07492 | val_0_rmse: 0.31265 |  0:00:14s\n",
      "epoch 65 | loss: 0.07599 | val_0_rmse: 0.26226 |  0:00:15s\n",
      "epoch 66 | loss: 0.11391 | val_0_rmse: 0.36596 |  0:00:15s\n",
      "epoch 67 | loss: 0.17919 | val_0_rmse: 0.30959 |  0:00:15s\n",
      "epoch 68 | loss: 0.12533 | val_0_rmse: 0.30949 |  0:00:15s\n",
      "epoch 69 | loss: 0.12512 | val_0_rmse: 0.31905 |  0:00:16s\n",
      "epoch 70 | loss: 0.09523 | val_0_rmse: 0.24541 |  0:00:16s\n",
      "epoch 71 | loss: 0.09701 | val_0_rmse: 0.28749 |  0:00:16s\n",
      "epoch 72 | loss: 0.08623 | val_0_rmse: 0.22498 |  0:00:16s\n",
      "epoch 73 | loss: 0.07589 | val_0_rmse: 0.31708 |  0:00:16s\n",
      "epoch 74 | loss: 0.07254 | val_0_rmse: 0.2473  |  0:00:17s\n",
      "epoch 75 | loss: 0.06997 | val_0_rmse: 0.24821 |  0:00:17s\n",
      "epoch 76 | loss: 0.06677 | val_0_rmse: 0.27642 |  0:00:17s\n",
      "epoch 77 | loss: 0.08037 | val_0_rmse: 0.26944 |  0:00:17s\n",
      "epoch 78 | loss: 0.06767 | val_0_rmse: 0.24351 |  0:00:18s\n",
      "epoch 79 | loss: 0.06033 | val_0_rmse: 0.25617 |  0:00:18s\n",
      "epoch 80 | loss: 0.06948 | val_0_rmse: 0.3008  |  0:00:18s\n",
      "epoch 81 | loss: 0.09045 | val_0_rmse: 0.23858 |  0:00:19s\n",
      "epoch 82 | loss: 0.05806 | val_0_rmse: 0.28011 |  0:00:19s\n",
      "epoch 83 | loss: 0.05728 | val_0_rmse: 0.22928 |  0:00:19s\n",
      "epoch 84 | loss: 0.0539  | val_0_rmse: 0.2324  |  0:00:19s\n",
      "epoch 85 | loss: 0.0544  | val_0_rmse: 0.29019 |  0:00:20s\n",
      "epoch 86 | loss: 0.08908 | val_0_rmse: 0.22225 |  0:00:20s\n",
      "epoch 87 | loss: 0.07401 | val_0_rmse: 0.24859 |  0:00:20s\n",
      "epoch 88 | loss: 0.05634 | val_0_rmse: 0.22714 |  0:00:20s\n",
      "epoch 89 | loss: 0.06146 | val_0_rmse: 0.23112 |  0:00:21s\n",
      "epoch 90 | loss: 0.05026 | val_0_rmse: 0.24234 |  0:00:21s\n",
      "epoch 91 | loss: 0.04538 | val_0_rmse: 0.21853 |  0:00:21s\n",
      "epoch 92 | loss: 0.04404 | val_0_rmse: 0.23064 |  0:00:22s\n",
      "epoch 93 | loss: 0.05111 | val_0_rmse: 0.23389 |  0:00:22s\n",
      "epoch 94 | loss: 0.04422 | val_0_rmse: 0.22274 |  0:00:22s\n",
      "epoch 95 | loss: 0.04322 | val_0_rmse: 0.23023 |  0:00:23s\n",
      "epoch 96 | loss: 0.04288 | val_0_rmse: 0.23578 |  0:00:23s\n",
      "epoch 97 | loss: 0.04902 | val_0_rmse: 0.22164 |  0:00:23s\n",
      "epoch 98 | loss: 0.05351 | val_0_rmse: 0.24494 |  0:00:24s\n",
      "epoch 99 | loss: 0.05141 | val_0_rmse: 0.23008 |  0:00:24s\n",
      "epoch 100| loss: 0.04511 | val_0_rmse: 0.23013 |  0:00:24s\n",
      "epoch 101| loss: 0.05609 | val_0_rmse: 0.22412 |  0:00:25s\n",
      "epoch 102| loss: 0.05594 | val_0_rmse: 0.22797 |  0:00:25s\n",
      "epoch 103| loss: 0.06294 | val_0_rmse: 0.25443 |  0:00:25s\n",
      "epoch 104| loss: 0.05971 | val_0_rmse: 0.22943 |  0:00:25s\n",
      "epoch 105| loss: 0.04824 | val_0_rmse: 0.22004 |  0:00:26s\n",
      "epoch 106| loss: 0.06449 | val_0_rmse: 0.27349 |  0:00:26s\n",
      "epoch 107| loss: 0.05995 | val_0_rmse: 0.21737 |  0:00:26s\n",
      "epoch 108| loss: 0.04575 | val_0_rmse: 0.20957 |  0:00:26s\n",
      "epoch 109| loss: 0.03962 | val_0_rmse: 0.21542 |  0:00:27s\n",
      "epoch 110| loss: 0.04323 | val_0_rmse: 0.21973 |  0:00:27s\n",
      "epoch 111| loss: 0.04052 | val_0_rmse: 0.21892 |  0:00:27s\n",
      "epoch 112| loss: 0.04365 | val_0_rmse: 0.2245  |  0:00:28s\n",
      "epoch 113| loss: 0.04479 | val_0_rmse: 0.21562 |  0:00:28s\n",
      "epoch 114| loss: 0.04486 | val_0_rmse: 0.21921 |  0:00:28s\n",
      "epoch 115| loss: 0.0498  | val_0_rmse: 0.22857 |  0:00:29s\n",
      "epoch 116| loss: 0.03956 | val_0_rmse: 0.21184 |  0:00:29s\n",
      "epoch 117| loss: 0.03853 | val_0_rmse: 0.21005 |  0:00:29s\n",
      "epoch 118| loss: 0.03968 | val_0_rmse: 0.22491 |  0:00:29s\n",
      "epoch 119| loss: 0.04106 | val_0_rmse: 0.24589 |  0:00:30s\n",
      "epoch 120| loss: 0.04408 | val_0_rmse: 0.2124  |  0:00:30s\n",
      "epoch 121| loss: 0.04557 | val_0_rmse: 0.2401  |  0:00:30s\n",
      "epoch 122| loss: 0.0425  | val_0_rmse: 0.20662 |  0:00:30s\n",
      "epoch 123| loss: 0.04542 | val_0_rmse: 0.21785 |  0:00:30s\n",
      "epoch 124| loss: 0.03921 | val_0_rmse: 0.23254 |  0:00:31s\n",
      "epoch 125| loss: 0.04153 | val_0_rmse: 0.20919 |  0:00:31s\n",
      "epoch 126| loss: 0.03625 | val_0_rmse: 0.21897 |  0:00:31s\n",
      "epoch 127| loss: 0.03747 | val_0_rmse: 0.22643 |  0:00:32s\n",
      "epoch 128| loss: 0.03935 | val_0_rmse: 0.2337  |  0:00:32s\n",
      "epoch 129| loss: 0.03904 | val_0_rmse: 0.21154 |  0:00:32s\n",
      "epoch 130| loss: 0.04189 | val_0_rmse: 0.21163 |  0:00:33s\n",
      "epoch 131| loss: 0.03864 | val_0_rmse: 0.21574 |  0:00:33s\n",
      "epoch 132| loss: 0.0376  | val_0_rmse: 0.21564 |  0:00:33s\n",
      "epoch 133| loss: 0.03579 | val_0_rmse: 0.22428 |  0:00:33s\n",
      "epoch 134| loss: 0.04084 | val_0_rmse: 0.22324 |  0:00:34s\n",
      "epoch 135| loss: 0.0369  | val_0_rmse: 0.24263 |  0:00:34s\n",
      "epoch 136| loss: 0.04331 | val_0_rmse: 0.21214 |  0:00:34s\n",
      "epoch 137| loss: 0.03778 | val_0_rmse: 0.21063 |  0:00:34s\n",
      "epoch 138| loss: 0.03471 | val_0_rmse: 0.22475 |  0:00:35s\n",
      "epoch 139| loss: 0.0338  | val_0_rmse: 0.21532 |  0:00:35s\n",
      "epoch 140| loss: 0.03571 | val_0_rmse: 0.21486 |  0:00:35s\n",
      "epoch 141| loss: 0.03956 | val_0_rmse: 0.21595 |  0:00:35s\n",
      "epoch 142| loss: 0.03558 | val_0_rmse: 0.20609 |  0:00:36s\n",
      "epoch 143| loss: 0.03316 | val_0_rmse: 0.22591 |  0:00:36s\n",
      "epoch 144| loss: 0.03436 | val_0_rmse: 0.21433 |  0:00:36s\n",
      "epoch 145| loss: 0.03415 | val_0_rmse: 0.21919 |  0:00:36s\n",
      "epoch 146| loss: 0.03728 | val_0_rmse: 0.21566 |  0:00:37s\n",
      "epoch 147| loss: 0.0428  | val_0_rmse: 0.23063 |  0:00:37s\n",
      "epoch 148| loss: 0.04035 | val_0_rmse: 0.2359  |  0:00:37s\n",
      "epoch 149| loss: 0.04024 | val_0_rmse: 0.21219 |  0:00:38s\n",
      "epoch 150| loss: 0.0394  | val_0_rmse: 0.21359 |  0:00:38s\n",
      "epoch 151| loss: 0.03934 | val_0_rmse: 0.21893 |  0:00:38s\n",
      "epoch 152| loss: 0.04359 | val_0_rmse: 0.23388 |  0:00:38s\n",
      "epoch 153| loss: 0.0465  | val_0_rmse: 0.20847 |  0:00:39s\n",
      "epoch 154| loss: 0.03569 | val_0_rmse: 0.22089 |  0:00:39s\n",
      "epoch 155| loss: 0.03887 | val_0_rmse: 0.21313 |  0:00:39s\n",
      "epoch 156| loss: 0.03238 | val_0_rmse: 0.21984 |  0:00:39s\n",
      "epoch 157| loss: 0.03567 | val_0_rmse: 0.2145  |  0:00:40s\n",
      "epoch 158| loss: 0.03586 | val_0_rmse: 0.21214 |  0:00:40s\n",
      "epoch 159| loss: 0.03103 | val_0_rmse: 0.22076 |  0:00:40s\n",
      "epoch 160| loss: 0.02999 | val_0_rmse: 0.21379 |  0:00:40s\n",
      "epoch 161| loss: 0.02979 | val_0_rmse: 0.21573 |  0:00:40s\n",
      "epoch 162| loss: 0.02963 | val_0_rmse: 0.20416 |  0:00:41s\n",
      "epoch 163| loss: 0.02952 | val_0_rmse: 0.21005 |  0:00:41s\n",
      "epoch 164| loss: 0.03216 | val_0_rmse: 0.21799 |  0:00:41s\n",
      "epoch 165| loss: 0.02964 | val_0_rmse: 0.2148  |  0:00:42s\n",
      "epoch 166| loss: 0.03192 | val_0_rmse: 0.20722 |  0:00:42s\n",
      "epoch 167| loss: 0.03091 | val_0_rmse: 0.20288 |  0:00:42s\n",
      "epoch 168| loss: 0.03215 | val_0_rmse: 0.21561 |  0:00:42s\n",
      "epoch 169| loss: 0.03803 | val_0_rmse: 0.24929 |  0:00:43s\n",
      "epoch 170| loss: 0.0451  | val_0_rmse: 0.21818 |  0:00:43s\n",
      "epoch 171| loss: 0.04787 | val_0_rmse: 0.26286 |  0:00:43s\n",
      "epoch 172| loss: 0.05105 | val_0_rmse: 0.23697 |  0:00:43s\n",
      "epoch 173| loss: 0.06478 | val_0_rmse: 0.22507 |  0:00:44s\n",
      "epoch 174| loss: 0.03622 | val_0_rmse: 0.24441 |  0:00:44s\n",
      "epoch 175| loss: 0.04647 | val_0_rmse: 0.26499 |  0:00:44s\n",
      "epoch 176| loss: 0.05123 | val_0_rmse: 0.22627 |  0:00:44s\n",
      "epoch 177| loss: 0.04428 | val_0_rmse: 0.25147 |  0:00:45s\n",
      "epoch 178| loss: 0.04931 | val_0_rmse: 0.21911 |  0:00:45s\n",
      "epoch 179| loss: 0.04561 | val_0_rmse: 0.24851 |  0:00:45s\n",
      "epoch 180| loss: 0.04803 | val_0_rmse: 0.2361  |  0:00:45s\n",
      "epoch 181| loss: 0.04684 | val_0_rmse: 0.23591 |  0:00:46s\n",
      "epoch 182| loss: 0.041   | val_0_rmse: 0.22729 |  0:00:46s\n",
      "epoch 183| loss: 0.04862 | val_0_rmse: 0.22044 |  0:00:46s\n",
      "epoch 184| loss: 0.03783 | val_0_rmse: 0.23441 |  0:00:46s\n",
      "epoch 185| loss: 0.04962 | val_0_rmse: 0.22314 |  0:00:47s\n",
      "epoch 186| loss: 0.04219 | val_0_rmse: 0.2222  |  0:00:47s\n",
      "epoch 187| loss: 0.04096 | val_0_rmse: 0.23726 |  0:00:47s\n",
      "epoch 188| loss: 0.04552 | val_0_rmse: 0.21061 |  0:00:47s\n",
      "epoch 189| loss: 0.04011 | val_0_rmse: 0.23164 |  0:00:47s\n",
      "epoch 190| loss: 0.05155 | val_0_rmse: 0.21827 |  0:00:48s\n",
      "epoch 191| loss: 0.0432  | val_0_rmse: 0.21671 |  0:00:48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:34:47,290] Trial 60 finished with value: 0.20288379570615467 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2267164927805225, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'sparsemax', 'lr': 0.018088223150800883, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 192| loss: 0.03754 | val_0_rmse: 0.2094  |  0:00:48s\n",
      "\n",
      "Early stopping occurred at epoch 192 with best_epoch = 167 and best_val_0_rmse = 0.20288\n",
      "Trial 060 | rmse_log=0.20288 | RMSE$=45,380 | MAE$=27,240 | MAPE=15.04% | n_d/n_a=24/16 steps=3 lr=0.01809 batch=512 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 125.17573| val_0_rmse: 10.9109 |  0:00:00s\n",
      "epoch 1  | loss: 91.45936| val_0_rmse: 9.8637  |  0:00:00s\n",
      "epoch 2  | loss: 62.42622| val_0_rmse: 8.63691 |  0:00:00s\n",
      "epoch 3  | loss: 42.92383| val_0_rmse: 7.35245 |  0:00:01s\n",
      "epoch 4  | loss: 24.02189| val_0_rmse: 6.0145  |  0:00:01s\n",
      "epoch 5  | loss: 16.70706| val_0_rmse: 4.81139 |  0:00:01s\n",
      "epoch 6  | loss: 14.25927| val_0_rmse: 3.89277 |  0:00:01s\n",
      "epoch 7  | loss: 11.34738| val_0_rmse: 3.50599 |  0:00:02s\n",
      "epoch 8  | loss: 7.47572 | val_0_rmse: 3.65597 |  0:00:02s\n",
      "epoch 9  | loss: 3.93921 | val_0_rmse: 3.65448 |  0:00:02s\n",
      "epoch 10 | loss: 3.5016  | val_0_rmse: 3.01338 |  0:00:02s\n",
      "epoch 11 | loss: 1.91044 | val_0_rmse: 1.94334 |  0:00:02s\n",
      "epoch 12 | loss: 1.48119 | val_0_rmse: 1.94661 |  0:00:03s\n",
      "epoch 13 | loss: 0.9615  | val_0_rmse: 1.86724 |  0:00:03s\n",
      "epoch 14 | loss: 0.69283 | val_0_rmse: 1.24024 |  0:00:03s\n",
      "epoch 15 | loss: 0.60017 | val_0_rmse: 1.44264 |  0:00:03s\n",
      "epoch 16 | loss: 0.5354  | val_0_rmse: 0.92928 |  0:00:04s\n",
      "epoch 17 | loss: 0.44514 | val_0_rmse: 0.99428 |  0:00:04s\n",
      "epoch 18 | loss: 0.36827 | val_0_rmse: 0.80044 |  0:00:04s\n",
      "epoch 19 | loss: 0.28355 | val_0_rmse: 0.66727 |  0:00:05s\n",
      "epoch 20 | loss: 0.31604 | val_0_rmse: 0.72832 |  0:00:05s\n",
      "epoch 21 | loss: 0.25597 | val_0_rmse: 0.63749 |  0:00:05s\n",
      "epoch 22 | loss: 0.22978 | val_0_rmse: 0.57128 |  0:00:05s\n",
      "epoch 23 | loss: 0.26354 | val_0_rmse: 0.74074 |  0:00:06s\n",
      "epoch 24 | loss: 0.25249 | val_0_rmse: 0.59222 |  0:00:06s\n",
      "epoch 25 | loss: 0.21396 | val_0_rmse: 0.63548 |  0:00:06s\n",
      "epoch 26 | loss: 0.20051 | val_0_rmse: 0.53825 |  0:00:06s\n",
      "epoch 27 | loss: 0.17053 | val_0_rmse: 0.45631 |  0:00:07s\n",
      "epoch 28 | loss: 0.17487 | val_0_rmse: 0.46096 |  0:00:07s\n",
      "epoch 29 | loss: 0.11182 | val_0_rmse: 0.38553 |  0:00:07s\n",
      "epoch 30 | loss: 0.11493 | val_0_rmse: 0.32497 |  0:00:07s\n",
      "epoch 31 | loss: 0.13484 | val_0_rmse: 0.35119 |  0:00:08s\n",
      "epoch 32 | loss: 0.09827 | val_0_rmse: 0.41497 |  0:00:08s\n",
      "epoch 33 | loss: 0.1048  | val_0_rmse: 0.32247 |  0:00:08s\n",
      "epoch 34 | loss: 0.09858 | val_0_rmse: 0.28527 |  0:00:08s\n",
      "epoch 35 | loss: 0.08048 | val_0_rmse: 0.33454 |  0:00:09s\n",
      "epoch 36 | loss: 0.09529 | val_0_rmse: 0.31781 |  0:00:09s\n",
      "epoch 37 | loss: 0.0798  | val_0_rmse: 0.2808  |  0:00:09s\n",
      "epoch 38 | loss: 0.0735  | val_0_rmse: 0.30374 |  0:00:10s\n",
      "epoch 39 | loss: 0.09372 | val_0_rmse: 0.28348 |  0:00:10s\n",
      "epoch 40 | loss: 0.07631 | val_0_rmse: 0.24825 |  0:00:10s\n",
      "epoch 41 | loss: 0.07113 | val_0_rmse: 0.25378 |  0:00:10s\n",
      "epoch 42 | loss: 0.06596 | val_0_rmse: 0.28298 |  0:00:11s\n",
      "epoch 43 | loss: 0.06528 | val_0_rmse: 0.25746 |  0:00:11s\n",
      "epoch 44 | loss: 0.05993 | val_0_rmse: 0.2361  |  0:00:11s\n",
      "epoch 45 | loss: 0.07646 | val_0_rmse: 0.26334 |  0:00:11s\n",
      "epoch 46 | loss: 0.06134 | val_0_rmse: 0.26276 |  0:00:12s\n",
      "epoch 47 | loss: 0.05791 | val_0_rmse: 0.23957 |  0:00:12s\n",
      "epoch 48 | loss: 0.08143 | val_0_rmse: 0.249   |  0:00:12s\n",
      "epoch 49 | loss: 0.05824 | val_0_rmse: 0.22597 |  0:00:12s\n",
      "epoch 50 | loss: 0.06072 | val_0_rmse: 0.21911 |  0:00:13s\n",
      "epoch 51 | loss: 0.05213 | val_0_rmse: 0.23636 |  0:00:13s\n",
      "epoch 52 | loss: 0.0544  | val_0_rmse: 0.27302 |  0:00:13s\n",
      "epoch 53 | loss: 0.09036 | val_0_rmse: 0.23754 |  0:00:13s\n",
      "epoch 54 | loss: 0.05336 | val_0_rmse: 0.24394 |  0:00:14s\n",
      "epoch 55 | loss: 0.06798 | val_0_rmse: 0.26039 |  0:00:14s\n",
      "epoch 56 | loss: 0.06409 | val_0_rmse: 0.22512 |  0:00:14s\n",
      "epoch 57 | loss: 0.05136 | val_0_rmse: 0.24527 |  0:00:14s\n",
      "epoch 58 | loss: 0.04827 | val_0_rmse: 0.22614 |  0:00:15s\n",
      "epoch 59 | loss: 0.05285 | val_0_rmse: 0.23354 |  0:00:15s\n",
      "epoch 60 | loss: 0.05818 | val_0_rmse: 0.25456 |  0:00:15s\n",
      "epoch 61 | loss: 0.05742 | val_0_rmse: 0.25257 |  0:00:15s\n",
      "epoch 62 | loss: 0.06574 | val_0_rmse: 0.29336 |  0:00:16s\n",
      "epoch 63 | loss: 0.08728 | val_0_rmse: 0.23528 |  0:00:16s\n",
      "epoch 64 | loss: 0.05239 | val_0_rmse: 0.23171 |  0:00:16s\n",
      "epoch 65 | loss: 0.04705 | val_0_rmse: 0.22726 |  0:00:16s\n",
      "epoch 66 | loss: 0.04512 | val_0_rmse: 0.22352 |  0:00:16s\n",
      "epoch 67 | loss: 0.04476 | val_0_rmse: 0.23646 |  0:00:17s\n",
      "epoch 68 | loss: 0.07498 | val_0_rmse: 0.22892 |  0:00:17s\n",
      "epoch 69 | loss: 0.06572 | val_0_rmse: 0.23775 |  0:00:17s\n",
      "epoch 70 | loss: 0.06238 | val_0_rmse: 0.22436 |  0:00:18s\n",
      "epoch 71 | loss: 0.06255 | val_0_rmse: 0.23897 |  0:00:18s\n",
      "epoch 72 | loss: 0.0627  | val_0_rmse: 0.25566 |  0:00:18s\n",
      "epoch 73 | loss: 0.08125 | val_0_rmse: 0.21247 |  0:00:18s\n",
      "epoch 74 | loss: 0.05707 | val_0_rmse: 0.23558 |  0:00:19s\n",
      "epoch 75 | loss: 0.05084 | val_0_rmse: 0.21069 |  0:00:19s\n",
      "epoch 76 | loss: 0.04499 | val_0_rmse: 0.22037 |  0:00:19s\n",
      "epoch 77 | loss: 0.05855 | val_0_rmse: 0.22848 |  0:00:19s\n",
      "epoch 78 | loss: 0.0488  | val_0_rmse: 0.209   |  0:00:20s\n",
      "epoch 79 | loss: 0.05014 | val_0_rmse: 0.21033 |  0:00:20s\n",
      "epoch 80 | loss: 0.04787 | val_0_rmse: 0.21196 |  0:00:20s\n",
      "epoch 81 | loss: 0.04907 | val_0_rmse: 0.20619 |  0:00:20s\n",
      "epoch 82 | loss: 0.04998 | val_0_rmse: 0.20059 |  0:00:21s\n",
      "epoch 83 | loss: 0.05061 | val_0_rmse: 0.23706 |  0:00:21s\n",
      "epoch 84 | loss: 0.04932 | val_0_rmse: 0.19612 |  0:00:21s\n",
      "epoch 85 | loss: 0.04518 | val_0_rmse: 0.21057 |  0:00:21s\n",
      "epoch 86 | loss: 0.04936 | val_0_rmse: 0.19941 |  0:00:22s\n",
      "epoch 87 | loss: 0.04891 | val_0_rmse: 0.2189  |  0:00:22s\n",
      "epoch 88 | loss: 0.03941 | val_0_rmse: 0.19716 |  0:00:22s\n",
      "epoch 89 | loss: 0.04223 | val_0_rmse: 0.19069 |  0:00:22s\n",
      "epoch 90 | loss: 0.03818 | val_0_rmse: 0.21849 |  0:00:23s\n",
      "epoch 91 | loss: 0.03893 | val_0_rmse: 0.19707 |  0:00:23s\n",
      "epoch 92 | loss: 0.03811 | val_0_rmse: 0.19415 |  0:00:23s\n",
      "epoch 93 | loss: 0.03078 | val_0_rmse: 0.20695 |  0:00:23s\n",
      "epoch 94 | loss: 0.03378 | val_0_rmse: 0.19141 |  0:00:24s\n",
      "epoch 95 | loss: 0.03573 | val_0_rmse: 0.19223 |  0:00:24s\n",
      "epoch 96 | loss: 0.03696 | val_0_rmse: 0.20133 |  0:00:24s\n",
      "epoch 97 | loss: 0.04674 | val_0_rmse: 0.18972 |  0:00:24s\n",
      "epoch 98 | loss: 0.03271 | val_0_rmse: 0.19139 |  0:00:24s\n",
      "epoch 99 | loss: 0.0303  | val_0_rmse: 0.19871 |  0:00:25s\n",
      "epoch 100| loss: 0.03097 | val_0_rmse: 0.19462 |  0:00:25s\n",
      "epoch 101| loss: 0.02865 | val_0_rmse: 0.19152 |  0:00:25s\n",
      "epoch 102| loss: 0.03369 | val_0_rmse: 0.21762 |  0:00:25s\n",
      "epoch 103| loss: 0.03718 | val_0_rmse: 0.20201 |  0:00:26s\n",
      "epoch 104| loss: 0.03569 | val_0_rmse: 0.22416 |  0:00:26s\n",
      "epoch 105| loss: 0.04094 | val_0_rmse: 0.19883 |  0:00:26s\n",
      "epoch 106| loss: 0.0343  | val_0_rmse: 0.21834 |  0:00:26s\n",
      "epoch 107| loss: 0.03221 | val_0_rmse: 0.197   |  0:00:27s\n",
      "epoch 108| loss: 0.03694 | val_0_rmse: 0.20921 |  0:00:27s\n",
      "epoch 109| loss: 0.03735 | val_0_rmse: 0.19234 |  0:00:27s\n",
      "epoch 110| loss: 0.041   | val_0_rmse: 0.2224  |  0:00:27s\n",
      "epoch 111| loss: 0.0384  | val_0_rmse: 0.1957  |  0:00:27s\n",
      "epoch 112| loss: 0.04031 | val_0_rmse: 0.22182 |  0:00:28s\n",
      "epoch 113| loss: 0.04165 | val_0_rmse: 0.19255 |  0:00:28s\n",
      "epoch 114| loss: 0.03636 | val_0_rmse: 0.20341 |  0:00:28s\n",
      "epoch 115| loss: 0.04036 | val_0_rmse: 0.20087 |  0:00:28s\n",
      "epoch 116| loss: 0.0395  | val_0_rmse: 0.20272 |  0:00:29s\n",
      "epoch 117| loss: 0.04362 | val_0_rmse: 0.20276 |  0:00:29s\n",
      "epoch 118| loss: 0.03843 | val_0_rmse: 0.20363 |  0:00:29s\n",
      "epoch 119| loss: 0.03809 | val_0_rmse: 0.19755 |  0:00:29s\n",
      "epoch 120| loss: 0.04547 | val_0_rmse: 0.22021 |  0:00:30s\n",
      "epoch 121| loss: 0.03349 | val_0_rmse: 0.19449 |  0:00:30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:35:18,281] Trial 61 finished with value: 0.1897246309551643 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.3060051323901143, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.016317927836304584, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 122| loss: 0.03808 | val_0_rmse: 0.19572 |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 122 with best_epoch = 97 and best_val_0_rmse = 0.18972\n",
      "Trial 061 | rmse_log=0.18972 | RMSE$=35,306 | MAE$=23,779 | MAPE=14.82% | n_d/n_a=24/16 steps=3 lr=0.01632 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.00152| val_0_rmse: 10.78005|  0:00:00s\n",
      "epoch 1  | loss: 83.34329| val_0_rmse: 9.30126 |  0:00:00s\n",
      "epoch 2  | loss: 52.11131| val_0_rmse: 7.70611 |  0:00:00s\n",
      "epoch 3  | loss: 30.23231| val_0_rmse: 6.04037 |  0:00:00s\n",
      "epoch 4  | loss: 17.35822| val_0_rmse: 4.54628 |  0:00:01s\n",
      "epoch 5  | loss: 14.95161| val_0_rmse: 3.5624  |  0:00:01s\n",
      "epoch 6  | loss: 12.72857| val_0_rmse: 3.56783 |  0:00:01s\n",
      "epoch 7  | loss: 6.33348 | val_0_rmse: 4.09747 |  0:00:01s\n",
      "epoch 8  | loss: 4.28242 | val_0_rmse: 3.95867 |  0:00:02s\n",
      "epoch 9  | loss: 2.83705 | val_0_rmse: 2.59957 |  0:00:02s\n",
      "epoch 10 | loss: 1.69303 | val_0_rmse: 1.83658 |  0:00:02s\n",
      "epoch 11 | loss: 1.38093 | val_0_rmse: 2.10907 |  0:00:02s\n",
      "epoch 12 | loss: 1.15688 | val_0_rmse: 1.33782 |  0:00:02s\n",
      "epoch 13 | loss: 0.63324 | val_0_rmse: 1.24973 |  0:00:03s\n",
      "epoch 14 | loss: 0.56039 | val_0_rmse: 1.01934 |  0:00:03s\n",
      "epoch 15 | loss: 0.40751 | val_0_rmse: 0.9904  |  0:00:03s\n",
      "epoch 16 | loss: 0.42227 | val_0_rmse: 0.84028 |  0:00:04s\n",
      "epoch 17 | loss: 0.30681 | val_0_rmse: 0.85762 |  0:00:04s\n",
      "epoch 18 | loss: 0.30319 | val_0_rmse: 0.59197 |  0:00:04s\n",
      "epoch 19 | loss: 0.29806 | val_0_rmse: 0.64857 |  0:00:04s\n",
      "epoch 20 | loss: 0.21196 | val_0_rmse: 0.61712 |  0:00:05s\n",
      "epoch 21 | loss: 0.16461 | val_0_rmse: 0.48528 |  0:00:05s\n",
      "epoch 22 | loss: 0.14436 | val_0_rmse: 0.40323 |  0:00:05s\n",
      "epoch 23 | loss: 0.13744 | val_0_rmse: 0.54935 |  0:00:05s\n",
      "epoch 24 | loss: 0.15223 | val_0_rmse: 0.31734 |  0:00:06s\n",
      "epoch 25 | loss: 0.14806 | val_0_rmse: 0.48942 |  0:00:06s\n",
      "epoch 26 | loss: 0.17456 | val_0_rmse: 0.29203 |  0:00:06s\n",
      "epoch 27 | loss: 0.14807 | val_0_rmse: 0.36098 |  0:00:06s\n",
      "epoch 28 | loss: 0.12425 | val_0_rmse: 0.27533 |  0:00:07s\n",
      "epoch 29 | loss: 0.11028 | val_0_rmse: 0.37007 |  0:00:07s\n",
      "epoch 30 | loss: 0.11386 | val_0_rmse: 0.25357 |  0:00:07s\n",
      "epoch 31 | loss: 0.10136 | val_0_rmse: 0.35027 |  0:00:07s\n",
      "epoch 32 | loss: 0.11293 | val_0_rmse: 0.29004 |  0:00:08s\n",
      "epoch 33 | loss: 0.11668 | val_0_rmse: 0.27763 |  0:00:08s\n",
      "epoch 34 | loss: 0.07726 | val_0_rmse: 0.24038 |  0:00:08s\n",
      "epoch 35 | loss: 0.08456 | val_0_rmse: 0.27243 |  0:00:08s\n",
      "epoch 36 | loss: 0.07194 | val_0_rmse: 0.2337  |  0:00:09s\n",
      "epoch 37 | loss: 0.07847 | val_0_rmse: 0.26604 |  0:00:09s\n",
      "epoch 38 | loss: 0.06259 | val_0_rmse: 0.2511  |  0:00:09s\n",
      "epoch 39 | loss: 0.07762 | val_0_rmse: 0.25193 |  0:00:09s\n",
      "epoch 40 | loss: 0.06919 | val_0_rmse: 0.24737 |  0:00:09s\n",
      "epoch 41 | loss: 0.05754 | val_0_rmse: 0.26628 |  0:00:10s\n",
      "epoch 42 | loss: 0.07428 | val_0_rmse: 0.2396  |  0:00:10s\n",
      "epoch 43 | loss: 0.07012 | val_0_rmse: 0.24763 |  0:00:10s\n",
      "epoch 44 | loss: 0.05488 | val_0_rmse: 0.23053 |  0:00:10s\n",
      "epoch 45 | loss: 0.05028 | val_0_rmse: 0.24151 |  0:00:11s\n",
      "epoch 46 | loss: 0.0534  | val_0_rmse: 0.23553 |  0:00:11s\n",
      "epoch 47 | loss: 0.04388 | val_0_rmse: 0.20535 |  0:00:11s\n",
      "epoch 48 | loss: 0.04945 | val_0_rmse: 0.2091  |  0:00:11s\n",
      "epoch 49 | loss: 0.04701 | val_0_rmse: 0.21908 |  0:00:12s\n",
      "epoch 50 | loss: 0.05471 | val_0_rmse: 0.26252 |  0:00:12s\n",
      "epoch 51 | loss: 0.10429 | val_0_rmse: 0.26673 |  0:00:12s\n",
      "epoch 52 | loss: 0.13515 | val_0_rmse: 0.30247 |  0:00:12s\n",
      "epoch 53 | loss: 0.08899 | val_0_rmse: 0.28209 |  0:00:13s\n",
      "epoch 54 | loss: 0.12153 | val_0_rmse: 0.27275 |  0:00:13s\n",
      "epoch 55 | loss: 0.07633 | val_0_rmse: 0.23864 |  0:00:13s\n",
      "epoch 56 | loss: 0.05718 | val_0_rmse: 0.22286 |  0:00:13s\n",
      "epoch 57 | loss: 0.05604 | val_0_rmse: 0.2165  |  0:00:14s\n",
      "epoch 58 | loss: 0.04637 | val_0_rmse: 0.21712 |  0:00:14s\n",
      "epoch 59 | loss: 0.05428 | val_0_rmse: 0.23853 |  0:00:14s\n",
      "epoch 60 | loss: 0.04596 | val_0_rmse: 0.23183 |  0:00:14s\n",
      "epoch 61 | loss: 0.0579  | val_0_rmse: 0.26603 |  0:00:14s\n",
      "epoch 62 | loss: 0.06678 | val_0_rmse: 0.22967 |  0:00:15s\n",
      "epoch 63 | loss: 0.05804 | val_0_rmse: 0.23355 |  0:00:15s\n",
      "epoch 64 | loss: 0.05633 | val_0_rmse: 0.22106 |  0:00:15s\n",
      "epoch 65 | loss: 0.04231 | val_0_rmse: 0.2011  |  0:00:15s\n",
      "epoch 66 | loss: 0.0363  | val_0_rmse: 0.20007 |  0:00:16s\n",
      "epoch 67 | loss: 0.03886 | val_0_rmse: 0.2095  |  0:00:16s\n",
      "epoch 68 | loss: 0.0323  | val_0_rmse: 0.21791 |  0:00:16s\n",
      "epoch 69 | loss: 0.03532 | val_0_rmse: 0.21711 |  0:00:16s\n",
      "epoch 70 | loss: 0.03343 | val_0_rmse: 0.20288 |  0:00:17s\n",
      "epoch 71 | loss: 0.03531 | val_0_rmse: 0.22709 |  0:00:17s\n",
      "epoch 72 | loss: 0.03232 | val_0_rmse: 0.21046 |  0:00:17s\n",
      "epoch 73 | loss: 0.03266 | val_0_rmse: 0.2174  |  0:00:17s\n",
      "epoch 74 | loss: 0.03332 | val_0_rmse: 0.22908 |  0:00:18s\n",
      "epoch 75 | loss: 0.04276 | val_0_rmse: 0.21841 |  0:00:18s\n",
      "epoch 76 | loss: 0.04316 | val_0_rmse: 0.19815 |  0:00:18s\n",
      "epoch 77 | loss: 0.03626 | val_0_rmse: 0.23378 |  0:00:18s\n",
      "epoch 78 | loss: 0.04125 | val_0_rmse: 0.19729 |  0:00:19s\n",
      "epoch 79 | loss: 0.03862 | val_0_rmse: 0.21406 |  0:00:19s\n",
      "epoch 80 | loss: 0.03124 | val_0_rmse: 0.19195 |  0:00:19s\n",
      "epoch 81 | loss: 0.03181 | val_0_rmse: 0.21306 |  0:00:19s\n",
      "epoch 82 | loss: 0.03842 | val_0_rmse: 0.20762 |  0:00:19s\n",
      "epoch 83 | loss: 0.03011 | val_0_rmse: 0.19685 |  0:00:20s\n",
      "epoch 84 | loss: 0.02854 | val_0_rmse: 0.18951 |  0:00:20s\n",
      "epoch 85 | loss: 0.02877 | val_0_rmse: 0.19481 |  0:00:20s\n",
      "epoch 86 | loss: 0.02924 | val_0_rmse: 0.19119 |  0:00:20s\n",
      "epoch 87 | loss: 0.02748 | val_0_rmse: 0.19061 |  0:00:21s\n",
      "epoch 88 | loss: 0.03006 | val_0_rmse: 0.19005 |  0:00:21s\n",
      "epoch 89 | loss: 0.03397 | val_0_rmse: 0.22539 |  0:00:21s\n",
      "epoch 90 | loss: 0.04225 | val_0_rmse: 0.2005  |  0:00:21s\n",
      "epoch 91 | loss: 0.04113 | val_0_rmse: 0.19847 |  0:00:22s\n",
      "epoch 92 | loss: 0.03078 | val_0_rmse: 0.20238 |  0:00:22s\n",
      "epoch 93 | loss: 0.03122 | val_0_rmse: 0.19198 |  0:00:22s\n",
      "epoch 94 | loss: 0.03066 | val_0_rmse: 0.21081 |  0:00:22s\n",
      "epoch 95 | loss: 0.04747 | val_0_rmse: 0.21463 |  0:00:22s\n",
      "epoch 96 | loss: 0.04579 | val_0_rmse: 0.20775 |  0:00:23s\n",
      "epoch 97 | loss: 0.03822 | val_0_rmse: 0.19778 |  0:00:23s\n",
      "epoch 98 | loss: 0.04294 | val_0_rmse: 0.19293 |  0:00:23s\n",
      "epoch 99 | loss: 0.04728 | val_0_rmse: 0.21813 |  0:00:23s\n",
      "epoch 100| loss: 0.04743 | val_0_rmse: 0.24694 |  0:00:24s\n",
      "epoch 101| loss: 0.04741 | val_0_rmse: 0.20718 |  0:00:24s\n",
      "epoch 102| loss: 0.0436  | val_0_rmse: 0.19508 |  0:00:24s\n",
      "epoch 103| loss: 0.0312  | val_0_rmse: 0.19105 |  0:00:24s\n",
      "epoch 104| loss: 0.02629 | val_0_rmse: 0.19219 |  0:00:25s\n",
      "epoch 105| loss: 0.02619 | val_0_rmse: 0.20898 |  0:00:25s\n",
      "epoch 106| loss: 0.0266  | val_0_rmse: 0.21535 |  0:00:25s\n",
      "epoch 107| loss: 0.0401  | val_0_rmse: 0.21964 |  0:00:25s\n",
      "epoch 108| loss: 0.03529 | val_0_rmse: 0.21128 |  0:00:26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:35:44,962] Trial 62 finished with value: 0.18950765030520383 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2396085795026706, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.01881499607704024, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 109| loss: 0.03252 | val_0_rmse: 0.24181 |  0:00:26s\n",
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 84 and best_val_0_rmse = 0.18951\n",
      "Trial 062 | rmse_log=0.18951 | RMSE$=40,456 | MAE$=22,697 | MAPE=13.51% | n_d/n_a=24/16 steps=3 lr=0.01881 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 125.15786| val_0_rmse: 10.88586|  0:00:00s\n",
      "epoch 1  | loss: 89.09884| val_0_rmse: 9.7385  |  0:00:00s\n",
      "epoch 2  | loss: 58.55739| val_0_rmse: 8.27874 |  0:00:00s\n",
      "epoch 3  | loss: 34.21806| val_0_rmse: 6.80704 |  0:00:00s\n",
      "epoch 4  | loss: 23.15265| val_0_rmse: 5.34184 |  0:00:01s\n",
      "epoch 5  | loss: 15.53063| val_0_rmse: 3.97467 |  0:00:01s\n",
      "epoch 6  | loss: 12.35263| val_0_rmse: 3.09373 |  0:00:01s\n",
      "epoch 7  | loss: 9.50736 | val_0_rmse: 2.96569 |  0:00:01s\n",
      "epoch 8  | loss: 5.18137 | val_0_rmse: 3.32022 |  0:00:02s\n",
      "epoch 9  | loss: 3.77796 | val_0_rmse: 3.30597 |  0:00:02s\n",
      "epoch 10 | loss: 3.52513 | val_0_rmse: 2.55171 |  0:00:02s\n",
      "epoch 11 | loss: 1.77374 | val_0_rmse: 1.76486 |  0:00:02s\n",
      "epoch 12 | loss: 1.63755 | val_0_rmse: 2.00832 |  0:00:03s\n",
      "epoch 13 | loss: 0.89364 | val_0_rmse: 1.64815 |  0:00:03s\n",
      "epoch 14 | loss: 0.89378 | val_0_rmse: 1.4807  |  0:00:03s\n",
      "epoch 15 | loss: 0.98238 | val_0_rmse: 1.49142 |  0:00:03s\n",
      "epoch 16 | loss: 0.61471 | val_0_rmse: 1.28334 |  0:00:04s\n",
      "epoch 17 | loss: 0.3667  | val_0_rmse: 0.93948 |  0:00:04s\n",
      "epoch 18 | loss: 0.417   | val_0_rmse: 1.01694 |  0:00:04s\n",
      "epoch 19 | loss: 0.35738 | val_0_rmse: 1.03143 |  0:00:04s\n",
      "epoch 20 | loss: 0.3015  | val_0_rmse: 0.86005 |  0:00:05s\n",
      "epoch 21 | loss: 0.22475 | val_0_rmse: 0.75907 |  0:00:05s\n",
      "epoch 22 | loss: 0.24698 | val_0_rmse: 0.98671 |  0:00:05s\n",
      "epoch 23 | loss: 0.2876  | val_0_rmse: 0.43254 |  0:00:05s\n",
      "epoch 24 | loss: 0.32301 | val_0_rmse: 0.755   |  0:00:06s\n",
      "epoch 25 | loss: 0.3705  | val_0_rmse: 0.52026 |  0:00:06s\n",
      "epoch 26 | loss: 0.29155 | val_0_rmse: 0.30809 |  0:00:06s\n",
      "epoch 27 | loss: 0.34855 | val_0_rmse: 0.57431 |  0:00:06s\n",
      "epoch 28 | loss: 0.20988 | val_0_rmse: 0.34157 |  0:00:06s\n",
      "epoch 29 | loss: 0.21086 | val_0_rmse: 0.41099 |  0:00:07s\n",
      "epoch 30 | loss: 0.14034 | val_0_rmse: 0.35098 |  0:00:07s\n",
      "epoch 31 | loss: 0.16377 | val_0_rmse: 0.34621 |  0:00:07s\n",
      "epoch 32 | loss: 0.14349 | val_0_rmse: 0.42475 |  0:00:07s\n",
      "epoch 33 | loss: 0.127   | val_0_rmse: 0.34366 |  0:00:08s\n",
      "epoch 34 | loss: 0.12132 | val_0_rmse: 0.32043 |  0:00:08s\n",
      "epoch 35 | loss: 0.13116 | val_0_rmse: 0.35907 |  0:00:08s\n",
      "epoch 36 | loss: 0.1261  | val_0_rmse: 0.37079 |  0:00:08s\n",
      "epoch 37 | loss: 0.1091  | val_0_rmse: 0.36175 |  0:00:09s\n",
      "epoch 38 | loss: 0.10259 | val_0_rmse: 0.29161 |  0:00:09s\n",
      "epoch 39 | loss: 0.09522 | val_0_rmse: 0.31002 |  0:00:09s\n",
      "epoch 40 | loss: 0.08769 | val_0_rmse: 0.33339 |  0:00:09s\n",
      "epoch 41 | loss: 0.08771 | val_0_rmse: 0.27091 |  0:00:10s\n",
      "epoch 42 | loss: 0.08285 | val_0_rmse: 0.28559 |  0:00:10s\n",
      "epoch 43 | loss: 0.08459 | val_0_rmse: 0.30388 |  0:00:10s\n",
      "epoch 44 | loss: 0.09372 | val_0_rmse: 0.23757 |  0:00:10s\n",
      "epoch 45 | loss: 0.07821 | val_0_rmse: 0.2702  |  0:00:10s\n",
      "epoch 46 | loss: 0.07456 | val_0_rmse: 0.29409 |  0:00:11s\n",
      "epoch 47 | loss: 0.08347 | val_0_rmse: 0.24577 |  0:00:11s\n",
      "epoch 48 | loss: 0.08404 | val_0_rmse: 0.24954 |  0:00:11s\n",
      "epoch 49 | loss: 0.06721 | val_0_rmse: 0.26524 |  0:00:11s\n",
      "epoch 50 | loss: 0.06978 | val_0_rmse: 0.24316 |  0:00:12s\n",
      "epoch 51 | loss: 0.08127 | val_0_rmse: 0.29583 |  0:00:12s\n",
      "epoch 52 | loss: 0.07046 | val_0_rmse: 0.2659  |  0:00:12s\n",
      "epoch 53 | loss: 0.08667 | val_0_rmse: 0.31761 |  0:00:12s\n",
      "epoch 54 | loss: 0.09037 | val_0_rmse: 0.27956 |  0:00:12s\n",
      "epoch 55 | loss: 0.12109 | val_0_rmse: 0.28765 |  0:00:13s\n",
      "epoch 56 | loss: 0.11786 | val_0_rmse: 0.2554  |  0:00:13s\n",
      "epoch 57 | loss: 0.07315 | val_0_rmse: 0.28414 |  0:00:13s\n",
      "epoch 58 | loss: 0.10679 | val_0_rmse: 0.26643 |  0:00:13s\n",
      "epoch 59 | loss: 0.09655 | val_0_rmse: 0.27542 |  0:00:14s\n",
      "epoch 60 | loss: 0.07967 | val_0_rmse: 0.25686 |  0:00:14s\n",
      "epoch 61 | loss: 0.08404 | val_0_rmse: 0.26708 |  0:00:14s\n",
      "epoch 62 | loss: 0.09205 | val_0_rmse: 0.26803 |  0:00:14s\n",
      "epoch 63 | loss: 0.08689 | val_0_rmse: 0.26118 |  0:00:15s\n",
      "epoch 64 | loss: 0.06902 | val_0_rmse: 0.24946 |  0:00:15s\n",
      "epoch 65 | loss: 0.04976 | val_0_rmse: 0.25098 |  0:00:15s\n",
      "epoch 66 | loss: 0.0492  | val_0_rmse: 0.28386 |  0:00:15s\n",
      "epoch 67 | loss: 0.06111 | val_0_rmse: 0.24648 |  0:00:16s\n",
      "epoch 68 | loss: 0.05994 | val_0_rmse: 0.23661 |  0:00:16s\n",
      "epoch 69 | loss: 0.05475 | val_0_rmse: 0.22659 |  0:00:16s\n",
      "epoch 70 | loss: 0.05743 | val_0_rmse: 0.23965 |  0:00:16s\n",
      "epoch 71 | loss: 0.05346 | val_0_rmse: 0.24803 |  0:00:17s\n",
      "epoch 72 | loss: 0.05986 | val_0_rmse: 0.25028 |  0:00:17s\n",
      "epoch 73 | loss: 0.05969 | val_0_rmse: 0.27335 |  0:00:17s\n",
      "epoch 74 | loss: 0.0734  | val_0_rmse: 0.24012 |  0:00:17s\n",
      "epoch 75 | loss: 0.06453 | val_0_rmse: 0.24361 |  0:00:17s\n",
      "epoch 76 | loss: 0.0563  | val_0_rmse: 0.2524  |  0:00:18s\n",
      "epoch 77 | loss: 0.05227 | val_0_rmse: 0.2235  |  0:00:18s\n",
      "epoch 78 | loss: 0.04794 | val_0_rmse: 0.22735 |  0:00:18s\n",
      "epoch 79 | loss: 0.0481  | val_0_rmse: 0.22394 |  0:00:18s\n",
      "epoch 80 | loss: 0.0474  | val_0_rmse: 0.2205  |  0:00:19s\n",
      "epoch 81 | loss: 0.0542  | val_0_rmse: 0.22364 |  0:00:19s\n",
      "epoch 82 | loss: 0.04794 | val_0_rmse: 0.21234 |  0:00:19s\n",
      "epoch 83 | loss: 0.04897 | val_0_rmse: 0.21399 |  0:00:19s\n",
      "epoch 84 | loss: 0.03763 | val_0_rmse: 0.21743 |  0:00:20s\n",
      "epoch 85 | loss: 0.04065 | val_0_rmse: 0.2203  |  0:00:20s\n",
      "epoch 86 | loss: 0.03919 | val_0_rmse: 0.21099 |  0:00:20s\n",
      "epoch 87 | loss: 0.04134 | val_0_rmse: 0.21456 |  0:00:20s\n",
      "epoch 88 | loss: 0.05227 | val_0_rmse: 0.25031 |  0:00:20s\n",
      "epoch 89 | loss: 0.05231 | val_0_rmse: 0.22182 |  0:00:21s\n",
      "epoch 90 | loss: 0.04917 | val_0_rmse: 0.21798 |  0:00:21s\n",
      "epoch 91 | loss: 0.04376 | val_0_rmse: 0.23274 |  0:00:21s\n",
      "epoch 92 | loss: 0.04284 | val_0_rmse: 0.21508 |  0:00:21s\n",
      "epoch 93 | loss: 0.03616 | val_0_rmse: 0.20581 |  0:00:22s\n",
      "epoch 94 | loss: 0.03858 | val_0_rmse: 0.20562 |  0:00:22s\n",
      "epoch 95 | loss: 0.04225 | val_0_rmse: 0.2096  |  0:00:22s\n",
      "epoch 96 | loss: 0.03799 | val_0_rmse: 0.23775 |  0:00:22s\n",
      "epoch 97 | loss: 0.05072 | val_0_rmse: 0.23069 |  0:00:23s\n",
      "epoch 98 | loss: 0.04844 | val_0_rmse: 0.26636 |  0:00:23s\n",
      "epoch 99 | loss: 0.04965 | val_0_rmse: 0.24683 |  0:00:23s\n",
      "epoch 100| loss: 0.05772 | val_0_rmse: 0.2283  |  0:00:23s\n",
      "epoch 101| loss: 0.04431 | val_0_rmse: 0.2226  |  0:00:24s\n",
      "epoch 102| loss: 0.04083 | val_0_rmse: 0.21206 |  0:00:24s\n",
      "epoch 103| loss: 0.03778 | val_0_rmse: 0.20193 |  0:00:24s\n",
      "epoch 104| loss: 0.032   | val_0_rmse: 0.2069  |  0:00:24s\n",
      "epoch 105| loss: 0.0336  | val_0_rmse: 0.21675 |  0:00:24s\n",
      "epoch 106| loss: 0.03646 | val_0_rmse: 0.207   |  0:00:25s\n",
      "epoch 107| loss: 0.0309  | val_0_rmse: 0.20465 |  0:00:25s\n",
      "epoch 108| loss: 0.03381 | val_0_rmse: 0.20654 |  0:00:25s\n",
      "epoch 109| loss: 0.03334 | val_0_rmse: 0.20892 |  0:00:25s\n",
      "epoch 110| loss: 0.02933 | val_0_rmse: 0.21197 |  0:00:26s\n",
      "epoch 111| loss: 0.03625 | val_0_rmse: 0.21171 |  0:00:26s\n",
      "epoch 112| loss: 0.02917 | val_0_rmse: 0.22668 |  0:00:26s\n",
      "epoch 113| loss: 0.03703 | val_0_rmse: 0.22982 |  0:00:26s\n",
      "epoch 114| loss: 0.03423 | val_0_rmse: 0.22452 |  0:00:27s\n",
      "epoch 115| loss: 0.03732 | val_0_rmse: 0.22104 |  0:00:27s\n",
      "epoch 116| loss: 0.02984 | val_0_rmse: 0.26038 |  0:00:27s\n",
      "epoch 117| loss: 0.04733 | val_0_rmse: 0.27288 |  0:00:27s\n",
      "epoch 118| loss: 0.10001 | val_0_rmse: 0.22567 |  0:00:28s\n",
      "epoch 119| loss: 0.07202 | val_0_rmse: 0.31559 |  0:00:28s\n",
      "epoch 120| loss: 0.08578 | val_0_rmse: 0.30217 |  0:00:28s\n",
      "epoch 121| loss: 0.08942 | val_0_rmse: 0.24707 |  0:00:28s\n",
      "epoch 122| loss: 0.08199 | val_0_rmse: 0.22917 |  0:00:28s\n",
      "epoch 123| loss: 0.06075 | val_0_rmse: 0.28555 |  0:00:29s\n",
      "epoch 124| loss: 0.0689  | val_0_rmse: 0.29152 |  0:00:29s\n",
      "epoch 125| loss: 0.08404 | val_0_rmse: 0.24338 |  0:00:29s\n",
      "epoch 126| loss: 0.09255 | val_0_rmse: 0.23338 |  0:00:29s\n",
      "epoch 127| loss: 0.04524 | val_0_rmse: 0.26026 |  0:00:30s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:36:15,632] Trial 63 finished with value: 0.2019302222057123 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.3036842714183725, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.017539320495069396, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 128| loss: 0.05072 | val_0_rmse: 0.28301 |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 128 with best_epoch = 103 and best_val_0_rmse = 0.20193\n",
      "Trial 063 | rmse_log=0.20193 | RMSE$=36,159 | MAE$=23,970 | MAPE=15.22% | n_d/n_a=24/16 steps=3 lr=0.01754 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 126.4833| val_0_rmse: 11.13326|  0:00:00s\n",
      "epoch 1  | loss: 94.58538| val_0_rmse: 10.27769|  0:00:00s\n",
      "epoch 2  | loss: 70.40874| val_0_rmse: 9.30653 |  0:00:00s\n",
      "epoch 3  | loss: 49.88062| val_0_rmse: 8.25285 |  0:00:01s\n",
      "epoch 4  | loss: 33.54339| val_0_rmse: 7.15233 |  0:00:01s\n",
      "epoch 5  | loss: 20.78038| val_0_rmse: 5.87604 |  0:00:01s\n",
      "epoch 6  | loss: 17.30044| val_0_rmse: 4.77813 |  0:00:01s\n",
      "epoch 7  | loss: 11.65416| val_0_rmse: 3.8677  |  0:00:02s\n",
      "epoch 8  | loss: 9.13988 | val_0_rmse: 3.41898 |  0:00:02s\n",
      "epoch 9  | loss: 5.28869 | val_0_rmse: 3.51467 |  0:00:02s\n",
      "epoch 10 | loss: 3.61403 | val_0_rmse: 3.37968 |  0:00:02s\n",
      "epoch 11 | loss: 2.65962 | val_0_rmse: 2.32128 |  0:00:03s\n",
      "epoch 12 | loss: 1.58738 | val_0_rmse: 1.92897 |  0:00:03s\n",
      "epoch 13 | loss: 1.07777 | val_0_rmse: 1.99055 |  0:00:03s\n",
      "epoch 14 | loss: 0.94594 | val_0_rmse: 1.41577 |  0:00:03s\n",
      "epoch 15 | loss: 0.75201 | val_0_rmse: 1.37283 |  0:00:03s\n",
      "epoch 16 | loss: 0.63391 | val_0_rmse: 1.25271 |  0:00:04s\n",
      "epoch 17 | loss: 0.48199 | val_0_rmse: 1.06749 |  0:00:04s\n",
      "epoch 18 | loss: 0.3899  | val_0_rmse: 1.01418 |  0:00:04s\n",
      "epoch 19 | loss: 0.37975 | val_0_rmse: 0.99899 |  0:00:05s\n",
      "epoch 20 | loss: 0.33035 | val_0_rmse: 0.8818  |  0:00:05s\n",
      "epoch 21 | loss: 0.32198 | val_0_rmse: 0.6434  |  0:00:05s\n",
      "epoch 22 | loss: 0.26763 | val_0_rmse: 0.75848 |  0:00:05s\n",
      "epoch 23 | loss: 0.24573 | val_0_rmse: 0.63329 |  0:00:06s\n",
      "epoch 24 | loss: 0.21938 | val_0_rmse: 0.62028 |  0:00:06s\n",
      "epoch 25 | loss: 0.2912  | val_0_rmse: 0.68815 |  0:00:06s\n",
      "epoch 26 | loss: 0.1895  | val_0_rmse: 0.38266 |  0:00:06s\n",
      "epoch 27 | loss: 0.216   | val_0_rmse: 0.62996 |  0:00:07s\n",
      "epoch 28 | loss: 0.24481 | val_0_rmse: 0.32507 |  0:00:07s\n",
      "epoch 29 | loss: 0.20032 | val_0_rmse: 0.50871 |  0:00:07s\n",
      "epoch 30 | loss: 0.15314 | val_0_rmse: 0.26099 |  0:00:08s\n",
      "epoch 31 | loss: 0.17233 | val_0_rmse: 0.51449 |  0:00:08s\n",
      "epoch 32 | loss: 0.21236 | val_0_rmse: 0.25875 |  0:00:08s\n",
      "epoch 33 | loss: 0.15612 | val_0_rmse: 0.35844 |  0:00:08s\n",
      "epoch 34 | loss: 0.10611 | val_0_rmse: 0.27085 |  0:00:09s\n",
      "epoch 35 | loss: 0.10848 | val_0_rmse: 0.33052 |  0:00:09s\n",
      "epoch 36 | loss: 0.09126 | val_0_rmse: 0.25094 |  0:00:09s\n",
      "epoch 37 | loss: 0.09255 | val_0_rmse: 0.27245 |  0:00:09s\n",
      "epoch 38 | loss: 0.11144 | val_0_rmse: 0.27058 |  0:00:10s\n",
      "epoch 39 | loss: 0.08807 | val_0_rmse: 0.243   |  0:00:10s\n",
      "epoch 40 | loss: 0.08666 | val_0_rmse: 0.33031 |  0:00:10s\n",
      "epoch 41 | loss: 0.12531 | val_0_rmse: 0.2301  |  0:00:10s\n",
      "epoch 42 | loss: 0.11602 | val_0_rmse: 0.29304 |  0:00:10s\n",
      "epoch 43 | loss: 0.11073 | val_0_rmse: 0.2597  |  0:00:11s\n",
      "epoch 44 | loss: 0.08    | val_0_rmse: 0.2815  |  0:00:11s\n",
      "epoch 45 | loss: 0.07403 | val_0_rmse: 0.23885 |  0:00:11s\n",
      "epoch 46 | loss: 0.0938  | val_0_rmse: 0.27447 |  0:00:11s\n",
      "epoch 47 | loss: 0.08832 | val_0_rmse: 0.26317 |  0:00:12s\n",
      "epoch 48 | loss: 0.08016 | val_0_rmse: 0.22239 |  0:00:12s\n",
      "epoch 49 | loss: 0.09262 | val_0_rmse: 0.22383 |  0:00:12s\n",
      "epoch 50 | loss: 0.09291 | val_0_rmse: 0.27884 |  0:00:12s\n",
      "epoch 51 | loss: 0.08437 | val_0_rmse: 0.26741 |  0:00:13s\n",
      "epoch 52 | loss: 0.11131 | val_0_rmse: 0.32004 |  0:00:13s\n",
      "epoch 53 | loss: 0.10974 | val_0_rmse: 0.27873 |  0:00:13s\n",
      "epoch 54 | loss: 0.09254 | val_0_rmse: 0.2323  |  0:00:13s\n",
      "epoch 55 | loss: 0.10849 | val_0_rmse: 0.25426 |  0:00:14s\n",
      "epoch 56 | loss: 0.08156 | val_0_rmse: 0.23742 |  0:00:14s\n",
      "epoch 57 | loss: 0.09856 | val_0_rmse: 0.29342 |  0:00:14s\n",
      "epoch 58 | loss: 0.09089 | val_0_rmse: 0.23382 |  0:00:14s\n",
      "epoch 59 | loss: 0.07187 | val_0_rmse: 0.24875 |  0:00:15s\n",
      "epoch 60 | loss: 0.06359 | val_0_rmse: 0.22291 |  0:00:15s\n",
      "epoch 61 | loss: 0.05697 | val_0_rmse: 0.22246 |  0:00:15s\n",
      "epoch 62 | loss: 0.05226 | val_0_rmse: 0.22599 |  0:00:15s\n",
      "epoch 63 | loss: 0.05408 | val_0_rmse: 0.24442 |  0:00:16s\n",
      "epoch 64 | loss: 0.07088 | val_0_rmse: 0.25709 |  0:00:16s\n",
      "epoch 65 | loss: 0.10183 | val_0_rmse: 0.22529 |  0:00:16s\n",
      "epoch 66 | loss: 0.07439 | val_0_rmse: 0.22351 |  0:00:16s\n",
      "epoch 67 | loss: 0.05317 | val_0_rmse: 0.22934 |  0:00:16s\n",
      "epoch 68 | loss: 0.05545 | val_0_rmse: 0.22653 |  0:00:17s\n",
      "epoch 69 | loss: 0.0605  | val_0_rmse: 0.23615 |  0:00:17s\n",
      "epoch 70 | loss: 0.05742 | val_0_rmse: 0.24304 |  0:00:17s\n",
      "epoch 71 | loss: 0.05607 | val_0_rmse: 0.2351  |  0:00:17s\n",
      "epoch 72 | loss: 0.0586  | val_0_rmse: 0.24496 |  0:00:18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:36:34,456] Trial 64 finished with value: 0.22238657023613156 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.3391260241767349, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.01484660841192797, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73 | loss: 0.05593 | val_0_rmse: 0.23444 |  0:00:18s\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 48 and best_val_0_rmse = 0.22239\n",
      "Trial 064 | rmse_log=0.22239 | RMSE$=45,620 | MAE$=30,478 | MAPE=16.91% | n_d/n_a=24/16 steps=3 lr=0.01485 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 122.95125| val_0_rmse: 10.65833|  0:00:00s\n",
      "epoch 1  | loss: 80.43151| val_0_rmse: 9.27455 |  0:00:00s\n",
      "epoch 2  | loss: 50.66633| val_0_rmse: 7.61387 |  0:00:00s\n",
      "epoch 3  | loss: 25.39425| val_0_rmse: 5.79776 |  0:00:01s\n",
      "epoch 4  | loss: 15.15798| val_0_rmse: 4.23706 |  0:00:01s\n",
      "epoch 5  | loss: 12.63119| val_0_rmse: 3.42687 |  0:00:01s\n",
      "epoch 6  | loss: 10.82443| val_0_rmse: 3.44852 |  0:00:01s\n",
      "epoch 7  | loss: 4.02728 | val_0_rmse: 3.81049 |  0:00:02s\n",
      "epoch 8  | loss: 4.39886 | val_0_rmse: 3.45553 |  0:00:02s\n",
      "epoch 9  | loss: 2.35915 | val_0_rmse: 2.47106 |  0:00:02s\n",
      "epoch 10 | loss: 1.27352 | val_0_rmse: 2.44913 |  0:00:02s\n",
      "epoch 11 | loss: 0.95406 | val_0_rmse: 1.85147 |  0:00:03s\n",
      "epoch 12 | loss: 0.85443 | val_0_rmse: 1.57034 |  0:00:03s\n",
      "epoch 13 | loss: 0.52045 | val_0_rmse: 1.26981 |  0:00:03s\n",
      "epoch 14 | loss: 0.35581 | val_0_rmse: 1.33972 |  0:00:03s\n",
      "epoch 15 | loss: 0.52011 | val_0_rmse: 0.93988 |  0:00:04s\n",
      "epoch 16 | loss: 0.28169 | val_0_rmse: 0.97519 |  0:00:04s\n",
      "epoch 17 | loss: 0.2849  | val_0_rmse: 0.64576 |  0:00:04s\n",
      "epoch 18 | loss: 0.22178 | val_0_rmse: 0.74645 |  0:00:04s\n",
      "epoch 19 | loss: 0.25432 | val_0_rmse: 0.60004 |  0:00:05s\n",
      "epoch 20 | loss: 0.26365 | val_0_rmse: 0.59058 |  0:00:05s\n",
      "epoch 21 | loss: 0.19012 | val_0_rmse: 0.51907 |  0:00:05s\n",
      "epoch 22 | loss: 0.15438 | val_0_rmse: 0.48342 |  0:00:05s\n",
      "epoch 23 | loss: 0.16895 | val_0_rmse: 0.52027 |  0:00:05s\n",
      "epoch 24 | loss: 0.1937  | val_0_rmse: 0.41002 |  0:00:06s\n",
      "epoch 25 | loss: 0.20994 | val_0_rmse: 0.47027 |  0:00:06s\n",
      "epoch 26 | loss: 0.17206 | val_0_rmse: 0.40434 |  0:00:06s\n",
      "epoch 27 | loss: 0.2192  | val_0_rmse: 0.38728 |  0:00:06s\n",
      "epoch 28 | loss: 0.15471 | val_0_rmse: 0.30579 |  0:00:07s\n",
      "epoch 29 | loss: 0.1445  | val_0_rmse: 0.41688 |  0:00:07s\n",
      "epoch 30 | loss: 0.11778 | val_0_rmse: 0.26364 |  0:00:07s\n",
      "epoch 31 | loss: 0.09067 | val_0_rmse: 0.29849 |  0:00:08s\n",
      "epoch 32 | loss: 0.09948 | val_0_rmse: 0.25779 |  0:00:08s\n",
      "epoch 33 | loss: 0.08188 | val_0_rmse: 0.26964 |  0:00:08s\n",
      "epoch 34 | loss: 0.09459 | val_0_rmse: 0.33166 |  0:00:08s\n",
      "epoch 35 | loss: 0.11012 | val_0_rmse: 0.25414 |  0:00:08s\n",
      "epoch 36 | loss: 0.09008 | val_0_rmse: 0.3141  |  0:00:09s\n",
      "epoch 37 | loss: 0.08573 | val_0_rmse: 0.25696 |  0:00:09s\n",
      "epoch 38 | loss: 0.07637 | val_0_rmse: 0.25231 |  0:00:09s\n",
      "epoch 39 | loss: 0.08521 | val_0_rmse: 0.30352 |  0:00:09s\n",
      "epoch 40 | loss: 0.10655 | val_0_rmse: 0.25675 |  0:00:10s\n",
      "epoch 41 | loss: 0.12536 | val_0_rmse: 0.25302 |  0:00:10s\n",
      "epoch 42 | loss: 0.07855 | val_0_rmse: 0.24953 |  0:00:10s\n",
      "epoch 43 | loss: 0.06456 | val_0_rmse: 0.26865 |  0:00:10s\n",
      "epoch 44 | loss: 0.10457 | val_0_rmse: 0.22877 |  0:00:10s\n",
      "epoch 45 | loss: 0.07326 | val_0_rmse: 0.25303 |  0:00:11s\n",
      "epoch 46 | loss: 0.09164 | val_0_rmse: 0.22839 |  0:00:11s\n",
      "epoch 47 | loss: 0.07139 | val_0_rmse: 0.24018 |  0:00:11s\n",
      "epoch 48 | loss: 0.06564 | val_0_rmse: 0.27202 |  0:00:11s\n",
      "epoch 49 | loss: 0.07034 | val_0_rmse: 0.26226 |  0:00:12s\n",
      "epoch 50 | loss: 0.08594 | val_0_rmse: 0.30002 |  0:00:12s\n",
      "epoch 51 | loss: 0.09037 | val_0_rmse: 0.31254 |  0:00:12s\n",
      "epoch 52 | loss: 0.09505 | val_0_rmse: 0.24975 |  0:00:12s\n",
      "epoch 53 | loss: 0.05639 | val_0_rmse: 0.23194 |  0:00:13s\n",
      "epoch 54 | loss: 0.05585 | val_0_rmse: 0.26514 |  0:00:13s\n",
      "epoch 55 | loss: 0.06778 | val_0_rmse: 0.24911 |  0:00:13s\n",
      "epoch 56 | loss: 0.06368 | val_0_rmse: 0.23814 |  0:00:13s\n",
      "epoch 57 | loss: 0.05507 | val_0_rmse: 0.23956 |  0:00:13s\n",
      "epoch 58 | loss: 0.0479  | val_0_rmse: 0.21956 |  0:00:14s\n",
      "epoch 59 | loss: 0.04884 | val_0_rmse: 0.22877 |  0:00:14s\n",
      "epoch 60 | loss: 0.0555  | val_0_rmse: 0.22176 |  0:00:14s\n",
      "epoch 61 | loss: 0.0611  | val_0_rmse: 0.21921 |  0:00:14s\n",
      "epoch 62 | loss: 0.04675 | val_0_rmse: 0.22974 |  0:00:15s\n",
      "epoch 63 | loss: 0.05091 | val_0_rmse: 0.20736 |  0:00:15s\n",
      "epoch 64 | loss: 0.04456 | val_0_rmse: 0.20892 |  0:00:15s\n",
      "epoch 65 | loss: 0.04645 | val_0_rmse: 0.22165 |  0:00:15s\n",
      "epoch 66 | loss: 0.04321 | val_0_rmse: 0.20542 |  0:00:16s\n",
      "epoch 67 | loss: 0.04625 | val_0_rmse: 0.20567 |  0:00:16s\n",
      "epoch 68 | loss: 0.04526 | val_0_rmse: 0.21367 |  0:00:16s\n",
      "epoch 69 | loss: 0.04792 | val_0_rmse: 0.19742 |  0:00:16s\n",
      "epoch 70 | loss: 0.05239 | val_0_rmse: 0.22919 |  0:00:17s\n",
      "epoch 71 | loss: 0.04612 | val_0_rmse: 0.19583 |  0:00:17s\n",
      "epoch 72 | loss: 0.04535 | val_0_rmse: 0.19758 |  0:00:17s\n",
      "epoch 73 | loss: 0.03946 | val_0_rmse: 0.26669 |  0:00:17s\n",
      "epoch 74 | loss: 0.05985 | val_0_rmse: 0.22147 |  0:00:17s\n",
      "epoch 75 | loss: 0.04324 | val_0_rmse: 0.2071  |  0:00:18s\n",
      "epoch 76 | loss: 0.03962 | val_0_rmse: 0.22619 |  0:00:18s\n",
      "epoch 77 | loss: 0.04191 | val_0_rmse: 0.22795 |  0:00:18s\n",
      "epoch 78 | loss: 0.04108 | val_0_rmse: 0.2088  |  0:00:18s\n",
      "epoch 79 | loss: 0.03848 | val_0_rmse: 0.20869 |  0:00:19s\n",
      "epoch 80 | loss: 0.03998 | val_0_rmse: 0.20098 |  0:00:19s\n",
      "epoch 81 | loss: 0.03812 | val_0_rmse: 0.22589 |  0:00:19s\n",
      "epoch 82 | loss: 0.05364 | val_0_rmse: 0.20028 |  0:00:19s\n",
      "epoch 83 | loss: 0.04123 | val_0_rmse: 0.2145  |  0:00:20s\n",
      "epoch 84 | loss: 0.04352 | val_0_rmse: 0.21281 |  0:00:20s\n",
      "epoch 85 | loss: 0.04933 | val_0_rmse: 0.1882  |  0:00:20s\n",
      "epoch 86 | loss: 0.04843 | val_0_rmse: 0.18984 |  0:00:20s\n",
      "epoch 87 | loss: 0.03761 | val_0_rmse: 0.20347 |  0:00:21s\n",
      "epoch 88 | loss: 0.04023 | val_0_rmse: 0.183   |  0:00:21s\n",
      "epoch 89 | loss: 0.041   | val_0_rmse: 0.23987 |  0:00:21s\n",
      "epoch 90 | loss: 0.06745 | val_0_rmse: 0.21768 |  0:00:21s\n",
      "epoch 91 | loss: 0.09539 | val_0_rmse: 0.20187 |  0:00:22s\n",
      "epoch 92 | loss: 0.05943 | val_0_rmse: 0.21087 |  0:00:22s\n",
      "epoch 93 | loss: 0.05718 | val_0_rmse: 0.22783 |  0:00:22s\n",
      "epoch 94 | loss: 0.0543  | val_0_rmse: 0.23147 |  0:00:22s\n",
      "epoch 95 | loss: 0.05256 | val_0_rmse: 0.23666 |  0:00:22s\n",
      "epoch 96 | loss: 0.05265 | val_0_rmse: 0.2701  |  0:00:23s\n",
      "epoch 97 | loss: 0.07919 | val_0_rmse: 0.1954  |  0:00:23s\n",
      "epoch 98 | loss: 0.04553 | val_0_rmse: 0.24607 |  0:00:23s\n",
      "epoch 99 | loss: 0.04698 | val_0_rmse: 0.22176 |  0:00:23s\n",
      "epoch 100| loss: 0.05476 | val_0_rmse: 0.25986 |  0:00:24s\n",
      "epoch 101| loss: 0.05587 | val_0_rmse: 0.23267 |  0:00:24s\n",
      "epoch 102| loss: 0.05656 | val_0_rmse: 0.22486 |  0:00:24s\n",
      "epoch 103| loss: 0.0515  | val_0_rmse: 0.25935 |  0:00:24s\n",
      "epoch 104| loss: 0.06322 | val_0_rmse: 0.21504 |  0:00:25s\n",
      "epoch 105| loss: 0.0436  | val_0_rmse: 0.23809 |  0:00:25s\n",
      "epoch 106| loss: 0.06028 | val_0_rmse: 0.23846 |  0:00:25s\n",
      "epoch 107| loss: 0.04633 | val_0_rmse: 0.23321 |  0:00:25s\n",
      "epoch 108| loss: 0.04651 | val_0_rmse: 0.24605 |  0:00:25s\n",
      "epoch 109| loss: 0.05659 | val_0_rmse: 0.20763 |  0:00:26s\n",
      "epoch 110| loss: 0.04855 | val_0_rmse: 0.24345 |  0:00:26s\n",
      "epoch 111| loss: 0.06544 | val_0_rmse: 0.20854 |  0:00:26s\n",
      "epoch 112| loss: 0.04311 | val_0_rmse: 0.23495 |  0:00:26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:37:01,956] Trial 65 finished with value: 0.18299613967515574 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.3757031217482896, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.019904720210600198, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 113| loss: 0.0498  | val_0_rmse: 0.19015 |  0:00:27s\n",
      "\n",
      "Early stopping occurred at epoch 113 with best_epoch = 88 and best_val_0_rmse = 0.183\n",
      "Trial 065 | rmse_log=0.18300 | RMSE$=35,013 | MAE$=23,124 | MAPE=14.17% | n_d/n_a=24/16 steps=3 lr=0.01990 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 125.42067| val_0_rmse: 11.0336 |  0:00:00s\n",
      "epoch 1  | loss: 90.09871| val_0_rmse: 10.04737|  0:00:00s\n",
      "epoch 2  | loss: 61.46206| val_0_rmse: 8.88589 |  0:00:00s\n",
      "epoch 3  | loss: 37.95277| val_0_rmse: 7.60974 |  0:00:00s\n",
      "epoch 4  | loss: 23.24179| val_0_rmse: 6.18169 |  0:00:01s\n",
      "epoch 5  | loss: 14.96675| val_0_rmse: 4.81091 |  0:00:01s\n",
      "epoch 6  | loss: 11.74319| val_0_rmse: 3.84178 |  0:00:01s\n",
      "epoch 7  | loss: 10.57937| val_0_rmse: 3.5833  |  0:00:01s\n",
      "epoch 8  | loss: 4.90203 | val_0_rmse: 3.72714 |  0:00:02s\n",
      "epoch 9  | loss: 3.52235 | val_0_rmse: 3.61463 |  0:00:02s\n",
      "epoch 10 | loss: 2.60608 | val_0_rmse: 2.64815 |  0:00:02s\n",
      "epoch 11 | loss: 1.615   | val_0_rmse: 1.99152 |  0:00:02s\n",
      "epoch 12 | loss: 1.2825  | val_0_rmse: 2.07607 |  0:00:03s\n",
      "epoch 13 | loss: 0.95462 | val_0_rmse: 1.40345 |  0:00:03s\n",
      "epoch 14 | loss: 0.78346 | val_0_rmse: 1.31789 |  0:00:03s\n",
      "epoch 15 | loss: 0.63344 | val_0_rmse: 1.07829 |  0:00:03s\n",
      "epoch 16 | loss: 0.50293 | val_0_rmse: 1.01617 |  0:00:04s\n",
      "epoch 17 | loss: 0.3605  | val_0_rmse: 0.898   |  0:00:04s\n",
      "epoch 18 | loss: 0.42951 | val_0_rmse: 0.62377 |  0:00:04s\n",
      "epoch 19 | loss: 0.38679 | val_0_rmse: 0.73313 |  0:00:04s\n",
      "epoch 20 | loss: 0.30029 | val_0_rmse: 0.62    |  0:00:05s\n",
      "epoch 21 | loss: 0.24699 | val_0_rmse: 0.51327 |  0:00:05s\n",
      "epoch 22 | loss: 0.22235 | val_0_rmse: 0.62908 |  0:00:05s\n",
      "epoch 23 | loss: 0.23899 | val_0_rmse: 0.5361  |  0:00:05s\n",
      "epoch 24 | loss: 0.21633 | val_0_rmse: 0.46569 |  0:00:06s\n",
      "epoch 25 | loss: 0.20302 | val_0_rmse: 0.62178 |  0:00:06s\n",
      "epoch 26 | loss: 0.20105 | val_0_rmse: 0.35089 |  0:00:06s\n",
      "epoch 27 | loss: 0.19438 | val_0_rmse: 0.45828 |  0:00:06s\n",
      "epoch 28 | loss: 0.16671 | val_0_rmse: 0.32472 |  0:00:07s\n",
      "epoch 29 | loss: 0.18808 | val_0_rmse: 0.43555 |  0:00:07s\n",
      "epoch 30 | loss: 0.14273 | val_0_rmse: 0.41426 |  0:00:07s\n",
      "epoch 31 | loss: 0.12971 | val_0_rmse: 0.34056 |  0:00:07s\n",
      "epoch 32 | loss: 0.13752 | val_0_rmse: 0.29893 |  0:00:07s\n",
      "epoch 33 | loss: 0.09323 | val_0_rmse: 0.31434 |  0:00:08s\n",
      "epoch 34 | loss: 0.10018 | val_0_rmse: 0.30791 |  0:00:08s\n",
      "epoch 35 | loss: 0.10009 | val_0_rmse: 0.34058 |  0:00:08s\n",
      "epoch 36 | loss: 0.09818 | val_0_rmse: 0.29558 |  0:00:08s\n",
      "epoch 37 | loss: 0.09266 | val_0_rmse: 0.3152  |  0:00:09s\n",
      "epoch 38 | loss: 0.0939  | val_0_rmse: 0.34003 |  0:00:09s\n",
      "epoch 39 | loss: 0.09036 | val_0_rmse: 0.28336 |  0:00:09s\n",
      "epoch 40 | loss: 0.0829  | val_0_rmse: 0.28432 |  0:00:09s\n",
      "epoch 41 | loss: 0.08813 | val_0_rmse: 0.28064 |  0:00:10s\n",
      "epoch 42 | loss: 0.08553 | val_0_rmse: 0.27455 |  0:00:10s\n",
      "epoch 43 | loss: 0.07339 | val_0_rmse: 0.29935 |  0:00:10s\n",
      "epoch 44 | loss: 0.06947 | val_0_rmse: 0.25964 |  0:00:10s\n",
      "epoch 45 | loss: 0.07751 | val_0_rmse: 0.26059 |  0:00:11s\n",
      "epoch 46 | loss: 0.07304 | val_0_rmse: 0.26984 |  0:00:11s\n",
      "epoch 47 | loss: 0.07176 | val_0_rmse: 0.27401 |  0:00:11s\n",
      "epoch 48 | loss: 0.07174 | val_0_rmse: 0.27806 |  0:00:11s\n",
      "epoch 49 | loss: 0.07839 | val_0_rmse: 0.28691 |  0:00:12s\n",
      "epoch 50 | loss: 0.08631 | val_0_rmse: 0.28905 |  0:00:12s\n",
      "epoch 51 | loss: 0.08238 | val_0_rmse: 0.25747 |  0:00:12s\n",
      "epoch 52 | loss: 0.09143 | val_0_rmse: 0.27523 |  0:00:12s\n",
      "epoch 53 | loss: 0.05903 | val_0_rmse: 0.25078 |  0:00:12s\n",
      "epoch 54 | loss: 0.06018 | val_0_rmse: 0.24419 |  0:00:13s\n",
      "epoch 55 | loss: 0.05727 | val_0_rmse: 0.26463 |  0:00:13s\n",
      "epoch 56 | loss: 0.06165 | val_0_rmse: 0.28384 |  0:00:13s\n",
      "epoch 57 | loss: 0.0669  | val_0_rmse: 0.26906 |  0:00:13s\n",
      "epoch 58 | loss: 0.0724  | val_0_rmse: 0.28432 |  0:00:14s\n",
      "epoch 59 | loss: 0.07215 | val_0_rmse: 0.27662 |  0:00:14s\n",
      "epoch 60 | loss: 0.07472 | val_0_rmse: 0.29557 |  0:00:14s\n",
      "epoch 61 | loss: 0.07294 | val_0_rmse: 0.25862 |  0:00:14s\n",
      "epoch 62 | loss: 0.06353 | val_0_rmse: 0.29961 |  0:00:15s\n",
      "epoch 63 | loss: 0.08198 | val_0_rmse: 0.28138 |  0:00:15s\n",
      "epoch 64 | loss: 0.0995  | val_0_rmse: 0.27926 |  0:00:15s\n",
      "epoch 65 | loss: 0.06513 | val_0_rmse: 0.2368  |  0:00:15s\n",
      "epoch 66 | loss: 0.05721 | val_0_rmse: 0.25001 |  0:00:15s\n",
      "epoch 67 | loss: 0.05769 | val_0_rmse: 0.27424 |  0:00:16s\n",
      "epoch 68 | loss: 0.10182 | val_0_rmse: 0.24348 |  0:00:16s\n",
      "epoch 69 | loss: 0.06557 | val_0_rmse: 0.23705 |  0:00:16s\n",
      "epoch 70 | loss: 0.09457 | val_0_rmse: 0.28847 |  0:00:16s\n",
      "epoch 71 | loss: 0.08254 | val_0_rmse: 0.32151 |  0:00:16s\n",
      "epoch 72 | loss: 0.12404 | val_0_rmse: 0.25614 |  0:00:17s\n",
      "epoch 73 | loss: 0.07751 | val_0_rmse: 0.2334  |  0:00:17s\n",
      "epoch 74 | loss: 0.05929 | val_0_rmse: 0.23328 |  0:00:17s\n",
      "epoch 75 | loss: 0.05313 | val_0_rmse: 0.24199 |  0:00:17s\n",
      "epoch 76 | loss: 0.06594 | val_0_rmse: 0.24752 |  0:00:18s\n",
      "epoch 77 | loss: 0.0604  | val_0_rmse: 0.22725 |  0:00:18s\n",
      "epoch 78 | loss: 0.04362 | val_0_rmse: 0.22719 |  0:00:18s\n",
      "epoch 79 | loss: 0.04767 | val_0_rmse: 0.23731 |  0:00:18s\n",
      "epoch 80 | loss: 0.04771 | val_0_rmse: 0.23425 |  0:00:19s\n",
      "epoch 81 | loss: 0.0464  | val_0_rmse: 0.2351  |  0:00:19s\n",
      "epoch 82 | loss: 0.04316 | val_0_rmse: 0.23473 |  0:00:19s\n",
      "epoch 83 | loss: 0.05221 | val_0_rmse: 0.22809 |  0:00:19s\n",
      "epoch 84 | loss: 0.04505 | val_0_rmse: 0.2352  |  0:00:19s\n",
      "epoch 85 | loss: 0.05372 | val_0_rmse: 0.23062 |  0:00:20s\n",
      "epoch 86 | loss: 0.04258 | val_0_rmse: 0.24808 |  0:00:20s\n",
      "epoch 87 | loss: 0.05245 | val_0_rmse: 0.21596 |  0:00:20s\n",
      "epoch 88 | loss: 0.0403  | val_0_rmse: 0.21544 |  0:00:20s\n",
      "epoch 89 | loss: 0.03592 | val_0_rmse: 0.22868 |  0:00:21s\n",
      "epoch 90 | loss: 0.04085 | val_0_rmse: 0.22222 |  0:00:21s\n",
      "epoch 91 | loss: 0.04191 | val_0_rmse: 0.21802 |  0:00:21s\n",
      "epoch 92 | loss: 0.04113 | val_0_rmse: 0.21581 |  0:00:21s\n",
      "epoch 93 | loss: 0.03594 | val_0_rmse: 0.21086 |  0:00:22s\n",
      "epoch 94 | loss: 0.03729 | val_0_rmse: 0.21645 |  0:00:22s\n",
      "epoch 95 | loss: 0.03901 | val_0_rmse: 0.2155  |  0:00:22s\n",
      "epoch 96 | loss: 0.03936 | val_0_rmse: 0.21647 |  0:00:22s\n",
      "epoch 97 | loss: 0.03546 | val_0_rmse: 0.22436 |  0:00:23s\n",
      "epoch 98 | loss: 0.03646 | val_0_rmse: 0.22243 |  0:00:23s\n",
      "epoch 99 | loss: 0.04326 | val_0_rmse: 0.21879 |  0:00:23s\n",
      "epoch 100| loss: 0.03681 | val_0_rmse: 0.22744 |  0:00:23s\n",
      "epoch 101| loss: 0.0439  | val_0_rmse: 0.23977 |  0:00:24s\n",
      "epoch 102| loss: 0.0422  | val_0_rmse: 0.22302 |  0:00:24s\n",
      "epoch 103| loss: 0.03891 | val_0_rmse: 0.2278  |  0:00:24s\n",
      "epoch 104| loss: 0.04179 | val_0_rmse: 0.23259 |  0:00:24s\n",
      "epoch 105| loss: 0.04097 | val_0_rmse: 0.21665 |  0:00:24s\n",
      "epoch 106| loss: 0.03676 | val_0_rmse: 0.22298 |  0:00:25s\n",
      "epoch 107| loss: 0.0343  | val_0_rmse: 0.21441 |  0:00:25s\n",
      "epoch 108| loss: 0.03173 | val_0_rmse: 0.21894 |  0:00:25s\n",
      "epoch 109| loss: 0.03324 | val_0_rmse: 0.22701 |  0:00:25s\n",
      "epoch 110| loss: 0.03616 | val_0_rmse: 0.22575 |  0:00:25s\n",
      "epoch 111| loss: 0.035   | val_0_rmse: 0.21721 |  0:00:26s\n",
      "epoch 112| loss: 0.03287 | val_0_rmse: 0.22923 |  0:00:26s\n",
      "epoch 113| loss: 0.03538 | val_0_rmse: 0.22716 |  0:00:26s\n",
      "epoch 114| loss: 0.03791 | val_0_rmse: 0.22113 |  0:00:26s\n",
      "epoch 115| loss: 0.03287 | val_0_rmse: 0.2222  |  0:00:27s\n",
      "epoch 116| loss: 0.03379 | val_0_rmse: 0.23335 |  0:00:27s\n",
      "epoch 117| loss: 0.04183 | val_0_rmse: 0.24307 |  0:00:27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:37:30,226] Trial 66 finished with value: 0.210863126845509 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2579579184658392, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.016408152452082516, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 118| loss: 0.05325 | val_0_rmse: 0.24078 |  0:00:27s\n",
      "\n",
      "Early stopping occurred at epoch 118 with best_epoch = 93 and best_val_0_rmse = 0.21086\n",
      "Trial 066 | rmse_log=0.21086 | RMSE$=40,201 | MAE$=26,939 | MAPE=16.08% | n_d/n_a=24/16 steps=3 lr=0.01641 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 125.62225| val_0_rmse: 11.53427|  0:00:00s\n",
      "epoch 1  | loss: 107.95474| val_0_rmse: 11.1607 |  0:00:00s\n",
      "epoch 2  | loss: 90.08437| val_0_rmse: 10.68428|  0:00:00s\n",
      "epoch 3  | loss: 76.77302| val_0_rmse: 10.19387|  0:00:00s\n",
      "epoch 4  | loss: 64.1987 | val_0_rmse: 9.59783 |  0:00:00s\n",
      "epoch 5  | loss: 52.5496 | val_0_rmse: 8.90927 |  0:00:01s\n",
      "epoch 6  | loss: 43.10016| val_0_rmse: 8.17405 |  0:00:01s\n",
      "epoch 7  | loss: 32.704  | val_0_rmse: 7.28652 |  0:00:01s\n",
      "epoch 8  | loss: 26.70918| val_0_rmse: 6.42162 |  0:00:01s\n",
      "epoch 9  | loss: 20.0508 | val_0_rmse: 5.38196 |  0:00:01s\n",
      "epoch 10 | loss: 16.60307| val_0_rmse: 4.40396 |  0:00:02s\n",
      "epoch 11 | loss: 12.26008| val_0_rmse: 3.62227 |  0:00:02s\n",
      "epoch 12 | loss: 11.17201| val_0_rmse: 2.95239 |  0:00:02s\n",
      "epoch 13 | loss: 10.6533 | val_0_rmse: 2.47144 |  0:00:02s\n",
      "epoch 14 | loss: 9.15072 | val_0_rmse: 2.21291 |  0:00:02s\n",
      "epoch 15 | loss: 7.33238 | val_0_rmse: 2.30699 |  0:00:02s\n",
      "epoch 16 | loss: 5.56016 | val_0_rmse: 2.36121 |  0:00:03s\n",
      "epoch 17 | loss: 4.43606 | val_0_rmse: 2.39766 |  0:00:03s\n",
      "epoch 18 | loss: 3.49088 | val_0_rmse: 2.52784 |  0:00:03s\n",
      "epoch 19 | loss: 2.724   | val_0_rmse: 2.38118 |  0:00:03s\n",
      "epoch 20 | loss: 2.20265 | val_0_rmse: 2.04314 |  0:00:03s\n",
      "epoch 21 | loss: 1.75074 | val_0_rmse: 1.5531  |  0:00:03s\n",
      "epoch 22 | loss: 1.33208 | val_0_rmse: 1.13982 |  0:00:04s\n",
      "epoch 23 | loss: 1.24173 | val_0_rmse: 1.02773 |  0:00:04s\n",
      "epoch 24 | loss: 1.09764 | val_0_rmse: 1.11715 |  0:00:04s\n",
      "epoch 25 | loss: 0.89397 | val_0_rmse: 1.1002  |  0:00:04s\n",
      "epoch 26 | loss: 0.79312 | val_0_rmse: 0.83024 |  0:00:04s\n",
      "epoch 27 | loss: 0.71666 | val_0_rmse: 0.60657 |  0:00:04s\n",
      "epoch 28 | loss: 0.59986 | val_0_rmse: 0.52031 |  0:00:05s\n",
      "epoch 29 | loss: 0.50461 | val_0_rmse: 0.6317  |  0:00:05s\n",
      "epoch 30 | loss: 0.51741 | val_0_rmse: 0.67918 |  0:00:05s\n",
      "epoch 31 | loss: 0.61989 | val_0_rmse: 0.59528 |  0:00:05s\n",
      "epoch 32 | loss: 0.41504 | val_0_rmse: 0.37624 |  0:00:05s\n",
      "epoch 33 | loss: 0.41994 | val_0_rmse: 0.31157 |  0:00:05s\n",
      "epoch 34 | loss: 0.48035 | val_0_rmse: 0.31159 |  0:00:06s\n",
      "epoch 35 | loss: 0.37659 | val_0_rmse: 0.39236 |  0:00:06s\n",
      "epoch 36 | loss: 0.37622 | val_0_rmse: 0.45477 |  0:00:06s\n",
      "epoch 37 | loss: 0.37092 | val_0_rmse: 0.40641 |  0:00:06s\n",
      "epoch 38 | loss: 0.28906 | val_0_rmse: 0.30894 |  0:00:06s\n",
      "epoch 39 | loss: 0.28548 | val_0_rmse: 0.29141 |  0:00:07s\n",
      "epoch 40 | loss: 0.32844 | val_0_rmse: 0.30767 |  0:00:07s\n",
      "epoch 41 | loss: 0.22817 | val_0_rmse: 0.31527 |  0:00:07s\n",
      "epoch 42 | loss: 0.31962 | val_0_rmse: 0.28057 |  0:00:07s\n",
      "epoch 43 | loss: 0.24076 | val_0_rmse: 0.27595 |  0:00:07s\n",
      "epoch 44 | loss: 0.21413 | val_0_rmse: 0.29063 |  0:00:07s\n",
      "epoch 45 | loss: 0.19693 | val_0_rmse: 0.26988 |  0:00:07s\n",
      "epoch 46 | loss: 0.17602 | val_0_rmse: 0.27706 |  0:00:08s\n",
      "epoch 47 | loss: 0.21776 | val_0_rmse: 0.27365 |  0:00:08s\n",
      "epoch 48 | loss: 0.21255 | val_0_rmse: 0.25823 |  0:00:08s\n",
      "epoch 49 | loss: 0.1606  | val_0_rmse: 0.27026 |  0:00:08s\n",
      "epoch 50 | loss: 0.1558  | val_0_rmse: 0.2907  |  0:00:08s\n",
      "epoch 51 | loss: 0.16663 | val_0_rmse: 0.26427 |  0:00:08s\n",
      "epoch 52 | loss: 0.13383 | val_0_rmse: 0.27393 |  0:00:09s\n",
      "epoch 53 | loss: 0.11899 | val_0_rmse: 0.26966 |  0:00:09s\n",
      "epoch 54 | loss: 0.13818 | val_0_rmse: 0.26222 |  0:00:09s\n",
      "epoch 55 | loss: 0.1199  | val_0_rmse: 0.26945 |  0:00:09s\n",
      "epoch 56 | loss: 0.12297 | val_0_rmse: 0.25806 |  0:00:09s\n",
      "epoch 57 | loss: 0.10181 | val_0_rmse: 0.25496 |  0:00:09s\n",
      "epoch 58 | loss: 0.11433 | val_0_rmse: 0.25432 |  0:00:10s\n",
      "epoch 59 | loss: 0.11409 | val_0_rmse: 0.25487 |  0:00:10s\n",
      "epoch 60 | loss: 0.11214 | val_0_rmse: 0.24419 |  0:00:10s\n",
      "epoch 61 | loss: 0.08467 | val_0_rmse: 0.27494 |  0:00:10s\n",
      "epoch 62 | loss: 0.1001  | val_0_rmse: 0.25877 |  0:00:10s\n",
      "epoch 63 | loss: 0.14823 | val_0_rmse: 0.23944 |  0:00:11s\n",
      "epoch 64 | loss: 0.0792  | val_0_rmse: 0.2556  |  0:00:11s\n",
      "epoch 65 | loss: 0.08749 | val_0_rmse: 0.22993 |  0:00:11s\n",
      "epoch 66 | loss: 0.07252 | val_0_rmse: 0.25711 |  0:00:11s\n",
      "epoch 67 | loss: 0.07743 | val_0_rmse: 0.26949 |  0:00:11s\n",
      "epoch 68 | loss: 0.09176 | val_0_rmse: 0.2265  |  0:00:11s\n",
      "epoch 69 | loss: 0.07036 | val_0_rmse: 0.22992 |  0:00:12s\n",
      "epoch 70 | loss: 0.07416 | val_0_rmse: 0.22442 |  0:00:12s\n",
      "epoch 71 | loss: 0.06943 | val_0_rmse: 0.22703 |  0:00:12s\n",
      "epoch 72 | loss: 0.07124 | val_0_rmse: 0.22087 |  0:00:12s\n",
      "epoch 73 | loss: 0.06112 | val_0_rmse: 0.21971 |  0:00:12s\n",
      "epoch 74 | loss: 0.06701 | val_0_rmse: 0.21556 |  0:00:12s\n",
      "epoch 75 | loss: 0.07    | val_0_rmse: 0.21839 |  0:00:13s\n",
      "epoch 76 | loss: 0.07199 | val_0_rmse: 0.23112 |  0:00:13s\n",
      "epoch 77 | loss: 0.06126 | val_0_rmse: 0.23415 |  0:00:13s\n",
      "epoch 78 | loss: 0.06953 | val_0_rmse: 0.22151 |  0:00:13s\n",
      "epoch 79 | loss: 0.05144 | val_0_rmse: 0.21726 |  0:00:13s\n",
      "epoch 80 | loss: 0.06253 | val_0_rmse: 0.23308 |  0:00:13s\n",
      "epoch 81 | loss: 0.07313 | val_0_rmse: 0.22215 |  0:00:13s\n",
      "epoch 82 | loss: 0.06244 | val_0_rmse: 0.22591 |  0:00:14s\n",
      "epoch 83 | loss: 0.06672 | val_0_rmse: 0.24468 |  0:00:14s\n",
      "epoch 84 | loss: 0.06183 | val_0_rmse: 0.23511 |  0:00:14s\n",
      "epoch 85 | loss: 0.05691 | val_0_rmse: 0.22681 |  0:00:14s\n",
      "epoch 86 | loss: 0.06599 | val_0_rmse: 0.22329 |  0:00:14s\n",
      "epoch 87 | loss: 0.05347 | val_0_rmse: 0.23002 |  0:00:14s\n",
      "epoch 88 | loss: 0.0579  | val_0_rmse: 0.22173 |  0:00:15s\n",
      "epoch 89 | loss: 0.07139 | val_0_rmse: 0.2302  |  0:00:15s\n",
      "epoch 90 | loss: 0.04785 | val_0_rmse: 0.27301 |  0:00:15s\n",
      "epoch 91 | loss: 0.07208 | val_0_rmse: 0.25128 |  0:00:15s\n",
      "epoch 92 | loss: 0.05641 | val_0_rmse: 0.23397 |  0:00:15s\n",
      "epoch 93 | loss: 0.08574 | val_0_rmse: 0.2464  |  0:00:15s\n",
      "epoch 94 | loss: 0.09305 | val_0_rmse: 0.21376 |  0:00:16s\n",
      "epoch 95 | loss: 0.05566 | val_0_rmse: 0.27033 |  0:00:16s\n",
      "epoch 96 | loss: 0.09585 | val_0_rmse: 0.2721  |  0:00:16s\n",
      "epoch 97 | loss: 0.08469 | val_0_rmse: 0.22113 |  0:00:16s\n",
      "epoch 98 | loss: 0.06347 | val_0_rmse: 0.2209  |  0:00:16s\n",
      "epoch 99 | loss: 0.05603 | val_0_rmse: 0.23142 |  0:00:16s\n",
      "epoch 100| loss: 0.05328 | val_0_rmse: 0.2252  |  0:00:17s\n",
      "epoch 101| loss: 0.04839 | val_0_rmse: 0.21886 |  0:00:17s\n",
      "epoch 102| loss: 0.05665 | val_0_rmse: 0.22025 |  0:00:17s\n",
      "epoch 103| loss: 0.05241 | val_0_rmse: 0.24107 |  0:00:17s\n",
      "epoch 104| loss: 0.04858 | val_0_rmse: 0.22698 |  0:00:17s\n",
      "epoch 105| loss: 0.04775 | val_0_rmse: 0.21911 |  0:00:17s\n",
      "epoch 106| loss: 0.05085 | val_0_rmse: 0.22894 |  0:00:18s\n",
      "epoch 107| loss: 0.04691 | val_0_rmse: 0.22819 |  0:00:18s\n",
      "epoch 108| loss: 0.04779 | val_0_rmse: 0.23306 |  0:00:18s\n",
      "epoch 109| loss: 0.05187 | val_0_rmse: 0.24688 |  0:00:18s\n",
      "epoch 110| loss: 0.04514 | val_0_rmse: 0.23789 |  0:00:18s\n",
      "epoch 111| loss: 0.04616 | val_0_rmse: 0.2131  |  0:00:18s\n",
      "epoch 112| loss: 0.0538  | val_0_rmse: 0.21565 |  0:00:19s\n",
      "epoch 113| loss: 0.04394 | val_0_rmse: 0.24136 |  0:00:19s\n",
      "epoch 114| loss: 0.04045 | val_0_rmse: 0.23836 |  0:00:19s\n",
      "epoch 115| loss: 0.04619 | val_0_rmse: 0.22349 |  0:00:19s\n",
      "epoch 116| loss: 0.03695 | val_0_rmse: 0.20768 |  0:00:19s\n",
      "epoch 117| loss: 0.03969 | val_0_rmse: 0.20936 |  0:00:19s\n",
      "epoch 118| loss: 0.04128 | val_0_rmse: 0.22933 |  0:00:20s\n",
      "epoch 119| loss: 0.04139 | val_0_rmse: 0.22127 |  0:00:20s\n",
      "epoch 120| loss: 0.03953 | val_0_rmse: 0.20882 |  0:00:20s\n",
      "epoch 121| loss: 0.04642 | val_0_rmse: 0.21282 |  0:00:20s\n",
      "epoch 122| loss: 0.03906 | val_0_rmse: 0.21012 |  0:00:20s\n",
      "epoch 123| loss: 0.03561 | val_0_rmse: 0.20631 |  0:00:20s\n",
      "epoch 124| loss: 0.04133 | val_0_rmse: 0.21266 |  0:00:21s\n",
      "epoch 125| loss: 0.03826 | val_0_rmse: 0.20524 |  0:00:21s\n",
      "epoch 126| loss: 0.04107 | val_0_rmse: 0.21057 |  0:00:21s\n",
      "epoch 127| loss: 0.03794 | val_0_rmse: 0.22589 |  0:00:21s\n",
      "epoch 128| loss: 0.0407  | val_0_rmse: 0.20488 |  0:00:21s\n",
      "epoch 129| loss: 0.04066 | val_0_rmse: 0.2112  |  0:00:21s\n",
      "epoch 130| loss: 0.03708 | val_0_rmse: 0.21216 |  0:00:22s\n",
      "epoch 131| loss: 0.03375 | val_0_rmse: 0.21166 |  0:00:22s\n",
      "epoch 132| loss: 0.03915 | val_0_rmse: 0.21282 |  0:00:22s\n",
      "epoch 133| loss: 0.03469 | val_0_rmse: 0.20166 |  0:00:22s\n",
      "epoch 134| loss: 0.03502 | val_0_rmse: 0.20316 |  0:00:22s\n",
      "epoch 135| loss: 0.03579 | val_0_rmse: 0.21414 |  0:00:22s\n",
      "epoch 136| loss: 0.03583 | val_0_rmse: 0.20399 |  0:00:23s\n",
      "epoch 137| loss: 0.03833 | val_0_rmse: 0.21465 |  0:00:23s\n",
      "epoch 138| loss: 0.03417 | val_0_rmse: 0.21204 |  0:00:23s\n",
      "epoch 139| loss: 0.03816 | val_0_rmse: 0.2021  |  0:00:23s\n",
      "epoch 140| loss: 0.03556 | val_0_rmse: 0.21527 |  0:00:23s\n",
      "epoch 141| loss: 0.03481 | val_0_rmse: 0.21033 |  0:00:23s\n",
      "epoch 142| loss: 0.03252 | val_0_rmse: 0.20686 |  0:00:24s\n",
      "epoch 143| loss: 0.03337 | val_0_rmse: 0.20578 |  0:00:24s\n",
      "epoch 144| loss: 0.03207 | val_0_rmse: 0.21094 |  0:00:24s\n",
      "epoch 145| loss: 0.03057 | val_0_rmse: 0.20632 |  0:00:24s\n",
      "epoch 146| loss: 0.03225 | val_0_rmse: 0.21446 |  0:00:24s\n",
      "epoch 147| loss: 0.02902 | val_0_rmse: 0.2084  |  0:00:24s\n",
      "epoch 148| loss: 0.0297  | val_0_rmse: 0.20525 |  0:00:24s\n",
      "epoch 149| loss: 0.03128 | val_0_rmse: 0.21488 |  0:00:25s\n",
      "epoch 150| loss: 0.0311  | val_0_rmse: 0.20351 |  0:00:25s\n",
      "epoch 151| loss: 0.02895 | val_0_rmse: 0.20895 |  0:00:25s\n",
      "epoch 152| loss: 0.0307  | val_0_rmse: 0.21207 |  0:00:25s\n",
      "epoch 153| loss: 0.03439 | val_0_rmse: 0.20842 |  0:00:25s\n",
      "epoch 154| loss: 0.0342  | val_0_rmse: 0.21557 |  0:00:25s\n",
      "epoch 155| loss: 0.03586 | val_0_rmse: 0.19668 |  0:00:26s\n",
      "epoch 156| loss: 0.03293 | val_0_rmse: 0.20219 |  0:00:26s\n",
      "epoch 157| loss: 0.0276  | val_0_rmse: 0.21679 |  0:00:26s\n",
      "epoch 158| loss: 0.02972 | val_0_rmse: 0.20465 |  0:00:26s\n",
      "epoch 159| loss: 0.03084 | val_0_rmse: 0.20661 |  0:00:26s\n",
      "epoch 160| loss: 0.02783 | val_0_rmse: 0.20299 |  0:00:26s\n",
      "epoch 161| loss: 0.02912 | val_0_rmse: 0.20123 |  0:00:27s\n",
      "epoch 162| loss: 0.03187 | val_0_rmse: 0.21637 |  0:00:27s\n",
      "epoch 163| loss: 0.02899 | val_0_rmse: 0.1976  |  0:00:27s\n",
      "epoch 164| loss: 0.02743 | val_0_rmse: 0.20583 |  0:00:27s\n",
      "epoch 165| loss: 0.02619 | val_0_rmse: 0.21602 |  0:00:27s\n",
      "epoch 166| loss: 0.03096 | val_0_rmse: 0.19822 |  0:00:27s\n",
      "epoch 167| loss: 0.03095 | val_0_rmse: 0.20928 |  0:00:28s\n",
      "epoch 168| loss: 0.02792 | val_0_rmse: 0.20058 |  0:00:28s\n",
      "epoch 169| loss: 0.02906 | val_0_rmse: 0.20565 |  0:00:28s\n",
      "epoch 170| loss: 0.02866 | val_0_rmse: 0.20955 |  0:00:28s\n",
      "epoch 171| loss: 0.02886 | val_0_rmse: 0.19935 |  0:00:28s\n",
      "epoch 172| loss: 0.02872 | val_0_rmse: 0.22513 |  0:00:28s\n",
      "epoch 173| loss: 0.02966 | val_0_rmse: 0.20512 |  0:00:29s\n",
      "epoch 174| loss: 0.02752 | val_0_rmse: 0.20411 |  0:00:29s\n",
      "epoch 175| loss: 0.02766 | val_0_rmse: 0.20395 |  0:00:29s\n",
      "epoch 176| loss: 0.02728 | val_0_rmse: 0.20246 |  0:00:29s\n",
      "epoch 177| loss: 0.02647 | val_0_rmse: 0.22066 |  0:00:29s\n",
      "epoch 178| loss: 0.02932 | val_0_rmse: 0.20244 |  0:00:29s\n",
      "epoch 179| loss: 0.02547 | val_0_rmse: 0.20407 |  0:00:30s\n",
      "epoch 180| loss: 0.03072 | val_0_rmse: 0.23237 |  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 180 with best_epoch = 155 and best_val_0_rmse = 0.19668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:38:00,748] Trial 67 finished with value: 0.19667770801028214 and parameters: {'n_d': 24, 'n_a': 48, 'n_steps': 3, 'gamma': 1.3758418192229467, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.019798314563181575, 'batch_size': 2048, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 067 | rmse_log=0.19668 | RMSE$=39,485 | MAE$=24,082 | MAPE=14.59% | n_d/n_a=24/48 steps=3 lr=0.01980 batch=2048 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 183.31224| val_0_rmse: 11.80422|  0:00:00s\n",
      "epoch 1  | loss: 144.79488| val_0_rmse: 11.35592|  0:00:00s\n",
      "epoch 2  | loss: 117.82063| val_0_rmse: 10.89529|  0:00:00s\n",
      "epoch 3  | loss: 94.05116| val_0_rmse: 10.24931|  0:00:01s\n",
      "epoch 4  | loss: 73.00313| val_0_rmse: 9.41677 |  0:00:01s\n",
      "epoch 5  | loss: 52.9918 | val_0_rmse: 8.41967 |  0:00:01s\n",
      "epoch 6  | loss: 37.17975| val_0_rmse: 7.24132 |  0:00:02s\n",
      "epoch 7  | loss: 28.57299| val_0_rmse: 6.05123 |  0:00:02s\n",
      "epoch 8  | loss: 17.37117| val_0_rmse: 4.83211 |  0:00:02s\n",
      "epoch 9  | loss: 15.75021| val_0_rmse: 3.86925 |  0:00:03s\n",
      "epoch 10 | loss: 14.65573| val_0_rmse: 3.2155  |  0:00:03s\n",
      "epoch 11 | loss: 13.72539| val_0_rmse: 3.01565 |  0:00:03s\n",
      "epoch 12 | loss: 12.22487| val_0_rmse: 3.03413 |  0:00:04s\n",
      "epoch 13 | loss: 8.71743 | val_0_rmse: 3.32118 |  0:00:04s\n",
      "epoch 14 | loss: 5.48778 | val_0_rmse: 3.45222 |  0:00:04s\n",
      "epoch 15 | loss: 4.68811 | val_0_rmse: 3.10207 |  0:00:04s\n",
      "epoch 16 | loss: 3.67872 | val_0_rmse: 2.52329 |  0:00:05s\n",
      "epoch 17 | loss: 2.74965 | val_0_rmse: 2.00923 |  0:00:05s\n",
      "epoch 18 | loss: 2.01978 | val_0_rmse: 2.11901 |  0:00:05s\n",
      "epoch 19 | loss: 1.36414 | val_0_rmse: 2.13503 |  0:00:06s\n",
      "epoch 20 | loss: 1.32754 | val_0_rmse: 1.44101 |  0:00:06s\n",
      "epoch 21 | loss: 1.16184 | val_0_rmse: 1.1917  |  0:00:06s\n",
      "epoch 22 | loss: 0.86386 | val_0_rmse: 1.49546 |  0:00:06s\n",
      "epoch 23 | loss: 0.95981 | val_0_rmse: 0.97107 |  0:00:07s\n",
      "epoch 24 | loss: 0.93854 | val_0_rmse: 0.77656 |  0:00:07s\n",
      "epoch 25 | loss: 0.71591 | val_0_rmse: 0.87926 |  0:00:07s\n",
      "epoch 26 | loss: 0.7487  | val_0_rmse: 0.66905 |  0:00:08s\n",
      "epoch 27 | loss: 0.65931 | val_0_rmse: 0.77628 |  0:00:08s\n",
      "epoch 28 | loss: 0.57941 | val_0_rmse: 0.75764 |  0:00:08s\n",
      "epoch 29 | loss: 0.48277 | val_0_rmse: 0.63145 |  0:00:08s\n",
      "epoch 30 | loss: 0.37457 | val_0_rmse: 0.61323 |  0:00:09s\n",
      "epoch 31 | loss: 0.39974 | val_0_rmse: 0.68442 |  0:00:09s\n",
      "epoch 32 | loss: 0.39734 | val_0_rmse: 0.50316 |  0:00:09s\n",
      "epoch 33 | loss: 0.32208 | val_0_rmse: 0.60988 |  0:00:10s\n",
      "epoch 34 | loss: 0.35136 | val_0_rmse: 0.39005 |  0:00:10s\n",
      "epoch 35 | loss: 0.42714 | val_0_rmse: 0.64609 |  0:00:10s\n",
      "epoch 36 | loss: 0.29292 | val_0_rmse: 0.44923 |  0:00:10s\n",
      "epoch 37 | loss: 0.28102 | val_0_rmse: 0.44878 |  0:00:11s\n",
      "epoch 38 | loss: 0.28552 | val_0_rmse: 0.38957 |  0:00:11s\n",
      "epoch 39 | loss: 0.21997 | val_0_rmse: 0.52574 |  0:00:11s\n",
      "epoch 40 | loss: 0.20218 | val_0_rmse: 0.41363 |  0:00:11s\n",
      "epoch 41 | loss: 0.25445 | val_0_rmse: 0.47273 |  0:00:12s\n",
      "epoch 42 | loss: 0.20611 | val_0_rmse: 0.46451 |  0:00:12s\n",
      "epoch 43 | loss: 0.22795 | val_0_rmse: 0.46098 |  0:00:12s\n",
      "epoch 44 | loss: 0.21252 | val_0_rmse: 0.35325 |  0:00:13s\n",
      "epoch 45 | loss: 0.20934 | val_0_rmse: 0.53325 |  0:00:13s\n",
      "epoch 46 | loss: 0.17614 | val_0_rmse: 0.43048 |  0:00:13s\n",
      "epoch 47 | loss: 0.16846 | val_0_rmse: 0.38832 |  0:00:13s\n",
      "epoch 48 | loss: 0.15628 | val_0_rmse: 0.41099 |  0:00:14s\n",
      "epoch 49 | loss: 0.16985 | val_0_rmse: 0.34214 |  0:00:14s\n",
      "epoch 50 | loss: 0.20207 | val_0_rmse: 0.39513 |  0:00:14s\n",
      "epoch 51 | loss: 0.16722 | val_0_rmse: 0.31714 |  0:00:15s\n",
      "epoch 52 | loss: 0.20518 | val_0_rmse: 0.40534 |  0:00:15s\n",
      "epoch 53 | loss: 0.17642 | val_0_rmse: 0.30505 |  0:00:15s\n",
      "epoch 54 | loss: 0.17975 | val_0_rmse: 0.44913 |  0:00:15s\n",
      "epoch 55 | loss: 0.20022 | val_0_rmse: 0.33063 |  0:00:16s\n",
      "epoch 56 | loss: 0.19434 | val_0_rmse: 0.39067 |  0:00:16s\n",
      "epoch 57 | loss: 0.18141 | val_0_rmse: 0.31875 |  0:00:16s\n",
      "epoch 58 | loss: 0.15654 | val_0_rmse: 0.34206 |  0:00:17s\n",
      "epoch 59 | loss: 0.14031 | val_0_rmse: 0.31424 |  0:00:17s\n",
      "epoch 60 | loss: 0.13341 | val_0_rmse: 0.32183 |  0:00:17s\n",
      "epoch 61 | loss: 0.15448 | val_0_rmse: 0.31108 |  0:00:17s\n",
      "epoch 62 | loss: 0.16443 | val_0_rmse: 0.32188 |  0:00:18s\n",
      "epoch 63 | loss: 0.16526 | val_0_rmse: 0.30117 |  0:00:18s\n",
      "epoch 64 | loss: 0.13696 | val_0_rmse: 0.29896 |  0:00:18s\n",
      "epoch 65 | loss: 0.12331 | val_0_rmse: 0.30101 |  0:00:18s\n",
      "epoch 66 | loss: 0.14281 | val_0_rmse: 0.31659 |  0:00:19s\n",
      "epoch 67 | loss: 0.13341 | val_0_rmse: 0.35617 |  0:00:19s\n",
      "epoch 68 | loss: 0.15048 | val_0_rmse: 0.29298 |  0:00:19s\n",
      "epoch 69 | loss: 0.14403 | val_0_rmse: 0.34896 |  0:00:20s\n",
      "epoch 70 | loss: 0.15602 | val_0_rmse: 0.28901 |  0:00:20s\n",
      "epoch 71 | loss: 0.1706  | val_0_rmse: 0.31022 |  0:00:20s\n",
      "epoch 72 | loss: 0.17538 | val_0_rmse: 0.419   |  0:00:20s\n",
      "epoch 73 | loss: 0.18441 | val_0_rmse: 0.37024 |  0:00:21s\n",
      "epoch 74 | loss: 0.26539 | val_0_rmse: 0.26562 |  0:00:21s\n",
      "epoch 75 | loss: 0.1187  | val_0_rmse: 0.30594 |  0:00:21s\n",
      "epoch 76 | loss: 0.10396 | val_0_rmse: 0.25711 |  0:00:21s\n",
      "epoch 77 | loss: 0.11339 | val_0_rmse: 0.34789 |  0:00:22s\n",
      "epoch 78 | loss: 0.12194 | val_0_rmse: 0.27212 |  0:00:22s\n",
      "epoch 79 | loss: 0.12597 | val_0_rmse: 0.36271 |  0:00:22s\n",
      "epoch 80 | loss: 0.1412  | val_0_rmse: 0.28791 |  0:00:23s\n",
      "epoch 81 | loss: 0.14981 | val_0_rmse: 0.27318 |  0:00:23s\n",
      "epoch 82 | loss: 0.09306 | val_0_rmse: 0.25368 |  0:00:23s\n",
      "epoch 83 | loss: 0.12793 | val_0_rmse: 0.27929 |  0:00:24s\n",
      "epoch 84 | loss: 0.10473 | val_0_rmse: 0.24834 |  0:00:24s\n",
      "epoch 85 | loss: 0.09883 | val_0_rmse: 0.2449  |  0:00:24s\n",
      "epoch 86 | loss: 0.08929 | val_0_rmse: 0.26307 |  0:00:24s\n",
      "epoch 87 | loss: 0.08849 | val_0_rmse: 0.26837 |  0:00:25s\n",
      "epoch 88 | loss: 0.08513 | val_0_rmse: 0.25085 |  0:00:25s\n",
      "epoch 89 | loss: 0.08494 | val_0_rmse: 0.24958 |  0:00:25s\n",
      "epoch 90 | loss: 0.07802 | val_0_rmse: 0.27166 |  0:00:26s\n",
      "epoch 91 | loss: 0.07789 | val_0_rmse: 0.31432 |  0:00:26s\n",
      "epoch 92 | loss: 0.08854 | val_0_rmse: 0.24649 |  0:00:26s\n",
      "epoch 93 | loss: 0.10229 | val_0_rmse: 0.30893 |  0:00:26s\n",
      "epoch 94 | loss: 0.10026 | val_0_rmse: 0.24332 |  0:00:27s\n",
      "epoch 95 | loss: 0.08698 | val_0_rmse: 0.32527 |  0:00:27s\n",
      "epoch 96 | loss: 0.10316 | val_0_rmse: 0.24525 |  0:00:27s\n",
      "epoch 97 | loss: 0.07798 | val_0_rmse: 0.3031  |  0:00:28s\n",
      "epoch 98 | loss: 0.08341 | val_0_rmse: 0.26694 |  0:00:28s\n",
      "epoch 99 | loss: 0.08661 | val_0_rmse: 0.30128 |  0:00:28s\n",
      "epoch 100| loss: 0.08351 | val_0_rmse: 0.26777 |  0:00:29s\n",
      "epoch 101| loss: 0.13597 | val_0_rmse: 0.24969 |  0:00:29s\n",
      "epoch 102| loss: 0.0982  | val_0_rmse: 0.35197 |  0:00:29s\n",
      "epoch 103| loss: 0.10058 | val_0_rmse: 0.27024 |  0:00:29s\n",
      "epoch 104| loss: 0.11308 | val_0_rmse: 0.28838 |  0:00:30s\n",
      "epoch 105| loss: 0.10058 | val_0_rmse: 0.25909 |  0:00:30s\n",
      "epoch 106| loss: 0.08444 | val_0_rmse: 0.2746  |  0:00:30s\n",
      "epoch 107| loss: 0.08594 | val_0_rmse: 0.36167 |  0:00:31s\n",
      "epoch 108| loss: 0.12851 | val_0_rmse: 0.22837 |  0:00:31s\n",
      "epoch 109| loss: 0.07502 | val_0_rmse: 0.23057 |  0:00:31s\n",
      "epoch 110| loss: 0.0635  | val_0_rmse: 0.23336 |  0:00:31s\n",
      "epoch 111| loss: 0.06469 | val_0_rmse: 0.2309  |  0:00:32s\n",
      "epoch 112| loss: 0.06916 | val_0_rmse: 0.25319 |  0:00:32s\n",
      "epoch 113| loss: 0.05896 | val_0_rmse: 0.2285  |  0:00:32s\n",
      "epoch 114| loss: 0.06031 | val_0_rmse: 0.2319  |  0:00:32s\n",
      "epoch 115| loss: 0.06133 | val_0_rmse: 0.23551 |  0:00:33s\n",
      "epoch 116| loss: 0.05711 | val_0_rmse: 0.21621 |  0:00:33s\n",
      "epoch 117| loss: 0.07112 | val_0_rmse: 0.24656 |  0:00:33s\n",
      "epoch 118| loss: 0.06813 | val_0_rmse: 0.23838 |  0:00:34s\n",
      "epoch 119| loss: 0.05834 | val_0_rmse: 0.21552 |  0:00:34s\n",
      "epoch 120| loss: 0.04566 | val_0_rmse: 0.23402 |  0:00:34s\n",
      "epoch 121| loss: 0.05372 | val_0_rmse: 0.22763 |  0:00:35s\n",
      "epoch 122| loss: 0.05631 | val_0_rmse: 0.24073 |  0:00:35s\n",
      "epoch 123| loss: 0.05048 | val_0_rmse: 0.23112 |  0:00:35s\n",
      "epoch 124| loss: 0.0544  | val_0_rmse: 0.22461 |  0:00:35s\n",
      "epoch 125| loss: 0.05184 | val_0_rmse: 0.24942 |  0:00:36s\n",
      "epoch 126| loss: 0.07067 | val_0_rmse: 0.2403  |  0:00:36s\n",
      "epoch 127| loss: 0.054   | val_0_rmse: 0.23561 |  0:00:36s\n",
      "epoch 128| loss: 0.06326 | val_0_rmse: 0.30652 |  0:00:37s\n",
      "epoch 129| loss: 0.07931 | val_0_rmse: 0.28589 |  0:00:37s\n",
      "epoch 130| loss: 0.11216 | val_0_rmse: 0.25519 |  0:00:37s\n",
      "epoch 131| loss: 0.07004 | val_0_rmse: 0.31888 |  0:00:37s\n",
      "epoch 132| loss: 0.07985 | val_0_rmse: 0.27942 |  0:00:38s\n",
      "epoch 133| loss: 0.13155 | val_0_rmse: 0.24031 |  0:00:38s\n",
      "epoch 134| loss: 0.05824 | val_0_rmse: 0.22467 |  0:00:38s\n",
      "epoch 135| loss: 0.05738 | val_0_rmse: 0.22528 |  0:00:39s\n",
      "epoch 136| loss: 0.05827 | val_0_rmse: 0.24404 |  0:00:39s\n",
      "epoch 137| loss: 0.05169 | val_0_rmse: 0.23522 |  0:00:39s\n",
      "epoch 138| loss: 0.04396 | val_0_rmse: 0.24705 |  0:00:39s\n",
      "epoch 139| loss: 0.04546 | val_0_rmse: 0.22014 |  0:00:40s\n",
      "epoch 140| loss: 0.04203 | val_0_rmse: 0.20993 |  0:00:40s\n",
      "epoch 141| loss: 0.04075 | val_0_rmse: 0.21894 |  0:00:40s\n",
      "epoch 142| loss: 0.04106 | val_0_rmse: 0.21955 |  0:00:41s\n",
      "epoch 143| loss: 0.04381 | val_0_rmse: 0.23321 |  0:00:41s\n",
      "epoch 144| loss: 0.04104 | val_0_rmse: 0.21217 |  0:00:41s\n",
      "epoch 145| loss: 0.0445  | val_0_rmse: 0.2135  |  0:00:42s\n",
      "epoch 146| loss: 0.0373  | val_0_rmse: 0.21841 |  0:00:42s\n",
      "epoch 147| loss: 0.05121 | val_0_rmse: 0.21943 |  0:00:42s\n",
      "epoch 148| loss: 0.05916 | val_0_rmse: 0.24486 |  0:00:42s\n",
      "epoch 149| loss: 0.04561 | val_0_rmse: 0.23995 |  0:00:43s\n",
      "epoch 150| loss: 0.04204 | val_0_rmse: 0.22954 |  0:00:43s\n",
      "epoch 151| loss: 0.04313 | val_0_rmse: 0.2255  |  0:00:43s\n",
      "epoch 152| loss: 0.04119 | val_0_rmse: 0.22533 |  0:00:44s\n",
      "epoch 153| loss: 0.03951 | val_0_rmse: 0.20758 |  0:00:44s\n",
      "epoch 154| loss: 0.03705 | val_0_rmse: 0.21236 |  0:00:44s\n",
      "epoch 155| loss: 0.03884 | val_0_rmse: 0.21393 |  0:00:44s\n",
      "epoch 156| loss: 0.03934 | val_0_rmse: 0.20542 |  0:00:45s\n",
      "epoch 157| loss: 0.03538 | val_0_rmse: 0.22561 |  0:00:45s\n",
      "epoch 158| loss: 0.03832 | val_0_rmse: 0.20559 |  0:00:45s\n",
      "epoch 159| loss: 0.03962 | val_0_rmse: 0.21866 |  0:00:46s\n",
      "epoch 160| loss: 0.04095 | val_0_rmse: 0.24975 |  0:00:46s\n",
      "epoch 161| loss: 0.04241 | val_0_rmse: 0.22996 |  0:00:46s\n",
      "epoch 162| loss: 0.03835 | val_0_rmse: 0.22828 |  0:00:46s\n",
      "epoch 163| loss: 0.04217 | val_0_rmse: 0.2282  |  0:00:47s\n",
      "epoch 164| loss: 0.04111 | val_0_rmse: 0.22449 |  0:00:47s\n",
      "epoch 165| loss: 0.03863 | val_0_rmse: 0.22182 |  0:00:47s\n",
      "epoch 166| loss: 0.03905 | val_0_rmse: 0.22772 |  0:00:48s\n",
      "epoch 167| loss: 0.04039 | val_0_rmse: 0.22581 |  0:00:48s\n",
      "epoch 168| loss: 0.03531 | val_0_rmse: 0.22717 |  0:00:48s\n",
      "epoch 169| loss: 0.03568 | val_0_rmse: 0.25989 |  0:00:48s\n",
      "epoch 170| loss: 0.04482 | val_0_rmse: 0.22629 |  0:00:49s\n",
      "epoch 171| loss: 0.03518 | val_0_rmse: 0.2201  |  0:00:49s\n",
      "epoch 172| loss: 0.03628 | val_0_rmse: 0.23671 |  0:00:49s\n",
      "epoch 173| loss: 0.05562 | val_0_rmse: 0.24046 |  0:00:49s\n",
      "epoch 174| loss: 0.05928 | val_0_rmse: 0.21076 |  0:00:50s\n",
      "epoch 175| loss: 0.03854 | val_0_rmse: 0.21201 |  0:00:50s\n",
      "epoch 176| loss: 0.03735 | val_0_rmse: 0.22863 |  0:00:50s\n",
      "epoch 177| loss: 0.04326 | val_0_rmse: 0.24285 |  0:00:51s\n",
      "epoch 178| loss: 0.05267 | val_0_rmse: 0.21944 |  0:00:51s\n",
      "epoch 179| loss: 0.04037 | val_0_rmse: 0.21448 |  0:00:51s\n",
      "epoch 180| loss: 0.0369  | val_0_rmse: 0.21125 |  0:00:52s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:38:53,506] Trial 68 finished with value: 0.2054186835715066 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 4, 'gamma': 1.2234632566106116, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.012841011224898892, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 181| loss: 0.03633 | val_0_rmse: 0.22722 |  0:00:52s\n",
      "\n",
      "Early stopping occurred at epoch 181 with best_epoch = 156 and best_val_0_rmse = 0.20542\n",
      "Trial 068 | rmse_log=0.20542 | RMSE$=40,246 | MAE$=25,386 | MAPE=15.03% | n_d/n_a=24/16 steps=4 lr=0.01284 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.02999| val_0_rmse: 10.83544|  0:00:00s\n",
      "epoch 1  | loss: 84.53145| val_0_rmse: 9.65797 |  0:00:00s\n",
      "epoch 2  | loss: 50.83433| val_0_rmse: 8.18827 |  0:00:00s\n",
      "epoch 3  | loss: 30.1988 | val_0_rmse: 6.62763 |  0:00:00s\n",
      "epoch 4  | loss: 18.46006| val_0_rmse: 5.15195 |  0:00:01s\n",
      "epoch 5  | loss: 14.23392| val_0_rmse: 3.97362 |  0:00:01s\n",
      "epoch 6  | loss: 11.13912| val_0_rmse: 3.42552 |  0:00:01s\n",
      "epoch 7  | loss: 6.39167 | val_0_rmse: 3.61776 |  0:00:01s\n",
      "epoch 8  | loss: 4.09122 | val_0_rmse: 3.44257 |  0:00:02s\n",
      "epoch 9  | loss: 2.5692  | val_0_rmse: 2.34568 |  0:00:02s\n",
      "epoch 10 | loss: 1.86144 | val_0_rmse: 2.02564 |  0:00:02s\n",
      "epoch 11 | loss: 0.97725 | val_0_rmse: 2.1701  |  0:00:02s\n",
      "epoch 12 | loss: 0.76178 | val_0_rmse: 1.63235 |  0:00:03s\n",
      "epoch 13 | loss: 0.64991 | val_0_rmse: 1.44003 |  0:00:03s\n",
      "epoch 14 | loss: 0.41964 | val_0_rmse: 1.21416 |  0:00:03s\n",
      "epoch 15 | loss: 0.33385 | val_0_rmse: 1.07577 |  0:00:03s\n",
      "epoch 16 | loss: 0.44435 | val_0_rmse: 0.98451 |  0:00:04s\n",
      "epoch 17 | loss: 0.36219 | val_0_rmse: 0.90597 |  0:00:04s\n",
      "epoch 18 | loss: 0.35135 | val_0_rmse: 0.5586  |  0:00:04s\n",
      "epoch 19 | loss: 0.4015  | val_0_rmse: 0.60438 |  0:00:04s\n",
      "epoch 20 | loss: 0.30905 | val_0_rmse: 0.57178 |  0:00:05s\n",
      "epoch 21 | loss: 0.26477 | val_0_rmse: 0.6865  |  0:00:05s\n",
      "epoch 22 | loss: 0.32595 | val_0_rmse: 0.4803  |  0:00:05s\n",
      "epoch 23 | loss: 0.24629 | val_0_rmse: 0.63886 |  0:00:05s\n",
      "epoch 24 | loss: 0.22351 | val_0_rmse: 0.32182 |  0:00:05s\n",
      "epoch 25 | loss: 0.24218 | val_0_rmse: 0.43675 |  0:00:06s\n",
      "epoch 26 | loss: 0.20273 | val_0_rmse: 0.35083 |  0:00:06s\n",
      "epoch 27 | loss: 0.13508 | val_0_rmse: 0.31168 |  0:00:06s\n",
      "epoch 28 | loss: 0.13434 | val_0_rmse: 0.38592 |  0:00:06s\n",
      "epoch 29 | loss: 0.14848 | val_0_rmse: 0.26144 |  0:00:07s\n",
      "epoch 30 | loss: 0.12648 | val_0_rmse: 0.40635 |  0:00:07s\n",
      "epoch 31 | loss: 0.12067 | val_0_rmse: 0.26388 |  0:00:07s\n",
      "epoch 32 | loss: 0.09992 | val_0_rmse: 0.23308 |  0:00:07s\n",
      "epoch 33 | loss: 0.11521 | val_0_rmse: 0.25103 |  0:00:08s\n",
      "epoch 34 | loss: 0.08678 | val_0_rmse: 0.27052 |  0:00:08s\n",
      "epoch 35 | loss: 0.07559 | val_0_rmse: 0.25657 |  0:00:08s\n",
      "epoch 36 | loss: 0.10222 | val_0_rmse: 0.24773 |  0:00:08s\n",
      "epoch 37 | loss: 0.10683 | val_0_rmse: 0.25023 |  0:00:08s\n",
      "epoch 38 | loss: 0.07926 | val_0_rmse: 0.25322 |  0:00:09s\n",
      "epoch 39 | loss: 0.08579 | val_0_rmse: 0.24318 |  0:00:09s\n",
      "epoch 40 | loss: 0.10253 | val_0_rmse: 0.27831 |  0:00:09s\n",
      "epoch 41 | loss: 0.08147 | val_0_rmse: 0.28523 |  0:00:09s\n",
      "epoch 42 | loss: 0.06971 | val_0_rmse: 0.24595 |  0:00:09s\n",
      "epoch 43 | loss: 0.08922 | val_0_rmse: 0.24514 |  0:00:10s\n",
      "epoch 44 | loss: 0.06777 | val_0_rmse: 0.24324 |  0:00:10s\n",
      "epoch 45 | loss: 0.07223 | val_0_rmse: 0.26757 |  0:00:10s\n",
      "epoch 46 | loss: 0.09338 | val_0_rmse: 0.29083 |  0:00:11s\n",
      "epoch 47 | loss: 0.10469 | val_0_rmse: 0.23207 |  0:00:11s\n",
      "epoch 48 | loss: 0.07365 | val_0_rmse: 0.27101 |  0:00:11s\n",
      "epoch 49 | loss: 0.07626 | val_0_rmse: 0.25785 |  0:00:11s\n",
      "epoch 50 | loss: 0.0876  | val_0_rmse: 0.30961 |  0:00:11s\n",
      "epoch 51 | loss: 0.12259 | val_0_rmse: 0.21811 |  0:00:12s\n",
      "epoch 52 | loss: 0.10571 | val_0_rmse: 0.23814 |  0:00:12s\n",
      "epoch 53 | loss: 0.06991 | val_0_rmse: 0.23516 |  0:00:12s\n",
      "epoch 54 | loss: 0.08239 | val_0_rmse: 0.25957 |  0:00:12s\n",
      "epoch 55 | loss: 0.09157 | val_0_rmse: 0.37297 |  0:00:13s\n",
      "epoch 56 | loss: 0.1976  | val_0_rmse: 0.247   |  0:00:13s\n",
      "epoch 57 | loss: 0.07767 | val_0_rmse: 0.22118 |  0:00:13s\n",
      "epoch 58 | loss: 0.08534 | val_0_rmse: 0.31754 |  0:00:13s\n",
      "epoch 59 | loss: 0.094   | val_0_rmse: 0.3192  |  0:00:13s\n",
      "epoch 60 | loss: 0.1531  | val_0_rmse: 0.25857 |  0:00:14s\n",
      "epoch 61 | loss: 0.08022 | val_0_rmse: 0.21931 |  0:00:14s\n",
      "epoch 62 | loss: 0.06033 | val_0_rmse: 0.21919 |  0:00:14s\n",
      "epoch 63 | loss: 0.04817 | val_0_rmse: 0.22217 |  0:00:14s\n",
      "epoch 64 | loss: 0.05229 | val_0_rmse: 0.21037 |  0:00:15s\n",
      "epoch 65 | loss: 0.04225 | val_0_rmse: 0.21122 |  0:00:15s\n",
      "epoch 66 | loss: 0.04281 | val_0_rmse: 0.21053 |  0:00:15s\n",
      "epoch 67 | loss: 0.04909 | val_0_rmse: 0.20795 |  0:00:15s\n",
      "epoch 68 | loss: 0.04916 | val_0_rmse: 0.22999 |  0:00:16s\n",
      "epoch 69 | loss: 0.04754 | val_0_rmse: 0.21754 |  0:00:16s\n",
      "epoch 70 | loss: 0.04019 | val_0_rmse: 0.21346 |  0:00:16s\n",
      "epoch 71 | loss: 0.04406 | val_0_rmse: 0.2385  |  0:00:16s\n",
      "epoch 72 | loss: 0.0434  | val_0_rmse: 0.22245 |  0:00:16s\n",
      "epoch 73 | loss: 0.04955 | val_0_rmse: 0.23598 |  0:00:17s\n",
      "epoch 74 | loss: 0.04937 | val_0_rmse: 0.23427 |  0:00:17s\n",
      "epoch 75 | loss: 0.0463  | val_0_rmse: 0.22625 |  0:00:17s\n",
      "epoch 76 | loss: 0.06107 | val_0_rmse: 0.2234  |  0:00:17s\n",
      "epoch 77 | loss: 0.0487  | val_0_rmse: 0.22313 |  0:00:18s\n",
      "epoch 78 | loss: 0.0454  | val_0_rmse: 0.22357 |  0:00:18s\n",
      "epoch 79 | loss: 0.04577 | val_0_rmse: 0.2117  |  0:00:18s\n",
      "epoch 80 | loss: 0.04899 | val_0_rmse: 0.19943 |  0:00:18s\n",
      "epoch 81 | loss: 0.05081 | val_0_rmse: 0.21711 |  0:00:19s\n",
      "epoch 82 | loss: 0.03948 | val_0_rmse: 0.20664 |  0:00:19s\n",
      "epoch 83 | loss: 0.04623 | val_0_rmse: 0.23311 |  0:00:19s\n",
      "epoch 84 | loss: 0.05311 | val_0_rmse: 0.25784 |  0:00:19s\n",
      "epoch 85 | loss: 0.10165 | val_0_rmse: 0.23022 |  0:00:20s\n",
      "epoch 86 | loss: 0.06579 | val_0_rmse: 0.22711 |  0:00:20s\n",
      "epoch 87 | loss: 0.05186 | val_0_rmse: 0.21665 |  0:00:20s\n",
      "epoch 88 | loss: 0.04368 | val_0_rmse: 0.21459 |  0:00:20s\n",
      "epoch 89 | loss: 0.03729 | val_0_rmse: 0.22539 |  0:00:20s\n",
      "epoch 90 | loss: 0.0394  | val_0_rmse: 0.21401 |  0:00:21s\n",
      "epoch 91 | loss: 0.04145 | val_0_rmse: 0.20175 |  0:00:21s\n",
      "epoch 92 | loss: 0.0378  | val_0_rmse: 0.20919 |  0:00:21s\n",
      "epoch 93 | loss: 0.0352  | val_0_rmse: 0.2159  |  0:00:21s\n",
      "epoch 94 | loss: 0.03411 | val_0_rmse: 0.20215 |  0:00:22s\n",
      "epoch 95 | loss: 0.03625 | val_0_rmse: 0.20267 |  0:00:22s\n",
      "epoch 96 | loss: 0.03687 | val_0_rmse: 0.24569 |  0:00:22s\n",
      "epoch 97 | loss: 0.04712 | val_0_rmse: 0.2124  |  0:00:22s\n",
      "epoch 98 | loss: 0.04245 | val_0_rmse: 0.26593 |  0:00:23s\n",
      "epoch 99 | loss: 0.05198 | val_0_rmse: 0.25646 |  0:00:23s\n",
      "epoch 100| loss: 0.07438 | val_0_rmse: 0.24776 |  0:00:23s\n",
      "epoch 101| loss: 0.05226 | val_0_rmse: 0.23219 |  0:00:23s\n",
      "epoch 102| loss: 0.05399 | val_0_rmse: 0.23735 |  0:00:24s\n",
      "epoch 103| loss: 0.06186 | val_0_rmse: 0.20823 |  0:00:24s\n",
      "epoch 104| loss: 0.05156 | val_0_rmse: 0.24253 |  0:00:24s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:39:18,733] Trial 69 finished with value: 0.19942912355478487 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.287902451884087, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.01825496875224158, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 105| loss: 0.071   | val_0_rmse: 0.20856 |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 105 with best_epoch = 80 and best_val_0_rmse = 0.19943\n",
      "Trial 069 | rmse_log=0.19943 | RMSE$=38,159 | MAE$=25,339 | MAPE=15.40% | n_d/n_a=24/16 steps=3 lr=0.01825 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 123.58256| val_0_rmse: 11.04222|  0:00:00s\n",
      "epoch 1  | loss: 84.15236| val_0_rmse: 10.07158|  0:00:00s\n",
      "epoch 2  | loss: 57.67991| val_0_rmse: 8.99873 |  0:00:00s\n",
      "epoch 3  | loss: 36.50919| val_0_rmse: 7.65692 |  0:00:01s\n",
      "epoch 4  | loss: 25.22164| val_0_rmse: 6.07733 |  0:00:01s\n",
      "epoch 5  | loss: 16.73779| val_0_rmse: 4.66257 |  0:00:01s\n",
      "epoch 6  | loss: 14.93449| val_0_rmse: 3.99556 |  0:00:02s\n",
      "epoch 7  | loss: 9.99709 | val_0_rmse: 3.80547 |  0:00:02s\n",
      "epoch 8  | loss: 5.70991 | val_0_rmse: 3.5601  |  0:00:02s\n",
      "epoch 9  | loss: 3.51214 | val_0_rmse: 2.53891 |  0:00:03s\n",
      "epoch 10 | loss: 2.49366 | val_0_rmse: 1.41565 |  0:00:03s\n",
      "epoch 11 | loss: 2.08616 | val_0_rmse: 1.43347 |  0:00:03s\n",
      "epoch 12 | loss: 1.51982 | val_0_rmse: 1.21395 |  0:00:03s\n",
      "epoch 13 | loss: 1.24093 | val_0_rmse: 0.88699 |  0:00:04s\n",
      "epoch 14 | loss: 1.04781 | val_0_rmse: 1.20881 |  0:00:04s\n",
      "epoch 15 | loss: 1.0853  | val_0_rmse: 0.92358 |  0:00:04s\n",
      "epoch 16 | loss: 0.74156 | val_0_rmse: 1.00093 |  0:00:05s\n",
      "epoch 17 | loss: 0.559   | val_0_rmse: 0.88524 |  0:00:05s\n",
      "epoch 18 | loss: 0.57891 | val_0_rmse: 0.7701  |  0:00:05s\n",
      "epoch 19 | loss: 0.50054 | val_0_rmse: 0.64872 |  0:00:05s\n",
      "epoch 20 | loss: 0.41269 | val_0_rmse: 0.61082 |  0:00:06s\n",
      "epoch 21 | loss: 0.45496 | val_0_rmse: 0.76576 |  0:00:06s\n",
      "epoch 22 | loss: 0.44557 | val_0_rmse: 0.44015 |  0:00:06s\n",
      "epoch 23 | loss: 0.37089 | val_0_rmse: 0.62129 |  0:00:07s\n",
      "epoch 24 | loss: 0.36752 | val_0_rmse: 0.38098 |  0:00:07s\n",
      "epoch 25 | loss: 0.37424 | val_0_rmse: 0.35294 |  0:00:07s\n",
      "epoch 26 | loss: 0.3592  | val_0_rmse: 0.56926 |  0:00:08s\n",
      "epoch 27 | loss: 0.32319 | val_0_rmse: 0.35568 |  0:00:08s\n",
      "epoch 28 | loss: 0.27231 | val_0_rmse: 0.42729 |  0:00:08s\n",
      "epoch 29 | loss: 0.20538 | val_0_rmse: 0.43929 |  0:00:08s\n",
      "epoch 30 | loss: 0.20314 | val_0_rmse: 0.37873 |  0:00:09s\n",
      "epoch 31 | loss: 0.23518 | val_0_rmse: 0.45183 |  0:00:09s\n",
      "epoch 32 | loss: 0.24895 | val_0_rmse: 0.32185 |  0:00:09s\n",
      "epoch 33 | loss: 0.2157  | val_0_rmse: 0.46836 |  0:00:10s\n",
      "epoch 34 | loss: 0.19516 | val_0_rmse: 0.35874 |  0:00:10s\n",
      "epoch 35 | loss: 0.1769  | val_0_rmse: 0.41312 |  0:00:10s\n",
      "epoch 36 | loss: 0.18915 | val_0_rmse: 0.27847 |  0:00:11s\n",
      "epoch 37 | loss: 0.19337 | val_0_rmse: 0.33702 |  0:00:11s\n",
      "epoch 38 | loss: 0.14577 | val_0_rmse: 0.27695 |  0:00:11s\n",
      "epoch 39 | loss: 0.14551 | val_0_rmse: 0.29901 |  0:00:11s\n",
      "epoch 40 | loss: 0.12324 | val_0_rmse: 0.33745 |  0:00:12s\n",
      "epoch 41 | loss: 0.13171 | val_0_rmse: 0.25819 |  0:00:12s\n",
      "epoch 42 | loss: 0.1244  | val_0_rmse: 0.27533 |  0:00:12s\n",
      "epoch 43 | loss: 0.11539 | val_0_rmse: 0.35548 |  0:00:12s\n",
      "epoch 44 | loss: 0.11571 | val_0_rmse: 0.3051  |  0:00:13s\n",
      "epoch 45 | loss: 0.09728 | val_0_rmse: 0.2858  |  0:00:13s\n",
      "epoch 46 | loss: 0.10348 | val_0_rmse: 0.41805 |  0:00:13s\n",
      "epoch 47 | loss: 0.1199  | val_0_rmse: 0.28002 |  0:00:14s\n",
      "epoch 48 | loss: 0.13375 | val_0_rmse: 0.47977 |  0:00:14s\n",
      "epoch 49 | loss: 0.15954 | val_0_rmse: 0.2581  |  0:00:14s\n",
      "epoch 50 | loss: 0.10736 | val_0_rmse: 0.3935  |  0:00:14s\n",
      "epoch 51 | loss: 0.13399 | val_0_rmse: 0.23724 |  0:00:15s\n",
      "epoch 52 | loss: 0.14222 | val_0_rmse: 0.36433 |  0:00:15s\n",
      "epoch 53 | loss: 0.10879 | val_0_rmse: 0.26123 |  0:00:15s\n",
      "epoch 54 | loss: 0.13184 | val_0_rmse: 0.36051 |  0:00:16s\n",
      "epoch 55 | loss: 0.14113 | val_0_rmse: 0.24127 |  0:00:16s\n",
      "epoch 56 | loss: 0.13799 | val_0_rmse: 0.32779 |  0:00:16s\n",
      "epoch 57 | loss: 0.10034 | val_0_rmse: 0.27339 |  0:00:16s\n",
      "epoch 58 | loss: 0.07408 | val_0_rmse: 0.25444 |  0:00:17s\n",
      "epoch 59 | loss: 0.0648  | val_0_rmse: 0.30165 |  0:00:17s\n",
      "epoch 60 | loss: 0.09799 | val_0_rmse: 0.25049 |  0:00:17s\n",
      "epoch 61 | loss: 0.09841 | val_0_rmse: 0.35435 |  0:00:18s\n",
      "epoch 62 | loss: 0.1021  | val_0_rmse: 0.25825 |  0:00:18s\n",
      "epoch 63 | loss: 0.09206 | val_0_rmse: 0.30721 |  0:00:18s\n",
      "epoch 64 | loss: 0.08446 | val_0_rmse: 0.26377 |  0:00:18s\n",
      "epoch 65 | loss: 0.09872 | val_0_rmse: 0.29934 |  0:00:19s\n",
      "epoch 66 | loss: 0.08854 | val_0_rmse: 0.27152 |  0:00:19s\n",
      "epoch 67 | loss: 0.09661 | val_0_rmse: 0.27721 |  0:00:19s\n",
      "epoch 68 | loss: 0.09487 | val_0_rmse: 0.27785 |  0:00:20s\n",
      "epoch 69 | loss: 0.12104 | val_0_rmse: 0.30898 |  0:00:20s\n",
      "epoch 70 | loss: 0.09388 | val_0_rmse: 0.24454 |  0:00:20s\n",
      "epoch 71 | loss: 0.07713 | val_0_rmse: 0.27917 |  0:00:20s\n",
      "epoch 72 | loss: 0.10981 | val_0_rmse: 0.22491 |  0:00:21s\n",
      "epoch 73 | loss: 0.07028 | val_0_rmse: 0.24708 |  0:00:21s\n",
      "epoch 74 | loss: 0.06927 | val_0_rmse: 0.23996 |  0:00:21s\n",
      "epoch 75 | loss: 0.05864 | val_0_rmse: 0.23309 |  0:00:21s\n",
      "epoch 76 | loss: 0.06598 | val_0_rmse: 0.24529 |  0:00:22s\n",
      "epoch 77 | loss: 0.07648 | val_0_rmse: 0.23679 |  0:00:22s\n",
      "epoch 78 | loss: 0.09341 | val_0_rmse: 0.30305 |  0:00:22s\n",
      "epoch 79 | loss: 0.09687 | val_0_rmse: 0.33965 |  0:00:23s\n",
      "epoch 80 | loss: 0.12862 | val_0_rmse: 0.3121  |  0:00:23s\n",
      "epoch 81 | loss: 0.14461 | val_0_rmse: 0.29574 |  0:00:23s\n",
      "epoch 82 | loss: 0.07992 | val_0_rmse: 0.23714 |  0:00:24s\n",
      "epoch 83 | loss: 0.06826 | val_0_rmse: 0.26954 |  0:00:24s\n",
      "epoch 84 | loss: 0.06197 | val_0_rmse: 0.2454  |  0:00:24s\n",
      "epoch 85 | loss: 0.05281 | val_0_rmse: 0.24755 |  0:00:24s\n",
      "epoch 86 | loss: 0.05715 | val_0_rmse: 0.24723 |  0:00:25s\n",
      "epoch 87 | loss: 0.0614  | val_0_rmse: 0.22598 |  0:00:25s\n",
      "epoch 88 | loss: 0.06069 | val_0_rmse: 0.23342 |  0:00:25s\n",
      "epoch 89 | loss: 0.0582  | val_0_rmse: 0.23257 |  0:00:26s\n",
      "epoch 90 | loss: 0.06297 | val_0_rmse: 0.2814  |  0:00:26s\n",
      "epoch 91 | loss: 0.06217 | val_0_rmse: 0.25509 |  0:00:26s\n",
      "epoch 92 | loss: 0.04955 | val_0_rmse: 0.23313 |  0:00:27s\n",
      "epoch 93 | loss: 0.05731 | val_0_rmse: 0.23328 |  0:00:27s\n",
      "epoch 94 | loss: 0.05851 | val_0_rmse: 0.27787 |  0:00:27s\n",
      "epoch 95 | loss: 0.05814 | val_0_rmse: 0.2391  |  0:00:28s\n",
      "epoch 96 | loss: 0.05202 | val_0_rmse: 0.24482 |  0:00:28s\n",
      "epoch 97 | loss: 0.05871 | val_0_rmse: 0.26978 |  0:00:28s\n",
      "\n",
      "Early stopping occurred at epoch 97 with best_epoch = 72 and best_val_0_rmse = 0.22491\n",
      "Trial 070 | rmse_log=0.22491 | RMSE$=43,682 | MAE$=27,789 | MAPE=17.12% | n_d/n_a=24/32 steps=4 lr=0.01595 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:39:47,734] Trial 70 finished with value: 0.2249090877333471 and parameters: {'n_d': 24, 'n_a': 32, 'n_steps': 4, 'gamma': 1.2543443602865667, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.0159519716336303, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n",
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 117.86799| val_0_rmse: 10.11045|  0:00:00s\n",
      "epoch 1  | loss: 87.55715| val_0_rmse: 8.82053 |  0:00:00s\n",
      "epoch 2  | loss: 65.45531| val_0_rmse: 7.5007  |  0:00:00s\n",
      "epoch 3  | loss: 46.34895| val_0_rmse: 6.19273 |  0:00:00s\n",
      "epoch 4  | loss: 34.47408| val_0_rmse: 4.94161 |  0:00:00s\n",
      "epoch 5  | loss: 20.40944| val_0_rmse: 3.8918  |  0:00:00s\n",
      "epoch 6  | loss: 11.82612| val_0_rmse: 3.28491 |  0:00:01s\n",
      "epoch 7  | loss: 11.34025| val_0_rmse: 3.42934 |  0:00:01s\n",
      "epoch 8  | loss: 4.36731 | val_0_rmse: 3.91816 |  0:00:01s\n",
      "epoch 9  | loss: 2.13787 | val_0_rmse: 3.57426 |  0:00:01s\n",
      "epoch 10 | loss: 1.65637 | val_0_rmse: 3.14679 |  0:00:01s\n",
      "epoch 11 | loss: 1.09687 | val_0_rmse: 3.29948 |  0:00:01s\n",
      "epoch 12 | loss: 0.63097 | val_0_rmse: 3.13099 |  0:00:02s\n",
      "epoch 13 | loss: 0.47204 | val_0_rmse: 2.70345 |  0:00:02s\n",
      "epoch 14 | loss: 0.36237 | val_0_rmse: 2.81373 |  0:00:02s\n",
      "epoch 15 | loss: 0.45161 | val_0_rmse: 2.30684 |  0:00:02s\n",
      "epoch 16 | loss: 0.49452 | val_0_rmse: 2.49438 |  0:00:02s\n",
      "epoch 17 | loss: 0.36502 | val_0_rmse: 1.88154 |  0:00:02s\n",
      "epoch 18 | loss: 0.43066 | val_0_rmse: 2.16916 |  0:00:03s\n",
      "epoch 19 | loss: 0.29334 | val_0_rmse: 1.71424 |  0:00:03s\n",
      "epoch 20 | loss: 0.32083 | val_0_rmse: 1.85249 |  0:00:03s\n",
      "epoch 21 | loss: 0.39853 | val_0_rmse: 1.51993 |  0:00:03s\n",
      "epoch 22 | loss: 0.21191 | val_0_rmse: 1.60961 |  0:00:03s\n",
      "epoch 23 | loss: 0.26361 | val_0_rmse: 1.39568 |  0:00:03s\n",
      "epoch 24 | loss: 0.23375 | val_0_rmse: 1.42382 |  0:00:04s\n",
      "epoch 25 | loss: 0.16122 | val_0_rmse: 1.36893 |  0:00:04s\n",
      "epoch 26 | loss: 0.16407 | val_0_rmse: 1.25133 |  0:00:04s\n",
      "epoch 27 | loss: 0.1586  | val_0_rmse: 1.17559 |  0:00:04s\n",
      "epoch 28 | loss: 0.11881 | val_0_rmse: 1.26179 |  0:00:04s\n",
      "epoch 29 | loss: 0.12298 | val_0_rmse: 1.20264 |  0:00:04s\n",
      "epoch 30 | loss: 0.11295 | val_0_rmse: 1.05974 |  0:00:05s\n",
      "epoch 31 | loss: 0.11318 | val_0_rmse: 1.08784 |  0:00:05s\n",
      "epoch 32 | loss: 0.21045 | val_0_rmse: 1.01142 |  0:00:05s\n",
      "epoch 33 | loss: 0.08977 | val_0_rmse: 0.88625 |  0:00:05s\n",
      "epoch 34 | loss: 0.10781 | val_0_rmse: 1.02782 |  0:00:05s\n",
      "epoch 35 | loss: 0.11752 | val_0_rmse: 0.93712 |  0:00:05s\n",
      "epoch 36 | loss: 0.08729 | val_0_rmse: 0.8095  |  0:00:06s\n",
      "epoch 37 | loss: 0.11291 | val_0_rmse: 0.84742 |  0:00:06s\n",
      "epoch 38 | loss: 0.1163  | val_0_rmse: 0.78525 |  0:00:06s\n",
      "epoch 39 | loss: 0.1241  | val_0_rmse: 0.73088 |  0:00:06s\n",
      "epoch 40 | loss: 0.08666 | val_0_rmse: 0.69775 |  0:00:06s\n",
      "epoch 41 | loss: 0.11449 | val_0_rmse: 0.73759 |  0:00:06s\n",
      "epoch 42 | loss: 0.12027 | val_0_rmse: 0.51605 |  0:00:07s\n",
      "epoch 43 | loss: 0.13351 | val_0_rmse: 0.84576 |  0:00:07s\n",
      "epoch 44 | loss: 0.17338 | val_0_rmse: 0.58127 |  0:00:07s\n",
      "epoch 45 | loss: 0.09809 | val_0_rmse: 0.63086 |  0:00:07s\n",
      "epoch 46 | loss: 0.09012 | val_0_rmse: 0.45989 |  0:00:07s\n",
      "epoch 47 | loss: 0.09208 | val_0_rmse: 0.65906 |  0:00:07s\n",
      "epoch 48 | loss: 0.09156 | val_0_rmse: 0.38382 |  0:00:07s\n",
      "epoch 49 | loss: 0.0945  | val_0_rmse: 0.62821 |  0:00:08s\n",
      "epoch 50 | loss: 0.10717 | val_0_rmse: 0.39227 |  0:00:08s\n",
      "epoch 51 | loss: 0.09694 | val_0_rmse: 0.5566  |  0:00:08s\n",
      "epoch 52 | loss: 0.07486 | val_0_rmse: 0.34797 |  0:00:08s\n",
      "epoch 53 | loss: 0.0754  | val_0_rmse: 0.51147 |  0:00:08s\n",
      "epoch 54 | loss: 0.06541 | val_0_rmse: 0.31476 |  0:00:08s\n",
      "epoch 55 | loss: 0.08017 | val_0_rmse: 0.46231 |  0:00:09s\n",
      "epoch 56 | loss: 0.08008 | val_0_rmse: 0.28229 |  0:00:09s\n",
      "epoch 57 | loss: 0.07709 | val_0_rmse: 0.44488 |  0:00:09s\n",
      "epoch 58 | loss: 0.06859 | val_0_rmse: 0.25107 |  0:00:09s\n",
      "epoch 59 | loss: 0.06276 | val_0_rmse: 0.40894 |  0:00:09s\n",
      "epoch 60 | loss: 0.08044 | val_0_rmse: 0.24837 |  0:00:09s\n",
      "epoch 61 | loss: 0.09943 | val_0_rmse: 0.33578 |  0:00:10s\n",
      "epoch 62 | loss: 0.063   | val_0_rmse: 0.26049 |  0:00:10s\n",
      "epoch 63 | loss: 0.04876 | val_0_rmse: 0.32231 |  0:00:10s\n",
      "epoch 64 | loss: 0.04532 | val_0_rmse: 0.23963 |  0:00:10s\n",
      "epoch 65 | loss: 0.07653 | val_0_rmse: 0.35438 |  0:00:10s\n",
      "epoch 66 | loss: 0.0861  | val_0_rmse: 0.24592 |  0:00:10s\n",
      "epoch 67 | loss: 0.07156 | val_0_rmse: 0.31818 |  0:00:11s\n",
      "epoch 68 | loss: 0.06723 | val_0_rmse: 0.29994 |  0:00:11s\n",
      "epoch 69 | loss: 0.04481 | val_0_rmse: 0.24094 |  0:00:11s\n",
      "epoch 70 | loss: 0.0519  | val_0_rmse: 0.29636 |  0:00:11s\n",
      "epoch 71 | loss: 0.05154 | val_0_rmse: 0.25257 |  0:00:11s\n",
      "epoch 72 | loss: 0.06324 | val_0_rmse: 0.33383 |  0:00:11s\n",
      "epoch 73 | loss: 0.05835 | val_0_rmse: 0.24259 |  0:00:12s\n",
      "epoch 74 | loss: 0.06557 | val_0_rmse: 0.26415 |  0:00:12s\n",
      "epoch 75 | loss: 0.04264 | val_0_rmse: 0.23564 |  0:00:12s\n",
      "epoch 76 | loss: 0.04434 | val_0_rmse: 0.29872 |  0:00:12s\n",
      "epoch 77 | loss: 0.05983 | val_0_rmse: 0.22913 |  0:00:12s\n",
      "epoch 78 | loss: 0.05736 | val_0_rmse: 0.28219 |  0:00:12s\n",
      "epoch 79 | loss: 0.05001 | val_0_rmse: 0.23395 |  0:00:13s\n",
      "epoch 80 | loss: 0.03783 | val_0_rmse: 0.23941 |  0:00:13s\n",
      "epoch 81 | loss: 0.04441 | val_0_rmse: 0.22314 |  0:00:13s\n",
      "epoch 82 | loss: 0.0352  | val_0_rmse: 0.23516 |  0:00:13s\n",
      "epoch 83 | loss: 0.03756 | val_0_rmse: 0.26214 |  0:00:13s\n",
      "epoch 84 | loss: 0.04016 | val_0_rmse: 0.21028 |  0:00:13s\n",
      "epoch 85 | loss: 0.04244 | val_0_rmse: 0.25489 |  0:00:13s\n",
      "epoch 86 | loss: 0.03708 | val_0_rmse: 0.22284 |  0:00:14s\n",
      "epoch 87 | loss: 0.03032 | val_0_rmse: 0.24946 |  0:00:14s\n",
      "epoch 88 | loss: 0.0412  | val_0_rmse: 0.21448 |  0:00:14s\n",
      "epoch 89 | loss: 0.04003 | val_0_rmse: 0.21389 |  0:00:14s\n",
      "epoch 90 | loss: 0.03402 | val_0_rmse: 0.26737 |  0:00:14s\n",
      "epoch 91 | loss: 0.04666 | val_0_rmse: 0.22905 |  0:00:14s\n",
      "epoch 92 | loss: 0.04584 | val_0_rmse: 0.24672 |  0:00:15s\n",
      "epoch 93 | loss: 0.04058 | val_0_rmse: 0.20732 |  0:00:15s\n",
      "epoch 94 | loss: 0.03489 | val_0_rmse: 0.23518 |  0:00:15s\n",
      "epoch 95 | loss: 0.05054 | val_0_rmse: 0.2219  |  0:00:15s\n",
      "epoch 96 | loss: 0.04535 | val_0_rmse: 0.24567 |  0:00:15s\n",
      "epoch 97 | loss: 0.03887 | val_0_rmse: 0.2135  |  0:00:16s\n",
      "epoch 98 | loss: 0.04464 | val_0_rmse: 0.22367 |  0:00:16s\n",
      "epoch 99 | loss: 0.05183 | val_0_rmse: 0.21446 |  0:00:16s\n",
      "epoch 100| loss: 0.03423 | val_0_rmse: 0.21542 |  0:00:16s\n",
      "epoch 101| loss: 0.03389 | val_0_rmse: 0.22244 |  0:00:16s\n",
      "epoch 102| loss: 0.03596 | val_0_rmse: 0.22134 |  0:00:16s\n",
      "epoch 103| loss: 0.03086 | val_0_rmse: 0.22817 |  0:00:17s\n",
      "epoch 104| loss: 0.03453 | val_0_rmse: 0.23345 |  0:00:17s\n",
      "epoch 105| loss: 0.03147 | val_0_rmse: 0.23089 |  0:00:17s\n",
      "epoch 106| loss: 0.03104 | val_0_rmse: 0.21565 |  0:00:17s\n",
      "epoch 107| loss: 0.02992 | val_0_rmse: 0.21853 |  0:00:17s\n",
      "epoch 108| loss: 0.02754 | val_0_rmse: 0.21753 |  0:00:17s\n",
      "epoch 109| loss: 0.02642 | val_0_rmse: 0.21054 |  0:00:17s\n",
      "epoch 110| loss: 0.02816 | val_0_rmse: 0.21728 |  0:00:18s\n",
      "epoch 111| loss: 0.02386 | val_0_rmse: 0.20844 |  0:00:18s\n",
      "epoch 112| loss: 0.03414 | val_0_rmse: 0.22625 |  0:00:18s\n",
      "epoch 113| loss: 0.02996 | val_0_rmse: 0.21734 |  0:00:18s\n",
      "epoch 114| loss: 0.02727 | val_0_rmse: 0.23755 |  0:00:18s\n",
      "epoch 115| loss: 0.04771 | val_0_rmse: 0.22433 |  0:00:18s\n",
      "epoch 116| loss: 0.0349  | val_0_rmse: 0.21588 |  0:00:19s\n",
      "epoch 117| loss: 0.02924 | val_0_rmse: 0.23758 |  0:00:19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:40:07,506] Trial 71 finished with value: 0.20731989506284507 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.4939906318642708, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.018845000926459736, 'batch_size': 512, 'virtual_batch_size': 256}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 118| loss: 0.03583 | val_0_rmse: 0.24834 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 118 with best_epoch = 93 and best_val_0_rmse = 0.20732\n",
      "Trial 071 | rmse_log=0.20732 | RMSE$=40,240 | MAE$=26,324 | MAPE=15.90% | n_d/n_a=16/16 steps=3 lr=0.01885 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 125.08173| val_0_rmse: 10.95411|  0:00:00s\n",
      "epoch 1  | loss: 87.98419| val_0_rmse: 9.82315 |  0:00:00s\n",
      "epoch 2  | loss: 60.28686| val_0_rmse: 8.49112 |  0:00:00s\n",
      "epoch 3  | loss: 37.19188| val_0_rmse: 7.22012 |  0:00:00s\n",
      "epoch 4  | loss: 22.95251| val_0_rmse: 5.99309 |  0:00:01s\n",
      "epoch 5  | loss: 17.99202| val_0_rmse: 4.83937 |  0:00:01s\n",
      "epoch 6  | loss: 14.37003| val_0_rmse: 3.98208 |  0:00:01s\n",
      "epoch 7  | loss: 9.53069 | val_0_rmse: 3.59746 |  0:00:01s\n",
      "epoch 8  | loss: 6.11356 | val_0_rmse: 3.58636 |  0:00:02s\n",
      "epoch 9  | loss: 2.59367 | val_0_rmse: 3.16434 |  0:00:02s\n",
      "epoch 10 | loss: 1.97171 | val_0_rmse: 2.21759 |  0:00:02s\n",
      "epoch 11 | loss: 1.65782 | val_0_rmse: 1.94624 |  0:00:02s\n",
      "epoch 12 | loss: 1.05815 | val_0_rmse: 1.87756 |  0:00:03s\n",
      "epoch 13 | loss: 0.82277 | val_0_rmse: 1.32402 |  0:00:03s\n",
      "epoch 14 | loss: 0.65675 | val_0_rmse: 1.35784 |  0:00:03s\n",
      "epoch 15 | loss: 0.47876 | val_0_rmse: 1.26865 |  0:00:03s\n",
      "epoch 16 | loss: 0.39424 | val_0_rmse: 1.14281 |  0:00:04s\n",
      "epoch 17 | loss: 0.28948 | val_0_rmse: 1.23377 |  0:00:04s\n",
      "epoch 18 | loss: 0.44482 | val_0_rmse: 0.83302 |  0:00:04s\n",
      "epoch 19 | loss: 0.31551 | val_0_rmse: 0.75854 |  0:00:04s\n",
      "epoch 20 | loss: 0.24639 | val_0_rmse: 0.7649  |  0:00:05s\n",
      "epoch 21 | loss: 0.22327 | val_0_rmse: 0.70008 |  0:00:05s\n",
      "epoch 22 | loss: 0.18344 | val_0_rmse: 0.57878 |  0:00:05s\n",
      "epoch 23 | loss: 0.17029 | val_0_rmse: 0.59178 |  0:00:05s\n",
      "epoch 24 | loss: 0.16272 | val_0_rmse: 0.60828 |  0:00:06s\n",
      "epoch 25 | loss: 0.21897 | val_0_rmse: 0.35714 |  0:00:06s\n",
      "epoch 26 | loss: 0.18437 | val_0_rmse: 0.75033 |  0:00:06s\n",
      "epoch 27 | loss: 0.43368 | val_0_rmse: 0.4451  |  0:00:06s\n",
      "epoch 28 | loss: 0.21507 | val_0_rmse: 0.45442 |  0:00:06s\n",
      "epoch 29 | loss: 0.20181 | val_0_rmse: 0.38638 |  0:00:07s\n",
      "epoch 30 | loss: 0.14188 | val_0_rmse: 0.31624 |  0:00:07s\n",
      "epoch 31 | loss: 0.11907 | val_0_rmse: 0.4012  |  0:00:07s\n",
      "epoch 32 | loss: 0.13543 | val_0_rmse: 0.27393 |  0:00:07s\n",
      "epoch 33 | loss: 0.13343 | val_0_rmse: 0.34791 |  0:00:08s\n",
      "epoch 34 | loss: 0.11617 | val_0_rmse: 0.25633 |  0:00:08s\n",
      "epoch 35 | loss: 0.0944  | val_0_rmse: 0.32546 |  0:00:08s\n",
      "epoch 36 | loss: 0.08683 | val_0_rmse: 0.25624 |  0:00:08s\n",
      "epoch 37 | loss: 0.08175 | val_0_rmse: 0.28162 |  0:00:09s\n",
      "epoch 38 | loss: 0.0814  | val_0_rmse: 0.27703 |  0:00:09s\n",
      "epoch 39 | loss: 0.09344 | val_0_rmse: 0.27259 |  0:00:09s\n",
      "epoch 40 | loss: 0.0807  | val_0_rmse: 0.30599 |  0:00:09s\n",
      "epoch 41 | loss: 0.076   | val_0_rmse: 0.31986 |  0:00:10s\n",
      "epoch 42 | loss: 0.07812 | val_0_rmse: 0.27779 |  0:00:10s\n",
      "epoch 43 | loss: 0.07388 | val_0_rmse: 0.25466 |  0:00:10s\n",
      "epoch 44 | loss: 0.08222 | val_0_rmse: 0.30885 |  0:00:10s\n",
      "epoch 45 | loss: 0.0636  | val_0_rmse: 0.27675 |  0:00:11s\n",
      "epoch 46 | loss: 0.06884 | val_0_rmse: 0.29806 |  0:00:11s\n",
      "epoch 47 | loss: 0.06039 | val_0_rmse: 0.24014 |  0:00:11s\n",
      "epoch 48 | loss: 0.06264 | val_0_rmse: 0.27148 |  0:00:11s\n",
      "epoch 49 | loss: 0.05695 | val_0_rmse: 0.25752 |  0:00:11s\n",
      "epoch 50 | loss: 0.06202 | val_0_rmse: 0.26884 |  0:00:12s\n",
      "epoch 51 | loss: 0.06447 | val_0_rmse: 0.24111 |  0:00:12s\n",
      "epoch 52 | loss: 0.10386 | val_0_rmse: 0.32677 |  0:00:12s\n",
      "epoch 53 | loss: 0.07589 | val_0_rmse: 0.26281 |  0:00:12s\n",
      "epoch 54 | loss: 0.06374 | val_0_rmse: 0.25269 |  0:00:13s\n",
      "epoch 55 | loss: 0.05771 | val_0_rmse: 0.25758 |  0:00:13s\n",
      "epoch 56 | loss: 0.05589 | val_0_rmse: 0.24319 |  0:00:13s\n",
      "epoch 57 | loss: 0.05951 | val_0_rmse: 0.28771 |  0:00:13s\n",
      "epoch 58 | loss: 0.07226 | val_0_rmse: 0.26535 |  0:00:14s\n",
      "epoch 59 | loss: 0.05997 | val_0_rmse: 0.25547 |  0:00:14s\n",
      "epoch 60 | loss: 0.04809 | val_0_rmse: 0.27781 |  0:00:14s\n",
      "epoch 61 | loss: 0.05922 | val_0_rmse: 0.24736 |  0:00:14s\n",
      "epoch 62 | loss: 0.06497 | val_0_rmse: 0.2597  |  0:00:14s\n",
      "epoch 63 | loss: 0.06302 | val_0_rmse: 0.25653 |  0:00:15s\n",
      "epoch 64 | loss: 0.05794 | val_0_rmse: 0.25144 |  0:00:15s\n",
      "epoch 65 | loss: 0.05498 | val_0_rmse: 0.23645 |  0:00:15s\n",
      "epoch 66 | loss: 0.04755 | val_0_rmse: 0.25942 |  0:00:15s\n",
      "epoch 67 | loss: 0.06754 | val_0_rmse: 0.28906 |  0:00:16s\n",
      "epoch 68 | loss: 0.10306 | val_0_rmse: 0.24008 |  0:00:16s\n",
      "epoch 69 | loss: 0.06467 | val_0_rmse: 0.28779 |  0:00:16s\n",
      "epoch 70 | loss: 0.08183 | val_0_rmse: 0.28882 |  0:00:16s\n",
      "epoch 71 | loss: 0.09664 | val_0_rmse: 0.27344 |  0:00:17s\n",
      "epoch 72 | loss: 0.07122 | val_0_rmse: 0.28371 |  0:00:17s\n",
      "epoch 73 | loss: 0.08547 | val_0_rmse: 0.29304 |  0:00:17s\n",
      "epoch 74 | loss: 0.08156 | val_0_rmse: 0.25852 |  0:00:17s\n",
      "epoch 75 | loss: 0.06757 | val_0_rmse: 0.25505 |  0:00:17s\n",
      "epoch 76 | loss: 0.07814 | val_0_rmse: 0.24383 |  0:00:18s\n",
      "epoch 77 | loss: 0.06279 | val_0_rmse: 0.25784 |  0:00:18s\n",
      "epoch 78 | loss: 0.06591 | val_0_rmse: 0.2455  |  0:00:18s\n",
      "epoch 79 | loss: 0.0725  | val_0_rmse: 0.2459  |  0:00:18s\n",
      "epoch 80 | loss: 0.05666 | val_0_rmse: 0.24103 |  0:00:19s\n",
      "epoch 81 | loss: 0.04493 | val_0_rmse: 0.28325 |  0:00:19s\n",
      "epoch 82 | loss: 0.09054 | val_0_rmse: 0.2446  |  0:00:19s\n",
      "epoch 83 | loss: 0.07012 | val_0_rmse: 0.24961 |  0:00:19s\n",
      "epoch 84 | loss: 0.06349 | val_0_rmse: 0.25805 |  0:00:19s\n",
      "epoch 85 | loss: 0.05761 | val_0_rmse: 0.21567 |  0:00:20s\n",
      "epoch 86 | loss: 0.06375 | val_0_rmse: 0.2647  |  0:00:20s\n",
      "epoch 87 | loss: 0.05458 | val_0_rmse: 0.22991 |  0:00:20s\n",
      "epoch 88 | loss: 0.04553 | val_0_rmse: 0.22805 |  0:00:20s\n",
      "epoch 89 | loss: 0.04823 | val_0_rmse: 0.22948 |  0:00:21s\n",
      "epoch 90 | loss: 0.0442  | val_0_rmse: 0.22752 |  0:00:21s\n",
      "epoch 91 | loss: 0.04437 | val_0_rmse: 0.25201 |  0:00:21s\n",
      "epoch 92 | loss: 0.04643 | val_0_rmse: 0.22112 |  0:00:21s\n",
      "epoch 93 | loss: 0.03995 | val_0_rmse: 0.22983 |  0:00:22s\n",
      "epoch 94 | loss: 0.03981 | val_0_rmse: 0.22742 |  0:00:22s\n",
      "epoch 95 | loss: 0.04476 | val_0_rmse: 0.23882 |  0:00:22s\n",
      "epoch 96 | loss: 0.04159 | val_0_rmse: 0.23487 |  0:00:22s\n",
      "epoch 97 | loss: 0.0525  | val_0_rmse: 0.24533 |  0:00:22s\n",
      "epoch 98 | loss: 0.04542 | val_0_rmse: 0.23368 |  0:00:23s\n",
      "epoch 99 | loss: 0.05031 | val_0_rmse: 0.26572 |  0:00:23s\n",
      "epoch 100| loss: 0.05392 | val_0_rmse: 0.26463 |  0:00:23s\n",
      "epoch 101| loss: 0.06145 | val_0_rmse: 0.26576 |  0:00:23s\n",
      "epoch 102| loss: 0.06741 | val_0_rmse: 0.26302 |  0:00:24s\n",
      "epoch 103| loss: 0.09518 | val_0_rmse: 0.21873 |  0:00:24s\n",
      "epoch 104| loss: 0.08246 | val_0_rmse: 0.2887  |  0:00:24s\n",
      "epoch 105| loss: 0.07513 | val_0_rmse: 0.32961 |  0:00:24s\n",
      "epoch 106| loss: 0.1219  | val_0_rmse: 0.2447  |  0:00:25s\n",
      "epoch 107| loss: 0.06391 | val_0_rmse: 0.21815 |  0:00:25s\n",
      "epoch 108| loss: 0.04677 | val_0_rmse: 0.22108 |  0:00:25s\n",
      "epoch 109| loss: 0.04972 | val_0_rmse: 0.24325 |  0:00:25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:40:34,046] Trial 72 finished with value: 0.2156722809413298 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.3482433742924025, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.017112050552316474, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 110| loss: 0.0456  | val_0_rmse: 0.22865 |  0:00:26s\n",
      "\n",
      "Early stopping occurred at epoch 110 with best_epoch = 85 and best_val_0_rmse = 0.21567\n",
      "Trial 072 | rmse_log=0.21567 | RMSE$=43,023 | MAE$=27,569 | MAPE=16.43% | n_d/n_a=24/16 steps=3 lr=0.01711 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 120.27113| val_0_rmse: 10.99592|  0:00:00s\n",
      "epoch 1  | loss: 78.80544| val_0_rmse: 9.83455 |  0:00:00s\n",
      "epoch 2  | loss: 51.3655 | val_0_rmse: 8.3916  |  0:00:00s\n",
      "epoch 3  | loss: 30.13897| val_0_rmse: 6.64359 |  0:00:01s\n",
      "epoch 4  | loss: 18.68832| val_0_rmse: 4.75937 |  0:00:01s\n",
      "epoch 5  | loss: 12.45432| val_0_rmse: 3.18594 |  0:00:01s\n",
      "epoch 6  | loss: 9.17148 | val_0_rmse: 2.31785 |  0:00:01s\n",
      "epoch 7  | loss: 6.17976 | val_0_rmse: 2.25436 |  0:00:02s\n",
      "epoch 8  | loss: 4.17202 | val_0_rmse: 2.61335 |  0:00:02s\n",
      "epoch 9  | loss: 3.08875 | val_0_rmse: 1.53166 |  0:00:02s\n",
      "epoch 10 | loss: 1.86887 | val_0_rmse: 1.16219 |  0:00:02s\n",
      "epoch 11 | loss: 1.61223 | val_0_rmse: 1.60276 |  0:00:03s\n",
      "epoch 12 | loss: 1.27503 | val_0_rmse: 1.00433 |  0:00:03s\n",
      "epoch 13 | loss: 0.86568 | val_0_rmse: 1.41128 |  0:00:03s\n",
      "epoch 14 | loss: 0.82196 | val_0_rmse: 0.80852 |  0:00:03s\n",
      "epoch 15 | loss: 0.60677 | val_0_rmse: 1.02548 |  0:00:04s\n",
      "epoch 16 | loss: 0.52482 | val_0_rmse: 0.69884 |  0:00:04s\n",
      "epoch 17 | loss: 0.44596 | val_0_rmse: 0.63815 |  0:00:04s\n",
      "epoch 18 | loss: 0.38883 | val_0_rmse: 0.82225 |  0:00:04s\n",
      "epoch 19 | loss: 0.46774 | val_0_rmse: 0.54828 |  0:00:04s\n",
      "epoch 20 | loss: 0.3584  | val_0_rmse: 0.68422 |  0:00:05s\n",
      "epoch 21 | loss: 0.26248 | val_0_rmse: 0.62252 |  0:00:05s\n",
      "epoch 22 | loss: 0.26924 | val_0_rmse: 0.52598 |  0:00:05s\n",
      "epoch 23 | loss: 0.19759 | val_0_rmse: 0.4049  |  0:00:05s\n",
      "epoch 24 | loss: 0.25352 | val_0_rmse: 0.50197 |  0:00:06s\n",
      "epoch 25 | loss: 0.1738  | val_0_rmse: 0.30007 |  0:00:06s\n",
      "epoch 26 | loss: 0.19212 | val_0_rmse: 0.40266 |  0:00:06s\n",
      "epoch 27 | loss: 0.17781 | val_0_rmse: 0.3643  |  0:00:06s\n",
      "epoch 28 | loss: 0.14545 | val_0_rmse: 0.29282 |  0:00:07s\n",
      "epoch 29 | loss: 0.13689 | val_0_rmse: 0.30898 |  0:00:07s\n",
      "epoch 30 | loss: 0.12725 | val_0_rmse: 0.31513 |  0:00:07s\n",
      "epoch 31 | loss: 0.24708 | val_0_rmse: 0.34093 |  0:00:07s\n",
      "epoch 32 | loss: 0.12882 | val_0_rmse: 0.26566 |  0:00:08s\n",
      "epoch 33 | loss: 0.11232 | val_0_rmse: 0.30439 |  0:00:08s\n",
      "epoch 34 | loss: 0.13608 | val_0_rmse: 0.27202 |  0:00:08s\n",
      "epoch 35 | loss: 0.10146 | val_0_rmse: 0.24188 |  0:00:08s\n",
      "epoch 36 | loss: 0.09347 | val_0_rmse: 0.2499  |  0:00:09s\n",
      "epoch 37 | loss: 0.09491 | val_0_rmse: 0.26087 |  0:00:09s\n",
      "epoch 38 | loss: 0.08149 | val_0_rmse: 0.23156 |  0:00:09s\n",
      "epoch 39 | loss: 0.07347 | val_0_rmse: 0.26687 |  0:00:09s\n",
      "epoch 40 | loss: 0.10141 | val_0_rmse: 0.24388 |  0:00:09s\n",
      "epoch 41 | loss: 0.08471 | val_0_rmse: 0.2403  |  0:00:10s\n",
      "epoch 42 | loss: 0.07195 | val_0_rmse: 0.24938 |  0:00:10s\n",
      "epoch 43 | loss: 0.09155 | val_0_rmse: 0.2498  |  0:00:10s\n",
      "epoch 44 | loss: 0.09712 | val_0_rmse: 0.29769 |  0:00:10s\n",
      "epoch 45 | loss: 0.1158  | val_0_rmse: 0.31111 |  0:00:11s\n",
      "epoch 46 | loss: 0.13505 | val_0_rmse: 0.26893 |  0:00:11s\n",
      "epoch 47 | loss: 0.09727 | val_0_rmse: 0.27544 |  0:00:11s\n",
      "epoch 48 | loss: 0.09114 | val_0_rmse: 0.31818 |  0:00:11s\n",
      "epoch 49 | loss: 0.09186 | val_0_rmse: 0.24835 |  0:00:12s\n",
      "epoch 50 | loss: 0.07529 | val_0_rmse: 0.2425  |  0:00:12s\n",
      "epoch 51 | loss: 0.08194 | val_0_rmse: 0.29042 |  0:00:12s\n",
      "epoch 52 | loss: 0.08691 | val_0_rmse: 0.25163 |  0:00:12s\n",
      "epoch 53 | loss: 0.0787  | val_0_rmse: 0.25929 |  0:00:13s\n",
      "epoch 54 | loss: 0.07083 | val_0_rmse: 0.24991 |  0:00:13s\n",
      "epoch 55 | loss: 0.08049 | val_0_rmse: 0.27032 |  0:00:13s\n",
      "epoch 56 | loss: 0.06177 | val_0_rmse: 0.24033 |  0:00:13s\n",
      "epoch 57 | loss: 0.04988 | val_0_rmse: 0.24101 |  0:00:13s\n",
      "epoch 58 | loss: 0.05746 | val_0_rmse: 0.2377  |  0:00:14s\n",
      "epoch 59 | loss: 0.0458  | val_0_rmse: 0.25009 |  0:00:14s\n",
      "epoch 60 | loss: 0.05249 | val_0_rmse: 0.23741 |  0:00:14s\n",
      "epoch 61 | loss: 0.04234 | val_0_rmse: 0.23122 |  0:00:14s\n",
      "epoch 62 | loss: 0.04652 | val_0_rmse: 0.26128 |  0:00:15s\n",
      "epoch 63 | loss: 0.05256 | val_0_rmse: 0.22005 |  0:00:15s\n",
      "epoch 64 | loss: 0.05082 | val_0_rmse: 0.2346  |  0:00:15s\n",
      "epoch 65 | loss: 0.05104 | val_0_rmse: 0.26646 |  0:00:15s\n",
      "epoch 66 | loss: 0.0584  | val_0_rmse: 0.22816 |  0:00:16s\n",
      "epoch 67 | loss: 0.05503 | val_0_rmse: 0.25448 |  0:00:16s\n",
      "epoch 68 | loss: 0.0458  | val_0_rmse: 0.23228 |  0:00:16s\n",
      "epoch 69 | loss: 0.05677 | val_0_rmse: 0.22785 |  0:00:16s\n",
      "epoch 70 | loss: 0.05436 | val_0_rmse: 0.21167 |  0:00:16s\n",
      "epoch 71 | loss: 0.04854 | val_0_rmse: 0.20687 |  0:00:17s\n",
      "epoch 72 | loss: 0.0466  | val_0_rmse: 0.22836 |  0:00:17s\n",
      "epoch 73 | loss: 0.04298 | val_0_rmse: 0.22273 |  0:00:17s\n",
      "epoch 74 | loss: 0.04124 | val_0_rmse: 0.22873 |  0:00:17s\n",
      "epoch 75 | loss: 0.05018 | val_0_rmse: 0.22206 |  0:00:18s\n",
      "epoch 76 | loss: 0.04251 | val_0_rmse: 0.22343 |  0:00:18s\n",
      "epoch 77 | loss: 0.04056 | val_0_rmse: 0.25572 |  0:00:18s\n",
      "epoch 78 | loss: 0.06524 | val_0_rmse: 0.24902 |  0:00:18s\n",
      "epoch 79 | loss: 0.08887 | val_0_rmse: 0.23195 |  0:00:19s\n",
      "epoch 80 | loss: 0.05783 | val_0_rmse: 0.24286 |  0:00:19s\n",
      "epoch 81 | loss: 0.06737 | val_0_rmse: 0.27624 |  0:00:19s\n",
      "epoch 82 | loss: 0.06653 | val_0_rmse: 0.38124 |  0:00:19s\n",
      "epoch 83 | loss: 0.13609 | val_0_rmse: 0.24328 |  0:00:20s\n",
      "epoch 84 | loss: 0.07077 | val_0_rmse: 0.22618 |  0:00:20s\n",
      "epoch 85 | loss: 0.05523 | val_0_rmse: 0.29004 |  0:00:20s\n",
      "epoch 86 | loss: 0.05451 | val_0_rmse: 0.228   |  0:00:20s\n",
      "epoch 87 | loss: 0.04192 | val_0_rmse: 0.21082 |  0:00:20s\n",
      "epoch 88 | loss: 0.04172 | val_0_rmse: 0.22367 |  0:00:21s\n",
      "epoch 89 | loss: 0.03705 | val_0_rmse: 0.20802 |  0:00:21s\n",
      "epoch 90 | loss: 0.03439 | val_0_rmse: 0.18815 |  0:00:21s\n",
      "epoch 91 | loss: 0.03224 | val_0_rmse: 0.20056 |  0:00:21s\n",
      "epoch 92 | loss: 0.03417 | val_0_rmse: 0.19412 |  0:00:22s\n",
      "epoch 93 | loss: 0.03112 | val_0_rmse: 0.19274 |  0:00:22s\n",
      "epoch 94 | loss: 0.03228 | val_0_rmse: 0.19577 |  0:00:22s\n",
      "epoch 95 | loss: 0.03254 | val_0_rmse: 0.20561 |  0:00:22s\n",
      "epoch 96 | loss: 0.0309  | val_0_rmse: 0.2225  |  0:00:23s\n",
      "epoch 97 | loss: 0.03228 | val_0_rmse: 0.2038  |  0:00:23s\n",
      "epoch 98 | loss: 0.03372 | val_0_rmse: 0.19692 |  0:00:23s\n",
      "epoch 99 | loss: 0.03471 | val_0_rmse: 0.1884  |  0:00:23s\n",
      "epoch 100| loss: 0.0401  | val_0_rmse: 0.20816 |  0:00:24s\n",
      "epoch 101| loss: 0.04558 | val_0_rmse: 0.22661 |  0:00:24s\n",
      "epoch 102| loss: 0.05378 | val_0_rmse: 0.21899 |  0:00:24s\n",
      "epoch 103| loss: 0.04982 | val_0_rmse: 0.23895 |  0:00:24s\n",
      "epoch 104| loss: 0.05267 | val_0_rmse: 0.20315 |  0:00:24s\n",
      "epoch 105| loss: 0.03705 | val_0_rmse: 0.21796 |  0:00:25s\n",
      "epoch 106| loss: 0.04    | val_0_rmse: 0.19069 |  0:00:25s\n",
      "epoch 107| loss: 0.02871 | val_0_rmse: 0.19805 |  0:00:25s\n",
      "epoch 108| loss: 0.03057 | val_0_rmse: 0.19571 |  0:00:25s\n",
      "epoch 109| loss: 0.0275  | val_0_rmse: 0.20514 |  0:00:26s\n",
      "epoch 110| loss: 0.02642 | val_0_rmse: 0.21135 |  0:00:26s\n",
      "epoch 111| loss: 0.03333 | val_0_rmse: 0.21456 |  0:00:26s\n",
      "epoch 112| loss: 0.02842 | val_0_rmse: 0.18827 |  0:00:26s\n",
      "epoch 113| loss: 0.02589 | val_0_rmse: 0.21481 |  0:00:27s\n",
      "epoch 114| loss: 0.04288 | val_0_rmse: 0.23897 |  0:00:27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:41:01,998] Trial 73 finished with value: 0.18814656980348465 and parameters: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.3269956118906108, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.015167556506165851, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 115| loss: 0.0497  | val_0_rmse: 0.24051 |  0:00:27s\n",
      "\n",
      "Early stopping occurred at epoch 115 with best_epoch = 90 and best_val_0_rmse = 0.18815\n",
      "Trial 073 | rmse_log=0.18815 | RMSE$=35,161 | MAE$=22,112 | MAPE=14.08% | n_d/n_a=32/64 steps=3 lr=0.01517 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 126.79518| val_0_rmse: 11.21452|  0:00:00s\n",
      "epoch 1  | loss: 97.89198| val_0_rmse: 10.43767|  0:00:00s\n",
      "epoch 2  | loss: 75.54707| val_0_rmse: 9.68776 |  0:00:00s\n",
      "epoch 3  | loss: 56.20253| val_0_rmse: 8.80127 |  0:00:00s\n",
      "epoch 4  | loss: 40.47893| val_0_rmse: 7.81023 |  0:00:01s\n",
      "epoch 5  | loss: 26.74376| val_0_rmse: 6.72206 |  0:00:01s\n",
      "epoch 6  | loss: 21.20273| val_0_rmse: 5.6384  |  0:00:01s\n",
      "epoch 7  | loss: 15.15194| val_0_rmse: 4.52607 |  0:00:02s\n",
      "epoch 8  | loss: 11.83302| val_0_rmse: 3.70765 |  0:00:02s\n",
      "epoch 9  | loss: 10.31781| val_0_rmse: 3.46621 |  0:00:02s\n",
      "epoch 10 | loss: 6.58541 | val_0_rmse: 3.43647 |  0:00:02s\n",
      "epoch 11 | loss: 3.82843 | val_0_rmse: 3.07075 |  0:00:03s\n",
      "epoch 12 | loss: 3.00272 | val_0_rmse: 2.30562 |  0:00:03s\n",
      "epoch 13 | loss: 1.66654 | val_0_rmse: 1.87796 |  0:00:03s\n",
      "epoch 14 | loss: 1.61395 | val_0_rmse: 1.68314 |  0:00:03s\n",
      "epoch 15 | loss: 1.08896 | val_0_rmse: 1.19912 |  0:00:03s\n",
      "epoch 16 | loss: 0.78205 | val_0_rmse: 1.14189 |  0:00:04s\n",
      "epoch 17 | loss: 0.72718 | val_0_rmse: 1.04136 |  0:00:04s\n",
      "epoch 18 | loss: 0.52611 | val_0_rmse: 0.88233 |  0:00:04s\n",
      "epoch 19 | loss: 0.44027 | val_0_rmse: 0.65182 |  0:00:04s\n",
      "epoch 20 | loss: 0.34951 | val_0_rmse: 0.69255 |  0:00:05s\n",
      "epoch 21 | loss: 0.34902 | val_0_rmse: 0.61712 |  0:00:05s\n",
      "epoch 22 | loss: 0.28893 | val_0_rmse: 0.52312 |  0:00:05s\n",
      "epoch 23 | loss: 0.25518 | val_0_rmse: 0.58265 |  0:00:05s\n",
      "epoch 24 | loss: 0.22402 | val_0_rmse: 0.63127 |  0:00:06s\n",
      "epoch 25 | loss: 0.22354 | val_0_rmse: 0.50038 |  0:00:06s\n",
      "epoch 26 | loss: 0.18971 | val_0_rmse: 0.52432 |  0:00:06s\n",
      "epoch 27 | loss: 0.19643 | val_0_rmse: 0.58459 |  0:00:06s\n",
      "epoch 28 | loss: 0.15516 | val_0_rmse: 0.58044 |  0:00:07s\n",
      "epoch 29 | loss: 0.19053 | val_0_rmse: 0.47577 |  0:00:07s\n",
      "epoch 30 | loss: 0.17481 | val_0_rmse: 0.41751 |  0:00:07s\n",
      "epoch 31 | loss: 0.13036 | val_0_rmse: 0.4852  |  0:00:07s\n",
      "epoch 32 | loss: 0.15012 | val_0_rmse: 0.37013 |  0:00:08s\n",
      "epoch 33 | loss: 0.16756 | val_0_rmse: 0.35678 |  0:00:08s\n",
      "epoch 34 | loss: 0.14824 | val_0_rmse: 0.34428 |  0:00:08s\n",
      "epoch 35 | loss: 0.11672 | val_0_rmse: 0.31719 |  0:00:08s\n",
      "epoch 36 | loss: 0.12591 | val_0_rmse: 0.33558 |  0:00:09s\n",
      "epoch 37 | loss: 0.11733 | val_0_rmse: 0.34338 |  0:00:09s\n",
      "epoch 38 | loss: 0.14233 | val_0_rmse: 0.30787 |  0:00:09s\n",
      "epoch 39 | loss: 0.11594 | val_0_rmse: 0.33926 |  0:00:09s\n",
      "epoch 40 | loss: 0.09774 | val_0_rmse: 0.28692 |  0:00:10s\n",
      "epoch 41 | loss: 0.1057  | val_0_rmse: 0.28205 |  0:00:10s\n",
      "epoch 42 | loss: 0.09401 | val_0_rmse: 0.29647 |  0:00:10s\n",
      "epoch 43 | loss: 0.10399 | val_0_rmse: 0.23755 |  0:00:10s\n",
      "epoch 44 | loss: 0.09754 | val_0_rmse: 0.32957 |  0:00:11s\n",
      "epoch 45 | loss: 0.11393 | val_0_rmse: 0.26086 |  0:00:11s\n",
      "epoch 46 | loss: 0.12577 | val_0_rmse: 0.24488 |  0:00:11s\n",
      "epoch 47 | loss: 0.1116  | val_0_rmse: 0.21395 |  0:00:11s\n",
      "epoch 48 | loss: 0.07579 | val_0_rmse: 0.21842 |  0:00:11s\n",
      "epoch 49 | loss: 0.07153 | val_0_rmse: 0.21628 |  0:00:12s\n",
      "epoch 50 | loss: 0.06633 | val_0_rmse: 0.25333 |  0:00:12s\n",
      "epoch 51 | loss: 0.07764 | val_0_rmse: 0.25386 |  0:00:12s\n",
      "epoch 52 | loss: 0.09883 | val_0_rmse: 0.22066 |  0:00:12s\n",
      "epoch 53 | loss: 0.0655  | val_0_rmse: 0.21402 |  0:00:13s\n",
      "epoch 54 | loss: 0.06226 | val_0_rmse: 0.21882 |  0:00:13s\n",
      "epoch 55 | loss: 0.05825 | val_0_rmse: 0.23523 |  0:00:13s\n",
      "epoch 56 | loss: 0.06964 | val_0_rmse: 0.21052 |  0:00:13s\n",
      "epoch 57 | loss: 0.05919 | val_0_rmse: 0.22552 |  0:00:14s\n",
      "epoch 58 | loss: 0.0556  | val_0_rmse: 0.22139 |  0:00:14s\n",
      "epoch 59 | loss: 0.06639 | val_0_rmse: 0.22205 |  0:00:14s\n",
      "epoch 60 | loss: 0.06044 | val_0_rmse: 0.22293 |  0:00:14s\n",
      "epoch 61 | loss: 0.05483 | val_0_rmse: 0.22113 |  0:00:14s\n",
      "epoch 62 | loss: 0.06163 | val_0_rmse: 0.2377  |  0:00:15s\n",
      "epoch 63 | loss: 0.05676 | val_0_rmse: 0.24043 |  0:00:15s\n",
      "epoch 64 | loss: 0.05998 | val_0_rmse: 0.24297 |  0:00:15s\n",
      "epoch 65 | loss: 0.06614 | val_0_rmse: 0.22965 |  0:00:15s\n",
      "epoch 66 | loss: 0.05438 | val_0_rmse: 0.21199 |  0:00:16s\n",
      "epoch 67 | loss: 0.05484 | val_0_rmse: 0.22822 |  0:00:16s\n",
      "epoch 68 | loss: 0.05003 | val_0_rmse: 0.22894 |  0:00:16s\n",
      "epoch 69 | loss: 0.04858 | val_0_rmse: 0.21614 |  0:00:16s\n",
      "epoch 70 | loss: 0.0597  | val_0_rmse: 0.22791 |  0:00:17s\n",
      "epoch 71 | loss: 0.06342 | val_0_rmse: 0.25048 |  0:00:17s\n",
      "epoch 72 | loss: 0.05745 | val_0_rmse: 0.21447 |  0:00:17s\n",
      "epoch 73 | loss: 0.04624 | val_0_rmse: 0.23212 |  0:00:17s\n",
      "epoch 74 | loss: 0.04624 | val_0_rmse: 0.2231  |  0:00:17s\n",
      "epoch 75 | loss: 0.04324 | val_0_rmse: 0.22235 |  0:00:18s\n",
      "epoch 76 | loss: 0.04328 | val_0_rmse: 0.22825 |  0:00:18s\n",
      "epoch 77 | loss: 0.04164 | val_0_rmse: 0.22369 |  0:00:18s\n",
      "epoch 78 | loss: 0.04786 | val_0_rmse: 0.22578 |  0:00:18s\n",
      "epoch 79 | loss: 0.05043 | val_0_rmse: 0.22308 |  0:00:19s\n",
      "epoch 80 | loss: 0.05233 | val_0_rmse: 0.24061 |  0:00:19s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:41:21,926] Trial 74 finished with value: 0.21052408740694264 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.3848346524202095, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.013593181887200441, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 13 with value: 0.17502611623133627.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 81 | loss: 0.05759 | val_0_rmse: 0.21669 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 81 with best_epoch = 56 and best_val_0_rmse = 0.21052\n",
      "Trial 074 | rmse_log=0.21052 | RMSE$=44,940 | MAE$=27,883 | MAPE=15.83% | n_d/n_a=24/16 steps=3 lr=0.01359 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.04973| val_0_rmse: 10.99165|  0:00:00s\n",
      "epoch 1  | loss: 89.06008| val_0_rmse: 9.9431  |  0:00:00s\n",
      "epoch 2  | loss: 59.40515| val_0_rmse: 8.74337 |  0:00:00s\n",
      "epoch 3  | loss: 38.15757| val_0_rmse: 7.40508 |  0:00:01s\n",
      "epoch 4  | loss: 23.74859| val_0_rmse: 5.91822 |  0:00:01s\n",
      "epoch 5  | loss: 14.49446| val_0_rmse: 4.49727 |  0:00:01s\n",
      "epoch 6  | loss: 11.89999| val_0_rmse: 3.49597 |  0:00:01s\n",
      "epoch 7  | loss: 9.40919 | val_0_rmse: 3.2735  |  0:00:02s\n",
      "epoch 8  | loss: 5.12708 | val_0_rmse: 3.25676 |  0:00:02s\n",
      "epoch 9  | loss: 3.27222 | val_0_rmse: 2.73516 |  0:00:02s\n",
      "epoch 10 | loss: 2.31068 | val_0_rmse: 1.89623 |  0:00:02s\n",
      "epoch 11 | loss: 1.48752 | val_0_rmse: 1.91827 |  0:00:03s\n",
      "epoch 12 | loss: 0.98775 | val_0_rmse: 1.75224 |  0:00:03s\n",
      "epoch 13 | loss: 0.87301 | val_0_rmse: 1.15747 |  0:00:03s\n",
      "epoch 14 | loss: 0.99645 | val_0_rmse: 1.19672 |  0:00:03s\n",
      "epoch 15 | loss: 0.72089 | val_0_rmse: 1.09848 |  0:00:04s\n",
      "epoch 16 | loss: 0.60768 | val_0_rmse: 0.87898 |  0:00:04s\n",
      "epoch 17 | loss: 0.55289 | val_0_rmse: 0.83893 |  0:00:04s\n",
      "epoch 18 | loss: 0.44769 | val_0_rmse: 0.91712 |  0:00:04s\n",
      "epoch 19 | loss: 0.41249 | val_0_rmse: 0.64369 |  0:00:04s\n",
      "epoch 20 | loss: 0.42459 | val_0_rmse: 0.79686 |  0:00:05s\n",
      "epoch 21 | loss: 0.37034 | val_0_rmse: 0.48618 |  0:00:05s\n",
      "epoch 22 | loss: 0.44724 | val_0_rmse: 0.81168 |  0:00:05s\n",
      "epoch 23 | loss: 0.41035 | val_0_rmse: 0.44587 |  0:00:05s\n",
      "epoch 24 | loss: 0.31928 | val_0_rmse: 1.08115 |  0:00:06s\n",
      "epoch 25 | loss: 0.38093 | val_0_rmse: 0.46043 |  0:00:06s\n",
      "epoch 26 | loss: 0.40426 | val_0_rmse: 0.7133  |  0:00:06s\n",
      "epoch 27 | loss: 0.32809 | val_0_rmse: 0.47409 |  0:00:06s\n",
      "epoch 28 | loss: 0.34675 | val_0_rmse: 0.4841  |  0:00:07s\n",
      "epoch 29 | loss: 0.26331 | val_0_rmse: 0.40301 |  0:00:07s\n",
      "epoch 30 | loss: 0.2826  | val_0_rmse: 0.54751 |  0:00:07s\n",
      "epoch 31 | loss: 0.20836 | val_0_rmse: 0.41264 |  0:00:07s\n",
      "epoch 32 | loss: 0.20897 | val_0_rmse: 0.60446 |  0:00:07s\n",
      "epoch 33 | loss: 0.24204 | val_0_rmse: 0.39281 |  0:00:08s\n",
      "epoch 34 | loss: 0.24509 | val_0_rmse: 0.50113 |  0:00:08s\n",
      "epoch 35 | loss: 0.18356 | val_0_rmse: 0.39132 |  0:00:08s\n",
      "epoch 36 | loss: 0.19041 | val_0_rmse: 0.52425 |  0:00:08s\n",
      "epoch 37 | loss: 0.17707 | val_0_rmse: 0.36114 |  0:00:09s\n",
      "epoch 38 | loss: 0.17863 | val_0_rmse: 0.51263 |  0:00:09s\n",
      "epoch 39 | loss: 0.14091 | val_0_rmse: 0.37194 |  0:00:09s\n",
      "epoch 40 | loss: 0.14292 | val_0_rmse: 0.4656  |  0:00:09s\n",
      "epoch 41 | loss: 0.14494 | val_0_rmse: 0.35195 |  0:00:10s\n",
      "epoch 42 | loss: 0.13493 | val_0_rmse: 0.54999 |  0:00:10s\n",
      "epoch 43 | loss: 0.18055 | val_0_rmse: 0.34379 |  0:00:10s\n",
      "epoch 44 | loss: 0.12357 | val_0_rmse: 0.39061 |  0:00:10s\n",
      "epoch 45 | loss: 0.13111 | val_0_rmse: 0.39766 |  0:00:11s\n",
      "epoch 46 | loss: 0.16074 | val_0_rmse: 0.41288 |  0:00:11s\n",
      "epoch 47 | loss: 0.11191 | val_0_rmse: 0.3226  |  0:00:11s\n",
      "epoch 48 | loss: 0.14168 | val_0_rmse: 0.36388 |  0:00:11s\n",
      "epoch 49 | loss: 0.12167 | val_0_rmse: 0.29938 |  0:00:11s\n",
      "epoch 50 | loss: 0.13097 | val_0_rmse: 0.41221 |  0:00:12s\n",
      "epoch 51 | loss: 0.12619 | val_0_rmse: 0.30826 |  0:00:12s\n",
      "epoch 52 | loss: 0.12613 | val_0_rmse: 0.37097 |  0:00:12s\n",
      "epoch 53 | loss: 0.10378 | val_0_rmse: 0.3073  |  0:00:12s\n",
      "epoch 54 | loss: 0.10905 | val_0_rmse: 0.38586 |  0:00:13s\n",
      "epoch 55 | loss: 0.13527 | val_0_rmse: 0.32512 |  0:00:13s\n",
      "epoch 56 | loss: 0.142   | val_0_rmse: 0.32251 |  0:00:13s\n",
      "epoch 57 | loss: 0.09975 | val_0_rmse: 0.28918 |  0:00:13s\n",
      "epoch 58 | loss: 0.08896 | val_0_rmse: 0.30547 |  0:00:14s\n",
      "epoch 59 | loss: 0.0816  | val_0_rmse: 0.34779 |  0:00:14s\n",
      "epoch 60 | loss: 0.09966 | val_0_rmse: 0.29923 |  0:00:14s\n",
      "epoch 61 | loss: 0.09957 | val_0_rmse: 0.33458 |  0:00:14s\n",
      "epoch 62 | loss: 0.10631 | val_0_rmse: 0.29603 |  0:00:14s\n",
      "epoch 63 | loss: 0.11164 | val_0_rmse: 0.31877 |  0:00:15s\n",
      "epoch 64 | loss: 0.0992  | val_0_rmse: 0.26801 |  0:00:15s\n",
      "epoch 65 | loss: 0.0904  | val_0_rmse: 0.2547  |  0:00:15s\n",
      "epoch 66 | loss: 0.08121 | val_0_rmse: 0.28477 |  0:00:15s\n",
      "epoch 67 | loss: 0.08898 | val_0_rmse: 0.26281 |  0:00:16s\n",
      "epoch 68 | loss: 0.07485 | val_0_rmse: 0.23902 |  0:00:16s\n",
      "epoch 69 | loss: 0.07417 | val_0_rmse: 0.24396 |  0:00:16s\n",
      "epoch 70 | loss: 0.07144 | val_0_rmse: 0.22611 |  0:00:16s\n",
      "epoch 71 | loss: 0.06113 | val_0_rmse: 0.25023 |  0:00:17s\n",
      "epoch 72 | loss: 0.06619 | val_0_rmse: 0.31383 |  0:00:17s\n",
      "epoch 73 | loss: 0.11103 | val_0_rmse: 0.24531 |  0:00:17s\n",
      "epoch 74 | loss: 0.07577 | val_0_rmse: 0.26926 |  0:00:17s\n",
      "epoch 75 | loss: 0.07233 | val_0_rmse: 0.24357 |  0:00:18s\n",
      "epoch 76 | loss: 0.06965 | val_0_rmse: 0.27296 |  0:00:18s\n",
      "epoch 77 | loss: 0.09358 | val_0_rmse: 0.22843 |  0:00:18s\n",
      "epoch 78 | loss: 0.05737 | val_0_rmse: 0.24378 |  0:00:18s\n",
      "epoch 79 | loss: 0.05363 | val_0_rmse: 0.22513 |  0:00:19s\n",
      "epoch 80 | loss: 0.0503  | val_0_rmse: 0.22794 |  0:00:19s\n",
      "epoch 81 | loss: 0.05369 | val_0_rmse: 0.22985 |  0:00:19s\n",
      "epoch 82 | loss: 0.06275 | val_0_rmse: 0.22439 |  0:00:19s\n",
      "epoch 83 | loss: 0.05071 | val_0_rmse: 0.24195 |  0:00:19s\n",
      "epoch 84 | loss: 0.05743 | val_0_rmse: 0.22406 |  0:00:20s\n",
      "epoch 85 | loss: 0.04777 | val_0_rmse: 0.22289 |  0:00:20s\n",
      "epoch 86 | loss: 0.05571 | val_0_rmse: 0.21825 |  0:00:20s\n",
      "epoch 87 | loss: 0.04588 | val_0_rmse: 0.23443 |  0:00:20s\n",
      "epoch 88 | loss: 0.04436 | val_0_rmse: 0.2314  |  0:00:21s\n",
      "epoch 89 | loss: 0.04445 | val_0_rmse: 0.23407 |  0:00:21s\n",
      "epoch 90 | loss: 0.04522 | val_0_rmse: 0.21302 |  0:00:21s\n",
      "epoch 91 | loss: 0.05564 | val_0_rmse: 0.23304 |  0:00:21s\n",
      "epoch 92 | loss: 0.04654 | val_0_rmse: 0.22725 |  0:00:22s\n",
      "epoch 93 | loss: 0.04954 | val_0_rmse: 0.22688 |  0:00:22s\n",
      "epoch 94 | loss: 0.04234 | val_0_rmse: 0.22413 |  0:00:22s\n",
      "epoch 95 | loss: 0.03592 | val_0_rmse: 0.20628 |  0:00:22s\n",
      "epoch 96 | loss: 0.04155 | val_0_rmse: 0.25136 |  0:00:23s\n",
      "epoch 97 | loss: 0.04765 | val_0_rmse: 0.22154 |  0:00:23s\n",
      "epoch 98 | loss: 0.04166 | val_0_rmse: 0.20725 |  0:00:23s\n",
      "epoch 99 | loss: 0.03869 | val_0_rmse: 0.22557 |  0:00:23s\n",
      "epoch 100| loss: 0.04156 | val_0_rmse: 0.21068 |  0:00:24s\n",
      "epoch 101| loss: 0.03877 | val_0_rmse: 0.20072 |  0:00:24s\n",
      "epoch 102| loss: 0.04802 | val_0_rmse: 0.20918 |  0:00:24s\n",
      "epoch 103| loss: 0.03904 | val_0_rmse: 0.1976  |  0:00:24s\n",
      "epoch 104| loss: 0.03501 | val_0_rmse: 0.21507 |  0:00:25s\n",
      "epoch 105| loss: 0.04194 | val_0_rmse: 0.19951 |  0:00:25s\n",
      "epoch 106| loss: 0.03489 | val_0_rmse: 0.20344 |  0:00:25s\n",
      "epoch 107| loss: 0.03566 | val_0_rmse: 0.20185 |  0:00:25s\n",
      "epoch 108| loss: 0.0361  | val_0_rmse: 0.19929 |  0:00:25s\n",
      "epoch 109| loss: 0.03412 | val_0_rmse: 0.19263 |  0:00:26s\n",
      "epoch 110| loss: 0.03467 | val_0_rmse: 0.19126 |  0:00:26s\n",
      "epoch 111| loss: 0.03398 | val_0_rmse: 0.20742 |  0:00:26s\n",
      "epoch 112| loss: 0.03595 | val_0_rmse: 0.19887 |  0:00:26s\n",
      "epoch 113| loss: 0.04379 | val_0_rmse: 0.19792 |  0:00:27s\n",
      "epoch 114| loss: 0.03521 | val_0_rmse: 0.20271 |  0:00:27s\n",
      "epoch 115| loss: 0.04069 | val_0_rmse: 0.25543 |  0:00:27s\n",
      "epoch 116| loss: 0.06009 | val_0_rmse: 0.21075 |  0:00:27s\n",
      "epoch 117| loss: 0.04626 | val_0_rmse: 0.25319 |  0:00:28s\n",
      "epoch 118| loss: 0.04826 | val_0_rmse: 0.2107  |  0:00:28s\n",
      "epoch 119| loss: 0.05946 | val_0_rmse: 0.23243 |  0:00:28s\n",
      "epoch 120| loss: 0.0518  | val_0_rmse: 0.2239  |  0:00:28s\n",
      "epoch 121| loss: 0.05159 | val_0_rmse: 0.22291 |  0:00:29s\n",
      "epoch 122| loss: 0.04134 | val_0_rmse: 0.22624 |  0:00:29s\n",
      "epoch 123| loss: 0.0522  | val_0_rmse: 0.21793 |  0:00:29s\n",
      "epoch 124| loss: 0.04816 | val_0_rmse: 0.21348 |  0:00:29s\n",
      "epoch 125| loss: 0.04589 | val_0_rmse: 0.22776 |  0:00:30s\n",
      "epoch 126| loss: 0.04312 | val_0_rmse: 0.2117  |  0:00:30s\n",
      "epoch 127| loss: 0.0409  | val_0_rmse: 0.22693 |  0:00:30s\n",
      "epoch 128| loss: 0.04631 | val_0_rmse: 0.21516 |  0:00:30s\n",
      "epoch 129| loss: 0.04916 | val_0_rmse: 0.22841 |  0:00:31s\n",
      "epoch 130| loss: 0.05038 | val_0_rmse: 0.20092 |  0:00:31s\n",
      "epoch 131| loss: 0.0437  | val_0_rmse: 0.24056 |  0:00:31s\n",
      "epoch 132| loss: 0.05195 | val_0_rmse: 0.18594 |  0:00:31s\n",
      "epoch 133| loss: 0.0334  | val_0_rmse: 0.20126 |  0:00:31s\n",
      "epoch 134| loss: 0.034   | val_0_rmse: 0.18291 |  0:00:32s\n",
      "epoch 135| loss: 0.03    | val_0_rmse: 0.19526 |  0:00:32s\n",
      "epoch 136| loss: 0.03489 | val_0_rmse: 0.17735 |  0:00:32s\n",
      "epoch 137| loss: 0.03281 | val_0_rmse: 0.17752 |  0:00:32s\n",
      "epoch 138| loss: 0.02858 | val_0_rmse: 0.186   |  0:00:32s\n",
      "epoch 139| loss: 0.03122 | val_0_rmse: 0.17315 |  0:00:33s\n",
      "epoch 140| loss: 0.02775 | val_0_rmse: 0.1777  |  0:00:33s\n",
      "epoch 141| loss: 0.03455 | val_0_rmse: 0.18014 |  0:00:33s\n",
      "epoch 142| loss: 0.03846 | val_0_rmse: 0.17251 |  0:00:33s\n",
      "epoch 143| loss: 0.03147 | val_0_rmse: 0.17737 |  0:00:34s\n",
      "epoch 144| loss: 0.02919 | val_0_rmse: 0.18539 |  0:00:34s\n",
      "epoch 145| loss: 0.02537 | val_0_rmse: 0.16698 |  0:00:34s\n",
      "epoch 146| loss: 0.0241  | val_0_rmse: 0.16825 |  0:00:34s\n",
      "epoch 147| loss: 0.02552 | val_0_rmse: 0.19099 |  0:00:35s\n",
      "epoch 148| loss: 0.02738 | val_0_rmse: 0.17075 |  0:00:35s\n",
      "epoch 149| loss: 0.02449 | val_0_rmse: 0.18804 |  0:00:35s\n",
      "epoch 150| loss: 0.0288  | val_0_rmse: 0.17177 |  0:00:35s\n",
      "epoch 151| loss: 0.02853 | val_0_rmse: 0.17384 |  0:00:36s\n",
      "epoch 152| loss: 0.02883 | val_0_rmse: 0.17793 |  0:00:36s\n",
      "epoch 153| loss: 0.02689 | val_0_rmse: 0.17907 |  0:00:36s\n",
      "epoch 154| loss: 0.02753 | val_0_rmse: 0.17293 |  0:00:36s\n",
      "epoch 155| loss: 0.0273  | val_0_rmse: 0.17627 |  0:00:36s\n",
      "epoch 156| loss: 0.02691 | val_0_rmse: 0.19885 |  0:00:37s\n",
      "epoch 157| loss: 0.0315  | val_0_rmse: 0.18374 |  0:00:37s\n",
      "epoch 158| loss: 0.03415 | val_0_rmse: 0.20615 |  0:00:37s\n",
      "epoch 159| loss: 0.03708 | val_0_rmse: 0.22638 |  0:00:37s\n",
      "epoch 160| loss: 0.04096 | val_0_rmse: 0.24609 |  0:00:38s\n",
      "epoch 161| loss: 0.05123 | val_0_rmse: 0.1815  |  0:00:38s\n",
      "epoch 162| loss: 0.03009 | val_0_rmse: 0.18926 |  0:00:38s\n",
      "epoch 163| loss: 0.02962 | val_0_rmse: 0.16873 |  0:00:38s\n",
      "epoch 164| loss: 0.02727 | val_0_rmse: 0.19425 |  0:00:39s\n",
      "epoch 165| loss: 0.03685 | val_0_rmse: 0.1827  |  0:00:39s\n",
      "epoch 166| loss: 0.02637 | val_0_rmse: 0.16541 |  0:00:39s\n",
      "epoch 167| loss: 0.02392 | val_0_rmse: 0.18323 |  0:00:39s\n",
      "epoch 168| loss: 0.03612 | val_0_rmse: 0.23568 |  0:00:40s\n",
      "epoch 169| loss: 0.07165 | val_0_rmse: 0.19132 |  0:00:40s\n",
      "epoch 170| loss: 0.04162 | val_0_rmse: 0.19379 |  0:00:40s\n",
      "epoch 171| loss: 0.03523 | val_0_rmse: 0.17885 |  0:00:40s\n",
      "epoch 172| loss: 0.03976 | val_0_rmse: 0.16845 |  0:00:40s\n",
      "epoch 173| loss: 0.02373 | val_0_rmse: 0.18194 |  0:00:41s\n",
      "epoch 174| loss: 0.02643 | val_0_rmse: 0.17327 |  0:00:41s\n",
      "epoch 175| loss: 0.02571 | val_0_rmse: 0.17235 |  0:00:41s\n",
      "epoch 176| loss: 0.02556 | val_0_rmse: 0.18487 |  0:00:41s\n",
      "epoch 177| loss: 0.03756 | val_0_rmse: 0.19819 |  0:00:42s\n",
      "epoch 178| loss: 0.02943 | val_0_rmse: 0.17279 |  0:00:42s\n",
      "epoch 179| loss: 0.02437 | val_0_rmse: 0.17435 |  0:00:42s\n",
      "epoch 180| loss: 0.02389 | val_0_rmse: 0.17212 |  0:00:42s\n",
      "epoch 181| loss: 0.02735 | val_0_rmse: 0.17395 |  0:00:43s\n",
      "epoch 182| loss: 0.02426 | val_0_rmse: 0.18218 |  0:00:43s\n",
      "epoch 183| loss: 0.02243 | val_0_rmse: 0.16248 |  0:00:43s\n",
      "epoch 184| loss: 0.0238  | val_0_rmse: 0.16987 |  0:00:43s\n",
      "epoch 185| loss: 0.02055 | val_0_rmse: 0.16004 |  0:00:44s\n",
      "epoch 186| loss: 0.01838 | val_0_rmse: 0.16518 |  0:00:44s\n",
      "epoch 187| loss: 0.0203  | val_0_rmse: 0.16789 |  0:00:44s\n",
      "epoch 188| loss: 0.02267 | val_0_rmse: 0.17588 |  0:00:44s\n",
      "epoch 189| loss: 0.02675 | val_0_rmse: 0.16433 |  0:00:44s\n",
      "epoch 190| loss: 0.01946 | val_0_rmse: 0.17515 |  0:00:45s\n",
      "epoch 191| loss: 0.02493 | val_0_rmse: 0.19964 |  0:00:45s\n",
      "epoch 192| loss: 0.03241 | val_0_rmse: 0.21683 |  0:00:45s\n",
      "epoch 193| loss: 0.03973 | val_0_rmse: 0.21004 |  0:00:45s\n",
      "epoch 194| loss: 0.05816 | val_0_rmse: 0.17397 |  0:00:46s\n",
      "epoch 195| loss: 0.03191 | val_0_rmse: 0.18369 |  0:00:46s\n",
      "epoch 196| loss: 0.02912 | val_0_rmse: 0.18904 |  0:00:46s\n",
      "epoch 197| loss: 0.02371 | val_0_rmse: 0.16034 |  0:00:46s\n",
      "epoch 198| loss: 0.02067 | val_0_rmse: 0.17245 |  0:00:47s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:42:09,699] Trial 75 finished with value: 0.1600434375972372 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.867196997241307, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.01777098127657113, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 199| loss: 0.0256  | val_0_rmse: 0.18082 |  0:00:47s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 185 and best_val_0_rmse = 0.16004\n",
      "Trial 075 | rmse_log=0.16004 | RMSE$=32,633 | MAE$=20,027 | MAPE=11.77% | n_d/n_a=24/16 steps=3 lr=0.01777 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 133.06943| val_0_rmse: 11.20688|  0:00:00s\n",
      "epoch 1  | loss: 107.103 | val_0_rmse: 10.61428|  0:00:00s\n",
      "epoch 2  | loss: 85.64265| val_0_rmse: 9.84582 |  0:00:00s\n",
      "epoch 3  | loss: 65.37457| val_0_rmse: 8.90142 |  0:00:00s\n",
      "epoch 4  | loss: 48.8436 | val_0_rmse: 7.82822 |  0:00:01s\n",
      "epoch 5  | loss: 36.78863| val_0_rmse: 6.75789 |  0:00:01s\n",
      "epoch 6  | loss: 24.7016 | val_0_rmse: 5.78413 |  0:00:01s\n",
      "epoch 7  | loss: 19.11762| val_0_rmse: 4.65713 |  0:00:01s\n",
      "epoch 8  | loss: 15.67284| val_0_rmse: 3.88161 |  0:00:01s\n",
      "epoch 9  | loss: 15.48426| val_0_rmse: 3.3044  |  0:00:02s\n",
      "epoch 10 | loss: 13.92822| val_0_rmse: 3.00114 |  0:00:02s\n",
      "epoch 11 | loss: 10.63115| val_0_rmse: 2.94511 |  0:00:02s\n",
      "epoch 12 | loss: 8.33689 | val_0_rmse: 3.09755 |  0:00:02s\n",
      "epoch 13 | loss: 5.51254 | val_0_rmse: 3.13588 |  0:00:02s\n",
      "epoch 14 | loss: 4.79199 | val_0_rmse: 2.91996 |  0:00:03s\n",
      "epoch 15 | loss: 3.45361 | val_0_rmse: 2.30207 |  0:00:03s\n",
      "epoch 16 | loss: 2.40153 | val_0_rmse: 1.70331 |  0:00:03s\n",
      "epoch 17 | loss: 2.07657 | val_0_rmse: 1.58683 |  0:00:03s\n",
      "epoch 18 | loss: 1.36142 | val_0_rmse: 1.56009 |  0:00:04s\n",
      "epoch 19 | loss: 1.22174 | val_0_rmse: 1.24388 |  0:00:04s\n",
      "epoch 20 | loss: 0.94367 | val_0_rmse: 0.98398 |  0:00:04s\n",
      "epoch 21 | loss: 0.84406 | val_0_rmse: 1.17207 |  0:00:04s\n",
      "epoch 22 | loss: 0.69989 | val_0_rmse: 1.2125  |  0:00:04s\n",
      "epoch 23 | loss: 0.83561 | val_0_rmse: 0.87744 |  0:00:05s\n",
      "epoch 24 | loss: 0.69483 | val_0_rmse: 0.81209 |  0:00:05s\n",
      "epoch 25 | loss: 0.56733 | val_0_rmse: 0.76903 |  0:00:05s\n",
      "epoch 26 | loss: 0.43078 | val_0_rmse: 0.64003 |  0:00:05s\n",
      "epoch 27 | loss: 0.4527  | val_0_rmse: 0.58513 |  0:00:05s\n",
      "epoch 28 | loss: 0.423   | val_0_rmse: 0.49747 |  0:00:06s\n",
      "epoch 29 | loss: 0.42183 | val_0_rmse: 0.56679 |  0:00:06s\n",
      "epoch 30 | loss: 0.35226 | val_0_rmse: 0.53308 |  0:00:06s\n",
      "epoch 31 | loss: 0.29411 | val_0_rmse: 0.50399 |  0:00:06s\n",
      "epoch 32 | loss: 0.26317 | val_0_rmse: 0.50875 |  0:00:06s\n",
      "epoch 33 | loss: 0.26017 | val_0_rmse: 0.50194 |  0:00:07s\n",
      "epoch 34 | loss: 0.25145 | val_0_rmse: 0.45864 |  0:00:07s\n",
      "epoch 35 | loss: 0.22383 | val_0_rmse: 0.51881 |  0:00:07s\n",
      "epoch 36 | loss: 0.23088 | val_0_rmse: 0.44686 |  0:00:07s\n",
      "epoch 37 | loss: 0.41749 | val_0_rmse: 0.77367 |  0:00:07s\n",
      "epoch 38 | loss: 0.43902 | val_0_rmse: 0.49444 |  0:00:08s\n",
      "epoch 39 | loss: 0.20538 | val_0_rmse: 0.46484 |  0:00:08s\n",
      "epoch 40 | loss: 0.29941 | val_0_rmse: 0.53735 |  0:00:08s\n",
      "epoch 41 | loss: 0.2981  | val_0_rmse: 0.40376 |  0:00:08s\n",
      "epoch 42 | loss: 0.2053  | val_0_rmse: 0.54669 |  0:00:08s\n",
      "epoch 43 | loss: 0.21596 | val_0_rmse: 0.45617 |  0:00:09s\n",
      "epoch 44 | loss: 0.19166 | val_0_rmse: 0.51083 |  0:00:09s\n",
      "epoch 45 | loss: 0.23436 | val_0_rmse: 0.43435 |  0:00:09s\n",
      "epoch 46 | loss: 0.16398 | val_0_rmse: 0.37779 |  0:00:09s\n",
      "epoch 47 | loss: 0.1708  | val_0_rmse: 0.3536  |  0:00:09s\n",
      "epoch 48 | loss: 0.19107 | val_0_rmse: 0.36661 |  0:00:10s\n",
      "epoch 49 | loss: 0.19254 | val_0_rmse: 0.51963 |  0:00:10s\n",
      "epoch 50 | loss: 0.1999  | val_0_rmse: 0.34361 |  0:00:10s\n",
      "epoch 51 | loss: 0.28545 | val_0_rmse: 0.40921 |  0:00:10s\n",
      "epoch 52 | loss: 0.21571 | val_0_rmse: 0.59622 |  0:00:10s\n",
      "epoch 53 | loss: 0.2714  | val_0_rmse: 0.39906 |  0:00:10s\n",
      "epoch 54 | loss: 0.17919 | val_0_rmse: 0.40378 |  0:00:11s\n",
      "epoch 55 | loss: 0.15044 | val_0_rmse: 0.40982 |  0:00:11s\n",
      "epoch 56 | loss: 0.1705  | val_0_rmse: 0.36601 |  0:00:11s\n",
      "epoch 57 | loss: 0.17487 | val_0_rmse: 0.46254 |  0:00:11s\n",
      "epoch 58 | loss: 0.17041 | val_0_rmse: 0.3729  |  0:00:12s\n",
      "epoch 59 | loss: 0.1418  | val_0_rmse: 0.41772 |  0:00:12s\n",
      "epoch 60 | loss: 0.15142 | val_0_rmse: 0.37853 |  0:00:12s\n",
      "epoch 61 | loss: 0.13747 | val_0_rmse: 0.41339 |  0:00:12s\n",
      "epoch 62 | loss: 0.13908 | val_0_rmse: 0.38713 |  0:00:12s\n",
      "epoch 63 | loss: 0.13871 | val_0_rmse: 0.38243 |  0:00:13s\n",
      "epoch 64 | loss: 0.12394 | val_0_rmse: 0.37749 |  0:00:13s\n",
      "epoch 65 | loss: 0.11881 | val_0_rmse: 0.3944  |  0:00:13s\n",
      "epoch 66 | loss: 0.11913 | val_0_rmse: 0.35692 |  0:00:13s\n",
      "epoch 67 | loss: 0.11763 | val_0_rmse: 0.43576 |  0:00:13s\n",
      "epoch 68 | loss: 0.16793 | val_0_rmse: 0.3515  |  0:00:14s\n",
      "epoch 69 | loss: 0.12822 | val_0_rmse: 0.35529 |  0:00:14s\n",
      "epoch 70 | loss: 0.11453 | val_0_rmse: 0.36535 |  0:00:14s\n",
      "epoch 71 | loss: 0.12323 | val_0_rmse: 0.31721 |  0:00:14s\n",
      "epoch 72 | loss: 0.14107 | val_0_rmse: 0.37158 |  0:00:14s\n",
      "epoch 73 | loss: 0.12543 | val_0_rmse: 0.34728 |  0:00:14s\n",
      "epoch 74 | loss: 0.10553 | val_0_rmse: 0.33952 |  0:00:15s\n",
      "epoch 75 | loss: 0.09873 | val_0_rmse: 0.36734 |  0:00:15s\n",
      "epoch 76 | loss: 0.10429 | val_0_rmse: 0.32279 |  0:00:15s\n",
      "epoch 77 | loss: 0.10377 | val_0_rmse: 0.32838 |  0:00:15s\n",
      "epoch 78 | loss: 0.09377 | val_0_rmse: 0.35363 |  0:00:15s\n",
      "epoch 79 | loss: 0.10678 | val_0_rmse: 0.37399 |  0:00:16s\n",
      "epoch 80 | loss: 0.15203 | val_0_rmse: 0.35509 |  0:00:16s\n",
      "epoch 81 | loss: 0.1327  | val_0_rmse: 0.35948 |  0:00:16s\n",
      "epoch 82 | loss: 0.17107 | val_0_rmse: 0.34224 |  0:00:16s\n",
      "epoch 83 | loss: 0.10817 | val_0_rmse: 0.33932 |  0:00:17s\n",
      "epoch 84 | loss: 0.09033 | val_0_rmse: 0.34007 |  0:00:17s\n",
      "epoch 85 | loss: 0.13461 | val_0_rmse: 0.35466 |  0:00:17s\n",
      "epoch 86 | loss: 0.12493 | val_0_rmse: 0.35929 |  0:00:17s\n",
      "epoch 87 | loss: 0.11353 | val_0_rmse: 0.30761 |  0:00:17s\n",
      "epoch 88 | loss: 0.11338 | val_0_rmse: 0.3191  |  0:00:18s\n",
      "epoch 89 | loss: 0.09263 | val_0_rmse: 0.30412 |  0:00:18s\n",
      "epoch 90 | loss: 0.09393 | val_0_rmse: 0.30747 |  0:00:18s\n",
      "epoch 91 | loss: 0.08283 | val_0_rmse: 0.32815 |  0:00:18s\n",
      "epoch 92 | loss: 0.0923  | val_0_rmse: 0.29465 |  0:00:18s\n",
      "epoch 93 | loss: 0.08021 | val_0_rmse: 0.28859 |  0:00:18s\n",
      "epoch 94 | loss: 0.07216 | val_0_rmse: 0.31093 |  0:00:19s\n",
      "epoch 95 | loss: 0.07886 | val_0_rmse: 0.29753 |  0:00:19s\n",
      "epoch 96 | loss: 0.0753  | val_0_rmse: 0.28802 |  0:00:19s\n",
      "epoch 97 | loss: 0.0832  | val_0_rmse: 0.30673 |  0:00:19s\n",
      "epoch 98 | loss: 0.0773  | val_0_rmse: 0.28283 |  0:00:20s\n",
      "epoch 99 | loss: 0.08339 | val_0_rmse: 0.29425 |  0:00:20s\n",
      "epoch 100| loss: 0.07281 | val_0_rmse: 0.3039  |  0:00:20s\n",
      "epoch 101| loss: 0.08393 | val_0_rmse: 0.30426 |  0:00:20s\n",
      "epoch 102| loss: 0.07642 | val_0_rmse: 0.29229 |  0:00:20s\n",
      "epoch 103| loss: 0.06966 | val_0_rmse: 0.30072 |  0:00:21s\n",
      "epoch 104| loss: 0.078   | val_0_rmse: 0.28134 |  0:00:21s\n",
      "epoch 105| loss: 0.06535 | val_0_rmse: 0.29086 |  0:00:21s\n",
      "epoch 106| loss: 0.06205 | val_0_rmse: 0.28265 |  0:00:21s\n",
      "epoch 107| loss: 0.06327 | val_0_rmse: 0.2841  |  0:00:21s\n",
      "epoch 108| loss: 0.06674 | val_0_rmse: 0.2556  |  0:00:21s\n",
      "epoch 109| loss: 0.07679 | val_0_rmse: 0.26701 |  0:00:22s\n",
      "epoch 110| loss: 0.06893 | val_0_rmse: 0.28023 |  0:00:22s\n",
      "epoch 111| loss: 0.06735 | val_0_rmse: 0.29773 |  0:00:22s\n",
      "epoch 112| loss: 0.08122 | val_0_rmse: 0.27045 |  0:00:22s\n",
      "epoch 113| loss: 0.09848 | val_0_rmse: 0.26989 |  0:00:22s\n",
      "epoch 114| loss: 0.07306 | val_0_rmse: 0.311   |  0:00:23s\n",
      "epoch 115| loss: 0.08232 | val_0_rmse: 0.27048 |  0:00:23s\n",
      "epoch 116| loss: 0.08364 | val_0_rmse: 0.25709 |  0:00:23s\n",
      "epoch 117| loss: 0.0738  | val_0_rmse: 0.26385 |  0:00:23s\n",
      "epoch 118| loss: 0.08431 | val_0_rmse: 0.27728 |  0:00:23s\n",
      "epoch 119| loss: 0.0709  | val_0_rmse: 0.30032 |  0:00:24s\n",
      "epoch 120| loss: 0.07725 | val_0_rmse: 0.26611 |  0:00:24s\n",
      "epoch 121| loss: 0.06961 | val_0_rmse: 0.27115 |  0:00:24s\n",
      "epoch 122| loss: 0.08564 | val_0_rmse: 0.28492 |  0:00:24s\n",
      "epoch 123| loss: 0.08547 | val_0_rmse: 0.281   |  0:00:25s\n",
      "epoch 124| loss: 0.07359 | val_0_rmse: 0.27322 |  0:00:25s\n",
      "epoch 125| loss: 0.0784  | val_0_rmse: 0.27126 |  0:00:25s\n",
      "epoch 126| loss: 0.07604 | val_0_rmse: 0.28855 |  0:00:25s\n",
      "epoch 127| loss: 0.08052 | val_0_rmse: 0.27806 |  0:00:25s\n",
      "epoch 128| loss: 0.07483 | val_0_rmse: 0.25939 |  0:00:25s\n",
      "epoch 129| loss: 0.08044 | val_0_rmse: 0.26612 |  0:00:26s\n",
      "epoch 130| loss: 0.08049 | val_0_rmse: 0.30952 |  0:00:26s\n",
      "epoch 131| loss: 0.07789 | val_0_rmse: 0.30208 |  0:00:26s\n",
      "epoch 132| loss: 0.07352 | val_0_rmse: 0.29048 |  0:00:26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:42:36,996] Trial 76 finished with value: 0.25559963095215454 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.9093297471743689, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'sparsemax', 'lr': 0.017615443146894286, 'batch_size': 1024, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 133| loss: 0.07584 | val_0_rmse: 0.27129 |  0:00:26s\n",
      "\n",
      "Early stopping occurred at epoch 133 with best_epoch = 108 and best_val_0_rmse = 0.2556\n",
      "Trial 076 | rmse_log=0.25560 | RMSE$=48,761 | MAE$=32,147 | MAPE=19.88% | n_d/n_a=24/16 steps=3 lr=0.01762 batch=1024 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 127.74795| val_0_rmse: 11.27968|  0:00:00s\n",
      "epoch 1  | loss: 99.54815| val_0_rmse: 10.55752|  0:00:00s\n",
      "epoch 2  | loss: 76.45936| val_0_rmse: 9.73497 |  0:00:00s\n",
      "epoch 3  | loss: 54.1188 | val_0_rmse: 8.79166 |  0:00:00s\n",
      "epoch 4  | loss: 39.97579| val_0_rmse: 7.82144 |  0:00:01s\n",
      "epoch 5  | loss: 29.31702| val_0_rmse: 6.89734 |  0:00:01s\n",
      "epoch 6  | loss: 19.27325| val_0_rmse: 5.90182 |  0:00:01s\n",
      "epoch 7  | loss: 15.4178 | val_0_rmse: 4.91809 |  0:00:02s\n",
      "epoch 8  | loss: 13.97903| val_0_rmse: 4.02378 |  0:00:02s\n",
      "epoch 9  | loss: 10.24124| val_0_rmse: 3.47204 |  0:00:02s\n",
      "epoch 10 | loss: 9.00536 | val_0_rmse: 3.14051 |  0:00:02s\n",
      "epoch 11 | loss: 5.3498  | val_0_rmse: 3.14947 |  0:00:02s\n",
      "epoch 12 | loss: 4.12418 | val_0_rmse: 3.17065 |  0:00:03s\n",
      "epoch 13 | loss: 2.71862 | val_0_rmse: 2.4862  |  0:00:03s\n",
      "epoch 14 | loss: 2.32338 | val_0_rmse: 1.94933 |  0:00:03s\n",
      "epoch 15 | loss: 1.71724 | val_0_rmse: 2.05947 |  0:00:03s\n",
      "epoch 16 | loss: 0.956   | val_0_rmse: 1.80946 |  0:00:04s\n",
      "epoch 17 | loss: 1.05053 | val_0_rmse: 1.4738  |  0:00:04s\n",
      "epoch 18 | loss: 0.85548 | val_0_rmse: 1.39237 |  0:00:04s\n",
      "epoch 19 | loss: 0.64412 | val_0_rmse: 1.01952 |  0:00:04s\n",
      "epoch 20 | loss: 0.4924  | val_0_rmse: 1.07354 |  0:00:05s\n",
      "epoch 21 | loss: 0.46187 | val_0_rmse: 0.89707 |  0:00:05s\n",
      "epoch 22 | loss: 0.54966 | val_0_rmse: 0.87534 |  0:00:05s\n",
      "epoch 23 | loss: 0.51591 | val_0_rmse: 0.71682 |  0:00:05s\n",
      "epoch 24 | loss: 0.43235 | val_0_rmse: 0.89242 |  0:00:06s\n",
      "epoch 25 | loss: 0.46606 | val_0_rmse: 0.67065 |  0:00:06s\n",
      "epoch 26 | loss: 0.36047 | val_0_rmse: 0.60183 |  0:00:06s\n",
      "epoch 27 | loss: 0.26555 | val_0_rmse: 0.42479 |  0:00:06s\n",
      "epoch 28 | loss: 0.26482 | val_0_rmse: 0.60216 |  0:00:07s\n",
      "epoch 29 | loss: 0.23512 | val_0_rmse: 0.462   |  0:00:07s\n",
      "epoch 30 | loss: 0.30845 | val_0_rmse: 0.61082 |  0:00:07s\n",
      "epoch 31 | loss: 0.25403 | val_0_rmse: 0.50163 |  0:00:07s\n",
      "epoch 32 | loss: 0.21704 | val_0_rmse: 0.53472 |  0:00:07s\n",
      "epoch 33 | loss: 0.25502 | val_0_rmse: 0.40153 |  0:00:08s\n",
      "epoch 34 | loss: 0.17997 | val_0_rmse: 0.37809 |  0:00:08s\n",
      "epoch 35 | loss: 0.17228 | val_0_rmse: 0.42326 |  0:00:08s\n",
      "epoch 36 | loss: 0.1846  | val_0_rmse: 0.37151 |  0:00:08s\n",
      "epoch 37 | loss: 0.16876 | val_0_rmse: 0.33765 |  0:00:09s\n",
      "epoch 38 | loss: 0.19372 | val_0_rmse: 0.3323  |  0:00:09s\n",
      "epoch 39 | loss: 0.20451 | val_0_rmse: 0.34263 |  0:00:09s\n",
      "epoch 40 | loss: 0.16227 | val_0_rmse: 0.38212 |  0:00:09s\n",
      "epoch 41 | loss: 0.16746 | val_0_rmse: 0.41215 |  0:00:10s\n",
      "epoch 42 | loss: 0.16171 | val_0_rmse: 0.36111 |  0:00:10s\n",
      "epoch 43 | loss: 0.13143 | val_0_rmse: 0.3615  |  0:00:10s\n",
      "epoch 44 | loss: 0.18638 | val_0_rmse: 0.27012 |  0:00:10s\n",
      "epoch 45 | loss: 0.15691 | val_0_rmse: 0.39554 |  0:00:11s\n",
      "epoch 46 | loss: 0.1273  | val_0_rmse: 0.37733 |  0:00:11s\n",
      "epoch 47 | loss: 0.17693 | val_0_rmse: 0.31592 |  0:00:11s\n",
      "epoch 48 | loss: 0.1111  | val_0_rmse: 0.30963 |  0:00:11s\n",
      "epoch 49 | loss: 0.1076  | val_0_rmse: 0.28944 |  0:00:12s\n",
      "epoch 50 | loss: 0.10442 | val_0_rmse: 0.3378  |  0:00:12s\n",
      "epoch 51 | loss: 0.10609 | val_0_rmse: 0.27936 |  0:00:12s\n",
      "epoch 52 | loss: 0.12369 | val_0_rmse: 0.30703 |  0:00:12s\n",
      "epoch 53 | loss: 0.11176 | val_0_rmse: 0.26923 |  0:00:12s\n",
      "epoch 54 | loss: 0.09609 | val_0_rmse: 0.26411 |  0:00:13s\n",
      "epoch 55 | loss: 0.08503 | val_0_rmse: 0.26282 |  0:00:13s\n",
      "epoch 56 | loss: 0.08372 | val_0_rmse: 0.24863 |  0:00:13s\n",
      "epoch 57 | loss: 0.08023 | val_0_rmse: 0.26044 |  0:00:14s\n",
      "epoch 58 | loss: 0.0862  | val_0_rmse: 0.25939 |  0:00:14s\n",
      "epoch 59 | loss: 0.07595 | val_0_rmse: 0.24267 |  0:00:14s\n",
      "epoch 60 | loss: 0.08473 | val_0_rmse: 0.25631 |  0:00:14s\n",
      "epoch 61 | loss: 0.07744 | val_0_rmse: 0.26627 |  0:00:15s\n",
      "epoch 62 | loss: 0.07416 | val_0_rmse: 0.26255 |  0:00:15s\n",
      "epoch 63 | loss: 0.07685 | val_0_rmse: 0.25579 |  0:00:15s\n",
      "epoch 64 | loss: 0.08668 | val_0_rmse: 0.27658 |  0:00:15s\n",
      "epoch 65 | loss: 0.08424 | val_0_rmse: 0.24733 |  0:00:15s\n",
      "epoch 66 | loss: 0.09336 | val_0_rmse: 0.28873 |  0:00:16s\n",
      "epoch 67 | loss: 0.09116 | val_0_rmse: 0.2931  |  0:00:16s\n",
      "epoch 68 | loss: 0.08419 | val_0_rmse: 0.2632  |  0:00:16s\n",
      "epoch 69 | loss: 0.07542 | val_0_rmse: 0.2772  |  0:00:16s\n",
      "epoch 70 | loss: 0.08744 | val_0_rmse: 0.30882 |  0:00:17s\n",
      "epoch 71 | loss: 0.07707 | val_0_rmse: 0.26831 |  0:00:17s\n",
      "epoch 72 | loss: 0.07104 | val_0_rmse: 0.28748 |  0:00:17s\n",
      "epoch 73 | loss: 0.07284 | val_0_rmse: 0.27056 |  0:00:17s\n",
      "epoch 74 | loss: 0.07458 | val_0_rmse: 0.26663 |  0:00:18s\n",
      "epoch 75 | loss: 0.06333 | val_0_rmse: 0.30147 |  0:00:18s\n",
      "epoch 76 | loss: 0.07244 | val_0_rmse: 0.28495 |  0:00:18s\n",
      "epoch 77 | loss: 0.08474 | val_0_rmse: 0.25784 |  0:00:18s\n",
      "epoch 78 | loss: 0.09224 | val_0_rmse: 0.26144 |  0:00:19s\n",
      "epoch 79 | loss: 0.07384 | val_0_rmse: 0.25486 |  0:00:19s\n",
      "epoch 80 | loss: 0.07507 | val_0_rmse: 0.25587 |  0:00:19s\n",
      "epoch 81 | loss: 0.06876 | val_0_rmse: 0.26415 |  0:00:19s\n",
      "epoch 82 | loss: 0.07111 | val_0_rmse: 0.24046 |  0:00:20s\n",
      "epoch 83 | loss: 0.06463 | val_0_rmse: 0.25521 |  0:00:20s\n",
      "epoch 84 | loss: 0.07008 | val_0_rmse: 0.25676 |  0:00:20s\n",
      "epoch 85 | loss: 0.05958 | val_0_rmse: 0.23183 |  0:00:20s\n",
      "epoch 86 | loss: 0.06186 | val_0_rmse: 0.26007 |  0:00:20s\n",
      "epoch 87 | loss: 0.06145 | val_0_rmse: 0.24153 |  0:00:21s\n",
      "epoch 88 | loss: 0.06406 | val_0_rmse: 0.24928 |  0:00:21s\n",
      "epoch 89 | loss: 0.06053 | val_0_rmse: 0.24929 |  0:00:21s\n",
      "epoch 90 | loss: 0.05582 | val_0_rmse: 0.23645 |  0:00:21s\n",
      "epoch 91 | loss: 0.05454 | val_0_rmse: 0.2389  |  0:00:22s\n",
      "epoch 92 | loss: 0.05701 | val_0_rmse: 0.26903 |  0:00:22s\n",
      "epoch 93 | loss: 0.06084 | val_0_rmse: 0.2556  |  0:00:22s\n",
      "epoch 94 | loss: 0.06801 | val_0_rmse: 0.25337 |  0:00:22s\n",
      "epoch 95 | loss: 0.06086 | val_0_rmse: 0.25602 |  0:00:23s\n",
      "epoch 96 | loss: 0.05525 | val_0_rmse: 0.26506 |  0:00:23s\n",
      "epoch 97 | loss: 0.07779 | val_0_rmse: 0.22117 |  0:00:23s\n",
      "epoch 98 | loss: 0.05533 | val_0_rmse: 0.26677 |  0:00:23s\n",
      "epoch 99 | loss: 0.06137 | val_0_rmse: 0.22103 |  0:00:23s\n",
      "epoch 100| loss: 0.05066 | val_0_rmse: 0.22507 |  0:00:24s\n",
      "epoch 101| loss: 0.05098 | val_0_rmse: 0.23883 |  0:00:24s\n",
      "epoch 102| loss: 0.04746 | val_0_rmse: 0.2163  |  0:00:24s\n",
      "epoch 103| loss: 0.04819 | val_0_rmse: 0.21426 |  0:00:24s\n",
      "epoch 104| loss: 0.04634 | val_0_rmse: 0.23266 |  0:00:25s\n",
      "epoch 105| loss: 0.0446  | val_0_rmse: 0.23016 |  0:00:25s\n",
      "epoch 106| loss: 0.04852 | val_0_rmse: 0.23076 |  0:00:25s\n",
      "epoch 107| loss: 0.04121 | val_0_rmse: 0.22991 |  0:00:25s\n",
      "epoch 108| loss: 0.0414  | val_0_rmse: 0.22884 |  0:00:26s\n",
      "epoch 109| loss: 0.04512 | val_0_rmse: 0.23526 |  0:00:26s\n",
      "epoch 110| loss: 0.04915 | val_0_rmse: 0.22741 |  0:00:26s\n",
      "epoch 111| loss: 0.04906 | val_0_rmse: 0.22021 |  0:00:26s\n",
      "epoch 112| loss: 0.04118 | val_0_rmse: 0.2289  |  0:00:27s\n",
      "epoch 113| loss: 0.04663 | val_0_rmse: 0.22008 |  0:00:27s\n",
      "epoch 114| loss: 0.04776 | val_0_rmse: 0.23392 |  0:00:27s\n",
      "epoch 115| loss: 0.05141 | val_0_rmse: 0.25045 |  0:00:27s\n",
      "epoch 116| loss: 0.05981 | val_0_rmse: 0.24018 |  0:00:28s\n",
      "epoch 117| loss: 0.06027 | val_0_rmse: 0.22412 |  0:00:28s\n",
      "epoch 118| loss: 0.05145 | val_0_rmse: 0.25663 |  0:00:28s\n",
      "epoch 119| loss: 0.06054 | val_0_rmse: 0.22208 |  0:00:28s\n",
      "epoch 120| loss: 0.04979 | val_0_rmse: 0.24099 |  0:00:28s\n",
      "epoch 121| loss: 0.04773 | val_0_rmse: 0.22509 |  0:00:29s\n",
      "epoch 122| loss: 0.05149 | val_0_rmse: 0.23485 |  0:00:29s\n",
      "epoch 123| loss: 0.04644 | val_0_rmse: 0.21641 |  0:00:29s\n",
      "epoch 124| loss: 0.04318 | val_0_rmse: 0.21877 |  0:00:29s\n",
      "epoch 125| loss: 0.04137 | val_0_rmse: 0.21574 |  0:00:30s\n",
      "epoch 126| loss: 0.04548 | val_0_rmse: 0.23005 |  0:00:30s\n",
      "epoch 127| loss: 0.0452  | val_0_rmse: 0.22325 |  0:00:30s\n",
      "epoch 128| loss: 0.04274 | val_0_rmse: 0.20949 |  0:00:30s\n",
      "epoch 129| loss: 0.0431  | val_0_rmse: 0.21845 |  0:00:31s\n",
      "epoch 130| loss: 0.04354 | val_0_rmse: 0.23329 |  0:00:31s\n",
      "epoch 131| loss: 0.04533 | val_0_rmse: 0.23637 |  0:00:31s\n",
      "epoch 132| loss: 0.04965 | val_0_rmse: 0.24069 |  0:00:31s\n",
      "epoch 133| loss: 0.04646 | val_0_rmse: 0.24381 |  0:00:32s\n",
      "epoch 134| loss: 0.04386 | val_0_rmse: 0.24007 |  0:00:32s\n",
      "epoch 135| loss: 0.0375  | val_0_rmse: 0.22634 |  0:00:32s\n",
      "epoch 136| loss: 0.03921 | val_0_rmse: 0.22652 |  0:00:32s\n",
      "epoch 137| loss: 0.03587 | val_0_rmse: 0.22794 |  0:00:33s\n",
      "epoch 138| loss: 0.04163 | val_0_rmse: 0.22579 |  0:00:33s\n",
      "epoch 139| loss: 0.03501 | val_0_rmse: 0.23809 |  0:00:33s\n",
      "epoch 140| loss: 0.04263 | val_0_rmse: 0.21919 |  0:00:33s\n",
      "epoch 141| loss: 0.03646 | val_0_rmse: 0.22053 |  0:00:33s\n",
      "epoch 142| loss: 0.04077 | val_0_rmse: 0.21346 |  0:00:34s\n",
      "epoch 143| loss: 0.03499 | val_0_rmse: 0.21086 |  0:00:34s\n",
      "epoch 144| loss: 0.03577 | val_0_rmse: 0.21177 |  0:00:34s\n",
      "epoch 145| loss: 0.03608 | val_0_rmse: 0.20538 |  0:00:34s\n",
      "epoch 146| loss: 0.03339 | val_0_rmse: 0.20533 |  0:00:35s\n",
      "epoch 147| loss: 0.03812 | val_0_rmse: 0.2078  |  0:00:35s\n",
      "epoch 148| loss: 0.0324  | val_0_rmse: 0.2096  |  0:00:35s\n",
      "epoch 149| loss: 0.03626 | val_0_rmse: 0.21623 |  0:00:35s\n",
      "epoch 150| loss: 0.04275 | val_0_rmse: 0.22618 |  0:00:36s\n",
      "epoch 151| loss: 0.04081 | val_0_rmse: 0.21283 |  0:00:36s\n",
      "epoch 152| loss: 0.03473 | val_0_rmse: 0.22716 |  0:00:36s\n",
      "epoch 153| loss: 0.05033 | val_0_rmse: 0.21029 |  0:00:36s\n",
      "epoch 154| loss: 0.03341 | val_0_rmse: 0.23038 |  0:00:36s\n",
      "epoch 155| loss: 0.04089 | val_0_rmse: 0.21678 |  0:00:37s\n",
      "epoch 156| loss: 0.04162 | val_0_rmse: 0.22181 |  0:00:37s\n",
      "epoch 157| loss: 0.0364  | val_0_rmse: 0.21379 |  0:00:37s\n",
      "epoch 158| loss: 0.04059 | val_0_rmse: 0.20785 |  0:00:37s\n",
      "epoch 159| loss: 0.03689 | val_0_rmse: 0.21325 |  0:00:38s\n",
      "epoch 160| loss: 0.03424 | val_0_rmse: 0.2192  |  0:00:38s\n",
      "epoch 161| loss: 0.03513 | val_0_rmse: 0.21952 |  0:00:38s\n",
      "epoch 162| loss: 0.03958 | val_0_rmse: 0.22644 |  0:00:38s\n",
      "epoch 163| loss: 0.03495 | val_0_rmse: 0.21774 |  0:00:38s\n",
      "epoch 164| loss: 0.03089 | val_0_rmse: 0.22223 |  0:00:39s\n",
      "epoch 165| loss: 0.03391 | val_0_rmse: 0.22864 |  0:00:39s\n",
      "epoch 166| loss: 0.03377 | val_0_rmse: 0.22205 |  0:00:39s\n",
      "epoch 167| loss: 0.03437 | val_0_rmse: 0.22205 |  0:00:39s\n",
      "epoch 168| loss: 0.03257 | val_0_rmse: 0.22519 |  0:00:40s\n",
      "epoch 169| loss: 0.03439 | val_0_rmse: 0.21394 |  0:00:40s\n",
      "epoch 170| loss: 0.03538 | val_0_rmse: 0.22264 |  0:00:40s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:43:18,236] Trial 77 finished with value: 0.205328167716117 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.8429738385887766, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.01219996630475322, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 171| loss: 0.03271 | val_0_rmse: 0.21347 |  0:00:40s\n",
      "\n",
      "Early stopping occurred at epoch 171 with best_epoch = 146 and best_val_0_rmse = 0.20533\n",
      "Trial 077 | rmse_log=0.20533 | RMSE$=39,595 | MAE$=24,807 | MAPE=15.35% | n_d/n_a=24/16 steps=3 lr=0.01220 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 151.79562| val_0_rmse: 11.09822|  0:00:00s\n",
      "epoch 1  | loss: 102.15006| val_0_rmse: 10.06899|  0:00:00s\n",
      "epoch 2  | loss: 66.55805| val_0_rmse: 8.67716 |  0:00:00s\n",
      "epoch 3  | loss: 38.76901| val_0_rmse: 6.94264 |  0:00:01s\n",
      "epoch 4  | loss: 21.78814| val_0_rmse: 5.04315 |  0:00:01s\n",
      "epoch 5  | loss: 15.04171| val_0_rmse: 3.53815 |  0:00:01s\n",
      "epoch 6  | loss: 18.02612| val_0_rmse: 3.34496 |  0:00:02s\n",
      "epoch 7  | loss: 9.25807 | val_0_rmse: 3.9052  |  0:00:02s\n",
      "epoch 8  | loss: 4.64781 | val_0_rmse: 3.90746 |  0:00:02s\n",
      "epoch 9  | loss: 3.78149 | val_0_rmse: 2.95225 |  0:00:03s\n",
      "epoch 10 | loss: 2.45373 | val_0_rmse: 1.98302 |  0:00:03s\n",
      "epoch 11 | loss: 1.88228 | val_0_rmse: 2.37718 |  0:00:03s\n",
      "epoch 12 | loss: 1.34372 | val_0_rmse: 1.90544 |  0:00:03s\n",
      "epoch 13 | loss: 1.16212 | val_0_rmse: 1.36598 |  0:00:04s\n",
      "epoch 14 | loss: 1.01995 | val_0_rmse: 1.55744 |  0:00:04s\n",
      "epoch 15 | loss: 0.87034 | val_0_rmse: 0.76776 |  0:00:04s\n",
      "epoch 16 | loss: 0.65608 | val_0_rmse: 1.33917 |  0:00:05s\n",
      "epoch 17 | loss: 0.92099 | val_0_rmse: 0.45684 |  0:00:05s\n",
      "epoch 18 | loss: 0.91036 | val_0_rmse: 1.10065 |  0:00:05s\n",
      "epoch 19 | loss: 1.01558 | val_0_rmse: 0.54833 |  0:00:05s\n",
      "epoch 20 | loss: 0.55485 | val_0_rmse: 0.53544 |  0:00:06s\n",
      "epoch 21 | loss: 0.42228 | val_0_rmse: 0.59307 |  0:00:06s\n",
      "epoch 22 | loss: 0.44673 | val_0_rmse: 0.47795 |  0:00:06s\n",
      "epoch 23 | loss: 0.35079 | val_0_rmse: 0.79886 |  0:00:07s\n",
      "epoch 24 | loss: 0.41096 | val_0_rmse: 0.48237 |  0:00:07s\n",
      "epoch 25 | loss: 0.46648 | val_0_rmse: 0.81107 |  0:00:07s\n",
      "epoch 26 | loss: 0.59038 | val_0_rmse: 0.44227 |  0:00:07s\n",
      "epoch 27 | loss: 0.44944 | val_0_rmse: 0.58351 |  0:00:08s\n",
      "epoch 28 | loss: 0.33391 | val_0_rmse: 0.69041 |  0:00:08s\n",
      "epoch 29 | loss: 0.37468 | val_0_rmse: 0.41806 |  0:00:08s\n",
      "epoch 30 | loss: 0.49043 | val_0_rmse: 0.83125 |  0:00:09s\n",
      "epoch 31 | loss: 0.31853 | val_0_rmse: 0.46792 |  0:00:09s\n",
      "epoch 32 | loss: 0.28112 | val_0_rmse: 0.62482 |  0:00:09s\n",
      "epoch 33 | loss: 0.22385 | val_0_rmse: 0.59862 |  0:00:09s\n",
      "epoch 34 | loss: 0.19048 | val_0_rmse: 0.52341 |  0:00:10s\n",
      "epoch 35 | loss: 0.2012  | val_0_rmse: 0.58939 |  0:00:10s\n",
      "epoch 36 | loss: 0.20057 | val_0_rmse: 0.34118 |  0:00:10s\n",
      "epoch 37 | loss: 0.14175 | val_0_rmse: 0.55358 |  0:00:11s\n",
      "epoch 38 | loss: 0.15585 | val_0_rmse: 0.45756 |  0:00:11s\n",
      "epoch 39 | loss: 0.10597 | val_0_rmse: 0.38382 |  0:00:11s\n",
      "epoch 40 | loss: 0.10474 | val_0_rmse: 0.354   |  0:00:11s\n",
      "epoch 41 | loss: 0.12614 | val_0_rmse: 0.30925 |  0:00:12s\n",
      "epoch 42 | loss: 0.1063  | val_0_rmse: 0.30679 |  0:00:12s\n",
      "epoch 43 | loss: 0.10907 | val_0_rmse: 0.36804 |  0:00:12s\n",
      "epoch 44 | loss: 0.11822 | val_0_rmse: 0.36765 |  0:00:13s\n",
      "epoch 45 | loss: 0.12896 | val_0_rmse: 0.2861  |  0:00:13s\n",
      "epoch 46 | loss: 0.09985 | val_0_rmse: 0.29915 |  0:00:13s\n",
      "epoch 47 | loss: 0.10962 | val_0_rmse: 0.30522 |  0:00:13s\n",
      "epoch 48 | loss: 0.11162 | val_0_rmse: 0.29209 |  0:00:14s\n",
      "epoch 49 | loss: 0.12397 | val_0_rmse: 0.40307 |  0:00:14s\n",
      "epoch 50 | loss: 0.11524 | val_0_rmse: 0.29151 |  0:00:14s\n",
      "epoch 51 | loss: 0.10614 | val_0_rmse: 0.26657 |  0:00:15s\n",
      "epoch 52 | loss: 0.09299 | val_0_rmse: 0.35197 |  0:00:15s\n",
      "epoch 53 | loss: 0.10207 | val_0_rmse: 0.31824 |  0:00:15s\n",
      "epoch 54 | loss: 0.07274 | val_0_rmse: 0.27463 |  0:00:16s\n",
      "epoch 55 | loss: 0.09613 | val_0_rmse: 0.2654  |  0:00:16s\n",
      "epoch 56 | loss: 0.09036 | val_0_rmse: 0.29182 |  0:00:16s\n",
      "epoch 57 | loss: 0.07929 | val_0_rmse: 0.26869 |  0:00:16s\n",
      "epoch 58 | loss: 0.07724 | val_0_rmse: 0.26101 |  0:00:17s\n",
      "epoch 59 | loss: 0.07184 | val_0_rmse: 0.26263 |  0:00:17s\n",
      "epoch 60 | loss: 0.07381 | val_0_rmse: 0.25907 |  0:00:17s\n",
      "epoch 61 | loss: 0.07494 | val_0_rmse: 0.2548  |  0:00:17s\n",
      "epoch 62 | loss: 0.08083 | val_0_rmse: 0.27972 |  0:00:18s\n",
      "epoch 63 | loss: 0.11254 | val_0_rmse: 0.34089 |  0:00:18s\n",
      "epoch 64 | loss: 0.13823 | val_0_rmse: 0.29334 |  0:00:18s\n",
      "epoch 65 | loss: 0.09063 | val_0_rmse: 0.35945 |  0:00:19s\n",
      "epoch 66 | loss: 0.13414 | val_0_rmse: 0.31759 |  0:00:19s\n",
      "epoch 67 | loss: 0.17906 | val_0_rmse: 0.29932 |  0:00:19s\n",
      "epoch 68 | loss: 0.10268 | val_0_rmse: 0.29915 |  0:00:20s\n",
      "epoch 69 | loss: 0.11178 | val_0_rmse: 0.28127 |  0:00:20s\n",
      "epoch 70 | loss: 0.08767 | val_0_rmse: 0.2706  |  0:00:20s\n",
      "epoch 71 | loss: 0.07348 | val_0_rmse: 0.31311 |  0:00:20s\n",
      "epoch 72 | loss: 0.09809 | val_0_rmse: 0.29992 |  0:00:21s\n",
      "epoch 73 | loss: 0.07602 | val_0_rmse: 0.2721  |  0:00:21s\n",
      "epoch 74 | loss: 0.06885 | val_0_rmse: 0.25761 |  0:00:21s\n",
      "epoch 75 | loss: 0.06601 | val_0_rmse: 0.27172 |  0:00:21s\n",
      "epoch 76 | loss: 0.06376 | val_0_rmse: 0.25678 |  0:00:22s\n",
      "epoch 77 | loss: 0.07067 | val_0_rmse: 0.33676 |  0:00:22s\n",
      "epoch 78 | loss: 0.08118 | val_0_rmse: 0.36998 |  0:00:22s\n",
      "epoch 79 | loss: 0.17137 | val_0_rmse: 0.33703 |  0:00:23s\n",
      "epoch 80 | loss: 0.14901 | val_0_rmse: 0.29901 |  0:00:23s\n",
      "epoch 81 | loss: 0.0943  | val_0_rmse: 0.31278 |  0:00:23s\n",
      "epoch 82 | loss: 0.09512 | val_0_rmse: 0.44459 |  0:00:24s\n",
      "epoch 83 | loss: 0.18049 | val_0_rmse: 0.25971 |  0:00:24s\n",
      "epoch 84 | loss: 0.10538 | val_0_rmse: 0.24294 |  0:00:24s\n",
      "epoch 85 | loss: 0.10565 | val_0_rmse: 0.40992 |  0:00:24s\n",
      "epoch 86 | loss: 0.12852 | val_0_rmse: 0.32376 |  0:00:25s\n",
      "epoch 87 | loss: 0.10794 | val_0_rmse: 0.34868 |  0:00:25s\n",
      "epoch 88 | loss: 0.17902 | val_0_rmse: 0.31991 |  0:00:25s\n",
      "epoch 89 | loss: 0.08498 | val_0_rmse: 0.31215 |  0:00:25s\n",
      "epoch 90 | loss: 0.07192 | val_0_rmse: 0.43096 |  0:00:26s\n",
      "epoch 91 | loss: 0.20365 | val_0_rmse: 0.25567 |  0:00:26s\n",
      "epoch 92 | loss: 0.08845 | val_0_rmse: 0.23765 |  0:00:26s\n",
      "epoch 93 | loss: 0.11128 | val_0_rmse: 0.42643 |  0:00:27s\n",
      "epoch 94 | loss: 0.13192 | val_0_rmse: 0.30567 |  0:00:27s\n",
      "epoch 95 | loss: 0.08402 | val_0_rmse: 0.34346 |  0:00:27s\n",
      "epoch 96 | loss: 0.19812 | val_0_rmse: 0.30534 |  0:00:27s\n",
      "epoch 97 | loss: 0.08    | val_0_rmse: 0.28463 |  0:00:28s\n",
      "epoch 98 | loss: 0.06321 | val_0_rmse: 0.28707 |  0:00:28s\n",
      "epoch 99 | loss: 0.06968 | val_0_rmse: 0.35558 |  0:00:28s\n",
      "epoch 100| loss: 0.10552 | val_0_rmse: 0.24274 |  0:00:29s\n",
      "epoch 101| loss: 0.05791 | val_0_rmse: 0.2657  |  0:00:29s\n",
      "epoch 102| loss: 0.04479 | val_0_rmse: 0.2469  |  0:00:29s\n",
      "epoch 103| loss: 0.04821 | val_0_rmse: 0.26058 |  0:00:29s\n",
      "epoch 104| loss: 0.05412 | val_0_rmse: 0.24145 |  0:00:30s\n",
      "epoch 105| loss: 0.05187 | val_0_rmse: 0.2371  |  0:00:30s\n",
      "epoch 106| loss: 0.05648 | val_0_rmse: 0.23242 |  0:00:30s\n",
      "epoch 107| loss: 0.04333 | val_0_rmse: 0.23283 |  0:00:30s\n",
      "epoch 108| loss: 0.0499  | val_0_rmse: 0.22567 |  0:00:31s\n",
      "epoch 109| loss: 0.05091 | val_0_rmse: 0.23008 |  0:00:31s\n",
      "epoch 110| loss: 0.04608 | val_0_rmse: 0.22358 |  0:00:31s\n",
      "epoch 111| loss: 0.04484 | val_0_rmse: 0.2333  |  0:00:32s\n",
      "epoch 112| loss: 0.05309 | val_0_rmse: 0.23952 |  0:00:32s\n",
      "epoch 113| loss: 0.04927 | val_0_rmse: 0.23218 |  0:00:32s\n",
      "epoch 114| loss: 0.0481  | val_0_rmse: 0.2575  |  0:00:32s\n",
      "epoch 115| loss: 0.04851 | val_0_rmse: 0.24305 |  0:00:33s\n",
      "epoch 116| loss: 0.0491  | val_0_rmse: 0.29934 |  0:00:33s\n",
      "epoch 117| loss: 0.0642  | val_0_rmse: 0.24059 |  0:00:33s\n",
      "epoch 118| loss: 0.04945 | val_0_rmse: 0.25369 |  0:00:34s\n",
      "epoch 119| loss: 0.04479 | val_0_rmse: 0.24399 |  0:00:34s\n",
      "epoch 120| loss: 0.05575 | val_0_rmse: 0.25059 |  0:00:34s\n",
      "epoch 121| loss: 0.05692 | val_0_rmse: 0.24253 |  0:00:34s\n",
      "epoch 122| loss: 0.04488 | val_0_rmse: 0.26471 |  0:00:35s\n",
      "epoch 123| loss: 0.06138 | val_0_rmse: 0.32664 |  0:00:35s\n",
      "epoch 124| loss: 0.12415 | val_0_rmse: 0.26607 |  0:00:35s\n",
      "epoch 125| loss: 0.09718 | val_0_rmse: 0.40005 |  0:00:36s\n",
      "epoch 126| loss: 0.11196 | val_0_rmse: 0.36365 |  0:00:36s\n",
      "epoch 127| loss: 0.11484 | val_0_rmse: 0.30043 |  0:00:36s\n",
      "epoch 128| loss: 0.1155  | val_0_rmse: 0.26268 |  0:00:36s\n",
      "epoch 129| loss: 0.08382 | val_0_rmse: 0.36868 |  0:00:37s\n",
      "epoch 130| loss: 0.09376 | val_0_rmse: 0.36352 |  0:00:37s\n",
      "epoch 131| loss: 0.10152 | val_0_rmse: 0.32005 |  0:00:37s\n",
      "epoch 132| loss: 0.14666 | val_0_rmse: 0.29837 |  0:00:38s\n",
      "epoch 133| loss: 0.07365 | val_0_rmse: 0.33725 |  0:00:38s\n",
      "epoch 134| loss: 0.07304 | val_0_rmse: 0.40995 |  0:00:38s\n",
      "epoch 135| loss: 0.15651 | val_0_rmse: 0.27471 |  0:00:39s\n",
      "\n",
      "Early stopping occurred at epoch 135 with best_epoch = 110 and best_val_0_rmse = 0.22358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:43:57,775] Trial 78 finished with value: 0.22358115170821538 and parameters: {'n_d': 24, 'n_a': 48, 'n_steps': 4, 'gamma': 1.739548962397883, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.019984491692587085, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 078 | rmse_log=0.22358 | RMSE$=42,427 | MAE$=28,867 | MAPE=17.54% | n_d/n_a=24/48 steps=4 lr=0.01998 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 180.13072| val_0_rmse: 11.3964 |  0:00:00s\n",
      "epoch 1  | loss: 131.86588| val_0_rmse: 10.73691|  0:00:00s\n",
      "epoch 2  | loss: 97.7965 | val_0_rmse: 10.07163|  0:00:00s\n",
      "epoch 3  | loss: 70.99181| val_0_rmse: 9.45389 |  0:00:00s\n",
      "epoch 4  | loss: 49.07634| val_0_rmse: 8.69193 |  0:00:01s\n",
      "epoch 5  | loss: 38.60986| val_0_rmse: 7.95983 |  0:00:01s\n",
      "epoch 6  | loss: 32.97631| val_0_rmse: 7.1996  |  0:00:01s\n",
      "epoch 7  | loss: 29.12722| val_0_rmse: 6.46149 |  0:00:01s\n",
      "epoch 8  | loss: 28.44938| val_0_rmse: 5.66273 |  0:00:01s\n",
      "epoch 9  | loss: 27.0864 | val_0_rmse: 4.95098 |  0:00:02s\n",
      "epoch 10 | loss: 29.26626| val_0_rmse: 4.41516 |  0:00:02s\n",
      "epoch 11 | loss: 29.79512| val_0_rmse: 4.05441 |  0:00:02s\n",
      "epoch 12 | loss: 26.01984| val_0_rmse: 3.73674 |  0:00:02s\n",
      "epoch 13 | loss: 21.03591| val_0_rmse: 3.65139 |  0:00:03s\n",
      "epoch 14 | loss: 15.79708| val_0_rmse: 3.64335 |  0:00:03s\n",
      "epoch 15 | loss: 11.48674| val_0_rmse: 3.73737 |  0:00:03s\n",
      "epoch 16 | loss: 7.97817 | val_0_rmse: 3.729   |  0:00:03s\n",
      "epoch 17 | loss: 7.294   | val_0_rmse: 3.67537 |  0:00:03s\n",
      "epoch 18 | loss: 6.61391 | val_0_rmse: 3.46791 |  0:00:04s\n",
      "epoch 19 | loss: 6.95418 | val_0_rmse: 3.14814 |  0:00:04s\n",
      "epoch 20 | loss: 5.73869 | val_0_rmse: 2.69827 |  0:00:04s\n",
      "epoch 21 | loss: 4.05976 | val_0_rmse: 2.02422 |  0:00:04s\n",
      "epoch 22 | loss: 3.41944 | val_0_rmse: 1.31377 |  0:00:04s\n",
      "epoch 23 | loss: 3.01887 | val_0_rmse: 0.8298  |  0:00:05s\n",
      "epoch 24 | loss: 3.23183 | val_0_rmse: 0.63602 |  0:00:05s\n",
      "epoch 25 | loss: 2.74676 | val_0_rmse: 0.66811 |  0:00:05s\n",
      "epoch 26 | loss: 2.20396 | val_0_rmse: 0.88878 |  0:00:05s\n",
      "epoch 27 | loss: 2.24165 | val_0_rmse: 0.99125 |  0:00:06s\n",
      "epoch 28 | loss: 2.0574  | val_0_rmse: 1.00614 |  0:00:06s\n",
      "epoch 29 | loss: 1.69046 | val_0_rmse: 0.81741 |  0:00:06s\n",
      "epoch 30 | loss: 1.42665 | val_0_rmse: 0.66026 |  0:00:06s\n",
      "epoch 31 | loss: 1.44751 | val_0_rmse: 0.54413 |  0:00:06s\n",
      "epoch 32 | loss: 1.55842 | val_0_rmse: 0.55801 |  0:00:07s\n",
      "epoch 33 | loss: 1.2321  | val_0_rmse: 0.69543 |  0:00:07s\n",
      "epoch 34 | loss: 1.15291 | val_0_rmse: 0.63869 |  0:00:07s\n",
      "epoch 35 | loss: 1.23564 | val_0_rmse: 0.52822 |  0:00:07s\n",
      "epoch 36 | loss: 0.88751 | val_0_rmse: 0.48124 |  0:00:07s\n",
      "epoch 37 | loss: 0.98818 | val_0_rmse: 0.52721 |  0:00:08s\n",
      "epoch 38 | loss: 0.85728 | val_0_rmse: 0.58331 |  0:00:08s\n",
      "epoch 39 | loss: 0.96708 | val_0_rmse: 0.55609 |  0:00:08s\n",
      "epoch 40 | loss: 0.81139 | val_0_rmse: 0.37622 |  0:00:08s\n",
      "epoch 41 | loss: 0.80765 | val_0_rmse: 0.3692  |  0:00:08s\n",
      "epoch 42 | loss: 0.87515 | val_0_rmse: 0.49386 |  0:00:09s\n",
      "epoch 43 | loss: 0.87167 | val_0_rmse: 0.43622 |  0:00:09s\n",
      "epoch 44 | loss: 0.74756 | val_0_rmse: 0.40882 |  0:00:09s\n",
      "epoch 45 | loss: 0.68573 | val_0_rmse: 0.43714 |  0:00:09s\n",
      "epoch 46 | loss: 0.67182 | val_0_rmse: 0.56659 |  0:00:09s\n",
      "epoch 47 | loss: 0.75489 | val_0_rmse: 0.87074 |  0:00:10s\n",
      "epoch 48 | loss: 1.57317 | val_0_rmse: 0.53243 |  0:00:10s\n",
      "epoch 49 | loss: 0.6686  | val_0_rmse: 0.52086 |  0:00:10s\n",
      "epoch 50 | loss: 1.13752 | val_0_rmse: 0.64068 |  0:00:10s\n",
      "epoch 51 | loss: 1.14414 | val_0_rmse: 0.4414  |  0:00:10s\n",
      "epoch 52 | loss: 0.56167 | val_0_rmse: 0.60099 |  0:00:11s\n",
      "epoch 53 | loss: 0.87209 | val_0_rmse: 0.73557 |  0:00:11s\n",
      "epoch 54 | loss: 1.16224 | val_0_rmse: 0.58681 |  0:00:11s\n",
      "epoch 55 | loss: 0.67936 | val_0_rmse: 0.35135 |  0:00:11s\n",
      "epoch 56 | loss: 0.74778 | val_0_rmse: 0.58448 |  0:00:11s\n",
      "epoch 57 | loss: 1.54766 | val_0_rmse: 0.4818  |  0:00:12s\n",
      "epoch 58 | loss: 1.11374 | val_0_rmse: 0.35113 |  0:00:12s\n",
      "epoch 59 | loss: 0.58327 | val_0_rmse: 0.57616 |  0:00:12s\n",
      "epoch 60 | loss: 1.00117 | val_0_rmse: 0.62402 |  0:00:12s\n",
      "epoch 61 | loss: 0.74446 | val_0_rmse: 0.45118 |  0:00:13s\n",
      "epoch 62 | loss: 0.47554 | val_0_rmse: 0.37005 |  0:00:13s\n",
      "epoch 63 | loss: 0.80337 | val_0_rmse: 0.3967  |  0:00:13s\n",
      "epoch 64 | loss: 0.9469  | val_0_rmse: 0.35268 |  0:00:13s\n",
      "epoch 65 | loss: 0.60999 | val_0_rmse: 0.40484 |  0:00:13s\n",
      "epoch 66 | loss: 0.40898 | val_0_rmse: 0.49981 |  0:00:14s\n",
      "epoch 67 | loss: 0.64151 | val_0_rmse: 0.46243 |  0:00:14s\n",
      "epoch 68 | loss: 0.48454 | val_0_rmse: 0.35203 |  0:00:14s\n",
      "epoch 69 | loss: 0.42426 | val_0_rmse: 0.36303 |  0:00:14s\n",
      "epoch 70 | loss: 0.52546 | val_0_rmse: 0.33516 |  0:00:14s\n",
      "epoch 71 | loss: 0.35036 | val_0_rmse: 0.39784 |  0:00:15s\n",
      "epoch 72 | loss: 0.44865 | val_0_rmse: 0.42334 |  0:00:15s\n",
      "epoch 73 | loss: 0.37755 | val_0_rmse: 0.37535 |  0:00:15s\n",
      "epoch 74 | loss: 0.36305 | val_0_rmse: 0.36247 |  0:00:15s\n",
      "epoch 75 | loss: 0.42346 | val_0_rmse: 0.33621 |  0:00:15s\n",
      "epoch 76 | loss: 0.32776 | val_0_rmse: 0.36915 |  0:00:16s\n",
      "epoch 77 | loss: 0.31146 | val_0_rmse: 0.40103 |  0:00:16s\n",
      "epoch 78 | loss: 0.34434 | val_0_rmse: 0.40683 |  0:00:16s\n",
      "epoch 79 | loss: 0.34059 | val_0_rmse: 0.3591  |  0:00:16s\n",
      "epoch 80 | loss: 0.37586 | val_0_rmse: 0.36133 |  0:00:16s\n",
      "epoch 81 | loss: 0.38681 | val_0_rmse: 0.31253 |  0:00:17s\n",
      "epoch 82 | loss: 0.26865 | val_0_rmse: 0.36983 |  0:00:17s\n",
      "epoch 83 | loss: 0.34323 | val_0_rmse: 0.35934 |  0:00:17s\n",
      "epoch 84 | loss: 0.29282 | val_0_rmse: 0.30094 |  0:00:17s\n",
      "epoch 85 | loss: 0.27629 | val_0_rmse: 0.30913 |  0:00:17s\n",
      "epoch 86 | loss: 0.26747 | val_0_rmse: 0.36353 |  0:00:18s\n",
      "epoch 87 | loss: 0.21647 | val_0_rmse: 0.35333 |  0:00:18s\n",
      "epoch 88 | loss: 0.21806 | val_0_rmse: 0.34369 |  0:00:18s\n",
      "epoch 89 | loss: 0.25925 | val_0_rmse: 0.35153 |  0:00:18s\n",
      "epoch 90 | loss: 0.24775 | val_0_rmse: 0.3793  |  0:00:18s\n",
      "epoch 91 | loss: 0.21797 | val_0_rmse: 0.38138 |  0:00:19s\n",
      "epoch 92 | loss: 0.2439  | val_0_rmse: 0.32659 |  0:00:19s\n",
      "epoch 93 | loss: 0.24872 | val_0_rmse: 0.31986 |  0:00:19s\n",
      "epoch 94 | loss: 0.25883 | val_0_rmse: 0.32794 |  0:00:19s\n",
      "epoch 95 | loss: 0.17699 | val_0_rmse: 0.32877 |  0:00:19s\n",
      "epoch 96 | loss: 0.19355 | val_0_rmse: 0.31914 |  0:00:20s\n",
      "epoch 97 | loss: 0.17355 | val_0_rmse: 0.3133  |  0:00:20s\n",
      "epoch 98 | loss: 0.15945 | val_0_rmse: 0.38371 |  0:00:20s\n",
      "epoch 99 | loss: 0.2157  | val_0_rmse: 0.38775 |  0:00:20s\n",
      "epoch 100| loss: 0.19189 | val_0_rmse: 0.31826 |  0:00:20s\n",
      "epoch 101| loss: 0.24576 | val_0_rmse: 0.32268 |  0:00:21s\n",
      "epoch 102| loss: 0.20373 | val_0_rmse: 0.34265 |  0:00:21s\n",
      "epoch 103| loss: 0.16324 | val_0_rmse: 0.34482 |  0:00:21s\n",
      "epoch 104| loss: 0.15564 | val_0_rmse: 0.33362 |  0:00:21s\n",
      "epoch 105| loss: 0.21089 | val_0_rmse: 0.32602 |  0:00:21s\n",
      "epoch 106| loss: 0.21435 | val_0_rmse: 0.42239 |  0:00:22s\n",
      "epoch 107| loss: 0.19585 | val_0_rmse: 0.4132  |  0:00:22s\n",
      "epoch 108| loss: 0.16948 | val_0_rmse: 0.32228 |  0:00:22s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:44:20,826] Trial 79 finished with value: 0.3009406572117342 and parameters: {'n_d': 64, 'n_a': 16, 'n_steps': 4, 'gamma': 1.6963920644544845, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.015836203421386134, 'batch_size': 2048, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 109| loss: 0.16788 | val_0_rmse: 0.30646 |  0:00:22s\n",
      "\n",
      "Early stopping occurred at epoch 109 with best_epoch = 84 and best_val_0_rmse = 0.30094\n",
      "Trial 079 | rmse_log=0.30094 | RMSE$=59,568 | MAE$=40,288 | MAPE=24.78% | n_d/n_a=64/16 steps=4 lr=0.01584 batch=2048 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 127.17847| val_0_rmse: 10.58931|  0:00:00s\n",
      "epoch 1  | loss: 92.86906| val_0_rmse: 9.18475 |  0:00:00s\n",
      "epoch 2  | loss: 63.67812| val_0_rmse: 7.38277 |  0:00:00s\n",
      "epoch 3  | loss: 35.4025 | val_0_rmse: 5.90556 |  0:00:00s\n",
      "epoch 4  | loss: 24.50231| val_0_rmse: 4.59565 |  0:00:00s\n",
      "epoch 5  | loss: 11.33164| val_0_rmse: 3.59922 |  0:00:00s\n",
      "epoch 6  | loss: 10.26622| val_0_rmse: 3.61374 |  0:00:01s\n",
      "epoch 7  | loss: 5.00563 | val_0_rmse: 4.62788 |  0:00:01s\n",
      "epoch 8  | loss: 2.6763  | val_0_rmse: 4.37659 |  0:00:01s\n",
      "epoch 9  | loss: 1.59628 | val_0_rmse: 3.30985 |  0:00:01s\n",
      "epoch 10 | loss: 1.51741 | val_0_rmse: 3.53145 |  0:00:01s\n",
      "epoch 11 | loss: 0.83486 | val_0_rmse: 3.26649 |  0:00:01s\n",
      "epoch 12 | loss: 0.64268 | val_0_rmse: 2.98375 |  0:00:02s\n",
      "epoch 13 | loss: 0.39385 | val_0_rmse: 2.46528 |  0:00:02s\n",
      "epoch 14 | loss: 0.51508 | val_0_rmse: 2.60989 |  0:00:02s\n",
      "epoch 15 | loss: 0.45726 | val_0_rmse: 2.28409 |  0:00:02s\n",
      "epoch 16 | loss: 0.3934  | val_0_rmse: 2.19696 |  0:00:02s\n",
      "epoch 17 | loss: 0.22946 | val_0_rmse: 2.21072 |  0:00:02s\n",
      "epoch 18 | loss: 0.20401 | val_0_rmse: 1.96939 |  0:00:03s\n",
      "epoch 19 | loss: 0.24102 | val_0_rmse: 1.87935 |  0:00:03s\n",
      "epoch 20 | loss: 0.22197 | val_0_rmse: 1.85388 |  0:00:03s\n",
      "epoch 21 | loss: 0.21208 | val_0_rmse: 1.6479  |  0:00:03s\n",
      "epoch 22 | loss: 0.15017 | val_0_rmse: 1.42151 |  0:00:03s\n",
      "epoch 23 | loss: 0.18805 | val_0_rmse: 1.65068 |  0:00:03s\n",
      "epoch 24 | loss: 0.19317 | val_0_rmse: 1.28128 |  0:00:04s\n",
      "epoch 25 | loss: 0.17415 | val_0_rmse: 1.58471 |  0:00:04s\n",
      "epoch 26 | loss: 0.20129 | val_0_rmse: 1.11858 |  0:00:04s\n",
      "epoch 27 | loss: 0.15188 | val_0_rmse: 1.36201 |  0:00:04s\n",
      "epoch 28 | loss: 0.15712 | val_0_rmse: 1.11479 |  0:00:04s\n",
      "epoch 29 | loss: 0.15983 | val_0_rmse: 1.2616  |  0:00:04s\n",
      "epoch 30 | loss: 0.12583 | val_0_rmse: 0.87409 |  0:00:04s\n",
      "epoch 31 | loss: 0.11716 | val_0_rmse: 1.19379 |  0:00:05s\n",
      "epoch 32 | loss: 0.15265 | val_0_rmse: 0.798   |  0:00:05s\n",
      "epoch 33 | loss: 0.14212 | val_0_rmse: 1.01541 |  0:00:05s\n",
      "epoch 34 | loss: 0.13282 | val_0_rmse: 0.74635 |  0:00:05s\n",
      "epoch 35 | loss: 0.14429 | val_0_rmse: 0.95752 |  0:00:05s\n",
      "epoch 36 | loss: 0.17111 | val_0_rmse: 0.77895 |  0:00:05s\n",
      "epoch 37 | loss: 0.13976 | val_0_rmse: 0.62827 |  0:00:06s\n",
      "epoch 38 | loss: 0.12416 | val_0_rmse: 0.80866 |  0:00:06s\n",
      "epoch 39 | loss: 0.08405 | val_0_rmse: 0.6149  |  0:00:06s\n",
      "epoch 40 | loss: 0.07534 | val_0_rmse: 0.72248 |  0:00:06s\n",
      "epoch 41 | loss: 0.08572 | val_0_rmse: 0.57955 |  0:00:06s\n",
      "epoch 42 | loss: 0.08964 | val_0_rmse: 0.65938 |  0:00:06s\n",
      "epoch 43 | loss: 0.07566 | val_0_rmse: 0.50702 |  0:00:07s\n",
      "epoch 44 | loss: 0.07136 | val_0_rmse: 0.70314 |  0:00:07s\n",
      "epoch 45 | loss: 0.06827 | val_0_rmse: 0.33291 |  0:00:07s\n",
      "epoch 46 | loss: 0.13236 | val_0_rmse: 0.53863 |  0:00:07s\n",
      "epoch 47 | loss: 0.06172 | val_0_rmse: 0.34754 |  0:00:07s\n",
      "epoch 48 | loss: 0.11997 | val_0_rmse: 0.4816  |  0:00:07s\n",
      "epoch 49 | loss: 0.08231 | val_0_rmse: 0.36271 |  0:00:07s\n",
      "epoch 50 | loss: 0.06924 | val_0_rmse: 0.40353 |  0:00:08s\n",
      "epoch 51 | loss: 0.08295 | val_0_rmse: 0.38231 |  0:00:08s\n",
      "epoch 52 | loss: 0.07175 | val_0_rmse: 0.37593 |  0:00:08s\n",
      "epoch 53 | loss: 0.09282 | val_0_rmse: 0.30398 |  0:00:08s\n",
      "epoch 54 | loss: 0.08864 | val_0_rmse: 0.36966 |  0:00:08s\n",
      "epoch 55 | loss: 0.0698  | val_0_rmse: 0.3258  |  0:00:08s\n",
      "epoch 56 | loss: 0.07261 | val_0_rmse: 0.44143 |  0:00:08s\n",
      "epoch 57 | loss: 0.10195 | val_0_rmse: 0.31702 |  0:00:09s\n",
      "epoch 58 | loss: 0.09349 | val_0_rmse: 0.42916 |  0:00:09s\n",
      "epoch 59 | loss: 0.07431 | val_0_rmse: 0.30762 |  0:00:09s\n",
      "epoch 60 | loss: 0.06276 | val_0_rmse: 0.40282 |  0:00:09s\n",
      "epoch 61 | loss: 0.05359 | val_0_rmse: 0.35301 |  0:00:09s\n",
      "epoch 62 | loss: 0.04374 | val_0_rmse: 0.36226 |  0:00:09s\n",
      "epoch 63 | loss: 0.04833 | val_0_rmse: 0.36392 |  0:00:09s\n",
      "epoch 64 | loss: 0.04464 | val_0_rmse: 0.42736 |  0:00:10s\n",
      "epoch 65 | loss: 0.06351 | val_0_rmse: 0.27473 |  0:00:10s\n",
      "epoch 66 | loss: 0.0445  | val_0_rmse: 0.38538 |  0:00:10s\n",
      "epoch 67 | loss: 0.05048 | val_0_rmse: 0.32626 |  0:00:10s\n",
      "epoch 68 | loss: 0.04512 | val_0_rmse: 0.25334 |  0:00:10s\n",
      "epoch 69 | loss: 0.04965 | val_0_rmse: 0.3371  |  0:00:10s\n",
      "epoch 70 | loss: 0.05846 | val_0_rmse: 0.28566 |  0:00:11s\n",
      "epoch 71 | loss: 0.05569 | val_0_rmse: 0.35068 |  0:00:11s\n",
      "epoch 72 | loss: 0.04948 | val_0_rmse: 0.25284 |  0:00:11s\n",
      "epoch 73 | loss: 0.04596 | val_0_rmse: 0.33146 |  0:00:11s\n",
      "epoch 74 | loss: 0.04497 | val_0_rmse: 0.26867 |  0:00:11s\n",
      "epoch 75 | loss: 0.03728 | val_0_rmse: 0.26707 |  0:00:11s\n",
      "epoch 76 | loss: 0.03341 | val_0_rmse: 0.24436 |  0:00:12s\n",
      "epoch 77 | loss: 0.03623 | val_0_rmse: 0.22189 |  0:00:12s\n",
      "epoch 78 | loss: 0.04646 | val_0_rmse: 0.29032 |  0:00:12s\n",
      "epoch 79 | loss: 0.03486 | val_0_rmse: 0.30263 |  0:00:12s\n",
      "epoch 80 | loss: 0.04711 | val_0_rmse: 0.23416 |  0:00:12s\n",
      "epoch 81 | loss: 0.03667 | val_0_rmse: 0.29656 |  0:00:12s\n",
      "epoch 82 | loss: 0.04283 | val_0_rmse: 0.24337 |  0:00:12s\n",
      "epoch 83 | loss: 0.04962 | val_0_rmse: 0.31413 |  0:00:13s\n",
      "epoch 84 | loss: 0.06716 | val_0_rmse: 0.29419 |  0:00:13s\n",
      "epoch 85 | loss: 0.11072 | val_0_rmse: 0.21025 |  0:00:13s\n",
      "epoch 86 | loss: 0.07052 | val_0_rmse: 0.31696 |  0:00:13s\n",
      "epoch 87 | loss: 0.06333 | val_0_rmse: 0.25711 |  0:00:13s\n",
      "epoch 88 | loss: 0.06454 | val_0_rmse: 0.25832 |  0:00:13s\n",
      "epoch 89 | loss: 0.04623 | val_0_rmse: 0.25933 |  0:00:14s\n",
      "epoch 90 | loss: 0.06607 | val_0_rmse: 0.23393 |  0:00:14s\n",
      "epoch 91 | loss: 0.03235 | val_0_rmse: 0.21407 |  0:00:14s\n",
      "epoch 92 | loss: 0.03366 | val_0_rmse: 0.24855 |  0:00:14s\n",
      "epoch 93 | loss: 0.03895 | val_0_rmse: 0.20537 |  0:00:14s\n",
      "epoch 94 | loss: 0.03079 | val_0_rmse: 0.21068 |  0:00:14s\n",
      "epoch 95 | loss: 0.0355  | val_0_rmse: 0.20429 |  0:00:15s\n",
      "epoch 96 | loss: 0.02815 | val_0_rmse: 0.21536 |  0:00:15s\n",
      "epoch 97 | loss: 0.03081 | val_0_rmse: 0.22505 |  0:00:15s\n",
      "epoch 98 | loss: 0.03005 | val_0_rmse: 0.21627 |  0:00:15s\n",
      "epoch 99 | loss: 0.03902 | val_0_rmse: 0.25606 |  0:00:15s\n",
      "epoch 100| loss: 0.03716 | val_0_rmse: 0.20921 |  0:00:15s\n",
      "epoch 101| loss: 0.02651 | val_0_rmse: 0.2179  |  0:00:16s\n",
      "epoch 102| loss: 0.02706 | val_0_rmse: 0.21415 |  0:00:16s\n",
      "epoch 103| loss: 0.03057 | val_0_rmse: 0.23943 |  0:00:16s\n",
      "epoch 104| loss: 0.03899 | val_0_rmse: 0.21933 |  0:00:16s\n",
      "epoch 105| loss: 0.03134 | val_0_rmse: 0.20975 |  0:00:16s\n",
      "epoch 106| loss: 0.03024 | val_0_rmse: 0.22371 |  0:00:16s\n",
      "epoch 107| loss: 0.02707 | val_0_rmse: 0.21211 |  0:00:16s\n",
      "epoch 108| loss: 0.02278 | val_0_rmse: 0.21148 |  0:00:17s\n",
      "epoch 109| loss: 0.02652 | val_0_rmse: 0.24746 |  0:00:17s\n",
      "epoch 110| loss: 0.0399  | val_0_rmse: 0.22307 |  0:00:17s\n",
      "epoch 111| loss: 0.02817 | val_0_rmse: 0.2194  |  0:00:17s\n",
      "epoch 112| loss: 0.03092 | val_0_rmse: 0.22776 |  0:00:17s\n",
      "epoch 113| loss: 0.03284 | val_0_rmse: 0.23829 |  0:00:18s\n",
      "epoch 114| loss: 0.03368 | val_0_rmse: 0.21544 |  0:00:18s\n",
      "epoch 115| loss: 0.03693 | val_0_rmse: 0.22386 |  0:00:18s\n",
      "epoch 116| loss: 0.03439 | val_0_rmse: 0.22053 |  0:00:18s\n",
      "epoch 117| loss: 0.02821 | val_0_rmse: 0.21839 |  0:00:18s\n",
      "epoch 118| loss: 0.03071 | val_0_rmse: 0.22539 |  0:00:18s\n",
      "epoch 119| loss: 0.02837 | val_0_rmse: 0.22387 |  0:00:19s\n",
      "epoch 120| loss: 0.02754 | val_0_rmse: 0.21167 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 120 with best_epoch = 95 and best_val_0_rmse = 0.20429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:44:40,560] Trial 80 finished with value: 0.2042932663587983 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.9804403149062009, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.019075753265433048, 'batch_size': 512, 'virtual_batch_size': 256}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 080 | rmse_log=0.20429 | RMSE$=41,775 | MAE$=26,472 | MAPE=15.96% | n_d/n_a=24/16 steps=3 lr=0.01908 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 112.77104| val_0_rmse: 11.04018|  0:00:00s\n",
      "epoch 1  | loss: 83.33627| val_0_rmse: 10.22282|  0:00:00s\n",
      "epoch 2  | loss: 61.09204| val_0_rmse: 9.14522 |  0:00:00s\n",
      "epoch 3  | loss: 41.05377| val_0_rmse: 7.73831 |  0:00:01s\n",
      "epoch 4  | loss: 25.50344| val_0_rmse: 6.17895 |  0:00:01s\n",
      "epoch 5  | loss: 15.58479| val_0_rmse: 4.58169 |  0:00:01s\n",
      "epoch 6  | loss: 15.18974| val_0_rmse: 3.59893 |  0:00:01s\n",
      "epoch 7  | loss: 12.00827| val_0_rmse: 3.35909 |  0:00:01s\n",
      "epoch 8  | loss: 7.84188 | val_0_rmse: 3.38832 |  0:00:02s\n",
      "epoch 9  | loss: 4.77207 | val_0_rmse: 3.51366 |  0:00:02s\n",
      "epoch 10 | loss: 3.1113  | val_0_rmse: 2.67726 |  0:00:02s\n",
      "epoch 11 | loss: 1.98937 | val_0_rmse: 1.81967 |  0:00:02s\n",
      "epoch 12 | loss: 1.37974 | val_0_rmse: 1.83689 |  0:00:03s\n",
      "epoch 13 | loss: 0.93341 | val_0_rmse: 1.32803 |  0:00:03s\n",
      "epoch 14 | loss: 0.69483 | val_0_rmse: 1.32375 |  0:00:03s\n",
      "epoch 15 | loss: 0.52471 | val_0_rmse: 0.94793 |  0:00:03s\n",
      "epoch 16 | loss: 0.532   | val_0_rmse: 1.01532 |  0:00:04s\n",
      "epoch 17 | loss: 0.47156 | val_0_rmse: 0.64371 |  0:00:04s\n",
      "epoch 18 | loss: 0.43889 | val_0_rmse: 0.90173 |  0:00:04s\n",
      "epoch 19 | loss: 0.43528 | val_0_rmse: 0.59624 |  0:00:04s\n",
      "epoch 20 | loss: 0.33707 | val_0_rmse: 0.79796 |  0:00:05s\n",
      "epoch 21 | loss: 0.3291  | val_0_rmse: 0.55959 |  0:00:05s\n",
      "epoch 22 | loss: 0.22646 | val_0_rmse: 0.59077 |  0:00:05s\n",
      "epoch 23 | loss: 0.27303 | val_0_rmse: 0.44277 |  0:00:05s\n",
      "epoch 24 | loss: 0.24998 | val_0_rmse: 0.44947 |  0:00:06s\n",
      "epoch 25 | loss: 0.28204 | val_0_rmse: 0.45487 |  0:00:06s\n",
      "epoch 26 | loss: 0.24975 | val_0_rmse: 0.5649  |  0:00:06s\n",
      "epoch 27 | loss: 0.20619 | val_0_rmse: 0.32075 |  0:00:06s\n",
      "epoch 28 | loss: 0.19882 | val_0_rmse: 0.52894 |  0:00:06s\n",
      "epoch 29 | loss: 0.22679 | val_0_rmse: 0.29328 |  0:00:07s\n",
      "epoch 30 | loss: 0.26725 | val_0_rmse: 0.38045 |  0:00:07s\n",
      "epoch 31 | loss: 0.21875 | val_0_rmse: 0.41944 |  0:00:07s\n",
      "epoch 32 | loss: 0.16942 | val_0_rmse: 0.29651 |  0:00:07s\n",
      "epoch 33 | loss: 0.12987 | val_0_rmse: 0.339   |  0:00:08s\n",
      "epoch 34 | loss: 0.11964 | val_0_rmse: 0.32437 |  0:00:08s\n",
      "epoch 35 | loss: 0.1094  | val_0_rmse: 0.29107 |  0:00:08s\n",
      "epoch 36 | loss: 0.10609 | val_0_rmse: 0.31111 |  0:00:08s\n",
      "epoch 37 | loss: 0.10728 | val_0_rmse: 0.28202 |  0:00:09s\n",
      "epoch 38 | loss: 0.10762 | val_0_rmse: 0.34352 |  0:00:09s\n",
      "epoch 39 | loss: 0.1021  | val_0_rmse: 0.27075 |  0:00:09s\n",
      "epoch 40 | loss: 0.09573 | val_0_rmse: 0.31886 |  0:00:09s\n",
      "epoch 41 | loss: 0.10976 | val_0_rmse: 0.27213 |  0:00:10s\n",
      "epoch 42 | loss: 0.09744 | val_0_rmse: 0.26828 |  0:00:10s\n",
      "epoch 43 | loss: 0.10178 | val_0_rmse: 0.24796 |  0:00:10s\n",
      "epoch 44 | loss: 0.1042  | val_0_rmse: 0.30583 |  0:00:10s\n",
      "epoch 45 | loss: 0.09904 | val_0_rmse: 0.25053 |  0:00:11s\n",
      "epoch 46 | loss: 0.07901 | val_0_rmse: 0.25659 |  0:00:11s\n",
      "epoch 47 | loss: 0.07399 | val_0_rmse: 0.27058 |  0:00:11s\n",
      "epoch 48 | loss: 0.07413 | val_0_rmse: 0.24617 |  0:00:11s\n",
      "epoch 49 | loss: 0.08068 | val_0_rmse: 0.28473 |  0:00:11s\n",
      "epoch 50 | loss: 0.0921  | val_0_rmse: 0.29618 |  0:00:12s\n",
      "epoch 51 | loss: 0.10432 | val_0_rmse: 0.28834 |  0:00:12s\n",
      "epoch 52 | loss: 0.11348 | val_0_rmse: 0.28872 |  0:00:12s\n",
      "epoch 53 | loss: 0.08858 | val_0_rmse: 0.26235 |  0:00:12s\n",
      "epoch 54 | loss: 0.07898 | val_0_rmse: 0.27121 |  0:00:13s\n",
      "epoch 55 | loss: 0.09629 | val_0_rmse: 0.25005 |  0:00:13s\n",
      "epoch 56 | loss: 0.07459 | val_0_rmse: 0.25928 |  0:00:13s\n",
      "epoch 57 | loss: 0.07091 | val_0_rmse: 0.25947 |  0:00:13s\n",
      "epoch 58 | loss: 0.07825 | val_0_rmse: 0.2602  |  0:00:14s\n",
      "epoch 59 | loss: 0.07124 | val_0_rmse: 0.27013 |  0:00:14s\n",
      "epoch 60 | loss: 0.0568  | val_0_rmse: 0.25843 |  0:00:14s\n",
      "epoch 61 | loss: 0.05688 | val_0_rmse: 0.28512 |  0:00:14s\n",
      "epoch 62 | loss: 0.05753 | val_0_rmse: 0.26048 |  0:00:14s\n",
      "epoch 63 | loss: 0.07112 | val_0_rmse: 0.30121 |  0:00:15s\n",
      "epoch 64 | loss: 0.08908 | val_0_rmse: 0.29449 |  0:00:15s\n",
      "epoch 65 | loss: 0.06199 | val_0_rmse: 0.26088 |  0:00:15s\n",
      "epoch 66 | loss: 0.0814  | val_0_rmse: 0.2809  |  0:00:15s\n",
      "epoch 67 | loss: 0.06486 | val_0_rmse: 0.25299 |  0:00:16s\n",
      "epoch 68 | loss: 0.06592 | val_0_rmse: 0.24995 |  0:00:16s\n",
      "epoch 69 | loss: 0.07152 | val_0_rmse: 0.29376 |  0:00:16s\n",
      "epoch 70 | loss: 0.07328 | val_0_rmse: 0.24864 |  0:00:16s\n",
      "epoch 71 | loss: 0.08472 | val_0_rmse: 0.28812 |  0:00:17s\n",
      "epoch 72 | loss: 0.07603 | val_0_rmse: 0.25769 |  0:00:17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:44:58,532] Trial 81 finished with value: 0.2461671152315157 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2162099658676668, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.016901682750302394, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73 | loss: 0.06331 | val_0_rmse: 0.2588  |  0:00:17s\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 48 and best_val_0_rmse = 0.24617\n",
      "Trial 081 | rmse_log=0.24617 | RMSE$=45,365 | MAE$=31,172 | MAPE=19.82% | n_d/n_a=16/16 steps=3 lr=0.01690 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.89872| val_0_rmse: 10.86213|  0:00:00s\n",
      "epoch 1  | loss: 84.82833| val_0_rmse: 9.54156 |  0:00:00s\n",
      "epoch 2  | loss: 57.23763| val_0_rmse: 8.04988 |  0:00:00s\n",
      "epoch 3  | loss: 34.19119| val_0_rmse: 6.49491 |  0:00:00s\n",
      "epoch 4  | loss: 21.34094| val_0_rmse: 5.03924 |  0:00:01s\n",
      "epoch 5  | loss: 15.93418| val_0_rmse: 4.07614 |  0:00:01s\n",
      "epoch 6  | loss: 12.40446| val_0_rmse: 3.69258 |  0:00:01s\n",
      "epoch 7  | loss: 8.57915 | val_0_rmse: 3.64973 |  0:00:01s\n",
      "epoch 8  | loss: 4.32398 | val_0_rmse: 3.50569 |  0:00:02s\n",
      "epoch 9  | loss: 3.04124 | val_0_rmse: 2.69632 |  0:00:02s\n",
      "epoch 10 | loss: 1.69653 | val_0_rmse: 1.74117 |  0:00:02s\n",
      "epoch 11 | loss: 1.89789 | val_0_rmse: 2.05095 |  0:00:02s\n",
      "epoch 12 | loss: 0.98062 | val_0_rmse: 1.60249 |  0:00:03s\n",
      "epoch 13 | loss: 0.64477 | val_0_rmse: 1.37893 |  0:00:03s\n",
      "epoch 14 | loss: 0.72135 | val_0_rmse: 1.14593 |  0:00:03s\n",
      "epoch 15 | loss: 0.56364 | val_0_rmse: 0.76753 |  0:00:03s\n",
      "epoch 16 | loss: 0.45175 | val_0_rmse: 0.89583 |  0:00:04s\n",
      "epoch 17 | loss: 0.37661 | val_0_rmse: 0.70745 |  0:00:04s\n",
      "epoch 18 | loss: 0.31262 | val_0_rmse: 1.0198  |  0:00:04s\n",
      "epoch 19 | loss: 0.23995 | val_0_rmse: 0.63045 |  0:00:04s\n",
      "epoch 20 | loss: 0.2216  | val_0_rmse: 0.58487 |  0:00:05s\n",
      "epoch 21 | loss: 0.22616 | val_0_rmse: 0.70138 |  0:00:05s\n",
      "epoch 22 | loss: 0.33424 | val_0_rmse: 0.37291 |  0:00:05s\n",
      "epoch 23 | loss: 0.25784 | val_0_rmse: 0.43699 |  0:00:05s\n",
      "epoch 24 | loss: 0.18006 | val_0_rmse: 0.35133 |  0:00:05s\n",
      "epoch 25 | loss: 0.146   | val_0_rmse: 0.42577 |  0:00:06s\n",
      "epoch 26 | loss: 0.16065 | val_0_rmse: 0.46303 |  0:00:06s\n",
      "epoch 27 | loss: 0.15484 | val_0_rmse: 0.39375 |  0:00:06s\n",
      "epoch 28 | loss: 0.13296 | val_0_rmse: 0.38319 |  0:00:06s\n",
      "epoch 29 | loss: 0.13376 | val_0_rmse: 0.35338 |  0:00:07s\n",
      "epoch 30 | loss: 0.13059 | val_0_rmse: 0.47243 |  0:00:07s\n",
      "epoch 31 | loss: 0.13177 | val_0_rmse: 0.38722 |  0:00:07s\n",
      "epoch 32 | loss: 0.0987  | val_0_rmse: 0.37436 |  0:00:07s\n",
      "epoch 33 | loss: 0.16072 | val_0_rmse: 0.37567 |  0:00:07s\n",
      "epoch 34 | loss: 0.09477 | val_0_rmse: 0.35221 |  0:00:08s\n",
      "epoch 35 | loss: 0.09495 | val_0_rmse: 0.30548 |  0:00:08s\n",
      "epoch 36 | loss: 0.10562 | val_0_rmse: 0.32608 |  0:00:08s\n",
      "epoch 37 | loss: 0.09362 | val_0_rmse: 0.31207 |  0:00:08s\n",
      "epoch 38 | loss: 0.10226 | val_0_rmse: 0.30294 |  0:00:09s\n",
      "epoch 39 | loss: 0.1164  | val_0_rmse: 0.36512 |  0:00:09s\n",
      "epoch 40 | loss: 0.09226 | val_0_rmse: 0.29275 |  0:00:09s\n",
      "epoch 41 | loss: 0.10024 | val_0_rmse: 0.2971  |  0:00:09s\n",
      "epoch 42 | loss: 0.08285 | val_0_rmse: 0.37619 |  0:00:09s\n",
      "epoch 43 | loss: 0.11316 | val_0_rmse: 0.29481 |  0:00:10s\n",
      "epoch 44 | loss: 0.08782 | val_0_rmse: 0.35206 |  0:00:10s\n",
      "epoch 45 | loss: 0.07057 | val_0_rmse: 0.32985 |  0:00:10s\n",
      "epoch 46 | loss: 0.08412 | val_0_rmse: 0.30686 |  0:00:11s\n",
      "epoch 47 | loss: 0.06549 | val_0_rmse: 0.36019 |  0:00:11s\n",
      "epoch 48 | loss: 0.09849 | val_0_rmse: 0.30426 |  0:00:11s\n",
      "epoch 49 | loss: 0.08051 | val_0_rmse: 0.33661 |  0:00:11s\n",
      "epoch 50 | loss: 0.06876 | val_0_rmse: 0.27375 |  0:00:11s\n",
      "epoch 51 | loss: 0.06055 | val_0_rmse: 0.3135  |  0:00:12s\n",
      "epoch 52 | loss: 0.0864  | val_0_rmse: 0.2647  |  0:00:12s\n",
      "epoch 53 | loss: 0.0646  | val_0_rmse: 0.27199 |  0:00:12s\n",
      "epoch 54 | loss: 0.06219 | val_0_rmse: 0.25969 |  0:00:12s\n",
      "epoch 55 | loss: 0.06129 | val_0_rmse: 0.27351 |  0:00:13s\n",
      "epoch 56 | loss: 0.06293 | val_0_rmse: 0.30285 |  0:00:13s\n",
      "epoch 57 | loss: 0.06387 | val_0_rmse: 0.24249 |  0:00:13s\n",
      "epoch 58 | loss: 0.06469 | val_0_rmse: 0.31185 |  0:00:13s\n",
      "epoch 59 | loss: 0.0648  | val_0_rmse: 0.24545 |  0:00:14s\n",
      "epoch 60 | loss: 0.06157 | val_0_rmse: 0.24486 |  0:00:14s\n",
      "epoch 61 | loss: 0.06284 | val_0_rmse: 0.27781 |  0:00:14s\n",
      "epoch 62 | loss: 0.05123 | val_0_rmse: 0.23193 |  0:00:14s\n",
      "epoch 63 | loss: 0.05486 | val_0_rmse: 0.27532 |  0:00:14s\n",
      "epoch 64 | loss: 0.07871 | val_0_rmse: 0.29826 |  0:00:15s\n",
      "epoch 65 | loss: 0.10923 | val_0_rmse: 0.24809 |  0:00:15s\n",
      "epoch 66 | loss: 0.07733 | val_0_rmse: 0.27369 |  0:00:15s\n",
      "epoch 67 | loss: 0.06764 | val_0_rmse: 0.26133 |  0:00:15s\n",
      "epoch 68 | loss: 0.06787 | val_0_rmse: 0.23544 |  0:00:16s\n",
      "epoch 69 | loss: 0.06362 | val_0_rmse: 0.301   |  0:00:16s\n",
      "epoch 70 | loss: 0.06583 | val_0_rmse: 0.23212 |  0:00:16s\n",
      "epoch 71 | loss: 0.05864 | val_0_rmse: 0.24747 |  0:00:16s\n",
      "epoch 72 | loss: 0.05214 | val_0_rmse: 0.24845 |  0:00:17s\n",
      "epoch 73 | loss: 0.05367 | val_0_rmse: 0.23988 |  0:00:17s\n",
      "epoch 74 | loss: 0.05321 | val_0_rmse: 0.25232 |  0:00:17s\n",
      "epoch 75 | loss: 0.05035 | val_0_rmse: 0.23389 |  0:00:17s\n",
      "epoch 76 | loss: 0.04504 | val_0_rmse: 0.24289 |  0:00:17s\n",
      "epoch 77 | loss: 0.04617 | val_0_rmse: 0.22077 |  0:00:18s\n",
      "epoch 78 | loss: 0.04341 | val_0_rmse: 0.23177 |  0:00:18s\n",
      "epoch 79 | loss: 0.04138 | val_0_rmse: 0.22443 |  0:00:18s\n",
      "epoch 80 | loss: 0.04652 | val_0_rmse: 0.24078 |  0:00:18s\n",
      "epoch 81 | loss: 0.04748 | val_0_rmse: 0.24122 |  0:00:19s\n",
      "epoch 82 | loss: 0.04778 | val_0_rmse: 0.22709 |  0:00:19s\n",
      "epoch 83 | loss: 0.04379 | val_0_rmse: 0.2694  |  0:00:19s\n",
      "epoch 84 | loss: 0.06505 | val_0_rmse: 0.24784 |  0:00:19s\n",
      "epoch 85 | loss: 0.09349 | val_0_rmse: 0.22675 |  0:00:20s\n",
      "epoch 86 | loss: 0.07879 | val_0_rmse: 0.2906  |  0:00:20s\n",
      "epoch 87 | loss: 0.06268 | val_0_rmse: 0.23208 |  0:00:20s\n",
      "epoch 88 | loss: 0.05634 | val_0_rmse: 0.22355 |  0:00:20s\n",
      "epoch 89 | loss: 0.06226 | val_0_rmse: 0.25732 |  0:00:20s\n",
      "epoch 90 | loss: 0.06042 | val_0_rmse: 0.24284 |  0:00:21s\n",
      "epoch 91 | loss: 0.05822 | val_0_rmse: 0.23257 |  0:00:21s\n",
      "epoch 92 | loss: 0.05214 | val_0_rmse: 0.24887 |  0:00:21s\n",
      "epoch 93 | loss: 0.05633 | val_0_rmse: 0.23078 |  0:00:21s\n",
      "epoch 94 | loss: 0.05255 | val_0_rmse: 0.23955 |  0:00:22s\n",
      "epoch 95 | loss: 0.056   | val_0_rmse: 0.23499 |  0:00:22s\n",
      "epoch 96 | loss: 0.05039 | val_0_rmse: 0.22912 |  0:00:22s\n",
      "epoch 97 | loss: 0.04783 | val_0_rmse: 0.2393  |  0:00:22s\n",
      "epoch 98 | loss: 0.04787 | val_0_rmse: 0.23972 |  0:00:23s\n",
      "epoch 99 | loss: 0.04808 | val_0_rmse: 0.236   |  0:00:23s\n",
      "epoch 100| loss: 0.04861 | val_0_rmse: 0.23623 |  0:00:23s\n",
      "epoch 101| loss: 0.05059 | val_0_rmse: 0.23361 |  0:00:23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:45:23,143] Trial 82 finished with value: 0.22076778823988163 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.4315201663264694, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.017999643555123353, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 102| loss: 0.05128 | val_0_rmse: 0.244   |  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 102 with best_epoch = 77 and best_val_0_rmse = 0.22077\n",
      "Trial 082 | rmse_log=0.22077 | RMSE$=46,544 | MAE$=28,540 | MAPE=16.98% | n_d/n_a=24/16 steps=3 lr=0.01800 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 127.1089| val_0_rmse: 11.11736|  0:00:00s\n",
      "epoch 1  | loss: 94.35444| val_0_rmse: 10.34535|  0:00:00s\n",
      "epoch 2  | loss: 70.39669| val_0_rmse: 9.46231 |  0:00:00s\n",
      "epoch 3  | loss: 50.32312| val_0_rmse: 8.50405 |  0:00:00s\n",
      "epoch 4  | loss: 34.78933| val_0_rmse: 7.41915 |  0:00:01s\n",
      "epoch 5  | loss: 22.78875| val_0_rmse: 6.27563 |  0:00:01s\n",
      "epoch 6  | loss: 17.02758| val_0_rmse: 5.18879 |  0:00:01s\n",
      "epoch 7  | loss: 13.91374| val_0_rmse: 4.11238 |  0:00:01s\n",
      "epoch 8  | loss: 12.84162| val_0_rmse: 3.55501 |  0:00:02s\n",
      "epoch 9  | loss: 8.32718 | val_0_rmse: 3.36766 |  0:00:02s\n",
      "epoch 10 | loss: 4.86403 | val_0_rmse: 3.43302 |  0:00:02s\n",
      "epoch 11 | loss: 3.82239 | val_0_rmse: 3.17561 |  0:00:02s\n",
      "epoch 12 | loss: 2.88212 | val_0_rmse: 2.25321 |  0:00:03s\n",
      "epoch 13 | loss: 2.03678 | val_0_rmse: 1.72415 |  0:00:03s\n",
      "epoch 14 | loss: 1.44974 | val_0_rmse: 2.0086  |  0:00:03s\n",
      "epoch 15 | loss: 1.45047 | val_0_rmse: 1.52347 |  0:00:03s\n",
      "epoch 16 | loss: 0.97799 | val_0_rmse: 0.97562 |  0:00:04s\n",
      "epoch 17 | loss: 0.689   | val_0_rmse: 1.13079 |  0:00:04s\n",
      "epoch 18 | loss: 0.69578 | val_0_rmse: 0.65463 |  0:00:04s\n",
      "epoch 19 | loss: 0.64256 | val_0_rmse: 0.72055 |  0:00:04s\n",
      "epoch 20 | loss: 0.66276 | val_0_rmse: 0.78374 |  0:00:05s\n",
      "epoch 21 | loss: 0.47883 | val_0_rmse: 0.65693 |  0:00:05s\n",
      "epoch 22 | loss: 0.38056 | val_0_rmse: 0.57028 |  0:00:05s\n",
      "epoch 23 | loss: 0.35568 | val_0_rmse: 0.67674 |  0:00:05s\n",
      "epoch 24 | loss: 0.42566 | val_0_rmse: 0.7456  |  0:00:05s\n",
      "epoch 25 | loss: 0.32638 | val_0_rmse: 0.63677 |  0:00:06s\n",
      "epoch 26 | loss: 0.29459 | val_0_rmse: 0.62115 |  0:00:06s\n",
      "epoch 27 | loss: 0.26587 | val_0_rmse: 0.4864  |  0:00:06s\n",
      "epoch 28 | loss: 0.30069 | val_0_rmse: 0.86043 |  0:00:06s\n",
      "epoch 29 | loss: 0.55325 | val_0_rmse: 0.99428 |  0:00:07s\n",
      "epoch 30 | loss: 0.48703 | val_0_rmse: 0.45315 |  0:00:07s\n",
      "epoch 31 | loss: 0.36086 | val_0_rmse: 0.56579 |  0:00:07s\n",
      "epoch 32 | loss: 0.192   | val_0_rmse: 0.5816  |  0:00:07s\n",
      "epoch 33 | loss: 0.21962 | val_0_rmse: 0.4346  |  0:00:07s\n",
      "epoch 34 | loss: 0.19558 | val_0_rmse: 0.56275 |  0:00:08s\n",
      "epoch 35 | loss: 0.17656 | val_0_rmse: 0.54994 |  0:00:08s\n",
      "epoch 36 | loss: 0.18845 | val_0_rmse: 0.50408 |  0:00:08s\n",
      "epoch 37 | loss: 0.19116 | val_0_rmse: 0.46951 |  0:00:08s\n",
      "epoch 38 | loss: 0.17266 | val_0_rmse: 0.44081 |  0:00:09s\n",
      "epoch 39 | loss: 0.14243 | val_0_rmse: 0.4751  |  0:00:09s\n",
      "epoch 40 | loss: 0.13594 | val_0_rmse: 0.43895 |  0:00:09s\n",
      "epoch 41 | loss: 0.11586 | val_0_rmse: 0.3803  |  0:00:09s\n",
      "epoch 42 | loss: 0.09895 | val_0_rmse: 0.38185 |  0:00:10s\n",
      "epoch 43 | loss: 0.11171 | val_0_rmse: 0.40895 |  0:00:10s\n",
      "epoch 44 | loss: 0.12637 | val_0_rmse: 0.31673 |  0:00:10s\n",
      "epoch 45 | loss: 0.12291 | val_0_rmse: 0.37789 |  0:00:10s\n",
      "epoch 46 | loss: 0.10386 | val_0_rmse: 0.3059  |  0:00:10s\n",
      "epoch 47 | loss: 0.1017  | val_0_rmse: 0.32867 |  0:00:11s\n",
      "epoch 48 | loss: 0.10203 | val_0_rmse: 0.39588 |  0:00:11s\n",
      "epoch 49 | loss: 0.12591 | val_0_rmse: 0.28679 |  0:00:11s\n",
      "epoch 50 | loss: 0.10597 | val_0_rmse: 0.28318 |  0:00:11s\n",
      "epoch 51 | loss: 0.07899 | val_0_rmse: 0.32264 |  0:00:12s\n",
      "epoch 52 | loss: 0.09134 | val_0_rmse: 0.28084 |  0:00:12s\n",
      "epoch 53 | loss: 0.09138 | val_0_rmse: 0.29566 |  0:00:12s\n",
      "epoch 54 | loss: 0.08275 | val_0_rmse: 0.31066 |  0:00:12s\n",
      "epoch 55 | loss: 0.08903 | val_0_rmse: 0.31403 |  0:00:12s\n",
      "epoch 56 | loss: 0.09779 | val_0_rmse: 0.30119 |  0:00:13s\n",
      "epoch 57 | loss: 0.08909 | val_0_rmse: 0.29554 |  0:00:13s\n",
      "epoch 58 | loss: 0.07788 | val_0_rmse: 0.31214 |  0:00:13s\n",
      "epoch 59 | loss: 0.08776 | val_0_rmse: 0.31774 |  0:00:13s\n",
      "epoch 60 | loss: 0.08934 | val_0_rmse: 0.29276 |  0:00:13s\n",
      "epoch 61 | loss: 0.08084 | val_0_rmse: 0.31217 |  0:00:14s\n",
      "epoch 62 | loss: 0.0869  | val_0_rmse: 0.29453 |  0:00:14s\n",
      "epoch 63 | loss: 0.09156 | val_0_rmse: 0.27916 |  0:00:14s\n",
      "epoch 64 | loss: 0.0748  | val_0_rmse: 0.31576 |  0:00:14s\n",
      "epoch 65 | loss: 0.07421 | val_0_rmse: 0.27559 |  0:00:15s\n",
      "epoch 66 | loss: 0.07493 | val_0_rmse: 0.28167 |  0:00:15s\n",
      "epoch 67 | loss: 0.06816 | val_0_rmse: 0.29007 |  0:00:15s\n",
      "epoch 68 | loss: 0.06355 | val_0_rmse: 0.26718 |  0:00:15s\n",
      "epoch 69 | loss: 0.06486 | val_0_rmse: 0.27125 |  0:00:16s\n",
      "epoch 70 | loss: 0.07298 | val_0_rmse: 0.29929 |  0:00:16s\n",
      "epoch 71 | loss: 0.07005 | val_0_rmse: 0.28242 |  0:00:16s\n",
      "epoch 72 | loss: 0.0661  | val_0_rmse: 0.26464 |  0:00:16s\n",
      "epoch 73 | loss: 0.06037 | val_0_rmse: 0.29123 |  0:00:17s\n",
      "epoch 74 | loss: 0.06367 | val_0_rmse: 0.27352 |  0:00:17s\n",
      "epoch 75 | loss: 0.06248 | val_0_rmse: 0.24973 |  0:00:17s\n",
      "epoch 76 | loss: 0.05339 | val_0_rmse: 0.27905 |  0:00:17s\n",
      "epoch 77 | loss: 0.05283 | val_0_rmse: 0.26415 |  0:00:17s\n",
      "epoch 78 | loss: 0.0632  | val_0_rmse: 0.25963 |  0:00:18s\n",
      "epoch 79 | loss: 0.0557  | val_0_rmse: 0.2759  |  0:00:18s\n",
      "epoch 80 | loss: 0.06008 | val_0_rmse: 0.25286 |  0:00:18s\n",
      "epoch 81 | loss: 0.05028 | val_0_rmse: 0.25175 |  0:00:18s\n",
      "epoch 82 | loss: 0.05682 | val_0_rmse: 0.25356 |  0:00:19s\n",
      "epoch 83 | loss: 0.05203 | val_0_rmse: 0.24642 |  0:00:19s\n",
      "epoch 84 | loss: 0.05166 | val_0_rmse: 0.24608 |  0:00:19s\n",
      "epoch 85 | loss: 0.05058 | val_0_rmse: 0.25597 |  0:00:19s\n",
      "epoch 86 | loss: 0.0563  | val_0_rmse: 0.26148 |  0:00:19s\n",
      "epoch 87 | loss: 0.05638 | val_0_rmse: 0.23104 |  0:00:20s\n",
      "epoch 88 | loss: 0.05301 | val_0_rmse: 0.24083 |  0:00:20s\n",
      "epoch 89 | loss: 0.04892 | val_0_rmse: 0.24322 |  0:00:20s\n",
      "epoch 90 | loss: 0.05012 | val_0_rmse: 0.23803 |  0:00:20s\n",
      "epoch 91 | loss: 0.05487 | val_0_rmse: 0.25588 |  0:00:21s\n",
      "epoch 92 | loss: 0.05616 | val_0_rmse: 0.23284 |  0:00:21s\n",
      "epoch 93 | loss: 0.04892 | val_0_rmse: 0.23114 |  0:00:21s\n",
      "epoch 94 | loss: 0.0395  | val_0_rmse: 0.23132 |  0:00:21s\n",
      "epoch 95 | loss: 0.04443 | val_0_rmse: 0.22079 |  0:00:21s\n",
      "epoch 96 | loss: 0.04267 | val_0_rmse: 0.25396 |  0:00:22s\n",
      "epoch 97 | loss: 0.05165 | val_0_rmse: 0.23174 |  0:00:22s\n",
      "epoch 98 | loss: 0.04537 | val_0_rmse: 0.23214 |  0:00:22s\n",
      "epoch 99 | loss: 0.04687 | val_0_rmse: 0.22444 |  0:00:22s\n",
      "epoch 100| loss: 0.04698 | val_0_rmse: 0.22919 |  0:00:23s\n",
      "epoch 101| loss: 0.04415 | val_0_rmse: 0.24903 |  0:00:23s\n",
      "epoch 102| loss: 0.04486 | val_0_rmse: 0.24988 |  0:00:23s\n",
      "epoch 103| loss: 0.05315 | val_0_rmse: 0.25229 |  0:00:23s\n",
      "epoch 104| loss: 0.05305 | val_0_rmse: 0.28037 |  0:00:23s\n",
      "epoch 105| loss: 0.06802 | val_0_rmse: 0.24557 |  0:00:24s\n",
      "epoch 106| loss: 0.04994 | val_0_rmse: 0.28069 |  0:00:24s\n",
      "epoch 107| loss: 0.05069 | val_0_rmse: 0.2704  |  0:00:24s\n",
      "epoch 108| loss: 0.06553 | val_0_rmse: 0.28271 |  0:00:24s\n",
      "epoch 109| loss: 0.10229 | val_0_rmse: 0.24014 |  0:00:25s\n",
      "epoch 110| loss: 0.06524 | val_0_rmse: 0.27178 |  0:00:25s\n",
      "epoch 111| loss: 0.06335 | val_0_rmse: 0.28333 |  0:00:25s\n",
      "epoch 112| loss: 0.06097 | val_0_rmse: 0.26078 |  0:00:25s\n",
      "epoch 113| loss: 0.05359 | val_0_rmse: 0.24361 |  0:00:25s\n",
      "epoch 114| loss: 0.04882 | val_0_rmse: 0.24928 |  0:00:26s\n",
      "epoch 115| loss: 0.0499  | val_0_rmse: 0.29594 |  0:00:26s\n",
      "epoch 116| loss: 0.07329 | val_0_rmse: 0.24652 |  0:00:26s\n",
      "epoch 117| loss: 0.06632 | val_0_rmse: 0.2483  |  0:00:26s\n",
      "epoch 118| loss: 0.05108 | val_0_rmse: 0.25002 |  0:00:26s\n",
      "epoch 119| loss: 0.07256 | val_0_rmse: 0.24547 |  0:00:27s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:45:50,863] Trial 83 finished with value: 0.22079266933047714 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.639813885519596, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.014138367561656005, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 120| loss: 0.04397 | val_0_rmse: 0.23031 |  0:00:27s\n",
      "\n",
      "Early stopping occurred at epoch 120 with best_epoch = 95 and best_val_0_rmse = 0.22079\n",
      "Trial 083 | rmse_log=0.22079 | RMSE$=48,455 | MAE$=29,470 | MAPE=17.10% | n_d/n_a=24/16 steps=3 lr=0.01414 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 120.65859| val_0_rmse: 11.0845 |  0:00:00s\n",
      "epoch 1  | loss: 80.9636 | val_0_rmse: 9.93458 |  0:00:00s\n",
      "epoch 2  | loss: 50.43545| val_0_rmse: 8.36585 |  0:00:00s\n",
      "epoch 3  | loss: 30.00916| val_0_rmse: 6.49796 |  0:00:01s\n",
      "epoch 4  | loss: 20.52426| val_0_rmse: 4.66596 |  0:00:01s\n",
      "epoch 5  | loss: 17.55085| val_0_rmse: 3.3844  |  0:00:01s\n",
      "epoch 6  | loss: 13.15413| val_0_rmse: 2.86497 |  0:00:01s\n",
      "epoch 7  | loss: 9.60296 | val_0_rmse: 2.98908 |  0:00:01s\n",
      "epoch 8  | loss: 4.32132 | val_0_rmse: 3.10817 |  0:00:02s\n",
      "epoch 9  | loss: 3.99886 | val_0_rmse: 2.43516 |  0:00:02s\n",
      "epoch 10 | loss: 2.05801 | val_0_rmse: 1.27427 |  0:00:02s\n",
      "epoch 11 | loss: 1.68417 | val_0_rmse: 1.35092 |  0:00:02s\n",
      "epoch 12 | loss: 1.37925 | val_0_rmse: 1.112   |  0:00:03s\n",
      "epoch 13 | loss: 1.17188 | val_0_rmse: 0.80136 |  0:00:03s\n",
      "epoch 14 | loss: 0.93157 | val_0_rmse: 0.92542 |  0:00:03s\n",
      "epoch 15 | loss: 0.90097 | val_0_rmse: 0.54074 |  0:00:03s\n",
      "epoch 16 | loss: 0.82199 | val_0_rmse: 0.88138 |  0:00:03s\n",
      "epoch 17 | loss: 0.6272  | val_0_rmse: 0.65614 |  0:00:04s\n",
      "epoch 18 | loss: 0.51509 | val_0_rmse: 0.70109 |  0:00:04s\n",
      "epoch 19 | loss: 0.54273 | val_0_rmse: 0.68627 |  0:00:04s\n",
      "epoch 20 | loss: 0.5593  | val_0_rmse: 0.47533 |  0:00:04s\n",
      "epoch 21 | loss: 0.41181 | val_0_rmse: 0.73518 |  0:00:05s\n",
      "epoch 22 | loss: 0.41818 | val_0_rmse: 0.59994 |  0:00:05s\n",
      "epoch 23 | loss: 0.35637 | val_0_rmse: 0.47107 |  0:00:05s\n",
      "epoch 24 | loss: 0.33842 | val_0_rmse: 0.61225 |  0:00:05s\n",
      "epoch 25 | loss: 0.31788 | val_0_rmse: 0.37451 |  0:00:05s\n",
      "epoch 26 | loss: 0.30369 | val_0_rmse: 0.60111 |  0:00:06s\n",
      "epoch 27 | loss: 0.31395 | val_0_rmse: 0.50365 |  0:00:06s\n",
      "epoch 28 | loss: 0.29556 | val_0_rmse: 0.39415 |  0:00:06s\n",
      "epoch 29 | loss: 0.19717 | val_0_rmse: 0.51858 |  0:00:06s\n",
      "epoch 30 | loss: 0.21961 | val_0_rmse: 0.49032 |  0:00:07s\n",
      "epoch 31 | loss: 0.20029 | val_0_rmse: 0.36968 |  0:00:07s\n",
      "epoch 32 | loss: 0.17511 | val_0_rmse: 0.37997 |  0:00:07s\n",
      "epoch 33 | loss: 0.1647  | val_0_rmse: 0.46629 |  0:00:07s\n",
      "epoch 34 | loss: 0.14886 | val_0_rmse: 0.3843  |  0:00:07s\n",
      "epoch 35 | loss: 0.16118 | val_0_rmse: 0.36854 |  0:00:08s\n",
      "epoch 36 | loss: 0.15307 | val_0_rmse: 0.51359 |  0:00:08s\n",
      "epoch 37 | loss: 0.19619 | val_0_rmse: 0.34849 |  0:00:08s\n",
      "epoch 38 | loss: 0.15896 | val_0_rmse: 0.45771 |  0:00:08s\n",
      "epoch 39 | loss: 0.14382 | val_0_rmse: 0.30723 |  0:00:09s\n",
      "epoch 40 | loss: 0.17615 | val_0_rmse: 0.37214 |  0:00:09s\n",
      "epoch 41 | loss: 0.12155 | val_0_rmse: 0.31089 |  0:00:09s\n",
      "epoch 42 | loss: 0.12777 | val_0_rmse: 0.37279 |  0:00:09s\n",
      "epoch 43 | loss: 0.16229 | val_0_rmse: 0.2858  |  0:00:10s\n",
      "epoch 44 | loss: 0.10795 | val_0_rmse: 0.33619 |  0:00:10s\n",
      "epoch 45 | loss: 0.09119 | val_0_rmse: 0.27275 |  0:00:10s\n",
      "epoch 46 | loss: 0.11977 | val_0_rmse: 0.30879 |  0:00:10s\n",
      "epoch 47 | loss: 0.10126 | val_0_rmse: 0.26535 |  0:00:11s\n",
      "epoch 48 | loss: 0.09342 | val_0_rmse: 0.27088 |  0:00:11s\n",
      "epoch 49 | loss: 0.10855 | val_0_rmse: 0.29351 |  0:00:11s\n",
      "epoch 50 | loss: 0.0872  | val_0_rmse: 0.28313 |  0:00:11s\n",
      "epoch 51 | loss: 0.08794 | val_0_rmse: 0.29133 |  0:00:11s\n",
      "epoch 52 | loss: 0.08567 | val_0_rmse: 0.33297 |  0:00:12s\n",
      "epoch 53 | loss: 0.13094 | val_0_rmse: 0.30775 |  0:00:12s\n",
      "epoch 54 | loss: 0.12358 | val_0_rmse: 0.2998  |  0:00:12s\n",
      "epoch 55 | loss: 0.0785  | val_0_rmse: 0.28279 |  0:00:12s\n",
      "epoch 56 | loss: 0.07417 | val_0_rmse: 0.30461 |  0:00:13s\n",
      "epoch 57 | loss: 0.08717 | val_0_rmse: 0.28801 |  0:00:13s\n",
      "epoch 58 | loss: 0.08471 | val_0_rmse: 0.30959 |  0:00:13s\n",
      "epoch 59 | loss: 0.08296 | val_0_rmse: 0.26388 |  0:00:13s\n",
      "epoch 60 | loss: 0.06941 | val_0_rmse: 0.26375 |  0:00:14s\n",
      "epoch 61 | loss: 0.06287 | val_0_rmse: 0.25212 |  0:00:14s\n",
      "epoch 62 | loss: 0.06137 | val_0_rmse: 0.25395 |  0:00:14s\n",
      "epoch 63 | loss: 0.05911 | val_0_rmse: 0.28508 |  0:00:14s\n",
      "epoch 64 | loss: 0.07676 | val_0_rmse: 0.26519 |  0:00:14s\n",
      "epoch 65 | loss: 0.05723 | val_0_rmse: 0.25815 |  0:00:15s\n",
      "epoch 66 | loss: 0.05313 | val_0_rmse: 0.252   |  0:00:15s\n",
      "epoch 67 | loss: 0.06054 | val_0_rmse: 0.2665  |  0:00:15s\n",
      "epoch 68 | loss: 0.06562 | val_0_rmse: 0.24834 |  0:00:15s\n",
      "epoch 69 | loss: 0.06094 | val_0_rmse: 0.25546 |  0:00:15s\n",
      "epoch 70 | loss: 0.05691 | val_0_rmse: 0.27435 |  0:00:16s\n",
      "epoch 71 | loss: 0.06897 | val_0_rmse: 0.27581 |  0:00:16s\n",
      "epoch 72 | loss: 0.07688 | val_0_rmse: 0.26512 |  0:00:16s\n",
      "epoch 73 | loss: 0.06521 | val_0_rmse: 0.25851 |  0:00:16s\n",
      "epoch 74 | loss: 0.05813 | val_0_rmse: 0.26378 |  0:00:17s\n",
      "epoch 75 | loss: 0.06472 | val_0_rmse: 0.2522  |  0:00:17s\n",
      "epoch 76 | loss: 0.05328 | val_0_rmse: 0.23825 |  0:00:17s\n",
      "epoch 77 | loss: 0.05402 | val_0_rmse: 0.25558 |  0:00:17s\n",
      "epoch 78 | loss: 0.05653 | val_0_rmse: 0.24701 |  0:00:18s\n",
      "epoch 79 | loss: 0.05242 | val_0_rmse: 0.25064 |  0:00:18s\n",
      "epoch 80 | loss: 0.05668 | val_0_rmse: 0.24114 |  0:00:18s\n",
      "epoch 81 | loss: 0.04457 | val_0_rmse: 0.26064 |  0:00:18s\n",
      "epoch 82 | loss: 0.05545 | val_0_rmse: 0.25713 |  0:00:18s\n",
      "epoch 83 | loss: 0.05755 | val_0_rmse: 0.27257 |  0:00:19s\n",
      "epoch 84 | loss: 0.0614  | val_0_rmse: 0.26327 |  0:00:19s\n",
      "epoch 85 | loss: 0.05242 | val_0_rmse: 0.24823 |  0:00:19s\n",
      "epoch 86 | loss: 0.05385 | val_0_rmse: 0.26465 |  0:00:19s\n",
      "epoch 87 | loss: 0.06134 | val_0_rmse: 0.25794 |  0:00:19s\n",
      "epoch 88 | loss: 0.04925 | val_0_rmse: 0.27375 |  0:00:20s\n",
      "epoch 89 | loss: 0.0681  | val_0_rmse: 0.2692  |  0:00:20s\n",
      "epoch 90 | loss: 0.07088 | val_0_rmse: 0.28804 |  0:00:20s\n",
      "epoch 91 | loss: 0.06495 | val_0_rmse: 0.29258 |  0:00:20s\n",
      "epoch 92 | loss: 0.08489 | val_0_rmse: 0.25051 |  0:00:21s\n",
      "epoch 93 | loss: 0.065   | val_0_rmse: 0.24281 |  0:00:21s\n",
      "epoch 94 | loss: 0.04751 | val_0_rmse: 0.26981 |  0:00:21s\n",
      "epoch 95 | loss: 0.04787 | val_0_rmse: 0.24015 |  0:00:21s\n",
      "epoch 96 | loss: 0.04283 | val_0_rmse: 0.23889 |  0:00:21s\n",
      "epoch 97 | loss: 0.04294 | val_0_rmse: 0.24754 |  0:00:22s\n",
      "epoch 98 | loss: 0.03863 | val_0_rmse: 0.24021 |  0:00:22s\n",
      "epoch 99 | loss: 0.04137 | val_0_rmse: 0.23628 |  0:00:22s\n",
      "epoch 100| loss: 0.04662 | val_0_rmse: 0.23893 |  0:00:22s\n",
      "epoch 101| loss: 0.04053 | val_0_rmse: 0.23308 |  0:00:23s\n",
      "epoch 102| loss: 0.04169 | val_0_rmse: 0.26511 |  0:00:23s\n",
      "epoch 103| loss: 0.04936 | val_0_rmse: 0.23672 |  0:00:23s\n",
      "epoch 104| loss: 0.05912 | val_0_rmse: 0.22712 |  0:00:23s\n",
      "epoch 105| loss: 0.06073 | val_0_rmse: 0.25357 |  0:00:23s\n",
      "epoch 106| loss: 0.04767 | val_0_rmse: 0.23624 |  0:00:24s\n",
      "epoch 107| loss: 0.04401 | val_0_rmse: 0.28341 |  0:00:24s\n",
      "epoch 108| loss: 0.06699 | val_0_rmse: 0.22355 |  0:00:24s\n",
      "epoch 109| loss: 0.0553  | val_0_rmse: 0.25104 |  0:00:24s\n",
      "epoch 110| loss: 0.05266 | val_0_rmse: 0.24917 |  0:00:25s\n",
      "epoch 111| loss: 0.05511 | val_0_rmse: 0.24204 |  0:00:25s\n",
      "epoch 112| loss: 0.04871 | val_0_rmse: 0.24804 |  0:00:25s\n",
      "epoch 113| loss: 0.05355 | val_0_rmse: 0.21936 |  0:00:25s\n",
      "epoch 114| loss: 0.05632 | val_0_rmse: 0.25423 |  0:00:26s\n",
      "epoch 115| loss: 0.04535 | val_0_rmse: 0.22375 |  0:00:26s\n",
      "epoch 116| loss: 0.04223 | val_0_rmse: 0.22259 |  0:00:26s\n",
      "epoch 117| loss: 0.03257 | val_0_rmse: 0.22159 |  0:00:26s\n",
      "epoch 118| loss: 0.03544 | val_0_rmse: 0.21515 |  0:00:26s\n",
      "epoch 119| loss: 0.0338  | val_0_rmse: 0.22824 |  0:00:27s\n",
      "epoch 120| loss: 0.03907 | val_0_rmse: 0.22549 |  0:00:27s\n",
      "epoch 121| loss: 0.04207 | val_0_rmse: 0.21523 |  0:00:27s\n",
      "epoch 122| loss: 0.03634 | val_0_rmse: 0.2636  |  0:00:27s\n",
      "epoch 123| loss: 0.08001 | val_0_rmse: 0.24937 |  0:00:27s\n",
      "epoch 124| loss: 0.05302 | val_0_rmse: 0.22643 |  0:00:28s\n",
      "epoch 125| loss: 0.04095 | val_0_rmse: 0.2831  |  0:00:28s\n",
      "epoch 126| loss: 0.08482 | val_0_rmse: 0.2242  |  0:00:28s\n",
      "epoch 127| loss: 0.0423  | val_0_rmse: 0.21632 |  0:00:28s\n",
      "epoch 128| loss: 0.03886 | val_0_rmse: 0.22311 |  0:00:29s\n",
      "epoch 129| loss: 0.04649 | val_0_rmse: 0.28577 |  0:00:29s\n",
      "epoch 130| loss: 0.08017 | val_0_rmse: 0.21707 |  0:00:29s\n",
      "epoch 131| loss: 0.03847 | val_0_rmse: 0.20751 |  0:00:29s\n",
      "epoch 132| loss: 0.03277 | val_0_rmse: 0.2096  |  0:00:30s\n",
      "epoch 133| loss: 0.03926 | val_0_rmse: 0.25457 |  0:00:30s\n",
      "epoch 134| loss: 0.04496 | val_0_rmse: 0.21838 |  0:00:30s\n",
      "epoch 135| loss: 0.02938 | val_0_rmse: 0.20885 |  0:00:30s\n",
      "epoch 136| loss: 0.0356  | val_0_rmse: 0.20308 |  0:00:30s\n",
      "epoch 137| loss: 0.03864 | val_0_rmse: 0.235   |  0:00:31s\n",
      "epoch 138| loss: 0.03599 | val_0_rmse: 0.19827 |  0:00:31s\n",
      "epoch 139| loss: 0.02871 | val_0_rmse: 0.19992 |  0:00:31s\n",
      "epoch 140| loss: 0.02918 | val_0_rmse: 0.24245 |  0:00:31s\n",
      "epoch 141| loss: 0.04821 | val_0_rmse: 0.22067 |  0:00:32s\n",
      "epoch 142| loss: 0.03933 | val_0_rmse: 0.22998 |  0:00:32s\n",
      "epoch 143| loss: 0.03705 | val_0_rmse: 0.21594 |  0:00:32s\n",
      "epoch 144| loss: 0.03815 | val_0_rmse: 0.20468 |  0:00:32s\n",
      "epoch 145| loss: 0.03716 | val_0_rmse: 0.22491 |  0:00:32s\n",
      "epoch 146| loss: 0.03518 | val_0_rmse: 0.21362 |  0:00:33s\n",
      "epoch 147| loss: 0.03216 | val_0_rmse: 0.22342 |  0:00:33s\n",
      "epoch 148| loss: 0.0338  | val_0_rmse: 0.20655 |  0:00:33s\n",
      "epoch 149| loss: 0.03081 | val_0_rmse: 0.22817 |  0:00:33s\n",
      "epoch 150| loss: 0.03911 | val_0_rmse: 0.23617 |  0:00:33s\n",
      "epoch 151| loss: 0.04982 | val_0_rmse: 0.25664 |  0:00:34s\n",
      "epoch 152| loss: 0.04642 | val_0_rmse: 0.21904 |  0:00:34s\n",
      "epoch 153| loss: 0.04191 | val_0_rmse: 0.20754 |  0:00:34s\n",
      "epoch 154| loss: 0.02765 | val_0_rmse: 0.23187 |  0:00:34s\n",
      "epoch 155| loss: 0.0338  | val_0_rmse: 0.2     |  0:00:35s\n",
      "epoch 156| loss: 0.02876 | val_0_rmse: 0.21585 |  0:00:35s\n",
      "epoch 157| loss: 0.03274 | val_0_rmse: 0.24397 |  0:00:35s\n",
      "epoch 158| loss: 0.03379 | val_0_rmse: 0.20092 |  0:00:35s\n",
      "epoch 159| loss: 0.02693 | val_0_rmse: 0.20383 |  0:00:35s\n",
      "epoch 160| loss: 0.03128 | val_0_rmse: 0.21807 |  0:00:36s\n",
      "epoch 161| loss: 0.03377 | val_0_rmse: 0.20702 |  0:00:36s\n",
      "epoch 162| loss: 0.02983 | val_0_rmse: 0.22311 |  0:00:36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:46:28,005] Trial 84 finished with value: 0.19827055839835328 and parameters: {'n_d': 32, 'n_a': 64, 'n_steps': 3, 'gamma': 1.804825431227398, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.015433970112865668, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 163| loss: 0.04604 | val_0_rmse: 0.25514 |  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 163 with best_epoch = 138 and best_val_0_rmse = 0.19827\n",
      "Trial 084 | rmse_log=0.19827 | RMSE$=42,205 | MAE$=26,329 | MAPE=14.93% | n_d/n_a=32/64 steps=3 lr=0.01543 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 110.13545| val_0_rmse: 9.87226 |  0:00:00s\n",
      "epoch 1  | loss: 62.24913| val_0_rmse: 8.37012 |  0:00:00s\n",
      "epoch 2  | loss: 40.81289| val_0_rmse: 6.65148 |  0:00:00s\n",
      "epoch 3  | loss: 24.7452 | val_0_rmse: 5.20494 |  0:00:00s\n",
      "epoch 4  | loss: 13.68187| val_0_rmse: 4.4839  |  0:00:00s\n",
      "epoch 5  | loss: 7.47701 | val_0_rmse: 4.36833 |  0:00:01s\n",
      "epoch 6  | loss: 3.39059 | val_0_rmse: 3.53705 |  0:00:01s\n",
      "epoch 7  | loss: 1.82419 | val_0_rmse: 2.86774 |  0:00:01s\n",
      "epoch 8  | loss: 1.09993 | val_0_rmse: 2.37051 |  0:00:01s\n",
      "epoch 9  | loss: 0.71174 | val_0_rmse: 1.86721 |  0:00:01s\n",
      "epoch 10 | loss: 0.84277 | val_0_rmse: 1.68457 |  0:00:02s\n",
      "epoch 11 | loss: 0.94871 | val_0_rmse: 1.82766 |  0:00:02s\n",
      "epoch 12 | loss: 0.45055 | val_0_rmse: 1.57067 |  0:00:02s\n",
      "epoch 13 | loss: 0.39505 | val_0_rmse: 1.38073 |  0:00:02s\n",
      "epoch 14 | loss: 0.3685  | val_0_rmse: 1.21817 |  0:00:02s\n",
      "epoch 15 | loss: 0.28305 | val_0_rmse: 1.23362 |  0:00:02s\n",
      "epoch 16 | loss: 0.38383 | val_0_rmse: 0.88535 |  0:00:03s\n",
      "epoch 17 | loss: 0.34891 | val_0_rmse: 0.82345 |  0:00:03s\n",
      "epoch 18 | loss: 0.2522  | val_0_rmse: 1.00494 |  0:00:03s\n",
      "epoch 19 | loss: 0.20941 | val_0_rmse: 0.8386  |  0:00:03s\n",
      "epoch 20 | loss: 0.18762 | val_0_rmse: 0.6986  |  0:00:03s\n",
      "epoch 21 | loss: 0.14883 | val_0_rmse: 0.62801 |  0:00:04s\n",
      "epoch 22 | loss: 0.15801 | val_0_rmse: 0.87109 |  0:00:04s\n",
      "epoch 23 | loss: 0.15503 | val_0_rmse: 0.57712 |  0:00:04s\n",
      "epoch 24 | loss: 0.15308 | val_0_rmse: 0.59957 |  0:00:04s\n",
      "epoch 25 | loss: 0.17446 | val_0_rmse: 0.32365 |  0:00:04s\n",
      "epoch 26 | loss: 0.16647 | val_0_rmse: 0.42664 |  0:00:04s\n",
      "epoch 27 | loss: 0.12532 | val_0_rmse: 0.24712 |  0:00:05s\n",
      "epoch 28 | loss: 0.14024 | val_0_rmse: 0.4057  |  0:00:05s\n",
      "epoch 29 | loss: 0.12488 | val_0_rmse: 0.25145 |  0:00:05s\n",
      "epoch 30 | loss: 0.0866  | val_0_rmse: 0.36121 |  0:00:05s\n",
      "epoch 31 | loss: 0.11531 | val_0_rmse: 0.22002 |  0:00:05s\n",
      "epoch 32 | loss: 0.0947  | val_0_rmse: 0.32528 |  0:00:06s\n",
      "epoch 33 | loss: 0.10403 | val_0_rmse: 0.23254 |  0:00:06s\n",
      "epoch 34 | loss: 0.07572 | val_0_rmse: 0.25491 |  0:00:06s\n",
      "epoch 35 | loss: 0.08938 | val_0_rmse: 0.24699 |  0:00:06s\n",
      "epoch 36 | loss: 0.09368 | val_0_rmse: 0.29654 |  0:00:06s\n",
      "epoch 37 | loss: 0.0649  | val_0_rmse: 0.25222 |  0:00:06s\n",
      "epoch 38 | loss: 0.05103 | val_0_rmse: 0.2481  |  0:00:07s\n",
      "epoch 39 | loss: 0.06029 | val_0_rmse: 0.27164 |  0:00:07s\n",
      "epoch 40 | loss: 0.06615 | val_0_rmse: 0.2377  |  0:00:07s\n",
      "epoch 41 | loss: 0.05174 | val_0_rmse: 0.2095  |  0:00:07s\n",
      "epoch 42 | loss: 0.05337 | val_0_rmse: 0.2316  |  0:00:07s\n",
      "epoch 43 | loss: 0.04638 | val_0_rmse: 0.25635 |  0:00:07s\n",
      "epoch 44 | loss: 0.07444 | val_0_rmse: 0.248   |  0:00:08s\n",
      "epoch 45 | loss: 0.06308 | val_0_rmse: 0.24873 |  0:00:08s\n",
      "epoch 46 | loss: 0.05578 | val_0_rmse: 0.2385  |  0:00:08s\n",
      "epoch 47 | loss: 0.05296 | val_0_rmse: 0.22794 |  0:00:08s\n",
      "epoch 48 | loss: 0.04934 | val_0_rmse: 0.27187 |  0:00:08s\n",
      "epoch 49 | loss: 0.05499 | val_0_rmse: 0.27918 |  0:00:08s\n",
      "epoch 50 | loss: 0.11165 | val_0_rmse: 0.23217 |  0:00:09s\n",
      "epoch 51 | loss: 0.06412 | val_0_rmse: 0.26859 |  0:00:09s\n",
      "epoch 52 | loss: 0.0781  | val_0_rmse: 0.31388 |  0:00:09s\n",
      "epoch 53 | loss: 0.07385 | val_0_rmse: 0.26396 |  0:00:09s\n",
      "epoch 54 | loss: 0.06769 | val_0_rmse: 0.21823 |  0:00:09s\n",
      "epoch 55 | loss: 0.06133 | val_0_rmse: 0.27513 |  0:00:10s\n",
      "epoch 56 | loss: 0.05219 | val_0_rmse: 0.32791 |  0:00:10s\n",
      "epoch 57 | loss: 0.08943 | val_0_rmse: 0.2639  |  0:00:10s\n",
      "epoch 58 | loss: 0.06752 | val_0_rmse: 0.2318  |  0:00:10s\n",
      "epoch 59 | loss: 0.0667  | val_0_rmse: 0.30264 |  0:00:10s\n",
      "epoch 60 | loss: 0.04788 | val_0_rmse: 0.35043 |  0:00:10s\n",
      "epoch 61 | loss: 0.06655 | val_0_rmse: 0.2414  |  0:00:11s\n",
      "epoch 62 | loss: 0.0499  | val_0_rmse: 0.24025 |  0:00:11s\n",
      "epoch 63 | loss: 0.04489 | val_0_rmse: 0.29915 |  0:00:11s\n",
      "epoch 64 | loss: 0.05044 | val_0_rmse: 0.2657  |  0:00:11s\n",
      "epoch 65 | loss: 0.0421  | val_0_rmse: 0.22116 |  0:00:11s\n",
      "epoch 66 | loss: 0.04832 | val_0_rmse: 0.21582 |  0:00:11s\n",
      "\n",
      "Early stopping occurred at epoch 66 with best_epoch = 41 and best_val_0_rmse = 0.2095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:46:40,170] Trial 85 finished with value: 0.20950017116262729 and parameters: {'n_d': 48, 'n_a': 32, 'n_steps': 3, 'gamma': 1.2886494879857149, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.018857738215876878, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 085 | rmse_log=0.20950 | RMSE$=42,801 | MAE$=26,946 | MAPE=15.81% | n_d/n_a=48/32 steps=3 lr=0.01886 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 129.81454| val_0_rmse: 10.88824|  0:00:00s\n",
      "epoch 1  | loss: 97.04466| val_0_rmse: 9.69069 |  0:00:00s\n",
      "epoch 2  | loss: 71.44995| val_0_rmse: 8.27314 |  0:00:00s\n",
      "epoch 3  | loss: 46.1344 | val_0_rmse: 6.75075 |  0:00:00s\n",
      "epoch 4  | loss: 27.2533 | val_0_rmse: 5.38999 |  0:00:00s\n",
      "epoch 5  | loss: 17.67909| val_0_rmse: 4.31433 |  0:00:00s\n",
      "epoch 6  | loss: 12.63602| val_0_rmse: 3.90799 |  0:00:01s\n",
      "epoch 7  | loss: 9.0706  | val_0_rmse: 4.33231 |  0:00:01s\n",
      "epoch 8  | loss: 4.64234 | val_0_rmse: 4.80116 |  0:00:01s\n",
      "epoch 9  | loss: 3.05598 | val_0_rmse: 4.90779 |  0:00:01s\n",
      "epoch 10 | loss: 1.64356 | val_0_rmse: 4.25974 |  0:00:01s\n",
      "epoch 11 | loss: 0.85727 | val_0_rmse: 4.01389 |  0:00:01s\n",
      "epoch 12 | loss: 0.61542 | val_0_rmse: 3.60829 |  0:00:02s\n",
      "epoch 13 | loss: 0.51497 | val_0_rmse: 3.48838 |  0:00:02s\n",
      "epoch 14 | loss: 0.58461 | val_0_rmse: 3.06794 |  0:00:02s\n",
      "epoch 15 | loss: 0.39545 | val_0_rmse: 3.09231 |  0:00:02s\n",
      "epoch 16 | loss: 0.38617 | val_0_rmse: 2.60276 |  0:00:02s\n",
      "epoch 17 | loss: 0.50776 | val_0_rmse: 2.72766 |  0:00:02s\n",
      "epoch 18 | loss: 0.50014 | val_0_rmse: 2.25144 |  0:00:02s\n",
      "epoch 19 | loss: 0.43706 | val_0_rmse: 2.07787 |  0:00:03s\n",
      "epoch 20 | loss: 0.27891 | val_0_rmse: 2.24977 |  0:00:03s\n",
      "epoch 21 | loss: 0.23933 | val_0_rmse: 2.00779 |  0:00:03s\n",
      "epoch 22 | loss: 0.24177 | val_0_rmse: 1.88291 |  0:00:03s\n",
      "epoch 23 | loss: 0.26599 | val_0_rmse: 1.70337 |  0:00:03s\n",
      "epoch 24 | loss: 0.18556 | val_0_rmse: 1.65899 |  0:00:03s\n",
      "epoch 25 | loss: 0.19415 | val_0_rmse: 1.55738 |  0:00:04s\n",
      "epoch 26 | loss: 0.2014  | val_0_rmse: 1.49802 |  0:00:04s\n",
      "epoch 27 | loss: 0.16876 | val_0_rmse: 1.23107 |  0:00:04s\n",
      "epoch 28 | loss: 0.13364 | val_0_rmse: 1.23603 |  0:00:04s\n",
      "epoch 29 | loss: 0.13362 | val_0_rmse: 1.16134 |  0:00:04s\n",
      "epoch 30 | loss: 0.13166 | val_0_rmse: 1.01235 |  0:00:04s\n",
      "epoch 31 | loss: 0.12828 | val_0_rmse: 0.95691 |  0:00:05s\n",
      "epoch 32 | loss: 0.10736 | val_0_rmse: 0.84058 |  0:00:05s\n",
      "epoch 33 | loss: 0.10965 | val_0_rmse: 0.87428 |  0:00:05s\n",
      "epoch 34 | loss: 0.09139 | val_0_rmse: 0.75176 |  0:00:05s\n",
      "epoch 35 | loss: 0.10567 | val_0_rmse: 0.73585 |  0:00:05s\n",
      "epoch 36 | loss: 0.09394 | val_0_rmse: 0.67029 |  0:00:05s\n",
      "epoch 37 | loss: 0.08126 | val_0_rmse: 0.73622 |  0:00:06s\n",
      "epoch 38 | loss: 0.11582 | val_0_rmse: 0.47866 |  0:00:06s\n",
      "epoch 39 | loss: 0.11406 | val_0_rmse: 0.67552 |  0:00:06s\n",
      "epoch 40 | loss: 0.11099 | val_0_rmse: 0.51361 |  0:00:06s\n",
      "epoch 41 | loss: 0.07188 | val_0_rmse: 0.49676 |  0:00:06s\n",
      "epoch 42 | loss: 0.06656 | val_0_rmse: 0.55484 |  0:00:06s\n",
      "epoch 43 | loss: 0.07765 | val_0_rmse: 0.49909 |  0:00:06s\n",
      "epoch 44 | loss: 0.0663  | val_0_rmse: 0.49507 |  0:00:07s\n",
      "epoch 45 | loss: 0.0567  | val_0_rmse: 0.54467 |  0:00:07s\n",
      "epoch 46 | loss: 0.07478 | val_0_rmse: 0.54164 |  0:00:07s\n",
      "epoch 47 | loss: 0.06309 | val_0_rmse: 0.34675 |  0:00:07s\n",
      "epoch 48 | loss: 0.11161 | val_0_rmse: 0.56134 |  0:00:07s\n",
      "epoch 49 | loss: 0.0838  | val_0_rmse: 0.40158 |  0:00:07s\n",
      "epoch 50 | loss: 0.07415 | val_0_rmse: 0.42072 |  0:00:07s\n",
      "epoch 51 | loss: 0.05701 | val_0_rmse: 0.35873 |  0:00:08s\n",
      "epoch 52 | loss: 0.07718 | val_0_rmse: 0.32583 |  0:00:08s\n",
      "epoch 53 | loss: 0.06794 | val_0_rmse: 0.46296 |  0:00:08s\n",
      "epoch 54 | loss: 0.06232 | val_0_rmse: 0.3025  |  0:00:08s\n",
      "epoch 55 | loss: 0.0557  | val_0_rmse: 0.46888 |  0:00:08s\n",
      "epoch 56 | loss: 0.05506 | val_0_rmse: 0.35862 |  0:00:08s\n",
      "epoch 57 | loss: 0.06609 | val_0_rmse: 0.31853 |  0:00:08s\n",
      "epoch 58 | loss: 0.06034 | val_0_rmse: 0.30488 |  0:00:09s\n",
      "epoch 59 | loss: 0.04767 | val_0_rmse: 0.35249 |  0:00:09s\n",
      "epoch 60 | loss: 0.05707 | val_0_rmse: 0.24432 |  0:00:09s\n",
      "epoch 61 | loss: 0.05823 | val_0_rmse: 0.3232  |  0:00:09s\n",
      "epoch 62 | loss: 0.04877 | val_0_rmse: 0.24271 |  0:00:09s\n",
      "epoch 63 | loss: 0.0469  | val_0_rmse: 0.26778 |  0:00:10s\n",
      "epoch 64 | loss: 0.04876 | val_0_rmse: 0.28216 |  0:00:10s\n",
      "epoch 65 | loss: 0.04409 | val_0_rmse: 0.23024 |  0:00:10s\n",
      "epoch 66 | loss: 0.05156 | val_0_rmse: 0.26677 |  0:00:10s\n",
      "epoch 67 | loss: 0.04682 | val_0_rmse: 0.29692 |  0:00:10s\n",
      "epoch 68 | loss: 0.04151 | val_0_rmse: 0.21841 |  0:00:10s\n",
      "epoch 69 | loss: 0.04408 | val_0_rmse: 0.29771 |  0:00:10s\n",
      "epoch 70 | loss: 0.07001 | val_0_rmse: 0.23485 |  0:00:11s\n",
      "epoch 71 | loss: 0.07086 | val_0_rmse: 0.27261 |  0:00:11s\n",
      "epoch 72 | loss: 0.03945 | val_0_rmse: 0.22792 |  0:00:11s\n",
      "epoch 73 | loss: 0.04005 | val_0_rmse: 0.25264 |  0:00:11s\n",
      "epoch 74 | loss: 0.03729 | val_0_rmse: 0.23387 |  0:00:11s\n",
      "epoch 75 | loss: 0.0392  | val_0_rmse: 0.25396 |  0:00:11s\n",
      "epoch 76 | loss: 0.03411 | val_0_rmse: 0.26065 |  0:00:11s\n",
      "epoch 77 | loss: 0.03767 | val_0_rmse: 0.22301 |  0:00:12s\n",
      "epoch 78 | loss: 0.03737 | val_0_rmse: 0.22501 |  0:00:12s\n",
      "epoch 79 | loss: 0.03483 | val_0_rmse: 0.28529 |  0:00:12s\n",
      "epoch 80 | loss: 0.05536 | val_0_rmse: 0.22324 |  0:00:12s\n",
      "epoch 81 | loss: 0.03775 | val_0_rmse: 0.30366 |  0:00:12s\n",
      "epoch 82 | loss: 0.12446 | val_0_rmse: 0.23738 |  0:00:12s\n",
      "epoch 83 | loss: 0.06427 | val_0_rmse: 0.30704 |  0:00:12s\n",
      "epoch 84 | loss: 0.09011 | val_0_rmse: 0.27069 |  0:00:13s\n",
      "epoch 85 | loss: 0.05507 | val_0_rmse: 0.21546 |  0:00:13s\n",
      "epoch 86 | loss: 0.02919 | val_0_rmse: 0.21349 |  0:00:13s\n",
      "epoch 87 | loss: 0.03043 | val_0_rmse: 0.21149 |  0:00:13s\n",
      "epoch 88 | loss: 0.0314  | val_0_rmse: 0.22214 |  0:00:13s\n",
      "epoch 89 | loss: 0.03179 | val_0_rmse: 0.22476 |  0:00:13s\n",
      "epoch 90 | loss: 0.04764 | val_0_rmse: 0.23358 |  0:00:14s\n",
      "epoch 91 | loss: 0.0325  | val_0_rmse: 0.21146 |  0:00:14s\n",
      "epoch 92 | loss: 0.0338  | val_0_rmse: 0.2663  |  0:00:14s\n",
      "epoch 93 | loss: 0.05011 | val_0_rmse: 0.21613 |  0:00:14s\n",
      "epoch 94 | loss: 0.04    | val_0_rmse: 0.20887 |  0:00:14s\n",
      "epoch 95 | loss: 0.03202 | val_0_rmse: 0.20467 |  0:00:14s\n",
      "epoch 96 | loss: 0.04916 | val_0_rmse: 0.21808 |  0:00:14s\n",
      "epoch 97 | loss: 0.03925 | val_0_rmse: 0.2238  |  0:00:15s\n",
      "epoch 98 | loss: 0.03634 | val_0_rmse: 0.20401 |  0:00:15s\n",
      "epoch 99 | loss: 0.05085 | val_0_rmse: 0.22747 |  0:00:15s\n",
      "epoch 100| loss: 0.03583 | val_0_rmse: 0.22651 |  0:00:15s\n",
      "epoch 101| loss: 0.03681 | val_0_rmse: 0.20647 |  0:00:15s\n",
      "epoch 102| loss: 0.0276  | val_0_rmse: 0.22054 |  0:00:15s\n",
      "epoch 103| loss: 0.03678 | val_0_rmse: 0.20619 |  0:00:15s\n",
      "epoch 104| loss: 0.02696 | val_0_rmse: 0.22783 |  0:00:16s\n",
      "epoch 105| loss: 0.06515 | val_0_rmse: 0.23573 |  0:00:16s\n",
      "epoch 106| loss: 0.03546 | val_0_rmse: 0.23478 |  0:00:16s\n",
      "epoch 107| loss: 0.03293 | val_0_rmse: 0.2696  |  0:00:16s\n",
      "epoch 108| loss: 0.06087 | val_0_rmse: 0.23628 |  0:00:16s\n",
      "epoch 109| loss: 0.0525  | val_0_rmse: 0.24129 |  0:00:16s\n",
      "epoch 110| loss: 0.04637 | val_0_rmse: 0.23485 |  0:00:16s\n",
      "epoch 111| loss: 0.05778 | val_0_rmse: 0.20417 |  0:00:17s\n",
      "epoch 112| loss: 0.04057 | val_0_rmse: 0.21219 |  0:00:17s\n",
      "epoch 113| loss: 0.02891 | val_0_rmse: 0.20904 |  0:00:17s\n",
      "epoch 114| loss: 0.02353 | val_0_rmse: 0.20426 |  0:00:17s\n",
      "epoch 115| loss: 0.02832 | val_0_rmse: 0.20672 |  0:00:17s\n",
      "epoch 116| loss: 0.02425 | val_0_rmse: 0.22063 |  0:00:17s\n",
      "epoch 117| loss: 0.0394  | val_0_rmse: 0.29111 |  0:00:18s\n",
      "epoch 118| loss: 0.10421 | val_0_rmse: 0.25129 |  0:00:18s\n",
      "epoch 119| loss: 0.05677 | val_0_rmse: 0.28659 |  0:00:18s\n",
      "epoch 120| loss: 0.06798 | val_0_rmse: 0.2886  |  0:00:18s\n",
      "epoch 121| loss: 0.06009 | val_0_rmse: 0.23477 |  0:00:18s\n",
      "epoch 122| loss: 0.07379 | val_0_rmse: 0.20512 |  0:00:18s\n",
      "epoch 123| loss: 0.05187 | val_0_rmse: 0.2839  |  0:00:18s\n",
      "\n",
      "Early stopping occurred at epoch 123 with best_epoch = 98 and best_val_0_rmse = 0.20401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:46:59,385] Trial 86 finished with value: 0.204008870034533 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.5704048380165612, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 1e-06, 'mask_type': 'entmax', 'lr': 0.016719495116155184, 'batch_size': 512, 'virtual_batch_size': 256}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 086 | rmse_log=0.20401 | RMSE$=45,743 | MAE$=26,485 | MAPE=14.66% | n_d/n_a=24/16 steps=3 lr=0.01672 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 147.39427| val_0_rmse: 10.43049|  0:00:00s\n",
      "epoch 1  | loss: 77.5067 | val_0_rmse: 8.46911 |  0:00:00s\n",
      "epoch 2  | loss: 37.80849| val_0_rmse: 6.55587 |  0:00:00s\n",
      "epoch 3  | loss: 20.63537| val_0_rmse: 4.79727 |  0:00:01s\n",
      "epoch 4  | loss: 22.09208| val_0_rmse: 3.80089 |  0:00:01s\n",
      "epoch 5  | loss: 19.37094| val_0_rmse: 4.23739 |  0:00:01s\n",
      "epoch 6  | loss: 10.80982| val_0_rmse: 4.68729 |  0:00:01s\n",
      "epoch 7  | loss: 8.85146 | val_0_rmse: 4.11599 |  0:00:02s\n",
      "epoch 8  | loss: 5.28249 | val_0_rmse: 2.26954 |  0:00:02s\n",
      "epoch 9  | loss: 3.47822 | val_0_rmse: 1.41345 |  0:00:02s\n",
      "epoch 10 | loss: 2.40925 | val_0_rmse: 1.95539 |  0:00:03s\n",
      "epoch 11 | loss: 2.06518 | val_0_rmse: 0.99103 |  0:00:03s\n",
      "epoch 12 | loss: 1.68459 | val_0_rmse: 1.33206 |  0:00:03s\n",
      "epoch 13 | loss: 1.13414 | val_0_rmse: 1.43099 |  0:00:03s\n",
      "epoch 14 | loss: 1.11085 | val_0_rmse: 0.76376 |  0:00:04s\n",
      "epoch 15 | loss: 0.8542  | val_0_rmse: 1.25709 |  0:00:04s\n",
      "epoch 16 | loss: 0.83127 | val_0_rmse: 0.60383 |  0:00:04s\n",
      "epoch 17 | loss: 0.62437 | val_0_rmse: 0.87153 |  0:00:04s\n",
      "epoch 18 | loss: 0.56251 | val_0_rmse: 0.52109 |  0:00:05s\n",
      "epoch 19 | loss: 0.49394 | val_0_rmse: 0.86318 |  0:00:05s\n",
      "epoch 20 | loss: 0.5     | val_0_rmse: 0.38887 |  0:00:05s\n",
      "epoch 21 | loss: 0.57508 | val_0_rmse: 0.72878 |  0:00:05s\n",
      "epoch 22 | loss: 0.47321 | val_0_rmse: 0.3695  |  0:00:06s\n",
      "epoch 23 | loss: 0.4304  | val_0_rmse: 0.86657 |  0:00:06s\n",
      "epoch 24 | loss: 0.57134 | val_0_rmse: 0.37485 |  0:00:06s\n",
      "epoch 25 | loss: 0.64171 | val_0_rmse: 0.62793 |  0:00:07s\n",
      "epoch 26 | loss: 0.43469 | val_0_rmse: 0.34391 |  0:00:07s\n",
      "epoch 27 | loss: 0.38738 | val_0_rmse: 0.74993 |  0:00:07s\n",
      "epoch 28 | loss: 0.45669 | val_0_rmse: 0.42474 |  0:00:07s\n",
      "epoch 29 | loss: 0.45029 | val_0_rmse: 0.68614 |  0:00:08s\n",
      "epoch 30 | loss: 0.38151 | val_0_rmse: 0.35413 |  0:00:08s\n",
      "epoch 31 | loss: 0.20948 | val_0_rmse: 0.46214 |  0:00:08s\n",
      "epoch 32 | loss: 0.22476 | val_0_rmse: 0.39719 |  0:00:09s\n",
      "epoch 33 | loss: 0.33094 | val_0_rmse: 0.4513  |  0:00:09s\n",
      "epoch 34 | loss: 0.23408 | val_0_rmse: 0.53978 |  0:00:09s\n",
      "epoch 35 | loss: 0.2204  | val_0_rmse: 0.40615 |  0:00:09s\n",
      "epoch 36 | loss: 0.19787 | val_0_rmse: 0.5257  |  0:00:10s\n",
      "epoch 37 | loss: 0.21824 | val_0_rmse: 0.42628 |  0:00:10s\n",
      "epoch 38 | loss: 0.15027 | val_0_rmse: 0.4777  |  0:00:10s\n",
      "epoch 39 | loss: 0.1708  | val_0_rmse: 0.59655 |  0:00:10s\n",
      "epoch 40 | loss: 0.19302 | val_0_rmse: 0.36513 |  0:00:11s\n",
      "epoch 41 | loss: 0.21778 | val_0_rmse: 0.41212 |  0:00:11s\n",
      "epoch 42 | loss: 0.15482 | val_0_rmse: 0.38967 |  0:00:11s\n",
      "epoch 43 | loss: 0.14672 | val_0_rmse: 0.41373 |  0:00:11s\n",
      "epoch 44 | loss: 0.16495 | val_0_rmse: 0.3551  |  0:00:12s\n",
      "epoch 45 | loss: 0.2291  | val_0_rmse: 0.33841 |  0:00:12s\n",
      "epoch 46 | loss: 0.16865 | val_0_rmse: 0.508   |  0:00:12s\n",
      "epoch 47 | loss: 0.20018 | val_0_rmse: 0.36115 |  0:00:13s\n",
      "epoch 48 | loss: 0.24696 | val_0_rmse: 0.46323 |  0:00:13s\n",
      "epoch 49 | loss: 0.19132 | val_0_rmse: 0.36532 |  0:00:13s\n",
      "epoch 50 | loss: 0.19297 | val_0_rmse: 0.47726 |  0:00:13s\n",
      "epoch 51 | loss: 0.18254 | val_0_rmse: 0.37905 |  0:00:14s\n",
      "epoch 52 | loss: 0.15721 | val_0_rmse: 0.45357 |  0:00:14s\n",
      "epoch 53 | loss: 0.16207 | val_0_rmse: 0.38085 |  0:00:14s\n",
      "epoch 54 | loss: 0.16326 | val_0_rmse: 0.44136 |  0:00:14s\n",
      "epoch 55 | loss: 0.1923  | val_0_rmse: 0.34964 |  0:00:15s\n",
      "epoch 56 | loss: 0.13929 | val_0_rmse: 0.4068  |  0:00:15s\n",
      "epoch 57 | loss: 0.15506 | val_0_rmse: 0.3881  |  0:00:15s\n",
      "epoch 58 | loss: 0.17112 | val_0_rmse: 0.38411 |  0:00:15s\n",
      "epoch 59 | loss: 0.14484 | val_0_rmse: 0.38698 |  0:00:16s\n",
      "epoch 60 | loss: 0.15709 | val_0_rmse: 0.3602  |  0:00:16s\n",
      "epoch 61 | loss: 0.13223 | val_0_rmse: 0.34377 |  0:00:16s\n",
      "epoch 62 | loss: 0.10708 | val_0_rmse: 0.3946  |  0:00:17s\n",
      "epoch 63 | loss: 0.13206 | val_0_rmse: 0.44628 |  0:00:17s\n",
      "epoch 64 | loss: 0.21711 | val_0_rmse: 0.45987 |  0:00:17s\n",
      "epoch 65 | loss: 0.42407 | val_0_rmse: 0.40004 |  0:00:17s\n",
      "epoch 66 | loss: 0.20771 | val_0_rmse: 0.41254 |  0:00:18s\n",
      "epoch 67 | loss: 0.18343 | val_0_rmse: 0.54536 |  0:00:18s\n",
      "epoch 68 | loss: 0.35692 | val_0_rmse: 0.35647 |  0:00:18s\n",
      "epoch 69 | loss: 0.25409 | val_0_rmse: 0.32045 |  0:00:18s\n",
      "epoch 70 | loss: 0.17079 | val_0_rmse: 0.46622 |  0:00:19s\n",
      "epoch 71 | loss: 0.19971 | val_0_rmse: 0.48462 |  0:00:19s\n",
      "epoch 72 | loss: 0.26523 | val_0_rmse: 0.36739 |  0:00:19s\n",
      "epoch 73 | loss: 0.26042 | val_0_rmse: 0.33503 |  0:00:20s\n",
      "epoch 74 | loss: 0.16504 | val_0_rmse: 0.45913 |  0:00:20s\n",
      "epoch 75 | loss: 0.19934 | val_0_rmse: 0.45735 |  0:00:20s\n",
      "epoch 76 | loss: 0.24304 | val_0_rmse: 0.37773 |  0:00:20s\n",
      "epoch 77 | loss: 0.21862 | val_0_rmse: 0.3377  |  0:00:21s\n",
      "epoch 78 | loss: 0.1617  | val_0_rmse: 0.45226 |  0:00:21s\n",
      "epoch 79 | loss: 0.18357 | val_0_rmse: 0.4433  |  0:00:21s\n",
      "epoch 80 | loss: 0.23214 | val_0_rmse: 0.35353 |  0:00:21s\n",
      "epoch 81 | loss: 0.22513 | val_0_rmse: 0.31039 |  0:00:22s\n",
      "epoch 82 | loss: 0.16121 | val_0_rmse: 0.46256 |  0:00:22s\n",
      "epoch 83 | loss: 0.18245 | val_0_rmse: 0.41393 |  0:00:22s\n",
      "epoch 84 | loss: 0.18512 | val_0_rmse: 0.3407  |  0:00:22s\n",
      "epoch 85 | loss: 0.13312 | val_0_rmse: 0.32398 |  0:00:23s\n",
      "epoch 86 | loss: 0.12506 | val_0_rmse: 0.29628 |  0:00:23s\n",
      "epoch 87 | loss: 0.10333 | val_0_rmse: 0.31649 |  0:00:23s\n",
      "epoch 88 | loss: 0.12832 | val_0_rmse: 0.32332 |  0:00:23s\n",
      "epoch 89 | loss: 0.11422 | val_0_rmse: 0.29957 |  0:00:24s\n",
      "epoch 90 | loss: 0.08866 | val_0_rmse: 0.30618 |  0:00:24s\n",
      "epoch 91 | loss: 0.09191 | val_0_rmse: 0.30695 |  0:00:24s\n",
      "epoch 92 | loss: 0.09743 | val_0_rmse: 0.29922 |  0:00:25s\n",
      "epoch 93 | loss: 0.09845 | val_0_rmse: 0.31271 |  0:00:25s\n",
      "epoch 94 | loss: 0.10476 | val_0_rmse: 0.32318 |  0:00:25s\n",
      "epoch 95 | loss: 0.10854 | val_0_rmse: 0.29651 |  0:00:25s\n",
      "epoch 96 | loss: 0.09292 | val_0_rmse: 0.29507 |  0:00:26s\n",
      "epoch 97 | loss: 0.08607 | val_0_rmse: 0.32077 |  0:00:26s\n",
      "epoch 98 | loss: 0.08773 | val_0_rmse: 0.30445 |  0:00:26s\n",
      "epoch 99 | loss: 0.0883  | val_0_rmse: 0.32801 |  0:00:26s\n",
      "epoch 100| loss: 0.09948 | val_0_rmse: 0.29935 |  0:00:27s\n",
      "epoch 101| loss: 0.10267 | val_0_rmse: 0.28774 |  0:00:27s\n",
      "epoch 102| loss: 0.10511 | val_0_rmse: 0.33067 |  0:00:27s\n",
      "epoch 103| loss: 0.10358 | val_0_rmse: 0.28205 |  0:00:27s\n",
      "epoch 104| loss: 0.10541 | val_0_rmse: 0.31056 |  0:00:28s\n",
      "epoch 105| loss: 0.12077 | val_0_rmse: 0.3225  |  0:00:28s\n",
      "epoch 106| loss: 0.11495 | val_0_rmse: 0.27146 |  0:00:28s\n",
      "epoch 107| loss: 0.11507 | val_0_rmse: 0.29676 |  0:00:29s\n",
      "epoch 108| loss: 0.11336 | val_0_rmse: 0.33078 |  0:00:29s\n",
      "epoch 109| loss: 0.09401 | val_0_rmse: 0.27254 |  0:00:29s\n",
      "epoch 110| loss: 0.09263 | val_0_rmse: 0.27206 |  0:00:29s\n",
      "epoch 111| loss: 0.1055  | val_0_rmse: 0.31723 |  0:00:30s\n",
      "epoch 112| loss: 0.09103 | val_0_rmse: 0.23575 |  0:00:30s\n",
      "epoch 113| loss: 0.05753 | val_0_rmse: 0.24433 |  0:00:30s\n",
      "epoch 114| loss: 0.06975 | val_0_rmse: 0.27035 |  0:00:30s\n",
      "epoch 115| loss: 0.08741 | val_0_rmse: 0.30661 |  0:00:31s\n",
      "epoch 116| loss: 0.07097 | val_0_rmse: 0.26096 |  0:00:31s\n",
      "epoch 117| loss: 0.06819 | val_0_rmse: 0.27003 |  0:00:31s\n",
      "epoch 118| loss: 0.07455 | val_0_rmse: 0.25591 |  0:00:31s\n",
      "epoch 119| loss: 0.0739  | val_0_rmse: 0.237   |  0:00:32s\n",
      "epoch 120| loss: 0.06675 | val_0_rmse: 0.25571 |  0:00:32s\n",
      "epoch 121| loss: 0.0655  | val_0_rmse: 0.33695 |  0:00:32s\n",
      "epoch 122| loss: 0.10815 | val_0_rmse: 0.29284 |  0:00:33s\n",
      "epoch 123| loss: 0.15565 | val_0_rmse: 0.2405  |  0:00:33s\n",
      "epoch 124| loss: 0.10021 | val_0_rmse: 0.25753 |  0:00:33s\n",
      "epoch 125| loss: 0.1004  | val_0_rmse: 0.31607 |  0:00:33s\n",
      "epoch 126| loss: 0.08565 | val_0_rmse: 0.26473 |  0:00:34s\n",
      "epoch 127| loss: 0.06645 | val_0_rmse: 0.2309  |  0:00:34s\n",
      "epoch 128| loss: 0.05998 | val_0_rmse: 0.25917 |  0:00:34s\n",
      "epoch 129| loss: 0.07428 | val_0_rmse: 0.26759 |  0:00:34s\n",
      "epoch 130| loss: 0.06397 | val_0_rmse: 0.25927 |  0:00:35s\n",
      "epoch 131| loss: 0.07489 | val_0_rmse: 0.22982 |  0:00:35s\n",
      "epoch 132| loss: 0.05618 | val_0_rmse: 0.26218 |  0:00:35s\n",
      "epoch 133| loss: 0.07158 | val_0_rmse: 0.27198 |  0:00:35s\n",
      "epoch 134| loss: 0.05942 | val_0_rmse: 0.26942 |  0:00:36s\n",
      "epoch 135| loss: 0.05884 | val_0_rmse: 0.25564 |  0:00:36s\n",
      "epoch 136| loss: 0.07575 | val_0_rmse: 0.26127 |  0:00:36s\n",
      "epoch 137| loss: 0.05892 | val_0_rmse: 0.25983 |  0:00:37s\n",
      "epoch 138| loss: 0.07076 | val_0_rmse: 0.26698 |  0:00:37s\n",
      "epoch 139| loss: 0.0616  | val_0_rmse: 0.27204 |  0:00:37s\n",
      "epoch 140| loss: 0.05663 | val_0_rmse: 0.2446  |  0:00:37s\n",
      "epoch 141| loss: 0.06023 | val_0_rmse: 0.24974 |  0:00:38s\n",
      "epoch 142| loss: 0.05692 | val_0_rmse: 0.24969 |  0:00:38s\n",
      "epoch 143| loss: 0.05693 | val_0_rmse: 0.2421  |  0:00:38s\n",
      "epoch 144| loss: 0.05323 | val_0_rmse: 0.23197 |  0:00:38s\n",
      "epoch 145| loss: 0.06122 | val_0_rmse: 0.24619 |  0:00:39s\n",
      "epoch 146| loss: 0.0495  | val_0_rmse: 0.24786 |  0:00:39s\n",
      "epoch 147| loss: 0.05239 | val_0_rmse: 0.25866 |  0:00:39s\n",
      "epoch 148| loss: 0.05005 | val_0_rmse: 0.23906 |  0:00:39s\n",
      "epoch 149| loss: 0.04991 | val_0_rmse: 0.23972 |  0:00:40s\n",
      "epoch 150| loss: 0.05186 | val_0_rmse: 0.24977 |  0:00:40s\n",
      "epoch 151| loss: 0.04637 | val_0_rmse: 0.24567 |  0:00:40s\n",
      "epoch 152| loss: 0.04647 | val_0_rmse: 0.24206 |  0:00:40s\n",
      "epoch 153| loss: 0.04573 | val_0_rmse: 0.23863 |  0:00:41s\n",
      "epoch 154| loss: 0.06571 | val_0_rmse: 0.24001 |  0:00:41s\n",
      "epoch 155| loss: 0.04613 | val_0_rmse: 0.24532 |  0:00:41s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:47:41,746] Trial 87 finished with value: 0.2298229491683592 and parameters: {'n_d': 48, 'n_a': 16, 'n_steps': 4, 'gamma': 1.2683866436668894, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'sparsemax', 'lr': 0.017668439258784117, 'batch_size': 512, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 156| loss: 0.04296 | val_0_rmse: 0.24403 |  0:00:41s\n",
      "\n",
      "Early stopping occurred at epoch 156 with best_epoch = 131 and best_val_0_rmse = 0.22982\n",
      "Trial 087 | rmse_log=0.22982 | RMSE$=45,657 | MAE$=30,415 | MAPE=18.36% | n_d/n_a=48/16 steps=4 lr=0.01767 batch=512 mask=sparsemax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 157.74851| val_0_rmse: 10.24644|  0:00:00s\n",
      "epoch 1  | loss: 88.92202| val_0_rmse: 8.57241 |  0:00:00s\n",
      "epoch 2  | loss: 47.24668| val_0_rmse: 6.756   |  0:00:00s\n",
      "epoch 3  | loss: 34.41661| val_0_rmse: 5.05151 |  0:00:00s\n",
      "epoch 4  | loss: 25.48839| val_0_rmse: 4.02614 |  0:00:00s\n",
      "epoch 5  | loss: 24.22229| val_0_rmse: 3.69021 |  0:00:01s\n",
      "epoch 6  | loss: 17.56521| val_0_rmse: 3.85678 |  0:00:01s\n",
      "epoch 7  | loss: 9.99071 | val_0_rmse: 4.11738 |  0:00:01s\n",
      "epoch 8  | loss: 6.69002 | val_0_rmse: 3.81623 |  0:00:01s\n",
      "epoch 9  | loss: 4.78268 | val_0_rmse: 2.61549 |  0:00:01s\n",
      "epoch 10 | loss: 2.74781 | val_0_rmse: 1.84871 |  0:00:02s\n",
      "epoch 11 | loss: 2.99646 | val_0_rmse: 1.82241 |  0:00:02s\n",
      "epoch 12 | loss: 1.76225 | val_0_rmse: 1.99569 |  0:00:02s\n",
      "epoch 13 | loss: 1.52136 | val_0_rmse: 1.64727 |  0:00:02s\n",
      "epoch 14 | loss: 1.51623 | val_0_rmse: 1.86264 |  0:00:02s\n",
      "epoch 15 | loss: 1.31419 | val_0_rmse: 1.63586 |  0:00:03s\n",
      "epoch 16 | loss: 0.93631 | val_0_rmse: 1.19134 |  0:00:03s\n",
      "epoch 17 | loss: 1.12699 | val_0_rmse: 1.56897 |  0:00:03s\n",
      "epoch 18 | loss: 0.96774 | val_0_rmse: 1.11603 |  0:00:03s\n",
      "epoch 19 | loss: 0.78715 | val_0_rmse: 1.21023 |  0:00:03s\n",
      "epoch 20 | loss: 0.89622 | val_0_rmse: 0.98937 |  0:00:03s\n",
      "epoch 21 | loss: 0.67679 | val_0_rmse: 0.79474 |  0:00:04s\n",
      "epoch 22 | loss: 0.64811 | val_0_rmse: 1.11265 |  0:00:04s\n",
      "epoch 23 | loss: 0.54591 | val_0_rmse: 1.07612 |  0:00:04s\n",
      "epoch 24 | loss: 0.73152 | val_0_rmse: 0.94345 |  0:00:04s\n",
      "epoch 25 | loss: 0.4755  | val_0_rmse: 0.82736 |  0:00:04s\n",
      "epoch 26 | loss: 0.56676 | val_0_rmse: 0.80042 |  0:00:04s\n",
      "epoch 27 | loss: 0.4749  | val_0_rmse: 0.77643 |  0:00:05s\n",
      "epoch 28 | loss: 0.49165 | val_0_rmse: 0.90529 |  0:00:05s\n",
      "epoch 29 | loss: 0.40691 | val_0_rmse: 0.64406 |  0:00:05s\n",
      "epoch 30 | loss: 0.35004 | val_0_rmse: 0.79941 |  0:00:05s\n",
      "epoch 31 | loss: 0.40118 | val_0_rmse: 0.61696 |  0:00:05s\n",
      "epoch 32 | loss: 0.33761 | val_0_rmse: 0.78889 |  0:00:05s\n",
      "epoch 33 | loss: 0.36099 | val_0_rmse: 0.49997 |  0:00:06s\n",
      "epoch 34 | loss: 0.29609 | val_0_rmse: 0.76589 |  0:00:06s\n",
      "epoch 35 | loss: 0.24937 | val_0_rmse: 0.53851 |  0:00:06s\n",
      "epoch 36 | loss: 0.26099 | val_0_rmse: 0.69741 |  0:00:06s\n",
      "epoch 37 | loss: 0.20804 | val_0_rmse: 0.40595 |  0:00:06s\n",
      "epoch 38 | loss: 0.1723  | val_0_rmse: 0.59931 |  0:00:07s\n",
      "epoch 39 | loss: 0.16421 | val_0_rmse: 0.42745 |  0:00:07s\n",
      "epoch 40 | loss: 0.11795 | val_0_rmse: 0.32353 |  0:00:07s\n",
      "epoch 41 | loss: 0.19622 | val_0_rmse: 0.69502 |  0:00:07s\n",
      "epoch 42 | loss: 0.22365 | val_0_rmse: 0.31553 |  0:00:07s\n",
      "epoch 43 | loss: 0.21459 | val_0_rmse: 0.49138 |  0:00:07s\n",
      "epoch 44 | loss: 0.1349  | val_0_rmse: 0.55869 |  0:00:08s\n",
      "epoch 45 | loss: 0.13285 | val_0_rmse: 0.40068 |  0:00:08s\n",
      "epoch 46 | loss: 0.17459 | val_0_rmse: 0.40136 |  0:00:08s\n",
      "epoch 47 | loss: 0.09655 | val_0_rmse: 0.54764 |  0:00:08s\n",
      "epoch 48 | loss: 0.12673 | val_0_rmse: 0.50973 |  0:00:08s\n",
      "epoch 49 | loss: 0.11268 | val_0_rmse: 0.35308 |  0:00:08s\n",
      "epoch 50 | loss: 0.09665 | val_0_rmse: 0.39681 |  0:00:09s\n",
      "epoch 51 | loss: 0.10607 | val_0_rmse: 0.49314 |  0:00:09s\n",
      "epoch 52 | loss: 0.12078 | val_0_rmse: 0.25517 |  0:00:09s\n",
      "epoch 53 | loss: 0.19598 | val_0_rmse: 0.36281 |  0:00:09s\n",
      "epoch 54 | loss: 0.09654 | val_0_rmse: 0.29184 |  0:00:09s\n",
      "epoch 55 | loss: 0.08417 | val_0_rmse: 0.33508 |  0:00:09s\n",
      "epoch 56 | loss: 0.0644  | val_0_rmse: 0.38429 |  0:00:10s\n",
      "epoch 57 | loss: 0.07322 | val_0_rmse: 0.36483 |  0:00:10s\n",
      "epoch 58 | loss: 0.08338 | val_0_rmse: 0.3001  |  0:00:10s\n",
      "epoch 59 | loss: 0.07021 | val_0_rmse: 0.29031 |  0:00:10s\n",
      "epoch 60 | loss: 0.06879 | val_0_rmse: 0.2435  |  0:00:10s\n",
      "epoch 61 | loss: 0.06033 | val_0_rmse: 0.32598 |  0:00:10s\n",
      "epoch 62 | loss: 0.05878 | val_0_rmse: 0.38095 |  0:00:11s\n",
      "epoch 63 | loss: 0.06081 | val_0_rmse: 0.25684 |  0:00:11s\n",
      "epoch 64 | loss: 0.05838 | val_0_rmse: 0.30404 |  0:00:11s\n",
      "epoch 65 | loss: 0.06506 | val_0_rmse: 0.40993 |  0:00:11s\n",
      "epoch 66 | loss: 0.08312 | val_0_rmse: 0.21377 |  0:00:11s\n",
      "epoch 67 | loss: 0.15121 | val_0_rmse: 0.279   |  0:00:11s\n",
      "epoch 68 | loss: 0.08536 | val_0_rmse: 0.26973 |  0:00:12s\n",
      "epoch 69 | loss: 0.06717 | val_0_rmse: 0.24769 |  0:00:12s\n",
      "epoch 70 | loss: 0.07891 | val_0_rmse: 0.29932 |  0:00:12s\n",
      "epoch 71 | loss: 0.06969 | val_0_rmse: 0.21508 |  0:00:12s\n",
      "epoch 72 | loss: 0.07277 | val_0_rmse: 0.28901 |  0:00:12s\n",
      "epoch 73 | loss: 0.08209 | val_0_rmse: 0.22231 |  0:00:12s\n",
      "epoch 74 | loss: 0.07354 | val_0_rmse: 0.2534  |  0:00:13s\n",
      "epoch 75 | loss: 0.07476 | val_0_rmse: 0.2231  |  0:00:13s\n",
      "epoch 76 | loss: 0.05953 | val_0_rmse: 0.23196 |  0:00:13s\n",
      "epoch 77 | loss: 0.07032 | val_0_rmse: 0.22929 |  0:00:13s\n",
      "epoch 78 | loss: 0.09016 | val_0_rmse: 0.24317 |  0:00:13s\n",
      "epoch 79 | loss: 0.05155 | val_0_rmse: 0.21577 |  0:00:14s\n",
      "epoch 80 | loss: 0.0574  | val_0_rmse: 0.23875 |  0:00:14s\n",
      "epoch 81 | loss: 0.06958 | val_0_rmse: 0.23068 |  0:00:14s\n",
      "epoch 82 | loss: 0.05801 | val_0_rmse: 0.21931 |  0:00:14s\n",
      "epoch 83 | loss: 0.04682 | val_0_rmse: 0.21471 |  0:00:14s\n",
      "epoch 84 | loss: 0.03672 | val_0_rmse: 0.21709 |  0:00:14s\n",
      "epoch 85 | loss: 0.03916 | val_0_rmse: 0.21996 |  0:00:15s\n",
      "epoch 86 | loss: 0.04553 | val_0_rmse: 0.20998 |  0:00:15s\n",
      "epoch 87 | loss: 0.04532 | val_0_rmse: 0.28097 |  0:00:15s\n",
      "epoch 88 | loss: 0.05359 | val_0_rmse: 0.28158 |  0:00:15s\n",
      "epoch 89 | loss: 0.08374 | val_0_rmse: 0.2642  |  0:00:15s\n",
      "epoch 90 | loss: 0.05088 | val_0_rmse: 0.27161 |  0:00:15s\n",
      "epoch 91 | loss: 0.06959 | val_0_rmse: 0.28109 |  0:00:16s\n",
      "epoch 92 | loss: 0.06777 | val_0_rmse: 0.2517  |  0:00:16s\n",
      "epoch 93 | loss: 0.0789  | val_0_rmse: 0.29501 |  0:00:16s\n",
      "epoch 94 | loss: 0.0586  | val_0_rmse: 0.25431 |  0:00:16s\n",
      "epoch 95 | loss: 0.06265 | val_0_rmse: 0.28216 |  0:00:16s\n",
      "epoch 96 | loss: 0.06486 | val_0_rmse: 0.25771 |  0:00:16s\n",
      "epoch 97 | loss: 0.07063 | val_0_rmse: 0.28894 |  0:00:17s\n",
      "epoch 98 | loss: 0.06034 | val_0_rmse: 0.24955 |  0:00:17s\n",
      "epoch 99 | loss: 0.06663 | val_0_rmse: 0.28912 |  0:00:17s\n",
      "epoch 100| loss: 0.05517 | val_0_rmse: 0.25493 |  0:00:17s\n",
      "epoch 101| loss: 0.05692 | val_0_rmse: 0.26683 |  0:00:17s\n",
      "epoch 102| loss: 0.0532  | val_0_rmse: 0.25167 |  0:00:17s\n",
      "epoch 103| loss: 0.06118 | val_0_rmse: 0.27583 |  0:00:18s\n",
      "epoch 104| loss: 0.05588 | val_0_rmse: 0.24991 |  0:00:18s\n",
      "epoch 105| loss: 0.05271 | val_0_rmse: 0.28532 |  0:00:18s\n",
      "epoch 106| loss: 0.0633  | val_0_rmse: 0.23964 |  0:00:18s\n",
      "epoch 107| loss: 0.05682 | val_0_rmse: 0.27507 |  0:00:18s\n",
      "epoch 108| loss: 0.05948 | val_0_rmse: 0.25227 |  0:00:18s\n",
      "epoch 109| loss: 0.05462 | val_0_rmse: 0.28862 |  0:00:19s\n",
      "epoch 110| loss: 0.05304 | val_0_rmse: 0.31332 |  0:00:19s\n",
      "epoch 111| loss: 0.13591 | val_0_rmse: 0.22516 |  0:00:19s\n",
      "\n",
      "Early stopping occurred at epoch 111 with best_epoch = 86 and best_val_0_rmse = 0.20998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:48:01,610] Trial 88 finished with value: 0.20997652734678732 and parameters: {'n_d': 64, 'n_a': 16, 'n_steps': 3, 'gamma': 1.3560672513897862, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.014749412281609078, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 088 | rmse_log=0.20998 | RMSE$=41,643 | MAE$=26,476 | MAPE=15.31% | n_d/n_a=64/16 steps=3 lr=0.01475 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 188.90643| val_0_rmse: 11.82938|  0:00:00s\n",
      "epoch 1  | loss: 150.17997| val_0_rmse: 11.33108|  0:00:00s\n",
      "epoch 2  | loss: 120.80571| val_0_rmse: 10.74743|  0:00:00s\n",
      "epoch 3  | loss: 97.3678 | val_0_rmse: 10.03585|  0:00:00s\n",
      "epoch 4  | loss: 76.05619| val_0_rmse: 9.15921 |  0:00:01s\n",
      "epoch 5  | loss: 58.13535| val_0_rmse: 8.06909 |  0:00:01s\n",
      "epoch 6  | loss: 41.55735| val_0_rmse: 6.7613  |  0:00:01s\n",
      "epoch 7  | loss: 30.92864| val_0_rmse: 5.46477 |  0:00:02s\n",
      "epoch 8  | loss: 22.34467| val_0_rmse: 4.17346 |  0:00:02s\n",
      "epoch 9  | loss: 15.46574| val_0_rmse: 2.98938 |  0:00:02s\n",
      "epoch 10 | loss: 15.2797 | val_0_rmse: 2.20672 |  0:00:02s\n",
      "epoch 11 | loss: 15.22746| val_0_rmse: 1.88616 |  0:00:03s\n",
      "epoch 12 | loss: 13.48808| val_0_rmse: 1.8636  |  0:00:03s\n",
      "epoch 13 | loss: 13.68341| val_0_rmse: 2.35069 |  0:00:03s\n",
      "epoch 14 | loss: 7.07837 | val_0_rmse: 2.88614 |  0:00:03s\n",
      "epoch 15 | loss: 4.88172 | val_0_rmse: 3.0581  |  0:00:04s\n",
      "epoch 16 | loss: 4.32299 | val_0_rmse: 2.5116  |  0:00:04s\n",
      "epoch 17 | loss: 2.71739 | val_0_rmse: 1.68564 |  0:00:04s\n",
      "epoch 18 | loss: 1.94053 | val_0_rmse: 1.42077 |  0:00:04s\n",
      "epoch 19 | loss: 1.56981 | val_0_rmse: 1.71764 |  0:00:05s\n",
      "epoch 20 | loss: 1.3926  | val_0_rmse: 1.21911 |  0:00:05s\n",
      "epoch 21 | loss: 0.94275 | val_0_rmse: 0.66161 |  0:00:05s\n",
      "epoch 22 | loss: 0.97781 | val_0_rmse: 0.98543 |  0:00:05s\n",
      "epoch 23 | loss: 0.7262  | val_0_rmse: 0.77864 |  0:00:06s\n",
      "epoch 24 | loss: 0.6272  | val_0_rmse: 0.61725 |  0:00:06s\n",
      "epoch 25 | loss: 0.66617 | val_0_rmse: 0.91602 |  0:00:06s\n",
      "epoch 26 | loss: 0.58258 | val_0_rmse: 0.6373  |  0:00:06s\n",
      "epoch 27 | loss: 0.57945 | val_0_rmse: 0.90291 |  0:00:07s\n",
      "epoch 28 | loss: 0.47651 | val_0_rmse: 0.6881  |  0:00:07s\n",
      "epoch 29 | loss: 0.38169 | val_0_rmse: 0.77321 |  0:00:07s\n",
      "epoch 30 | loss: 0.34597 | val_0_rmse: 0.66088 |  0:00:07s\n",
      "epoch 31 | loss: 0.33121 | val_0_rmse: 0.91191 |  0:00:08s\n",
      "epoch 32 | loss: 0.33593 | val_0_rmse: 0.6983  |  0:00:08s\n",
      "epoch 33 | loss: 0.28929 | val_0_rmse: 0.78459 |  0:00:08s\n",
      "epoch 34 | loss: 0.26909 | val_0_rmse: 0.35796 |  0:00:08s\n",
      "epoch 35 | loss: 0.34051 | val_0_rmse: 0.42889 |  0:00:08s\n",
      "epoch 36 | loss: 0.23791 | val_0_rmse: 0.54925 |  0:00:09s\n",
      "epoch 37 | loss: 0.28588 | val_0_rmse: 0.31129 |  0:00:09s\n",
      "epoch 38 | loss: 0.22958 | val_0_rmse: 0.48875 |  0:00:09s\n",
      "epoch 39 | loss: 0.26194 | val_0_rmse: 0.3063  |  0:00:09s\n",
      "epoch 40 | loss: 0.16845 | val_0_rmse: 0.34207 |  0:00:10s\n",
      "epoch 41 | loss: 0.17958 | val_0_rmse: 0.27135 |  0:00:10s\n",
      "epoch 42 | loss: 0.18889 | val_0_rmse: 0.4488  |  0:00:10s\n",
      "epoch 43 | loss: 0.18676 | val_0_rmse: 0.266   |  0:00:10s\n",
      "epoch 44 | loss: 0.20597 | val_0_rmse: 0.33655 |  0:00:11s\n",
      "epoch 45 | loss: 0.19876 | val_0_rmse: 0.32133 |  0:00:11s\n",
      "epoch 46 | loss: 0.20459 | val_0_rmse: 0.33882 |  0:00:11s\n",
      "epoch 47 | loss: 0.14072 | val_0_rmse: 0.28456 |  0:00:11s\n",
      "epoch 48 | loss: 0.20336 | val_0_rmse: 0.37085 |  0:00:12s\n",
      "epoch 49 | loss: 0.17328 | val_0_rmse: 0.31052 |  0:00:12s\n",
      "epoch 50 | loss: 0.14915 | val_0_rmse: 0.31381 |  0:00:12s\n",
      "epoch 51 | loss: 0.21631 | val_0_rmse: 0.44065 |  0:00:12s\n",
      "epoch 52 | loss: 0.24638 | val_0_rmse: 0.4096  |  0:00:12s\n",
      "epoch 53 | loss: 0.20602 | val_0_rmse: 0.24767 |  0:00:13s\n",
      "epoch 54 | loss: 0.18716 | val_0_rmse: 0.37634 |  0:00:13s\n",
      "epoch 55 | loss: 0.18849 | val_0_rmse: 0.38095 |  0:00:13s\n",
      "epoch 56 | loss: 0.14359 | val_0_rmse: 0.30264 |  0:00:13s\n",
      "epoch 57 | loss: 0.28235 | val_0_rmse: 0.27332 |  0:00:14s\n",
      "epoch 58 | loss: 0.1787  | val_0_rmse: 0.53065 |  0:00:14s\n",
      "epoch 59 | loss: 0.33794 | val_0_rmse: 0.6278  |  0:00:14s\n",
      "epoch 60 | loss: 0.38979 | val_0_rmse: 0.28331 |  0:00:14s\n",
      "epoch 61 | loss: 0.16924 | val_0_rmse: 0.27976 |  0:00:15s\n",
      "epoch 62 | loss: 0.18053 | val_0_rmse: 0.39127 |  0:00:15s\n",
      "epoch 63 | loss: 0.19363 | val_0_rmse: 0.4623  |  0:00:15s\n",
      "epoch 64 | loss: 0.20731 | val_0_rmse: 0.26128 |  0:00:15s\n",
      "epoch 65 | loss: 0.259   | val_0_rmse: 0.33935 |  0:00:16s\n",
      "epoch 66 | loss: 0.37623 | val_0_rmse: 0.3211  |  0:00:16s\n",
      "epoch 67 | loss: 0.11961 | val_0_rmse: 0.35678 |  0:00:16s\n",
      "epoch 68 | loss: 0.12334 | val_0_rmse: 0.24362 |  0:00:16s\n",
      "epoch 69 | loss: 0.08694 | val_0_rmse: 0.26253 |  0:00:17s\n",
      "epoch 70 | loss: 0.08375 | val_0_rmse: 0.24293 |  0:00:17s\n",
      "epoch 71 | loss: 0.11165 | val_0_rmse: 0.24616 |  0:00:17s\n",
      "epoch 72 | loss: 0.07469 | val_0_rmse: 0.24255 |  0:00:17s\n",
      "epoch 73 | loss: 0.08659 | val_0_rmse: 0.282   |  0:00:18s\n",
      "epoch 74 | loss: 0.07992 | val_0_rmse: 0.23771 |  0:00:18s\n",
      "epoch 75 | loss: 0.08093 | val_0_rmse: 0.32548 |  0:00:18s\n",
      "epoch 76 | loss: 0.12305 | val_0_rmse: 0.25226 |  0:00:18s\n",
      "epoch 77 | loss: 0.08152 | val_0_rmse: 0.27225 |  0:00:18s\n",
      "epoch 78 | loss: 0.14898 | val_0_rmse: 0.28743 |  0:00:19s\n",
      "epoch 79 | loss: 0.1101  | val_0_rmse: 0.28774 |  0:00:19s\n",
      "epoch 80 | loss: 0.09212 | val_0_rmse: 0.24346 |  0:00:19s\n",
      "epoch 81 | loss: 0.10857 | val_0_rmse: 0.23187 |  0:00:19s\n",
      "epoch 82 | loss: 0.07306 | val_0_rmse: 0.25029 |  0:00:20s\n",
      "epoch 83 | loss: 0.07889 | val_0_rmse: 0.24123 |  0:00:20s\n",
      "epoch 84 | loss: 0.09059 | val_0_rmse: 0.36168 |  0:00:20s\n",
      "epoch 85 | loss: 0.1691  | val_0_rmse: 0.3622  |  0:00:20s\n",
      "epoch 86 | loss: 0.14783 | val_0_rmse: 0.29594 |  0:00:21s\n",
      "epoch 87 | loss: 0.1578  | val_0_rmse: 0.34288 |  0:00:21s\n",
      "epoch 88 | loss: 0.16561 | val_0_rmse: 0.33298 |  0:00:21s\n",
      "epoch 89 | loss: 0.16805 | val_0_rmse: 0.42101 |  0:00:21s\n",
      "epoch 90 | loss: 0.2422  | val_0_rmse: 0.24383 |  0:00:22s\n",
      "epoch 91 | loss: 0.08469 | val_0_rmse: 0.27645 |  0:00:22s\n",
      "epoch 92 | loss: 0.08714 | val_0_rmse: 0.25744 |  0:00:22s\n",
      "epoch 93 | loss: 0.07239 | val_0_rmse: 0.24788 |  0:00:22s\n",
      "epoch 94 | loss: 0.06252 | val_0_rmse: 0.24686 |  0:00:23s\n",
      "epoch 95 | loss: 0.07568 | val_0_rmse: 0.27454 |  0:00:23s\n",
      "epoch 96 | loss: 0.08018 | val_0_rmse: 0.24424 |  0:00:23s\n",
      "epoch 97 | loss: 0.07195 | val_0_rmse: 0.24714 |  0:00:23s\n",
      "epoch 98 | loss: 0.05999 | val_0_rmse: 0.25204 |  0:00:24s\n",
      "epoch 99 | loss: 0.07654 | val_0_rmse: 0.23672 |  0:00:24s\n",
      "epoch 100| loss: 0.06402 | val_0_rmse: 0.2623  |  0:00:24s\n",
      "epoch 101| loss: 0.0604  | val_0_rmse: 0.23905 |  0:00:24s\n",
      "epoch 102| loss: 0.06752 | val_0_rmse: 0.29116 |  0:00:24s\n",
      "epoch 103| loss: 0.08729 | val_0_rmse: 0.30409 |  0:00:25s\n",
      "epoch 104| loss: 0.11164 | val_0_rmse: 0.32449 |  0:00:25s\n",
      "epoch 105| loss: 0.10634 | val_0_rmse: 0.27505 |  0:00:25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:48:27,864] Trial 89 finished with value: 0.23187305906270872 and parameters: {'n_d': 24, 'n_a': 16, 'n_steps': 4, 'gamma': 1.309631029599434, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0, 'mask_type': 'entmax', 'lr': 0.019220235100105374, 'batch_size': 1024, 'virtual_batch_size': 64}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 106| loss: 0.09869 | val_0_rmse: 0.24468 |  0:00:25s\n",
      "\n",
      "Early stopping occurred at epoch 106 with best_epoch = 81 and best_val_0_rmse = 0.23187\n",
      "Trial 089 | rmse_log=0.23187 | RMSE$=45,068 | MAE$=30,779 | MAPE=17.46% | n_d/n_a=24/16 steps=4 lr=0.01922 batch=1024 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 123.70939| val_0_rmse: 9.76692 |  0:00:00s\n",
      "epoch 1  | loss: 72.01832| val_0_rmse: 7.99603 |  0:00:00s\n",
      "epoch 2  | loss: 43.56627| val_0_rmse: 6.65126 |  0:00:00s\n",
      "epoch 3  | loss: 27.98163| val_0_rmse: 5.52259 |  0:00:00s\n",
      "epoch 4  | loss: 21.11265| val_0_rmse: 4.72791 |  0:00:00s\n",
      "epoch 5  | loss: 16.75206| val_0_rmse: 4.42946 |  0:00:01s\n",
      "epoch 6  | loss: 10.85423| val_0_rmse: 4.48233 |  0:00:01s\n",
      "epoch 7  | loss: 6.37393 | val_0_rmse: 4.58253 |  0:00:01s\n",
      "epoch 8  | loss: 4.20493 | val_0_rmse: 4.02041 |  0:00:01s\n",
      "epoch 9  | loss: 2.51765 | val_0_rmse: 3.17673 |  0:00:01s\n",
      "epoch 10 | loss: 1.75101 | val_0_rmse: 3.32536 |  0:00:02s\n",
      "epoch 11 | loss: 1.45676 | val_0_rmse: 2.54773 |  0:00:02s\n",
      "epoch 12 | loss: 1.44586 | val_0_rmse: 2.79648 |  0:00:02s\n",
      "epoch 13 | loss: 0.85313 | val_0_rmse: 2.12289 |  0:00:02s\n",
      "epoch 14 | loss: 0.83353 | val_0_rmse: 2.28552 |  0:00:02s\n",
      "epoch 15 | loss: 0.78149 | val_0_rmse: 1.80537 |  0:00:03s\n",
      "epoch 16 | loss: 0.66939 | val_0_rmse: 1.8788  |  0:00:03s\n",
      "epoch 17 | loss: 0.62444 | val_0_rmse: 1.36993 |  0:00:03s\n",
      "epoch 18 | loss: 0.61589 | val_0_rmse: 2.07886 |  0:00:03s\n",
      "epoch 19 | loss: 0.62924 | val_0_rmse: 1.43878 |  0:00:03s\n",
      "epoch 20 | loss: 0.50496 | val_0_rmse: 1.46346 |  0:00:03s\n",
      "epoch 21 | loss: 0.47071 | val_0_rmse: 0.98006 |  0:00:04s\n",
      "epoch 22 | loss: 0.34426 | val_0_rmse: 1.26397 |  0:00:04s\n",
      "epoch 23 | loss: 0.40295 | val_0_rmse: 0.80662 |  0:00:04s\n",
      "epoch 24 | loss: 0.49889 | val_0_rmse: 0.98139 |  0:00:04s\n",
      "epoch 25 | loss: 0.3074  | val_0_rmse: 0.9594  |  0:00:04s\n",
      "epoch 26 | loss: 0.2853  | val_0_rmse: 0.74839 |  0:00:04s\n",
      "epoch 27 | loss: 0.26014 | val_0_rmse: 0.91759 |  0:00:05s\n",
      "epoch 28 | loss: 0.21651 | val_0_rmse: 0.73844 |  0:00:05s\n",
      "epoch 29 | loss: 0.16683 | val_0_rmse: 0.62816 |  0:00:05s\n",
      "epoch 30 | loss: 0.16344 | val_0_rmse: 0.68845 |  0:00:05s\n",
      "epoch 31 | loss: 0.13735 | val_0_rmse: 0.62125 |  0:00:05s\n",
      "epoch 32 | loss: 0.15382 | val_0_rmse: 0.65922 |  0:00:05s\n",
      "epoch 33 | loss: 0.14857 | val_0_rmse: 0.53955 |  0:00:06s\n",
      "epoch 34 | loss: 0.13821 | val_0_rmse: 0.50231 |  0:00:06s\n",
      "epoch 35 | loss: 0.1266  | val_0_rmse: 0.6572  |  0:00:06s\n",
      "epoch 36 | loss: 0.16584 | val_0_rmse: 0.64708 |  0:00:06s\n",
      "epoch 37 | loss: 0.13365 | val_0_rmse: 0.43716 |  0:00:06s\n",
      "epoch 38 | loss: 0.12197 | val_0_rmse: 0.54294 |  0:00:07s\n",
      "epoch 39 | loss: 0.11553 | val_0_rmse: 0.48757 |  0:00:07s\n",
      "epoch 40 | loss: 0.08832 | val_0_rmse: 0.452   |  0:00:07s\n",
      "epoch 41 | loss: 0.09728 | val_0_rmse: 0.41785 |  0:00:07s\n",
      "epoch 42 | loss: 0.07895 | val_0_rmse: 0.44273 |  0:00:07s\n",
      "epoch 43 | loss: 0.08107 | val_0_rmse: 0.45407 |  0:00:08s\n",
      "epoch 44 | loss: 0.10291 | val_0_rmse: 0.34574 |  0:00:08s\n",
      "epoch 45 | loss: 0.10731 | val_0_rmse: 0.42064 |  0:00:08s\n",
      "epoch 46 | loss: 0.08306 | val_0_rmse: 0.40246 |  0:00:08s\n",
      "epoch 47 | loss: 0.07881 | val_0_rmse: 0.31063 |  0:00:08s\n",
      "epoch 48 | loss: 0.09213 | val_0_rmse: 0.30613 |  0:00:08s\n",
      "epoch 49 | loss: 0.07317 | val_0_rmse: 0.36106 |  0:00:09s\n",
      "epoch 50 | loss: 0.0764  | val_0_rmse: 0.25599 |  0:00:09s\n",
      "epoch 51 | loss: 0.09565 | val_0_rmse: 0.48409 |  0:00:09s\n",
      "epoch 52 | loss: 0.12129 | val_0_rmse: 0.26175 |  0:00:09s\n",
      "epoch 53 | loss: 0.12596 | val_0_rmse: 0.44067 |  0:00:09s\n",
      "epoch 54 | loss: 0.20408 | val_0_rmse: 0.34673 |  0:00:09s\n",
      "epoch 55 | loss: 0.11204 | val_0_rmse: 0.29392 |  0:00:10s\n",
      "epoch 56 | loss: 0.17563 | val_0_rmse: 0.47412 |  0:00:10s\n",
      "epoch 57 | loss: 0.2207  | val_0_rmse: 0.29772 |  0:00:10s\n",
      "epoch 58 | loss: 0.18337 | val_0_rmse: 0.26113 |  0:00:10s\n",
      "epoch 59 | loss: 0.12713 | val_0_rmse: 0.45192 |  0:00:10s\n",
      "epoch 60 | loss: 0.14162 | val_0_rmse: 0.30275 |  0:00:10s\n",
      "epoch 61 | loss: 0.19817 | val_0_rmse: 0.3217  |  0:00:11s\n",
      "epoch 62 | loss: 0.1327  | val_0_rmse: 0.26206 |  0:00:11s\n",
      "epoch 63 | loss: 0.09881 | val_0_rmse: 0.26452 |  0:00:11s\n",
      "epoch 64 | loss: 0.09431 | val_0_rmse: 0.28352 |  0:00:11s\n",
      "epoch 65 | loss: 0.08386 | val_0_rmse: 0.24738 |  0:00:11s\n",
      "epoch 66 | loss: 0.06344 | val_0_rmse: 0.28548 |  0:00:11s\n",
      "epoch 67 | loss: 0.07077 | val_0_rmse: 0.23847 |  0:00:12s\n",
      "epoch 68 | loss: 0.05966 | val_0_rmse: 0.29123 |  0:00:12s\n",
      "epoch 69 | loss: 0.07803 | val_0_rmse: 0.2449  |  0:00:12s\n",
      "epoch 70 | loss: 0.07416 | val_0_rmse: 0.2935  |  0:00:12s\n",
      "epoch 71 | loss: 0.06949 | val_0_rmse: 0.23907 |  0:00:12s\n",
      "epoch 72 | loss: 0.08147 | val_0_rmse: 0.29667 |  0:00:12s\n",
      "epoch 73 | loss: 0.08493 | val_0_rmse: 0.2381  |  0:00:13s\n",
      "epoch 74 | loss: 0.05752 | val_0_rmse: 0.23665 |  0:00:13s\n",
      "epoch 75 | loss: 0.06392 | val_0_rmse: 0.24573 |  0:00:13s\n",
      "epoch 76 | loss: 0.06664 | val_0_rmse: 0.28707 |  0:00:13s\n",
      "epoch 77 | loss: 0.062   | val_0_rmse: 0.23472 |  0:00:13s\n",
      "epoch 78 | loss: 0.03875 | val_0_rmse: 0.26434 |  0:00:14s\n",
      "epoch 79 | loss: 0.06656 | val_0_rmse: 0.2573  |  0:00:14s\n",
      "epoch 80 | loss: 0.05274 | val_0_rmse: 0.25697 |  0:00:14s\n",
      "epoch 81 | loss: 0.05621 | val_0_rmse: 0.25617 |  0:00:14s\n",
      "epoch 82 | loss: 0.0523  | val_0_rmse: 0.24037 |  0:00:14s\n",
      "epoch 83 | loss: 0.04115 | val_0_rmse: 0.23703 |  0:00:14s\n",
      "epoch 84 | loss: 0.05242 | val_0_rmse: 0.25769 |  0:00:15s\n",
      "epoch 85 | loss: 0.05605 | val_0_rmse: 0.21899 |  0:00:15s\n",
      "epoch 86 | loss: 0.0755  | val_0_rmse: 0.2762  |  0:00:15s\n",
      "epoch 87 | loss: 0.05195 | val_0_rmse: 0.24283 |  0:00:15s\n",
      "epoch 88 | loss: 0.04945 | val_0_rmse: 0.22096 |  0:00:15s\n",
      "epoch 89 | loss: 0.04401 | val_0_rmse: 0.24125 |  0:00:15s\n",
      "epoch 90 | loss: 0.04694 | val_0_rmse: 0.23536 |  0:00:16s\n",
      "epoch 91 | loss: 0.04479 | val_0_rmse: 0.25035 |  0:00:16s\n",
      "epoch 92 | loss: 0.07248 | val_0_rmse: 0.26174 |  0:00:16s\n",
      "epoch 93 | loss: 0.08468 | val_0_rmse: 0.21857 |  0:00:16s\n",
      "epoch 94 | loss: 0.04543 | val_0_rmse: 0.22134 |  0:00:16s\n",
      "epoch 95 | loss: 0.03503 | val_0_rmse: 0.21659 |  0:00:16s\n",
      "epoch 96 | loss: 0.03051 | val_0_rmse: 0.21884 |  0:00:17s\n",
      "epoch 97 | loss: 0.03751 | val_0_rmse: 0.20323 |  0:00:17s\n",
      "epoch 98 | loss: 0.03251 | val_0_rmse: 0.20607 |  0:00:17s\n",
      "epoch 99 | loss: 0.03162 | val_0_rmse: 0.20679 |  0:00:17s\n",
      "epoch 100| loss: 0.03274 | val_0_rmse: 0.21025 |  0:00:17s\n",
      "epoch 101| loss: 0.03324 | val_0_rmse: 0.20647 |  0:00:18s\n",
      "epoch 102| loss: 0.03371 | val_0_rmse: 0.2149  |  0:00:18s\n",
      "epoch 103| loss: 0.03795 | val_0_rmse: 0.20021 |  0:00:18s\n",
      "epoch 104| loss: 0.03565 | val_0_rmse: 0.20286 |  0:00:18s\n",
      "epoch 105| loss: 0.05115 | val_0_rmse: 0.22414 |  0:00:18s\n",
      "epoch 106| loss: 0.04083 | val_0_rmse: 0.22077 |  0:00:18s\n",
      "epoch 107| loss: 0.03519 | val_0_rmse: 0.19693 |  0:00:19s\n",
      "epoch 108| loss: 0.04125 | val_0_rmse: 0.18696 |  0:00:19s\n",
      "epoch 109| loss: 0.0379  | val_0_rmse: 0.21787 |  0:00:19s\n",
      "epoch 110| loss: 0.03888 | val_0_rmse: 0.22216 |  0:00:19s\n",
      "epoch 111| loss: 0.04027 | val_0_rmse: 0.18421 |  0:00:19s\n",
      "epoch 112| loss: 0.03572 | val_0_rmse: 0.18585 |  0:00:20s\n",
      "epoch 113| loss: 0.03867 | val_0_rmse: 0.22348 |  0:00:20s\n",
      "epoch 114| loss: 0.03464 | val_0_rmse: 0.20875 |  0:00:20s\n",
      "epoch 115| loss: 0.0333  | val_0_rmse: 0.20268 |  0:00:20s\n",
      "epoch 116| loss: 0.04012 | val_0_rmse: 0.2055  |  0:00:20s\n",
      "epoch 117| loss: 0.04218 | val_0_rmse: 0.2231  |  0:00:20s\n",
      "epoch 118| loss: 0.03688 | val_0_rmse: 0.21369 |  0:00:21s\n",
      "epoch 119| loss: 0.03332 | val_0_rmse: 0.21569 |  0:00:21s\n",
      "epoch 120| loss: 0.03585 | val_0_rmse: 0.21293 |  0:00:21s\n",
      "epoch 121| loss: 0.04008 | val_0_rmse: 0.18336 |  0:00:21s\n",
      "epoch 122| loss: 0.02712 | val_0_rmse: 0.18973 |  0:00:21s\n",
      "epoch 123| loss: 0.02964 | val_0_rmse: 0.1909  |  0:00:22s\n",
      "epoch 124| loss: 0.03221 | val_0_rmse: 0.23207 |  0:00:22s\n",
      "epoch 125| loss: 0.03284 | val_0_rmse: 0.18985 |  0:00:22s\n",
      "epoch 126| loss: 0.0369  | val_0_rmse: 0.21751 |  0:00:22s\n",
      "epoch 127| loss: 0.05396 | val_0_rmse: 0.20074 |  0:00:22s\n",
      "epoch 128| loss: 0.03779 | val_0_rmse: 0.19069 |  0:00:22s\n",
      "epoch 129| loss: 0.03475 | val_0_rmse: 0.19947 |  0:00:23s\n",
      "epoch 130| loss: 0.04375 | val_0_rmse: 0.24758 |  0:00:23s\n",
      "epoch 131| loss: 0.04428 | val_0_rmse: 0.22547 |  0:00:23s\n",
      "epoch 132| loss: 0.04898 | val_0_rmse: 0.29656 |  0:00:23s\n",
      "epoch 133| loss: 0.08573 | val_0_rmse: 0.2748  |  0:00:23s\n",
      "epoch 134| loss: 0.06751 | val_0_rmse: 0.2859  |  0:00:23s\n",
      "epoch 135| loss: 0.05887 | val_0_rmse: 0.31378 |  0:00:24s\n",
      "epoch 136| loss: 0.09021 | val_0_rmse: 0.28001 |  0:00:24s\n",
      "epoch 137| loss: 0.05178 | val_0_rmse: 0.31985 |  0:00:24s\n",
      "epoch 138| loss: 0.08377 | val_0_rmse: 0.26562 |  0:00:24s\n",
      "epoch 139| loss: 0.05293 | val_0_rmse: 0.29438 |  0:00:24s\n",
      "epoch 140| loss: 0.06508 | val_0_rmse: 0.2657  |  0:00:24s\n",
      "epoch 141| loss: 0.0569  | val_0_rmse: 0.406   |  0:00:25s\n",
      "epoch 142| loss: 0.1544  | val_0_rmse: 0.36592 |  0:00:25s\n",
      "epoch 143| loss: 0.0931  | val_0_rmse: 0.3558  |  0:00:25s\n",
      "epoch 144| loss: 0.04383 | val_0_rmse: 0.37155 |  0:00:25s\n",
      "epoch 145| loss: 0.03918 | val_0_rmse: 0.4153  |  0:00:25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:48:54,197] Trial 90 finished with value: 0.18335619743844317 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2402425438498506, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.01602412719182403, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 146| loss: 0.04149 | val_0_rmse: 0.46533 |  0:00:25s\n",
      "\n",
      "Early stopping occurred at epoch 146 with best_epoch = 121 and best_val_0_rmse = 0.18336\n",
      "Trial 090 | rmse_log=0.18336 | RMSE$=38,984 | MAE$=23,868 | MAPE=13.96% | n_d/n_a=48/24 steps=3 lr=0.01602 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 123.75597| val_0_rmse: 10.04527|  0:00:00s\n",
      "epoch 1  | loss: 74.96069| val_0_rmse: 8.644   |  0:00:00s\n",
      "epoch 2  | loss: 45.39458| val_0_rmse: 7.31396 |  0:00:00s\n",
      "epoch 3  | loss: 32.22216| val_0_rmse: 6.22762 |  0:00:00s\n",
      "epoch 4  | loss: 26.4505 | val_0_rmse: 5.36266 |  0:00:00s\n",
      "epoch 5  | loss: 19.34284| val_0_rmse: 5.10015 |  0:00:01s\n",
      "epoch 6  | loss: 13.22023| val_0_rmse: 5.31314 |  0:00:01s\n",
      "epoch 7  | loss: 6.85891 | val_0_rmse: 5.32046 |  0:00:01s\n",
      "epoch 8  | loss: 3.86779 | val_0_rmse: 4.62741 |  0:00:01s\n",
      "epoch 9  | loss: 2.26451 | val_0_rmse: 3.98546 |  0:00:01s\n",
      "epoch 10 | loss: 1.57184 | val_0_rmse: 3.90895 |  0:00:01s\n",
      "epoch 11 | loss: 1.29928 | val_0_rmse: 2.96234 |  0:00:02s\n",
      "epoch 12 | loss: 1.04845 | val_0_rmse: 2.92179 |  0:00:02s\n",
      "epoch 13 | loss: 1.23413 | val_0_rmse: 2.12966 |  0:00:02s\n",
      "epoch 14 | loss: 0.80018 | val_0_rmse: 2.12303 |  0:00:02s\n",
      "epoch 15 | loss: 0.79027 | val_0_rmse: 1.58342 |  0:00:02s\n",
      "epoch 16 | loss: 0.62072 | val_0_rmse: 1.80488 |  0:00:03s\n",
      "epoch 17 | loss: 0.5724  | val_0_rmse: 1.29252 |  0:00:03s\n",
      "epoch 18 | loss: 0.50591 | val_0_rmse: 1.29767 |  0:00:03s\n",
      "epoch 19 | loss: 0.70214 | val_0_rmse: 1.06367 |  0:00:03s\n",
      "epoch 20 | loss: 0.38119 | val_0_rmse: 1.11962 |  0:00:03s\n",
      "epoch 21 | loss: 0.39871 | val_0_rmse: 0.76309 |  0:00:03s\n",
      "epoch 22 | loss: 0.40123 | val_0_rmse: 0.82294 |  0:00:04s\n",
      "epoch 23 | loss: 0.23108 | val_0_rmse: 0.77696 |  0:00:04s\n",
      "epoch 24 | loss: 0.2895  | val_0_rmse: 0.63935 |  0:00:04s\n",
      "epoch 25 | loss: 0.2599  | val_0_rmse: 0.7545  |  0:00:04s\n",
      "epoch 26 | loss: 0.17579 | val_0_rmse: 0.57911 |  0:00:04s\n",
      "epoch 27 | loss: 0.1705  | val_0_rmse: 0.5811  |  0:00:05s\n",
      "epoch 28 | loss: 0.15929 | val_0_rmse: 0.55944 |  0:00:05s\n",
      "epoch 29 | loss: 0.14426 | val_0_rmse: 0.55566 |  0:00:05s\n",
      "epoch 30 | loss: 0.13797 | val_0_rmse: 0.79204 |  0:00:05s\n",
      "epoch 31 | loss: 0.18071 | val_0_rmse: 0.64301 |  0:00:05s\n",
      "epoch 32 | loss: 0.13627 | val_0_rmse: 0.60973 |  0:00:05s\n",
      "epoch 33 | loss: 0.21479 | val_0_rmse: 0.40761 |  0:00:06s\n",
      "epoch 34 | loss: 0.12308 | val_0_rmse: 0.59754 |  0:00:06s\n",
      "epoch 35 | loss: 0.14137 | val_0_rmse: 0.33554 |  0:00:06s\n",
      "epoch 36 | loss: 0.09986 | val_0_rmse: 0.4484  |  0:00:06s\n",
      "epoch 37 | loss: 0.12132 | val_0_rmse: 0.34227 |  0:00:06s\n",
      "epoch 38 | loss: 0.09498 | val_0_rmse: 0.46926 |  0:00:06s\n",
      "epoch 39 | loss: 0.13519 | val_0_rmse: 0.26982 |  0:00:07s\n",
      "epoch 40 | loss: 0.07596 | val_0_rmse: 0.34519 |  0:00:07s\n",
      "epoch 41 | loss: 0.10257 | val_0_rmse: 0.25354 |  0:00:07s\n",
      "epoch 42 | loss: 0.08964 | val_0_rmse: 0.41993 |  0:00:07s\n",
      "epoch 43 | loss: 0.07519 | val_0_rmse: 0.28584 |  0:00:07s\n",
      "epoch 44 | loss: 0.0696  | val_0_rmse: 0.27336 |  0:00:07s\n",
      "epoch 45 | loss: 0.06997 | val_0_rmse: 0.40701 |  0:00:08s\n",
      "epoch 46 | loss: 0.12915 | val_0_rmse: 0.22756 |  0:00:08s\n",
      "epoch 47 | loss: 0.08781 | val_0_rmse: 0.32423 |  0:00:08s\n",
      "epoch 48 | loss: 0.07717 | val_0_rmse: 0.21185 |  0:00:08s\n",
      "epoch 49 | loss: 0.10326 | val_0_rmse: 0.2732  |  0:00:08s\n",
      "epoch 50 | loss: 0.06397 | val_0_rmse: 0.22802 |  0:00:09s\n",
      "epoch 51 | loss: 0.1276  | val_0_rmse: 0.24533 |  0:00:09s\n",
      "epoch 52 | loss: 0.06501 | val_0_rmse: 0.26591 |  0:00:09s\n",
      "epoch 53 | loss: 0.11691 | val_0_rmse: 0.26285 |  0:00:09s\n",
      "epoch 54 | loss: 0.0727  | val_0_rmse: 0.23317 |  0:00:09s\n",
      "epoch 55 | loss: 0.13303 | val_0_rmse: 0.24048 |  0:00:09s\n",
      "epoch 56 | loss: 0.06154 | val_0_rmse: 0.2629  |  0:00:10s\n",
      "epoch 57 | loss: 0.0592  | val_0_rmse: 0.21448 |  0:00:10s\n",
      "epoch 58 | loss: 0.05545 | val_0_rmse: 0.2152  |  0:00:10s\n",
      "epoch 59 | loss: 0.05627 | val_0_rmse: 0.23918 |  0:00:10s\n",
      "epoch 60 | loss: 0.05068 | val_0_rmse: 0.24494 |  0:00:10s\n",
      "epoch 61 | loss: 0.05262 | val_0_rmse: 0.21334 |  0:00:10s\n",
      "epoch 62 | loss: 0.05186 | val_0_rmse: 0.20092 |  0:00:11s\n",
      "epoch 63 | loss: 0.05294 | val_0_rmse: 0.23652 |  0:00:11s\n",
      "epoch 64 | loss: 0.04862 | val_0_rmse: 0.23989 |  0:00:11s\n",
      "epoch 65 | loss: 0.05044 | val_0_rmse: 0.19877 |  0:00:11s\n",
      "epoch 66 | loss: 0.05013 | val_0_rmse: 0.2253  |  0:00:11s\n",
      "epoch 67 | loss: 0.06788 | val_0_rmse: 0.21162 |  0:00:11s\n",
      "epoch 68 | loss: 0.05933 | val_0_rmse: 0.25529 |  0:00:12s\n",
      "epoch 69 | loss: 0.0627  | val_0_rmse: 0.20141 |  0:00:12s\n",
      "epoch 70 | loss: 0.06187 | val_0_rmse: 0.22782 |  0:00:12s\n",
      "epoch 71 | loss: 0.06313 | val_0_rmse: 0.24577 |  0:00:12s\n",
      "epoch 72 | loss: 0.05837 | val_0_rmse: 0.21617 |  0:00:12s\n",
      "epoch 73 | loss: 0.05723 | val_0_rmse: 0.20585 |  0:00:13s\n",
      "epoch 74 | loss: 0.07705 | val_0_rmse: 0.28813 |  0:00:13s\n",
      "epoch 75 | loss: 0.06539 | val_0_rmse: 0.20702 |  0:00:13s\n",
      "epoch 76 | loss: 0.05338 | val_0_rmse: 0.2326  |  0:00:13s\n",
      "epoch 77 | loss: 0.08064 | val_0_rmse: 0.25082 |  0:00:13s\n",
      "epoch 78 | loss: 0.07166 | val_0_rmse: 0.26462 |  0:00:13s\n",
      "epoch 79 | loss: 0.06698 | val_0_rmse: 0.34348 |  0:00:14s\n",
      "epoch 80 | loss: 0.12495 | val_0_rmse: 0.28216 |  0:00:14s\n",
      "epoch 81 | loss: 0.12784 | val_0_rmse: 0.2192  |  0:00:14s\n",
      "epoch 82 | loss: 0.11218 | val_0_rmse: 0.36213 |  0:00:14s\n",
      "epoch 83 | loss: 0.13796 | val_0_rmse: 0.29858 |  0:00:14s\n",
      "epoch 84 | loss: 0.0926  | val_0_rmse: 0.27367 |  0:00:14s\n",
      "epoch 85 | loss: 0.17117 | val_0_rmse: 0.22482 |  0:00:14s\n",
      "epoch 86 | loss: 0.09589 | val_0_rmse: 0.34148 |  0:00:15s\n",
      "epoch 87 | loss: 0.09037 | val_0_rmse: 0.29323 |  0:00:15s\n",
      "epoch 88 | loss: 0.11541 | val_0_rmse: 0.2734  |  0:00:15s\n",
      "epoch 89 | loss: 0.14442 | val_0_rmse: 0.23635 |  0:00:15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:49:10,468] Trial 91 finished with value: 0.19877154281495168 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2390049037816793, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.01595898622097313, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 90 | loss: 0.07855 | val_0_rmse: 0.3041  |  0:00:15s\n",
      "\n",
      "Early stopping occurred at epoch 90 with best_epoch = 65 and best_val_0_rmse = 0.19877\n",
      "Trial 091 | rmse_log=0.19877 | RMSE$=36,442 | MAE$=23,452 | MAPE=14.72% | n_d/n_a=48/24 steps=3 lr=0.01596 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 122.01464| val_0_rmse: 9.6084  |  0:00:00s\n",
      "epoch 1  | loss: 65.3799 | val_0_rmse: 7.71171 |  0:00:00s\n",
      "epoch 2  | loss: 35.19692| val_0_rmse: 5.83277 |  0:00:00s\n",
      "epoch 3  | loss: 21.94209| val_0_rmse: 4.59269 |  0:00:00s\n",
      "epoch 4  | loss: 18.94124| val_0_rmse: 4.55584 |  0:00:00s\n",
      "epoch 5  | loss: 11.44827| val_0_rmse: 4.85519 |  0:00:01s\n",
      "epoch 6  | loss: 4.70076 | val_0_rmse: 4.70114 |  0:00:01s\n",
      "epoch 7  | loss: 3.55849 | val_0_rmse: 3.56464 |  0:00:01s\n",
      "epoch 8  | loss: 1.79998 | val_0_rmse: 3.32552 |  0:00:01s\n",
      "epoch 9  | loss: 1.5399  | val_0_rmse: 2.48206 |  0:00:01s\n",
      "epoch 10 | loss: 1.33513 | val_0_rmse: 2.40382 |  0:00:02s\n",
      "epoch 11 | loss: 1.0515  | val_0_rmse: 1.89167 |  0:00:02s\n",
      "epoch 12 | loss: 1.1262  | val_0_rmse: 1.99148 |  0:00:02s\n",
      "epoch 13 | loss: 0.80509 | val_0_rmse: 1.43799 |  0:00:02s\n",
      "epoch 14 | loss: 0.67081 | val_0_rmse: 1.62792 |  0:00:02s\n",
      "epoch 15 | loss: 0.79327 | val_0_rmse: 1.22023 |  0:00:02s\n",
      "epoch 16 | loss: 0.4106  | val_0_rmse: 1.26877 |  0:00:03s\n",
      "epoch 17 | loss: 0.46319 | val_0_rmse: 1.11139 |  0:00:03s\n",
      "epoch 18 | loss: 0.40442 | val_0_rmse: 1.09287 |  0:00:03s\n",
      "epoch 19 | loss: 0.25136 | val_0_rmse: 1.0303  |  0:00:03s\n",
      "epoch 20 | loss: 0.28709 | val_0_rmse: 0.88512 |  0:00:03s\n",
      "epoch 21 | loss: 0.272   | val_0_rmse: 0.64951 |  0:00:04s\n",
      "epoch 22 | loss: 0.29635 | val_0_rmse: 0.66711 |  0:00:04s\n",
      "epoch 23 | loss: 0.18042 | val_0_rmse: 0.72047 |  0:00:04s\n",
      "epoch 24 | loss: 0.19777 | val_0_rmse: 0.70886 |  0:00:04s\n",
      "epoch 25 | loss: 0.16609 | val_0_rmse: 0.79772 |  0:00:04s\n",
      "epoch 26 | loss: 0.12514 | val_0_rmse: 0.76887 |  0:00:04s\n",
      "epoch 27 | loss: 0.16902 | val_0_rmse: 0.87962 |  0:00:05s\n",
      "epoch 28 | loss: 0.18349 | val_0_rmse: 0.58956 |  0:00:05s\n",
      "epoch 29 | loss: 0.12699 | val_0_rmse: 0.51782 |  0:00:05s\n",
      "epoch 30 | loss: 0.16414 | val_0_rmse: 0.72216 |  0:00:05s\n",
      "epoch 31 | loss: 0.11179 | val_0_rmse: 0.79641 |  0:00:05s\n",
      "epoch 32 | loss: 0.17902 | val_0_rmse: 0.52373 |  0:00:06s\n",
      "epoch 33 | loss: 0.13475 | val_0_rmse: 0.37584 |  0:00:06s\n",
      "epoch 34 | loss: 0.11587 | val_0_rmse: 0.54804 |  0:00:06s\n",
      "epoch 35 | loss: 0.10315 | val_0_rmse: 0.3858  |  0:00:06s\n",
      "epoch 36 | loss: 0.10734 | val_0_rmse: 0.43153 |  0:00:06s\n",
      "epoch 37 | loss: 0.15772 | val_0_rmse: 0.23695 |  0:00:06s\n",
      "epoch 38 | loss: 0.15514 | val_0_rmse: 0.54443 |  0:00:07s\n",
      "epoch 39 | loss: 0.12119 | val_0_rmse: 0.24784 |  0:00:07s\n",
      "epoch 40 | loss: 0.24072 | val_0_rmse: 0.44401 |  0:00:07s\n",
      "epoch 41 | loss: 0.10372 | val_0_rmse: 0.21737 |  0:00:07s\n",
      "epoch 42 | loss: 0.11646 | val_0_rmse: 0.32421 |  0:00:07s\n",
      "epoch 43 | loss: 0.09384 | val_0_rmse: 0.3669  |  0:00:07s\n",
      "epoch 44 | loss: 0.12709 | val_0_rmse: 0.24103 |  0:00:08s\n",
      "epoch 45 | loss: 0.16273 | val_0_rmse: 0.23758 |  0:00:08s\n",
      "epoch 46 | loss: 0.08499 | val_0_rmse: 0.31076 |  0:00:08s\n",
      "epoch 47 | loss: 0.12821 | val_0_rmse: 0.26732 |  0:00:08s\n",
      "epoch 48 | loss: 0.17156 | val_0_rmse: 0.40832 |  0:00:08s\n",
      "epoch 49 | loss: 0.08551 | val_0_rmse: 0.22269 |  0:00:09s\n",
      "epoch 50 | loss: 0.14209 | val_0_rmse: 0.2404  |  0:00:09s\n",
      "epoch 51 | loss: 0.05691 | val_0_rmse: 0.26998 |  0:00:09s\n",
      "epoch 52 | loss: 0.05885 | val_0_rmse: 0.30021 |  0:00:09s\n",
      "epoch 53 | loss: 0.06896 | val_0_rmse: 0.26937 |  0:00:09s\n",
      "epoch 54 | loss: 0.05335 | val_0_rmse: 0.26466 |  0:00:09s\n",
      "epoch 55 | loss: 0.04943 | val_0_rmse: 0.25063 |  0:00:10s\n",
      "epoch 56 | loss: 0.05222 | val_0_rmse: 0.24878 |  0:00:10s\n",
      "epoch 57 | loss: 0.0486  | val_0_rmse: 0.22926 |  0:00:10s\n",
      "epoch 58 | loss: 0.04857 | val_0_rmse: 0.22245 |  0:00:10s\n",
      "epoch 59 | loss: 0.04666 | val_0_rmse: 0.2938  |  0:00:10s\n",
      "epoch 60 | loss: 0.06082 | val_0_rmse: 0.241   |  0:00:10s\n",
      "epoch 61 | loss: 0.05298 | val_0_rmse: 0.211   |  0:00:11s\n",
      "epoch 62 | loss: 0.0509  | val_0_rmse: 0.20812 |  0:00:11s\n",
      "epoch 63 | loss: 0.04867 | val_0_rmse: 0.25686 |  0:00:11s\n",
      "epoch 64 | loss: 0.04793 | val_0_rmse: 0.25631 |  0:00:11s\n",
      "epoch 65 | loss: 0.05103 | val_0_rmse: 0.20236 |  0:00:11s\n",
      "epoch 66 | loss: 0.07202 | val_0_rmse: 0.20606 |  0:00:11s\n",
      "epoch 67 | loss: 0.05211 | val_0_rmse: 0.25925 |  0:00:12s\n",
      "epoch 68 | loss: 0.04595 | val_0_rmse: 0.28519 |  0:00:12s\n",
      "epoch 69 | loss: 0.06127 | val_0_rmse: 0.21836 |  0:00:12s\n",
      "epoch 70 | loss: 0.04505 | val_0_rmse: 0.20339 |  0:00:12s\n",
      "epoch 71 | loss: 0.05067 | val_0_rmse: 0.20879 |  0:00:12s\n",
      "epoch 72 | loss: 0.05693 | val_0_rmse: 0.24102 |  0:00:12s\n",
      "epoch 73 | loss: 0.04387 | val_0_rmse: 0.25059 |  0:00:13s\n",
      "epoch 74 | loss: 0.05994 | val_0_rmse: 0.19929 |  0:00:13s\n",
      "epoch 75 | loss: 0.04912 | val_0_rmse: 0.2578  |  0:00:13s\n",
      "epoch 76 | loss: 0.0516  | val_0_rmse: 0.22705 |  0:00:13s\n",
      "epoch 77 | loss: 0.04813 | val_0_rmse: 0.21774 |  0:00:13s\n",
      "epoch 78 | loss: 0.03874 | val_0_rmse: 0.20107 |  0:00:14s\n",
      "epoch 79 | loss: 0.05128 | val_0_rmse: 0.26959 |  0:00:14s\n",
      "epoch 80 | loss: 0.04712 | val_0_rmse: 0.23044 |  0:00:14s\n",
      "epoch 81 | loss: 0.03687 | val_0_rmse: 0.20294 |  0:00:14s\n",
      "epoch 82 | loss: 0.03174 | val_0_rmse: 0.21208 |  0:00:14s\n",
      "epoch 83 | loss: 0.0328  | val_0_rmse: 0.22048 |  0:00:14s\n",
      "epoch 84 | loss: 0.04063 | val_0_rmse: 0.208   |  0:00:15s\n",
      "epoch 85 | loss: 0.03961 | val_0_rmse: 0.29832 |  0:00:15s\n",
      "epoch 86 | loss: 0.07469 | val_0_rmse: 0.198   |  0:00:15s\n",
      "epoch 87 | loss: 0.03766 | val_0_rmse: 0.19352 |  0:00:15s\n",
      "epoch 88 | loss: 0.04836 | val_0_rmse: 0.2581  |  0:00:15s\n",
      "epoch 89 | loss: 0.04613 | val_0_rmse: 0.20895 |  0:00:15s\n",
      "epoch 90 | loss: 0.0357  | val_0_rmse: 0.197   |  0:00:15s\n",
      "epoch 91 | loss: 0.0374  | val_0_rmse: 0.21292 |  0:00:16s\n",
      "epoch 92 | loss: 0.07097 | val_0_rmse: 0.23994 |  0:00:16s\n",
      "epoch 93 | loss: 0.08396 | val_0_rmse: 0.29054 |  0:00:16s\n",
      "epoch 94 | loss: 0.05301 | val_0_rmse: 0.19985 |  0:00:16s\n",
      "epoch 95 | loss: 0.04363 | val_0_rmse: 0.21123 |  0:00:16s\n",
      "epoch 96 | loss: 0.08184 | val_0_rmse: 0.30782 |  0:00:17s\n",
      "epoch 97 | loss: 0.07882 | val_0_rmse: 0.37964 |  0:00:17s\n",
      "epoch 98 | loss: 0.12105 | val_0_rmse: 0.25703 |  0:00:17s\n",
      "epoch 99 | loss: 0.06665 | val_0_rmse: 0.29908 |  0:00:17s\n",
      "epoch 100| loss: 0.14816 | val_0_rmse: 0.20093 |  0:00:17s\n",
      "epoch 101| loss: 0.11432 | val_0_rmse: 0.49084 |  0:00:17s\n",
      "epoch 102| loss: 0.16776 | val_0_rmse: 0.29866 |  0:00:18s\n",
      "epoch 103| loss: 0.11295 | val_0_rmse: 0.24679 |  0:00:18s\n",
      "epoch 104| loss: 0.07127 | val_0_rmse: 0.19272 |  0:00:18s\n",
      "epoch 105| loss: 0.03636 | val_0_rmse: 0.2782  |  0:00:18s\n",
      "epoch 106| loss: 0.05735 | val_0_rmse: 0.20441 |  0:00:18s\n",
      "epoch 107| loss: 0.03678 | val_0_rmse: 0.19199 |  0:00:18s\n",
      "epoch 108| loss: 0.02768 | val_0_rmse: 0.19242 |  0:00:19s\n",
      "epoch 109| loss: 0.04341 | val_0_rmse: 0.24451 |  0:00:19s\n",
      "epoch 110| loss: 0.05849 | val_0_rmse: 0.30395 |  0:00:19s\n",
      "epoch 111| loss: 0.13226 | val_0_rmse: 0.19069 |  0:00:19s\n",
      "epoch 112| loss: 0.12398 | val_0_rmse: 0.47309 |  0:00:19s\n",
      "epoch 113| loss: 0.16188 | val_0_rmse: 0.26916 |  0:00:19s\n",
      "epoch 114| loss: 0.07125 | val_0_rmse: 0.36753 |  0:00:20s\n",
      "epoch 115| loss: 0.20225 | val_0_rmse: 0.29102 |  0:00:20s\n",
      "epoch 116| loss: 0.07454 | val_0_rmse: 0.30451 |  0:00:20s\n",
      "epoch 117| loss: 0.07521 | val_0_rmse: 0.42147 |  0:00:20s\n",
      "epoch 118| loss: 0.15691 | val_0_rmse: 0.24716 |  0:00:20s\n",
      "epoch 119| loss: 0.13339 | val_0_rmse: 0.21357 |  0:00:21s\n",
      "epoch 120| loss: 0.07191 | val_0_rmse: 0.35487 |  0:00:21s\n",
      "epoch 121| loss: 0.07846 | val_0_rmse: 0.37594 |  0:00:21s\n",
      "epoch 122| loss: 0.17289 | val_0_rmse: 0.2594  |  0:00:21s\n",
      "epoch 123| loss: 0.09373 | val_0_rmse: 0.20834 |  0:00:21s\n",
      "epoch 124| loss: 0.07531 | val_0_rmse: 0.36181 |  0:00:21s\n",
      "epoch 125| loss: 0.10309 | val_0_rmse: 0.36723 |  0:00:22s\n",
      "epoch 126| loss: 0.11488 | val_0_rmse: 0.24364 |  0:00:22s\n",
      "epoch 127| loss: 0.09732 | val_0_rmse: 0.1892  |  0:00:22s\n",
      "epoch 128| loss: 0.06418 | val_0_rmse: 0.29615 |  0:00:22s\n",
      "epoch 129| loss: 0.05242 | val_0_rmse: 0.21132 |  0:00:22s\n",
      "epoch 130| loss: 0.03469 | val_0_rmse: 0.20915 |  0:00:22s\n",
      "epoch 131| loss: 0.03426 | val_0_rmse: 0.18506 |  0:00:23s\n",
      "epoch 132| loss: 0.05023 | val_0_rmse: 0.19771 |  0:00:23s\n",
      "epoch 133| loss: 0.06871 | val_0_rmse: 0.27449 |  0:00:23s\n",
      "epoch 134| loss: 0.07631 | val_0_rmse: 0.46581 |  0:00:23s\n",
      "epoch 135| loss: 0.19076 | val_0_rmse: 0.21696 |  0:00:23s\n",
      "epoch 136| loss: 0.08516 | val_0_rmse: 0.18984 |  0:00:24s\n",
      "epoch 137| loss: 0.07349 | val_0_rmse: 0.3958  |  0:00:24s\n",
      "epoch 138| loss: 0.08572 | val_0_rmse: 0.32693 |  0:00:24s\n",
      "epoch 139| loss: 0.12841 | val_0_rmse: 0.2825  |  0:00:24s\n",
      "epoch 140| loss: 0.11086 | val_0_rmse: 0.22493 |  0:00:24s\n",
      "epoch 141| loss: 0.07114 | val_0_rmse: 0.33506 |  0:00:25s\n",
      "epoch 142| loss: 0.08683 | val_0_rmse: 0.36648 |  0:00:25s\n",
      "epoch 143| loss: 0.10497 | val_0_rmse: 0.26872 |  0:00:25s\n",
      "epoch 144| loss: 0.143   | val_0_rmse: 0.23022 |  0:00:25s\n",
      "epoch 145| loss: 0.0562  | val_0_rmse: 0.31728 |  0:00:25s\n",
      "epoch 146| loss: 0.05907 | val_0_rmse: 0.3527  |  0:00:25s\n",
      "epoch 147| loss: 0.13256 | val_0_rmse: 0.29924 |  0:00:26s\n",
      "epoch 148| loss: 0.13952 | val_0_rmse: 0.26968 |  0:00:26s\n",
      "epoch 149| loss: 0.05779 | val_0_rmse: 0.26579 |  0:00:26s\n",
      "epoch 150| loss: 0.05066 | val_0_rmse: 0.42176 |  0:00:26s\n",
      "epoch 151| loss: 0.1492  | val_0_rmse: 0.24598 |  0:00:26s\n",
      "epoch 152| loss: 0.12928 | val_0_rmse: 0.22503 |  0:00:26s\n",
      "epoch 153| loss: 0.05819 | val_0_rmse: 0.29765 |  0:00:27s\n",
      "epoch 154| loss: 0.05892 | val_0_rmse: 0.36649 |  0:00:27s\n",
      "epoch 155| loss: 0.14877 | val_0_rmse: 0.28833 |  0:00:27s\n",
      "epoch 156| loss: 0.1316  | val_0_rmse: 0.25752 |  0:00:27s\n",
      "\n",
      "Early stopping occurred at epoch 156 with best_epoch = 131 and best_val_0_rmse = 0.18506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:49:38,398] Trial 92 finished with value: 0.18505559068172692 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2188700982343552, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.018354315398202016, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 092 | rmse_log=0.18506 | RMSE$=31,402 | MAE$=21,327 | MAPE=13.95% | n_d/n_a=48/24 steps=3 lr=0.01835 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 122.28423| val_0_rmse: 9.56945 |  0:00:00s\n",
      "epoch 1  | loss: 66.91282| val_0_rmse: 7.47295 |  0:00:00s\n",
      "epoch 2  | loss: 40.47143| val_0_rmse: 5.59472 |  0:00:00s\n",
      "epoch 3  | loss: 25.43899| val_0_rmse: 4.47812 |  0:00:00s\n",
      "epoch 4  | loss: 21.02753| val_0_rmse: 4.38535 |  0:00:00s\n",
      "epoch 5  | loss: 10.84658| val_0_rmse: 4.97457 |  0:00:01s\n",
      "epoch 6  | loss: 6.02952 | val_0_rmse: 5.15955 |  0:00:01s\n",
      "epoch 7  | loss: 4.44441 | val_0_rmse: 4.04947 |  0:00:01s\n",
      "epoch 8  | loss: 2.6935  | val_0_rmse: 3.4584  |  0:00:01s\n",
      "epoch 9  | loss: 1.70805 | val_0_rmse: 3.31083 |  0:00:01s\n",
      "epoch 10 | loss: 1.47574 | val_0_rmse: 2.68536 |  0:00:02s\n",
      "epoch 11 | loss: 1.31155 | val_0_rmse: 2.69133 |  0:00:02s\n",
      "epoch 12 | loss: 1.14829 | val_0_rmse: 2.18587 |  0:00:02s\n",
      "epoch 13 | loss: 0.8131  | val_0_rmse: 2.16289 |  0:00:02s\n",
      "epoch 14 | loss: 0.98448 | val_0_rmse: 1.67571 |  0:00:02s\n",
      "epoch 15 | loss: 0.69757 | val_0_rmse: 1.84592 |  0:00:03s\n",
      "epoch 16 | loss: 0.7577  | val_0_rmse: 1.37498 |  0:00:03s\n",
      "epoch 17 | loss: 0.67619 | val_0_rmse: 1.28422 |  0:00:03s\n",
      "epoch 18 | loss: 0.55773 | val_0_rmse: 1.28086 |  0:00:03s\n",
      "epoch 19 | loss: 0.53879 | val_0_rmse: 0.93386 |  0:00:03s\n",
      "epoch 20 | loss: 0.65027 | val_0_rmse: 1.45855 |  0:00:03s\n",
      "epoch 21 | loss: 0.5549  | val_0_rmse: 0.74259 |  0:00:04s\n",
      "epoch 22 | loss: 0.72319 | val_0_rmse: 1.04565 |  0:00:04s\n",
      "epoch 23 | loss: 0.30621 | val_0_rmse: 0.9226  |  0:00:04s\n",
      "epoch 24 | loss: 0.27727 | val_0_rmse: 1.04738 |  0:00:04s\n",
      "epoch 25 | loss: 0.26462 | val_0_rmse: 0.73046 |  0:00:04s\n",
      "epoch 26 | loss: 0.23541 | val_0_rmse: 0.95056 |  0:00:04s\n",
      "epoch 27 | loss: 0.38257 | val_0_rmse: 0.74523 |  0:00:05s\n",
      "epoch 28 | loss: 0.22079 | val_0_rmse: 0.76762 |  0:00:05s\n",
      "epoch 29 | loss: 0.28421 | val_0_rmse: 0.88987 |  0:00:05s\n",
      "epoch 30 | loss: 0.26058 | val_0_rmse: 0.64616 |  0:00:05s\n",
      "epoch 31 | loss: 0.23531 | val_0_rmse: 0.85042 |  0:00:05s\n",
      "epoch 32 | loss: 0.22005 | val_0_rmse: 0.86099 |  0:00:06s\n",
      "epoch 33 | loss: 0.19658 | val_0_rmse: 0.57581 |  0:00:06s\n",
      "epoch 34 | loss: 0.23482 | val_0_rmse: 0.68251 |  0:00:06s\n",
      "epoch 35 | loss: 0.11663 | val_0_rmse: 0.61584 |  0:00:06s\n",
      "epoch 36 | loss: 0.1771  | val_0_rmse: 0.68035 |  0:00:06s\n",
      "epoch 37 | loss: 0.19218 | val_0_rmse: 0.40861 |  0:00:06s\n",
      "epoch 38 | loss: 0.18761 | val_0_rmse: 0.71098 |  0:00:07s\n",
      "epoch 39 | loss: 0.16908 | val_0_rmse: 0.26332 |  0:00:07s\n",
      "epoch 40 | loss: 0.22892 | val_0_rmse: 0.65627 |  0:00:07s\n",
      "epoch 41 | loss: 0.15579 | val_0_rmse: 0.29578 |  0:00:07s\n",
      "epoch 42 | loss: 0.25352 | val_0_rmse: 0.38993 |  0:00:07s\n",
      "epoch 43 | loss: 0.123   | val_0_rmse: 0.50372 |  0:00:07s\n",
      "epoch 44 | loss: 0.1617  | val_0_rmse: 0.22309 |  0:00:08s\n",
      "epoch 45 | loss: 0.08793 | val_0_rmse: 0.32464 |  0:00:08s\n",
      "epoch 46 | loss: 0.08615 | val_0_rmse: 0.41391 |  0:00:08s\n",
      "epoch 47 | loss: 0.10358 | val_0_rmse: 0.20747 |  0:00:08s\n",
      "epoch 48 | loss: 0.1272  | val_0_rmse: 0.41877 |  0:00:08s\n",
      "epoch 49 | loss: 0.10059 | val_0_rmse: 0.2015  |  0:00:09s\n",
      "epoch 50 | loss: 0.12516 | val_0_rmse: 0.48413 |  0:00:09s\n",
      "epoch 51 | loss: 0.14671 | val_0_rmse: 0.2286  |  0:00:09s\n",
      "epoch 52 | loss: 0.27856 | val_0_rmse: 0.25934 |  0:00:09s\n",
      "epoch 53 | loss: 0.22469 | val_0_rmse: 0.5392  |  0:00:09s\n",
      "epoch 54 | loss: 0.22415 | val_0_rmse: 0.2523  |  0:00:09s\n",
      "epoch 55 | loss: 0.30619 | val_0_rmse: 0.34732 |  0:00:10s\n",
      "epoch 56 | loss: 0.14567 | val_0_rmse: 0.2485  |  0:00:10s\n",
      "epoch 57 | loss: 0.10366 | val_0_rmse: 0.20768 |  0:00:10s\n",
      "epoch 58 | loss: 0.0922  | val_0_rmse: 0.26266 |  0:00:10s\n",
      "epoch 59 | loss: 0.07963 | val_0_rmse: 0.19433 |  0:00:10s\n",
      "epoch 60 | loss: 0.09043 | val_0_rmse: 0.29051 |  0:00:10s\n",
      "epoch 61 | loss: 0.07441 | val_0_rmse: 0.19558 |  0:00:11s\n",
      "epoch 62 | loss: 0.06118 | val_0_rmse: 0.23058 |  0:00:11s\n",
      "epoch 63 | loss: 0.0985  | val_0_rmse: 0.20459 |  0:00:11s\n",
      "epoch 64 | loss: 0.06377 | val_0_rmse: 0.18603 |  0:00:11s\n",
      "epoch 65 | loss: 0.03867 | val_0_rmse: 0.317   |  0:00:11s\n",
      "epoch 66 | loss: 0.11697 | val_0_rmse: 0.19606 |  0:00:11s\n",
      "epoch 67 | loss: 0.09676 | val_0_rmse: 0.20074 |  0:00:12s\n",
      "epoch 68 | loss: 0.05275 | val_0_rmse: 0.17812 |  0:00:12s\n",
      "epoch 69 | loss: 0.04451 | val_0_rmse: 0.22243 |  0:00:12s\n",
      "epoch 70 | loss: 0.04296 | val_0_rmse: 0.17519 |  0:00:12s\n",
      "epoch 71 | loss: 0.06554 | val_0_rmse: 0.1698  |  0:00:12s\n",
      "epoch 72 | loss: 0.0344  | val_0_rmse: 0.19626 |  0:00:13s\n",
      "epoch 73 | loss: 0.04115 | val_0_rmse: 0.22889 |  0:00:13s\n",
      "epoch 74 | loss: 0.0428  | val_0_rmse: 0.18651 |  0:00:13s\n",
      "epoch 75 | loss: 0.058   | val_0_rmse: 0.34528 |  0:00:13s\n",
      "epoch 76 | loss: 0.09178 | val_0_rmse: 0.18655 |  0:00:13s\n",
      "epoch 77 | loss: 0.06402 | val_0_rmse: 0.29346 |  0:00:14s\n",
      "epoch 78 | loss: 0.06864 | val_0_rmse: 0.23189 |  0:00:14s\n",
      "epoch 79 | loss: 0.08334 | val_0_rmse: 0.25484 |  0:00:14s\n",
      "epoch 80 | loss: 0.059   | val_0_rmse: 0.2321  |  0:00:14s\n",
      "epoch 81 | loss: 0.08032 | val_0_rmse: 0.26316 |  0:00:14s\n",
      "epoch 82 | loss: 0.05739 | val_0_rmse: 0.21406 |  0:00:14s\n",
      "epoch 83 | loss: 0.07054 | val_0_rmse: 0.26053 |  0:00:15s\n",
      "epoch 84 | loss: 0.05004 | val_0_rmse: 0.22868 |  0:00:15s\n",
      "epoch 85 | loss: 0.06735 | val_0_rmse: 0.25158 |  0:00:15s\n",
      "epoch 86 | loss: 0.05747 | val_0_rmse: 0.2205  |  0:00:15s\n",
      "epoch 87 | loss: 0.06554 | val_0_rmse: 0.25202 |  0:00:15s\n",
      "epoch 88 | loss: 0.05369 | val_0_rmse: 0.23293 |  0:00:15s\n",
      "epoch 89 | loss: 0.06466 | val_0_rmse: 0.23875 |  0:00:16s\n",
      "epoch 90 | loss: 0.05458 | val_0_rmse: 0.24174 |  0:00:16s\n",
      "epoch 91 | loss: 0.06188 | val_0_rmse: 0.22946 |  0:00:16s\n",
      "epoch 92 | loss: 0.05628 | val_0_rmse: 0.25245 |  0:00:16s\n",
      "epoch 93 | loss: 0.06262 | val_0_rmse: 0.23391 |  0:00:16s\n",
      "epoch 94 | loss: 0.05788 | val_0_rmse: 0.2407  |  0:00:16s\n",
      "epoch 95 | loss: 0.05607 | val_0_rmse: 0.23879 |  0:00:17s\n",
      "epoch 96 | loss: 0.05819 | val_0_rmse: 0.23373 |  0:00:17s\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 71 and best_val_0_rmse = 0.1698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:49:56,045] Trial 93 finished with value: 0.16979877422634734 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2189664023148425, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.018201556597916575, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 093 | rmse_log=0.16980 | RMSE$=34,613 | MAE$=23,053 | MAPE=13.01% | n_d/n_a=48/24 steps=3 lr=0.01820 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 123.03526| val_0_rmse: 9.82558 |  0:00:00s\n",
      "epoch 1  | loss: 70.29098| val_0_rmse: 8.19958 |  0:00:00s\n",
      "epoch 2  | loss: 46.74646| val_0_rmse: 6.92931 |  0:00:00s\n",
      "epoch 3  | loss: 27.46983| val_0_rmse: 5.75939 |  0:00:00s\n",
      "epoch 4  | loss: 19.63554| val_0_rmse: 4.92786 |  0:00:00s\n",
      "epoch 5  | loss: 12.52617| val_0_rmse: 4.67781 |  0:00:01s\n",
      "epoch 6  | loss: 6.66096 | val_0_rmse: 4.97017 |  0:00:01s\n",
      "epoch 7  | loss: 4.21168 | val_0_rmse: 4.06826 |  0:00:01s\n",
      "epoch 8  | loss: 2.02822 | val_0_rmse: 3.15302 |  0:00:01s\n",
      "epoch 9  | loss: 1.61692 | val_0_rmse: 2.89498 |  0:00:01s\n",
      "epoch 10 | loss: 1.27426 | val_0_rmse: 2.56825 |  0:00:02s\n",
      "epoch 11 | loss: 0.87975 | val_0_rmse: 2.19448 |  0:00:02s\n",
      "epoch 12 | loss: 0.82245 | val_0_rmse: 2.15926 |  0:00:02s\n",
      "epoch 13 | loss: 0.84598 | val_0_rmse: 1.68223 |  0:00:02s\n",
      "epoch 14 | loss: 0.60404 | val_0_rmse: 1.73729 |  0:00:02s\n",
      "epoch 15 | loss: 0.55007 | val_0_rmse: 1.29917 |  0:00:02s\n",
      "epoch 16 | loss: 0.45532 | val_0_rmse: 1.4381  |  0:00:03s\n",
      "epoch 17 | loss: 0.59441 | val_0_rmse: 1.02012 |  0:00:03s\n",
      "epoch 18 | loss: 0.43243 | val_0_rmse: 1.30669 |  0:00:03s\n",
      "epoch 19 | loss: 0.40663 | val_0_rmse: 1.08014 |  0:00:03s\n",
      "epoch 20 | loss: 0.32348 | val_0_rmse: 0.81812 |  0:00:03s\n",
      "epoch 21 | loss: 0.32222 | val_0_rmse: 1.0497  |  0:00:03s\n",
      "epoch 22 | loss: 0.36494 | val_0_rmse: 0.78105 |  0:00:04s\n",
      "epoch 23 | loss: 0.31018 | val_0_rmse: 0.89357 |  0:00:04s\n",
      "epoch 24 | loss: 0.18986 | val_0_rmse: 0.85314 |  0:00:04s\n",
      "epoch 25 | loss: 0.20169 | val_0_rmse: 0.71409 |  0:00:04s\n",
      "epoch 26 | loss: 0.27628 | val_0_rmse: 0.82755 |  0:00:04s\n",
      "epoch 27 | loss: 0.21075 | val_0_rmse: 0.73964 |  0:00:05s\n",
      "epoch 28 | loss: 0.20637 | val_0_rmse: 0.59778 |  0:00:05s\n",
      "epoch 29 | loss: 0.15613 | val_0_rmse: 0.4556  |  0:00:05s\n",
      "epoch 30 | loss: 0.253   | val_0_rmse: 0.70581 |  0:00:05s\n",
      "epoch 31 | loss: 0.21481 | val_0_rmse: 0.50472 |  0:00:05s\n",
      "epoch 32 | loss: 0.17917 | val_0_rmse: 0.52288 |  0:00:05s\n",
      "epoch 33 | loss: 0.1975  | val_0_rmse: 0.62302 |  0:00:06s\n",
      "epoch 34 | loss: 0.16906 | val_0_rmse: 0.32469 |  0:00:06s\n",
      "epoch 35 | loss: 0.22206 | val_0_rmse: 0.55606 |  0:00:06s\n",
      "epoch 36 | loss: 0.1707  | val_0_rmse: 0.34774 |  0:00:06s\n",
      "epoch 37 | loss: 0.17058 | val_0_rmse: 0.51063 |  0:00:06s\n",
      "epoch 38 | loss: 0.13228 | val_0_rmse: 0.33615 |  0:00:06s\n",
      "epoch 39 | loss: 0.12001 | val_0_rmse: 0.51881 |  0:00:07s\n",
      "epoch 40 | loss: 0.08773 | val_0_rmse: 0.43192 |  0:00:07s\n",
      "epoch 41 | loss: 0.10864 | val_0_rmse: 0.41871 |  0:00:07s\n",
      "epoch 42 | loss: 0.0814  | val_0_rmse: 0.4067  |  0:00:07s\n",
      "epoch 43 | loss: 0.07803 | val_0_rmse: 0.32736 |  0:00:07s\n",
      "epoch 44 | loss: 0.07169 | val_0_rmse: 0.38667 |  0:00:07s\n",
      "epoch 45 | loss: 0.0888  | val_0_rmse: 0.45613 |  0:00:08s\n",
      "epoch 46 | loss: 0.08341 | val_0_rmse: 0.29562 |  0:00:08s\n",
      "epoch 47 | loss: 0.09013 | val_0_rmse: 0.26584 |  0:00:08s\n",
      "epoch 48 | loss: 0.0809  | val_0_rmse: 0.4135  |  0:00:08s\n",
      "epoch 49 | loss: 0.06488 | val_0_rmse: 0.23747 |  0:00:08s\n",
      "epoch 50 | loss: 0.11076 | val_0_rmse: 0.46614 |  0:00:09s\n",
      "epoch 51 | loss: 0.13161 | val_0_rmse: 0.23846 |  0:00:09s\n",
      "epoch 52 | loss: 0.09521 | val_0_rmse: 0.38331 |  0:00:09s\n",
      "epoch 53 | loss: 0.13222 | val_0_rmse: 0.24347 |  0:00:09s\n",
      "epoch 54 | loss: 0.12921 | val_0_rmse: 0.26638 |  0:00:09s\n",
      "epoch 55 | loss: 0.05873 | val_0_rmse: 0.24639 |  0:00:09s\n",
      "epoch 56 | loss: 0.09112 | val_0_rmse: 0.34882 |  0:00:10s\n",
      "epoch 57 | loss: 0.11694 | val_0_rmse: 0.24065 |  0:00:10s\n",
      "epoch 58 | loss: 0.09882 | val_0_rmse: 0.3055  |  0:00:10s\n",
      "epoch 59 | loss: 0.09101 | val_0_rmse: 0.27259 |  0:00:10s\n",
      "epoch 60 | loss: 0.08537 | val_0_rmse: 0.32599 |  0:00:10s\n",
      "epoch 61 | loss: 0.07167 | val_0_rmse: 0.34447 |  0:00:10s\n",
      "epoch 62 | loss: 0.10716 | val_0_rmse: 0.39745 |  0:00:11s\n",
      "epoch 63 | loss: 0.14993 | val_0_rmse: 0.26078 |  0:00:11s\n",
      "epoch 64 | loss: 0.05212 | val_0_rmse: 0.25655 |  0:00:11s\n",
      "epoch 65 | loss: 0.05332 | val_0_rmse: 0.30976 |  0:00:11s\n",
      "epoch 66 | loss: 0.06237 | val_0_rmse: 0.24584 |  0:00:11s\n",
      "epoch 67 | loss: 0.05937 | val_0_rmse: 0.22445 |  0:00:11s\n",
      "epoch 68 | loss: 0.08498 | val_0_rmse: 0.25997 |  0:00:12s\n",
      "epoch 69 | loss: 0.07888 | val_0_rmse: 0.30455 |  0:00:12s\n",
      "epoch 70 | loss: 0.06014 | val_0_rmse: 0.20803 |  0:00:12s\n",
      "epoch 71 | loss: 0.09059 | val_0_rmse: 0.31679 |  0:00:12s\n",
      "epoch 72 | loss: 0.07114 | val_0_rmse: 0.20013 |  0:00:12s\n",
      "epoch 73 | loss: 0.05011 | val_0_rmse: 0.26348 |  0:00:13s\n",
      "epoch 74 | loss: 0.07584 | val_0_rmse: 0.22684 |  0:00:13s\n",
      "epoch 75 | loss: 0.05805 | val_0_rmse: 0.21694 |  0:00:13s\n",
      "epoch 76 | loss: 0.06917 | val_0_rmse: 0.25544 |  0:00:13s\n",
      "epoch 77 | loss: 0.06768 | val_0_rmse: 0.25322 |  0:00:13s\n",
      "epoch 78 | loss: 0.04911 | val_0_rmse: 0.19699 |  0:00:13s\n",
      "epoch 79 | loss: 0.07549 | val_0_rmse: 0.29978 |  0:00:14s\n",
      "epoch 80 | loss: 0.06653 | val_0_rmse: 0.20704 |  0:00:14s\n",
      "epoch 81 | loss: 0.06169 | val_0_rmse: 0.22834 |  0:00:14s\n",
      "epoch 82 | loss: 0.05583 | val_0_rmse: 0.25254 |  0:00:14s\n",
      "epoch 83 | loss: 0.0575  | val_0_rmse: 0.25225 |  0:00:14s\n",
      "epoch 84 | loss: 0.06033 | val_0_rmse: 0.23368 |  0:00:14s\n",
      "epoch 85 | loss: 0.05714 | val_0_rmse: 0.23238 |  0:00:15s\n",
      "epoch 86 | loss: 0.05911 | val_0_rmse: 0.24655 |  0:00:15s\n",
      "epoch 87 | loss: 0.06367 | val_0_rmse: 0.24236 |  0:00:15s\n",
      "epoch 88 | loss: 0.05366 | val_0_rmse: 0.2401  |  0:00:15s\n",
      "epoch 89 | loss: 0.06438 | val_0_rmse: 0.23333 |  0:00:15s\n",
      "epoch 90 | loss: 0.04688 | val_0_rmse: 0.23554 |  0:00:16s\n",
      "epoch 91 | loss: 0.05665 | val_0_rmse: 0.23149 |  0:00:16s\n",
      "epoch 92 | loss: 0.05583 | val_0_rmse: 0.23708 |  0:00:16s\n",
      "epoch 93 | loss: 0.06419 | val_0_rmse: 0.2239  |  0:00:16s\n",
      "epoch 94 | loss: 0.05317 | val_0_rmse: 0.23293 |  0:00:16s\n",
      "epoch 95 | loss: 0.05112 | val_0_rmse: 0.21325 |  0:00:16s\n",
      "epoch 96 | loss: 0.05851 | val_0_rmse: 0.27125 |  0:00:17s\n",
      "epoch 97 | loss: 0.05094 | val_0_rmse: 0.23388 |  0:00:17s\n",
      "epoch 98 | loss: 0.05988 | val_0_rmse: 0.24611 |  0:00:17s\n",
      "epoch 99 | loss: 0.05262 | val_0_rmse: 0.22928 |  0:00:17s\n",
      "epoch 100| loss: 0.05576 | val_0_rmse: 0.24407 |  0:00:17s\n",
      "epoch 101| loss: 0.05443 | val_0_rmse: 0.23732 |  0:00:17s\n",
      "epoch 102| loss: 0.05118 | val_0_rmse: 0.21842 |  0:00:18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:50:14,678] Trial 94 finished with value: 0.19698748562991783 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2230223504697155, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.01705005715902686, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 103| loss: 0.05953 | val_0_rmse: 0.25052 |  0:00:18s\n",
      "\n",
      "Early stopping occurred at epoch 103 with best_epoch = 78 and best_val_0_rmse = 0.19699\n",
      "Trial 094 | rmse_log=0.19699 | RMSE$=38,468 | MAE$=23,247 | MAPE=14.50% | n_d/n_a=48/24 steps=3 lr=0.01705 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 125.11554| val_0_rmse: 9.95818 |  0:00:00s\n",
      "epoch 1  | loss: 74.78875| val_0_rmse: 8.3305  |  0:00:00s\n",
      "epoch 2  | loss: 43.65339| val_0_rmse: 6.70912 |  0:00:00s\n",
      "epoch 3  | loss: 32.13228| val_0_rmse: 5.49453 |  0:00:00s\n",
      "epoch 4  | loss: 28.48545| val_0_rmse: 4.74966 |  0:00:00s\n",
      "epoch 5  | loss: 19.9846 | val_0_rmse: 4.55597 |  0:00:01s\n",
      "epoch 6  | loss: 13.82733| val_0_rmse: 4.67193 |  0:00:01s\n",
      "epoch 7  | loss: 7.72513 | val_0_rmse: 4.85936 |  0:00:01s\n",
      "epoch 8  | loss: 5.20879 | val_0_rmse: 4.32191 |  0:00:01s\n",
      "epoch 9  | loss: 3.05705 | val_0_rmse: 3.38839 |  0:00:01s\n",
      "epoch 10 | loss: 1.82717 | val_0_rmse: 3.21826 |  0:00:02s\n",
      "epoch 11 | loss: 1.4872  | val_0_rmse: 2.41643 |  0:00:02s\n",
      "epoch 12 | loss: 1.39543 | val_0_rmse: 2.41177 |  0:00:02s\n",
      "epoch 13 | loss: 0.90642 | val_0_rmse: 2.24409 |  0:00:02s\n",
      "epoch 14 | loss: 0.73483 | val_0_rmse: 1.87694 |  0:00:02s\n",
      "epoch 15 | loss: 0.63003 | val_0_rmse: 1.99627 |  0:00:03s\n",
      "epoch 16 | loss: 0.59142 | val_0_rmse: 1.86908 |  0:00:03s\n",
      "epoch 17 | loss: 0.31612 | val_0_rmse: 1.71373 |  0:00:03s\n",
      "epoch 18 | loss: 0.44672 | val_0_rmse: 1.48719 |  0:00:03s\n",
      "epoch 19 | loss: 0.32926 | val_0_rmse: 1.38658 |  0:00:03s\n",
      "epoch 20 | loss: 0.38572 | val_0_rmse: 1.24646 |  0:00:03s\n",
      "epoch 21 | loss: 0.33281 | val_0_rmse: 1.08129 |  0:00:04s\n",
      "epoch 22 | loss: 0.30521 | val_0_rmse: 1.11121 |  0:00:04s\n",
      "epoch 23 | loss: 0.25474 | val_0_rmse: 1.0403  |  0:00:04s\n",
      "epoch 24 | loss: 0.20804 | val_0_rmse: 0.85124 |  0:00:04s\n",
      "epoch 25 | loss: 0.14611 | val_0_rmse: 0.92314 |  0:00:04s\n",
      "epoch 26 | loss: 0.17251 | val_0_rmse: 0.71213 |  0:00:04s\n",
      "epoch 27 | loss: 0.16915 | val_0_rmse: 0.70967 |  0:00:05s\n",
      "epoch 28 | loss: 0.18217 | val_0_rmse: 0.73663 |  0:00:05s\n",
      "epoch 29 | loss: 0.13235 | val_0_rmse: 0.62185 |  0:00:05s\n",
      "epoch 30 | loss: 0.13289 | val_0_rmse: 0.61198 |  0:00:05s\n",
      "epoch 31 | loss: 0.1399  | val_0_rmse: 0.63251 |  0:00:05s\n",
      "epoch 32 | loss: 0.12756 | val_0_rmse: 0.61909 |  0:00:06s\n",
      "epoch 33 | loss: 0.13825 | val_0_rmse: 0.4899  |  0:00:06s\n",
      "epoch 34 | loss: 0.15508 | val_0_rmse: 0.47577 |  0:00:06s\n",
      "epoch 35 | loss: 0.18348 | val_0_rmse: 0.27228 |  0:00:06s\n",
      "epoch 36 | loss: 0.16713 | val_0_rmse: 0.7904  |  0:00:06s\n",
      "epoch 37 | loss: 0.42532 | val_0_rmse: 0.3579  |  0:00:07s\n",
      "epoch 38 | loss: 0.22132 | val_0_rmse: 0.28744 |  0:00:07s\n",
      "epoch 39 | loss: 0.19092 | val_0_rmse: 0.64265 |  0:00:07s\n",
      "epoch 40 | loss: 0.39419 | val_0_rmse: 0.32415 |  0:00:07s\n",
      "epoch 41 | loss: 0.16808 | val_0_rmse: 0.26592 |  0:00:07s\n",
      "epoch 42 | loss: 0.183   | val_0_rmse: 0.37651 |  0:00:07s\n",
      "epoch 43 | loss: 0.09475 | val_0_rmse: 0.25723 |  0:00:08s\n",
      "epoch 44 | loss: 0.07283 | val_0_rmse: 0.25612 |  0:00:08s\n",
      "epoch 45 | loss: 0.06614 | val_0_rmse: 0.27527 |  0:00:08s\n",
      "epoch 46 | loss: 0.05695 | val_0_rmse: 0.27772 |  0:00:08s\n",
      "epoch 47 | loss: 0.06283 | val_0_rmse: 0.26533 |  0:00:08s\n",
      "epoch 48 | loss: 0.07198 | val_0_rmse: 0.23426 |  0:00:08s\n",
      "epoch 49 | loss: 0.04921 | val_0_rmse: 0.28056 |  0:00:09s\n",
      "epoch 50 | loss: 0.06754 | val_0_rmse: 0.28954 |  0:00:09s\n",
      "epoch 51 | loss: 0.09334 | val_0_rmse: 0.34947 |  0:00:09s\n",
      "epoch 52 | loss: 0.08976 | val_0_rmse: 0.25233 |  0:00:09s\n",
      "epoch 53 | loss: 0.07187 | val_0_rmse: 0.31596 |  0:00:09s\n",
      "epoch 54 | loss: 0.09075 | val_0_rmse: 0.25323 |  0:00:09s\n",
      "epoch 55 | loss: 0.07253 | val_0_rmse: 0.33777 |  0:00:10s\n",
      "epoch 56 | loss: 0.0863  | val_0_rmse: 0.27166 |  0:00:10s\n",
      "epoch 57 | loss: 0.07428 | val_0_rmse: 0.29546 |  0:00:10s\n",
      "epoch 58 | loss: 0.06623 | val_0_rmse: 0.29572 |  0:00:10s\n",
      "epoch 59 | loss: 0.08476 | val_0_rmse: 0.27366 |  0:00:10s\n",
      "epoch 60 | loss: 0.05594 | val_0_rmse: 0.28888 |  0:00:11s\n",
      "epoch 61 | loss: 0.07873 | val_0_rmse: 0.28939 |  0:00:11s\n",
      "epoch 62 | loss: 0.06508 | val_0_rmse: 0.28126 |  0:00:11s\n",
      "epoch 63 | loss: 0.06944 | val_0_rmse: 0.27399 |  0:00:11s\n",
      "epoch 64 | loss: 0.0726  | val_0_rmse: 0.25439 |  0:00:11s\n",
      "epoch 65 | loss: 0.05865 | val_0_rmse: 0.30893 |  0:00:11s\n",
      "epoch 66 | loss: 0.07569 | val_0_rmse: 0.26525 |  0:00:12s\n",
      "epoch 67 | loss: 0.06024 | val_0_rmse: 0.28407 |  0:00:12s\n",
      "epoch 68 | loss: 0.07508 | val_0_rmse: 0.27089 |  0:00:12s\n",
      "epoch 69 | loss: 0.0652  | val_0_rmse: 0.26307 |  0:00:12s\n",
      "epoch 70 | loss: 0.05509 | val_0_rmse: 0.27568 |  0:00:12s\n",
      "epoch 71 | loss: 0.06336 | val_0_rmse: 0.27876 |  0:00:13s\n",
      "epoch 72 | loss: 0.05596 | val_0_rmse: 0.2776  |  0:00:13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:50:28,407] Trial 95 finished with value: 0.23426280301560934 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2556423361876654, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.01528539836892887, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 73 | loss: 0.05447 | val_0_rmse: 0.27362 |  0:00:13s\n",
      "\n",
      "Early stopping occurred at epoch 73 with best_epoch = 48 and best_val_0_rmse = 0.23426\n",
      "Trial 095 | rmse_log=0.23426 | RMSE$=51,865 | MAE$=27,303 | MAPE=16.32% | n_d/n_a=48/24 steps=3 lr=0.01529 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 129.07311| val_0_rmse: 10.66357|  0:00:00s\n",
      "epoch 1  | loss: 88.54971| val_0_rmse: 9.74982 |  0:00:00s\n",
      "epoch 2  | loss: 66.94454| val_0_rmse: 8.83191 |  0:00:00s\n",
      "epoch 3  | loss: 45.80291| val_0_rmse: 7.79638 |  0:00:00s\n",
      "epoch 4  | loss: 33.04826| val_0_rmse: 6.75253 |  0:00:00s\n",
      "epoch 5  | loss: 22.08647| val_0_rmse: 5.75617 |  0:00:01s\n",
      "epoch 6  | loss: 20.32366| val_0_rmse: 4.87993 |  0:00:01s\n",
      "epoch 7  | loss: 17.29488| val_0_rmse: 4.37309 |  0:00:01s\n",
      "epoch 8  | loss: 12.12227| val_0_rmse: 4.0904  |  0:00:01s\n",
      "epoch 9  | loss: 8.32003 | val_0_rmse: 3.94056 |  0:00:01s\n",
      "epoch 10 | loss: 7.18838 | val_0_rmse: 3.72959 |  0:00:02s\n",
      "epoch 11 | loss: 3.62574 | val_0_rmse: 2.91247 |  0:00:02s\n",
      "epoch 12 | loss: 2.1757  | val_0_rmse: 2.21835 |  0:00:02s\n",
      "epoch 13 | loss: 2.22937 | val_0_rmse: 2.05928 |  0:00:02s\n",
      "epoch 14 | loss: 1.78589 | val_0_rmse: 1.82202 |  0:00:02s\n",
      "epoch 15 | loss: 1.2834  | val_0_rmse: 1.26205 |  0:00:03s\n",
      "epoch 16 | loss: 1.19899 | val_0_rmse: 1.44735 |  0:00:03s\n",
      "epoch 17 | loss: 0.85365 | val_0_rmse: 1.11006 |  0:00:03s\n",
      "epoch 18 | loss: 0.96989 | val_0_rmse: 1.36857 |  0:00:03s\n",
      "epoch 19 | loss: 0.98607 | val_0_rmse: 0.94925 |  0:00:03s\n",
      "epoch 20 | loss: 0.78789 | val_0_rmse: 0.93889 |  0:00:03s\n",
      "epoch 21 | loss: 0.74804 | val_0_rmse: 0.92863 |  0:00:04s\n",
      "epoch 22 | loss: 0.51584 | val_0_rmse: 0.78114 |  0:00:04s\n",
      "epoch 23 | loss: 0.60637 | val_0_rmse: 0.90015 |  0:00:04s\n",
      "epoch 24 | loss: 0.46276 | val_0_rmse: 0.89129 |  0:00:04s\n",
      "epoch 25 | loss: 0.43738 | val_0_rmse: 0.86929 |  0:00:04s\n",
      "epoch 26 | loss: 0.47821 | val_0_rmse: 1.0067  |  0:00:04s\n",
      "epoch 27 | loss: 0.33617 | val_0_rmse: 0.82154 |  0:00:05s\n",
      "epoch 28 | loss: 0.4155  | val_0_rmse: 0.7305  |  0:00:05s\n",
      "epoch 29 | loss: 0.30609 | val_0_rmse: 0.7126  |  0:00:05s\n",
      "epoch 30 | loss: 0.29149 | val_0_rmse: 0.84441 |  0:00:05s\n",
      "epoch 31 | loss: 0.35215 | val_0_rmse: 0.76009 |  0:00:06s\n",
      "epoch 32 | loss: 0.2568  | val_0_rmse: 0.67244 |  0:00:06s\n",
      "epoch 33 | loss: 0.24053 | val_0_rmse: 0.82912 |  0:00:06s\n",
      "epoch 34 | loss: 0.3411  | val_0_rmse: 0.72679 |  0:00:06s\n",
      "epoch 35 | loss: 0.24728 | val_0_rmse: 0.71026 |  0:00:06s\n",
      "epoch 36 | loss: 0.22799 | val_0_rmse: 0.63017 |  0:00:06s\n",
      "epoch 37 | loss: 0.15372 | val_0_rmse: 0.62309 |  0:00:07s\n",
      "epoch 38 | loss: 0.13226 | val_0_rmse: 0.75802 |  0:00:07s\n",
      "epoch 39 | loss: 0.13952 | val_0_rmse: 0.48017 |  0:00:07s\n",
      "epoch 40 | loss: 0.18451 | val_0_rmse: 0.82688 |  0:00:07s\n",
      "epoch 41 | loss: 0.21681 | val_0_rmse: 0.42771 |  0:00:07s\n",
      "epoch 42 | loss: 0.12336 | val_0_rmse: 0.56831 |  0:00:08s\n",
      "epoch 43 | loss: 0.11163 | val_0_rmse: 0.45416 |  0:00:08s\n",
      "epoch 44 | loss: 0.10235 | val_0_rmse: 0.38939 |  0:00:08s\n",
      "epoch 45 | loss: 0.1271  | val_0_rmse: 0.383   |  0:00:08s\n",
      "epoch 46 | loss: 0.11298 | val_0_rmse: 0.3165  |  0:00:08s\n",
      "epoch 47 | loss: 0.1348  | val_0_rmse: 0.41813 |  0:00:09s\n",
      "epoch 48 | loss: 0.11275 | val_0_rmse: 0.28115 |  0:00:09s\n",
      "epoch 49 | loss: 0.10959 | val_0_rmse: 0.37495 |  0:00:09s\n",
      "epoch 50 | loss: 0.08673 | val_0_rmse: 0.2693  |  0:00:09s\n",
      "epoch 51 | loss: 0.09081 | val_0_rmse: 0.37544 |  0:00:09s\n",
      "epoch 52 | loss: 0.09164 | val_0_rmse: 0.23656 |  0:00:10s\n",
      "epoch 53 | loss: 0.06735 | val_0_rmse: 0.32129 |  0:00:10s\n",
      "epoch 54 | loss: 0.08115 | val_0_rmse: 0.21705 |  0:00:10s\n",
      "epoch 55 | loss: 0.07155 | val_0_rmse: 0.24561 |  0:00:10s\n",
      "epoch 56 | loss: 0.06097 | val_0_rmse: 0.29686 |  0:00:10s\n",
      "epoch 57 | loss: 0.07165 | val_0_rmse: 0.22114 |  0:00:11s\n",
      "epoch 58 | loss: 0.09452 | val_0_rmse: 0.33587 |  0:00:11s\n",
      "epoch 59 | loss: 0.10077 | val_0_rmse: 0.22384 |  0:00:11s\n",
      "epoch 60 | loss: 0.10505 | val_0_rmse: 0.27507 |  0:00:11s\n",
      "epoch 61 | loss: 0.11033 | val_0_rmse: 0.23321 |  0:00:11s\n",
      "epoch 62 | loss: 0.07489 | val_0_rmse: 0.25051 |  0:00:11s\n",
      "epoch 63 | loss: 0.07832 | val_0_rmse: 0.31515 |  0:00:12s\n",
      "epoch 64 | loss: 0.09505 | val_0_rmse: 0.24393 |  0:00:12s\n",
      "epoch 65 | loss: 0.06734 | val_0_rmse: 0.29205 |  0:00:12s\n",
      "epoch 66 | loss: 0.07174 | val_0_rmse: 0.22022 |  0:00:12s\n",
      "epoch 67 | loss: 0.07227 | val_0_rmse: 0.34161 |  0:00:12s\n",
      "epoch 68 | loss: 0.10351 | val_0_rmse: 0.23035 |  0:00:12s\n",
      "epoch 69 | loss: 0.07828 | val_0_rmse: 0.3192  |  0:00:13s\n",
      "epoch 70 | loss: 0.06852 | val_0_rmse: 0.21988 |  0:00:13s\n",
      "epoch 71 | loss: 0.08131 | val_0_rmse: 0.31246 |  0:00:13s\n",
      "epoch 72 | loss: 0.07211 | val_0_rmse: 0.22932 |  0:00:13s\n",
      "epoch 73 | loss: 0.07475 | val_0_rmse: 0.27949 |  0:00:13s\n",
      "epoch 74 | loss: 0.06473 | val_0_rmse: 0.22706 |  0:00:13s\n",
      "epoch 75 | loss: 0.1018  | val_0_rmse: 0.27471 |  0:00:14s\n",
      "epoch 76 | loss: 0.09018 | val_0_rmse: 0.22328 |  0:00:14s\n",
      "epoch 77 | loss: 0.06767 | val_0_rmse: 0.21672 |  0:00:14s\n",
      "epoch 78 | loss: 0.05709 | val_0_rmse: 0.20955 |  0:00:14s\n",
      "epoch 79 | loss: 0.04713 | val_0_rmse: 0.2159  |  0:00:14s\n",
      "epoch 80 | loss: 0.04539 | val_0_rmse: 0.2082  |  0:00:15s\n",
      "epoch 81 | loss: 0.04864 | val_0_rmse: 0.22168 |  0:00:15s\n",
      "epoch 82 | loss: 0.0528  | val_0_rmse: 0.22908 |  0:00:15s\n",
      "epoch 83 | loss: 0.04195 | val_0_rmse: 0.22199 |  0:00:15s\n",
      "epoch 84 | loss: 0.0414  | val_0_rmse: 0.20904 |  0:00:15s\n",
      "epoch 85 | loss: 0.05595 | val_0_rmse: 0.21393 |  0:00:15s\n",
      "epoch 86 | loss: 0.04527 | val_0_rmse: 0.2178  |  0:00:16s\n",
      "epoch 87 | loss: 0.05656 | val_0_rmse: 0.21835 |  0:00:16s\n",
      "epoch 88 | loss: 0.03964 | val_0_rmse: 0.20825 |  0:00:16s\n",
      "epoch 89 | loss: 0.0414  | val_0_rmse: 0.20058 |  0:00:16s\n",
      "epoch 90 | loss: 0.03537 | val_0_rmse: 0.19924 |  0:00:16s\n",
      "epoch 91 | loss: 0.03443 | val_0_rmse: 0.2018  |  0:00:16s\n",
      "epoch 92 | loss: 0.0452  | val_0_rmse: 0.21319 |  0:00:17s\n",
      "epoch 93 | loss: 0.04156 | val_0_rmse: 0.19281 |  0:00:17s\n",
      "epoch 94 | loss: 0.04433 | val_0_rmse: 0.20189 |  0:00:17s\n",
      "epoch 95 | loss: 0.0447  | val_0_rmse: 0.19007 |  0:00:17s\n",
      "epoch 96 | loss: 0.03808 | val_0_rmse: 0.19916 |  0:00:17s\n",
      "epoch 97 | loss: 0.04774 | val_0_rmse: 0.23108 |  0:00:18s\n",
      "epoch 98 | loss: 0.04124 | val_0_rmse: 0.20331 |  0:00:18s\n",
      "epoch 99 | loss: 0.05134 | val_0_rmse: 0.26487 |  0:00:18s\n",
      "epoch 100| loss: 0.06178 | val_0_rmse: 0.19143 |  0:00:18s\n",
      "epoch 101| loss: 0.0372  | val_0_rmse: 0.20397 |  0:00:18s\n",
      "epoch 102| loss: 0.03419 | val_0_rmse: 0.18373 |  0:00:18s\n",
      "epoch 103| loss: 0.03422 | val_0_rmse: 0.19372 |  0:00:19s\n",
      "epoch 104| loss: 0.03578 | val_0_rmse: 0.21648 |  0:00:19s\n",
      "epoch 105| loss: 0.04802 | val_0_rmse: 0.20402 |  0:00:19s\n",
      "epoch 106| loss: 0.04023 | val_0_rmse: 0.27673 |  0:00:19s\n",
      "epoch 107| loss: 0.06863 | val_0_rmse: 0.20415 |  0:00:19s\n",
      "epoch 108| loss: 0.05598 | val_0_rmse: 0.23974 |  0:00:19s\n",
      "epoch 109| loss: 0.07973 | val_0_rmse: 0.19851 |  0:00:20s\n",
      "epoch 110| loss: 0.06087 | val_0_rmse: 0.20217 |  0:00:20s\n",
      "epoch 111| loss: 0.04533 | val_0_rmse: 0.18124 |  0:00:20s\n",
      "epoch 112| loss: 0.0497  | val_0_rmse: 0.20469 |  0:00:20s\n",
      "epoch 113| loss: 0.04382 | val_0_rmse: 0.20176 |  0:00:20s\n",
      "epoch 114| loss: 0.0366  | val_0_rmse: 0.18018 |  0:00:20s\n",
      "epoch 115| loss: 0.0519  | val_0_rmse: 0.21084 |  0:00:21s\n",
      "epoch 116| loss: 0.03709 | val_0_rmse: 0.17693 |  0:00:21s\n",
      "epoch 117| loss: 0.03388 | val_0_rmse: 0.17425 |  0:00:21s\n",
      "epoch 118| loss: 0.03377 | val_0_rmse: 0.18263 |  0:00:21s\n",
      "epoch 119| loss: 0.02814 | val_0_rmse: 0.17949 |  0:00:21s\n",
      "epoch 120| loss: 0.03221 | val_0_rmse: 0.19934 |  0:00:22s\n",
      "epoch 121| loss: 0.03986 | val_0_rmse: 0.20387 |  0:00:22s\n",
      "epoch 122| loss: 0.0343  | val_0_rmse: 0.20946 |  0:00:22s\n",
      "epoch 123| loss: 0.0375  | val_0_rmse: 0.22986 |  0:00:22s\n",
      "epoch 124| loss: 0.04966 | val_0_rmse: 0.22262 |  0:00:22s\n",
      "epoch 125| loss: 0.04067 | val_0_rmse: 0.18315 |  0:00:22s\n",
      "epoch 126| loss: 0.02873 | val_0_rmse: 0.17528 |  0:00:23s\n",
      "epoch 127| loss: 0.03644 | val_0_rmse: 0.20472 |  0:00:23s\n",
      "epoch 128| loss: 0.03584 | val_0_rmse: 0.19937 |  0:00:23s\n",
      "epoch 129| loss: 0.03475 | val_0_rmse: 0.2107  |  0:00:23s\n",
      "epoch 130| loss: 0.04725 | val_0_rmse: 0.23308 |  0:00:23s\n",
      "epoch 131| loss: 0.05006 | val_0_rmse: 0.22324 |  0:00:23s\n",
      "epoch 132| loss: 0.04502 | val_0_rmse: 0.24516 |  0:00:24s\n",
      "epoch 133| loss: 0.05318 | val_0_rmse: 0.19197 |  0:00:24s\n",
      "epoch 134| loss: 0.0309  | val_0_rmse: 0.2231  |  0:00:24s\n",
      "epoch 135| loss: 0.04851 | val_0_rmse: 0.19359 |  0:00:24s\n",
      "epoch 136| loss: 0.04334 | val_0_rmse: 0.19272 |  0:00:24s\n",
      "epoch 137| loss: 0.03104 | val_0_rmse: 0.18694 |  0:00:24s\n",
      "epoch 138| loss: 0.03102 | val_0_rmse: 0.20436 |  0:00:25s\n",
      "epoch 139| loss: 0.02745 | val_0_rmse: 0.18378 |  0:00:25s\n",
      "epoch 140| loss: 0.02788 | val_0_rmse: 0.1812  |  0:00:25s\n",
      "epoch 141| loss: 0.02982 | val_0_rmse: 0.19204 |  0:00:25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:50:54,621] Trial 96 finished with value: 0.17425441836253489 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2451686951843983, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.010824484923591733, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 142| loss: 0.02798 | val_0_rmse: 0.20058 |  0:00:25s\n",
      "\n",
      "Early stopping occurred at epoch 142 with best_epoch = 117 and best_val_0_rmse = 0.17425\n",
      "Trial 096 | rmse_log=0.17425 | RMSE$=38,835 | MAE$=22,020 | MAPE=12.36% | n_d/n_a=48/24 steps=3 lr=0.01082 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 126.05477| val_0_rmse: 10.3605 |  0:00:00s\n",
      "epoch 1  | loss: 80.48213| val_0_rmse: 8.94817 |  0:00:00s\n",
      "epoch 2  | loss: 50.9196 | val_0_rmse: 7.56011 |  0:00:00s\n",
      "epoch 3  | loss: 35.94134| val_0_rmse: 6.23868 |  0:00:00s\n",
      "epoch 4  | loss: 23.78151| val_0_rmse: 5.19548 |  0:00:00s\n",
      "epoch 5  | loss: 22.49297| val_0_rmse: 4.5435  |  0:00:01s\n",
      "epoch 6  | loss: 15.61287| val_0_rmse: 4.26379 |  0:00:01s\n",
      "epoch 7  | loss: 11.09365| val_0_rmse: 4.38104 |  0:00:01s\n",
      "epoch 8  | loss: 5.76336 | val_0_rmse: 4.50092 |  0:00:01s\n",
      "epoch 9  | loss: 4.20802 | val_0_rmse: 3.81243 |  0:00:01s\n",
      "epoch 10 | loss: 2.82727 | val_0_rmse: 2.59953 |  0:00:02s\n",
      "epoch 11 | loss: 2.50729 | val_0_rmse: 2.42936 |  0:00:02s\n",
      "epoch 12 | loss: 1.65181 | val_0_rmse: 2.29712 |  0:00:02s\n",
      "epoch 13 | loss: 1.13006 | val_0_rmse: 1.6917  |  0:00:02s\n",
      "epoch 14 | loss: 1.1479  | val_0_rmse: 1.71883 |  0:00:02s\n",
      "epoch 15 | loss: 0.8986  | val_0_rmse: 1.52485 |  0:00:02s\n",
      "epoch 16 | loss: 0.60275 | val_0_rmse: 1.29984 |  0:00:03s\n",
      "epoch 17 | loss: 0.68696 | val_0_rmse: 1.23654 |  0:00:03s\n",
      "epoch 18 | loss: 0.72881 | val_0_rmse: 1.26205 |  0:00:03s\n",
      "epoch 19 | loss: 0.70505 | val_0_rmse: 0.98065 |  0:00:03s\n",
      "epoch 20 | loss: 0.55871 | val_0_rmse: 1.11526 |  0:00:03s\n",
      "epoch 21 | loss: 0.47947 | val_0_rmse: 0.91291 |  0:00:04s\n",
      "epoch 22 | loss: 0.40951 | val_0_rmse: 1.03692 |  0:00:04s\n",
      "epoch 23 | loss: 0.30036 | val_0_rmse: 1.01302 |  0:00:04s\n",
      "epoch 24 | loss: 0.20329 | val_0_rmse: 0.87393 |  0:00:04s\n",
      "epoch 25 | loss: 0.2273  | val_0_rmse: 0.77412 |  0:00:04s\n",
      "epoch 26 | loss: 0.28276 | val_0_rmse: 0.83857 |  0:00:05s\n",
      "epoch 27 | loss: 0.26367 | val_0_rmse: 0.90341 |  0:00:05s\n",
      "epoch 28 | loss: 0.21953 | val_0_rmse: 0.62404 |  0:00:05s\n",
      "epoch 29 | loss: 0.25832 | val_0_rmse: 0.60342 |  0:00:05s\n",
      "epoch 30 | loss: 0.19575 | val_0_rmse: 0.73077 |  0:00:05s\n",
      "epoch 31 | loss: 0.23999 | val_0_rmse: 0.5909  |  0:00:05s\n",
      "epoch 32 | loss: 0.17397 | val_0_rmse: 0.74342 |  0:00:06s\n",
      "epoch 33 | loss: 0.16138 | val_0_rmse: 0.52909 |  0:00:06s\n",
      "epoch 34 | loss: 0.12494 | val_0_rmse: 0.6303  |  0:00:06s\n",
      "epoch 35 | loss: 0.1107  | val_0_rmse: 0.63953 |  0:00:06s\n",
      "epoch 36 | loss: 0.15124 | val_0_rmse: 0.54439 |  0:00:06s\n",
      "epoch 37 | loss: 0.14002 | val_0_rmse: 0.60716 |  0:00:06s\n",
      "epoch 38 | loss: 0.15848 | val_0_rmse: 0.54079 |  0:00:07s\n",
      "epoch 39 | loss: 0.14505 | val_0_rmse: 0.36663 |  0:00:07s\n",
      "epoch 40 | loss: 0.12117 | val_0_rmse: 0.41859 |  0:00:07s\n",
      "epoch 41 | loss: 0.10749 | val_0_rmse: 0.48003 |  0:00:07s\n",
      "epoch 42 | loss: 0.09841 | val_0_rmse: 0.33259 |  0:00:07s\n",
      "epoch 43 | loss: 0.08744 | val_0_rmse: 0.3153  |  0:00:07s\n",
      "epoch 44 | loss: 0.08364 | val_0_rmse: 0.36481 |  0:00:08s\n",
      "epoch 45 | loss: 0.08131 | val_0_rmse: 0.30651 |  0:00:08s\n",
      "epoch 46 | loss: 0.06883 | val_0_rmse: 0.26544 |  0:00:08s\n",
      "epoch 47 | loss: 0.0656  | val_0_rmse: 0.28149 |  0:00:08s\n",
      "epoch 48 | loss: 0.08222 | val_0_rmse: 0.36271 |  0:00:08s\n",
      "epoch 49 | loss: 0.11679 | val_0_rmse: 0.23111 |  0:00:09s\n",
      "epoch 50 | loss: 0.08191 | val_0_rmse: 0.2724  |  0:00:09s\n",
      "epoch 51 | loss: 0.06394 | val_0_rmse: 0.25204 |  0:00:09s\n",
      "epoch 52 | loss: 0.07384 | val_0_rmse: 0.2214  |  0:00:09s\n",
      "epoch 53 | loss: 0.0912  | val_0_rmse: 0.28765 |  0:00:09s\n",
      "epoch 54 | loss: 0.06219 | val_0_rmse: 0.22188 |  0:00:09s\n",
      "epoch 55 | loss: 0.06116 | val_0_rmse: 0.2507  |  0:00:10s\n",
      "epoch 56 | loss: 0.0581  | val_0_rmse: 0.25809 |  0:00:10s\n",
      "epoch 57 | loss: 0.05945 | val_0_rmse: 0.25818 |  0:00:10s\n",
      "epoch 58 | loss: 0.07664 | val_0_rmse: 0.21661 |  0:00:10s\n",
      "epoch 59 | loss: 0.05738 | val_0_rmse: 0.21845 |  0:00:10s\n",
      "epoch 60 | loss: 0.06185 | val_0_rmse: 0.25213 |  0:00:10s\n",
      "epoch 61 | loss: 0.05643 | val_0_rmse: 0.2346  |  0:00:11s\n",
      "epoch 62 | loss: 0.05016 | val_0_rmse: 0.21931 |  0:00:11s\n",
      "epoch 63 | loss: 0.06836 | val_0_rmse: 0.2314  |  0:00:11s\n",
      "epoch 64 | loss: 0.05404 | val_0_rmse: 0.24599 |  0:00:11s\n",
      "epoch 65 | loss: 0.05917 | val_0_rmse: 0.21345 |  0:00:11s\n",
      "epoch 66 | loss: 0.05105 | val_0_rmse: 0.21923 |  0:00:11s\n",
      "epoch 67 | loss: 0.05332 | val_0_rmse: 0.22137 |  0:00:12s\n",
      "epoch 68 | loss: 0.04593 | val_0_rmse: 0.21762 |  0:00:12s\n",
      "epoch 69 | loss: 0.04609 | val_0_rmse: 0.21547 |  0:00:12s\n",
      "epoch 70 | loss: 0.04439 | val_0_rmse: 0.2169  |  0:00:12s\n",
      "epoch 71 | loss: 0.046   | val_0_rmse: 0.20925 |  0:00:12s\n",
      "epoch 72 | loss: 0.05508 | val_0_rmse: 0.21672 |  0:00:13s\n",
      "epoch 73 | loss: 0.05865 | val_0_rmse: 0.23142 |  0:00:13s\n",
      "epoch 74 | loss: 0.0586  | val_0_rmse: 0.24128 |  0:00:13s\n",
      "epoch 75 | loss: 0.07447 | val_0_rmse: 0.22611 |  0:00:13s\n",
      "epoch 76 | loss: 0.04579 | val_0_rmse: 0.24379 |  0:00:13s\n",
      "epoch 77 | loss: 0.04925 | val_0_rmse: 0.214   |  0:00:13s\n",
      "epoch 78 | loss: 0.04312 | val_0_rmse: 0.22604 |  0:00:14s\n",
      "epoch 79 | loss: 0.04915 | val_0_rmse: 0.25229 |  0:00:14s\n",
      "epoch 80 | loss: 0.06658 | val_0_rmse: 0.23391 |  0:00:14s\n",
      "epoch 81 | loss: 0.085   | val_0_rmse: 0.21902 |  0:00:14s\n",
      "epoch 82 | loss: 0.0571  | val_0_rmse: 0.22467 |  0:00:14s\n",
      "epoch 83 | loss: 0.05563 | val_0_rmse: 0.23218 |  0:00:14s\n",
      "epoch 84 | loss: 0.05096 | val_0_rmse: 0.24981 |  0:00:15s\n",
      "epoch 85 | loss: 0.06335 | val_0_rmse: 0.24175 |  0:00:15s\n",
      "epoch 86 | loss: 0.06241 | val_0_rmse: 0.27486 |  0:00:15s\n",
      "epoch 87 | loss: 0.07326 | val_0_rmse: 0.25689 |  0:00:15s\n",
      "epoch 88 | loss: 0.05768 | val_0_rmse: 0.22569 |  0:00:15s\n",
      "epoch 89 | loss: 0.06051 | val_0_rmse: 0.2714  |  0:00:16s\n",
      "epoch 90 | loss: 0.06341 | val_0_rmse: 0.23753 |  0:00:16s\n",
      "epoch 91 | loss: 0.05163 | val_0_rmse: 0.26607 |  0:00:16s\n",
      "epoch 92 | loss: 0.06234 | val_0_rmse: 0.24253 |  0:00:16s\n",
      "epoch 93 | loss: 0.06076 | val_0_rmse: 0.24563 |  0:00:16s\n",
      "epoch 94 | loss: 0.05016 | val_0_rmse: 0.24875 |  0:00:16s\n",
      "epoch 95 | loss: 0.06816 | val_0_rmse: 0.25906 |  0:00:17s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:51:12,251] Trial 97 finished with value: 0.2092495688887525 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2720054993523553, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.013631858792737836, 'batch_size': 512, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 96 | loss: 0.05566 | val_0_rmse: 0.24146 |  0:00:17s\n",
      "\n",
      "Early stopping occurred at epoch 96 with best_epoch = 71 and best_val_0_rmse = 0.20925\n",
      "Trial 097 | rmse_log=0.20925 | RMSE$=42,477 | MAE$=25,817 | MAPE=15.47% | n_d/n_a=48/24 steps=3 lr=0.01363 batch=512 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 139.92575| val_0_rmse: 11.50525|  0:00:00s\n",
      "epoch 1  | loss: 122.80872| val_0_rmse: 11.21489|  0:00:00s\n",
      "epoch 2  | loss: 106.6167| val_0_rmse: 10.88968|  0:00:00s\n",
      "epoch 3  | loss: 96.31034| val_0_rmse: 10.58059|  0:00:00s\n",
      "epoch 4  | loss: 85.92559| val_0_rmse: 10.26998|  0:00:00s\n",
      "epoch 5  | loss: 77.08273| val_0_rmse: 9.94995 |  0:00:00s\n",
      "epoch 6  | loss: 67.27964| val_0_rmse: 9.57667 |  0:00:00s\n",
      "epoch 7  | loss: 57.05523| val_0_rmse: 9.20627 |  0:00:01s\n",
      "epoch 8  | loss: 53.70035| val_0_rmse: 8.8063  |  0:00:01s\n",
      "epoch 9  | loss: 42.76414| val_0_rmse: 8.39614 |  0:00:01s\n",
      "epoch 10 | loss: 38.11299| val_0_rmse: 7.98029 |  0:00:01s\n",
      "epoch 11 | loss: 33.05353| val_0_rmse: 7.54861 |  0:00:01s\n",
      "epoch 12 | loss: 31.51751| val_0_rmse: 7.12788 |  0:00:01s\n",
      "epoch 13 | loss: 28.38227| val_0_rmse: 6.69578 |  0:00:01s\n",
      "epoch 14 | loss: 31.19907| val_0_rmse: 6.2679  |  0:00:01s\n",
      "epoch 15 | loss: 23.66974| val_0_rmse: 5.82079 |  0:00:02s\n",
      "epoch 16 | loss: 19.96205| val_0_rmse: 5.3692  |  0:00:02s\n",
      "epoch 17 | loss: 20.80311| val_0_rmse: 4.93804 |  0:00:02s\n",
      "epoch 18 | loss: 19.84759| val_0_rmse: 4.53593 |  0:00:02s\n",
      "epoch 19 | loss: 16.34434| val_0_rmse: 4.16346 |  0:00:02s\n",
      "epoch 20 | loss: 15.93589| val_0_rmse: 3.8784  |  0:00:02s\n",
      "epoch 21 | loss: 13.1882 | val_0_rmse: 3.64342 |  0:00:02s\n",
      "epoch 22 | loss: 11.80566| val_0_rmse: 3.49661 |  0:00:02s\n",
      "epoch 23 | loss: 9.10752 | val_0_rmse: 3.31646 |  0:00:03s\n",
      "epoch 24 | loss: 7.53463 | val_0_rmse: 3.14051 |  0:00:03s\n",
      "epoch 25 | loss: 6.42224 | val_0_rmse: 3.03012 |  0:00:03s\n",
      "epoch 26 | loss: 5.49583 | val_0_rmse: 2.88402 |  0:00:03s\n",
      "epoch 27 | loss: 5.01797 | val_0_rmse: 2.72695 |  0:00:03s\n",
      "epoch 28 | loss: 4.26169 | val_0_rmse: 2.54388 |  0:00:03s\n",
      "epoch 29 | loss: 4.0458  | val_0_rmse: 2.29462 |  0:00:03s\n",
      "epoch 30 | loss: 3.2776  | val_0_rmse: 2.04154 |  0:00:04s\n",
      "epoch 31 | loss: 2.79914 | val_0_rmse: 1.77941 |  0:00:04s\n",
      "epoch 32 | loss: 2.42989 | val_0_rmse: 1.49535 |  0:00:04s\n",
      "epoch 33 | loss: 2.408   | val_0_rmse: 1.29751 |  0:00:04s\n",
      "epoch 34 | loss: 1.95474 | val_0_rmse: 1.21994 |  0:00:04s\n",
      "epoch 35 | loss: 2.25256 | val_0_rmse: 1.12519 |  0:00:04s\n",
      "epoch 36 | loss: 1.95388 | val_0_rmse: 1.15302 |  0:00:04s\n",
      "epoch 37 | loss: 1.41816 | val_0_rmse: 1.15447 |  0:00:04s\n",
      "epoch 38 | loss: 1.35339 | val_0_rmse: 1.09925 |  0:00:05s\n",
      "epoch 39 | loss: 1.15469 | val_0_rmse: 1.01838 |  0:00:05s\n",
      "epoch 40 | loss: 1.24594 | val_0_rmse: 0.95807 |  0:00:05s\n",
      "epoch 41 | loss: 0.82393 | val_0_rmse: 0.93117 |  0:00:05s\n",
      "epoch 42 | loss: 0.73753 | val_0_rmse: 0.94149 |  0:00:05s\n",
      "epoch 43 | loss: 1.04616 | val_0_rmse: 0.94527 |  0:00:05s\n",
      "epoch 44 | loss: 0.94574 | val_0_rmse: 0.89868 |  0:00:05s\n",
      "epoch 45 | loss: 0.87568 | val_0_rmse: 0.84456 |  0:00:05s\n",
      "epoch 46 | loss: 0.78588 | val_0_rmse: 0.88302 |  0:00:05s\n",
      "epoch 47 | loss: 0.73747 | val_0_rmse: 0.84139 |  0:00:06s\n",
      "epoch 48 | loss: 0.54468 | val_0_rmse: 0.86219 |  0:00:06s\n",
      "epoch 49 | loss: 0.61313 | val_0_rmse: 0.74967 |  0:00:06s\n",
      "epoch 50 | loss: 0.47003 | val_0_rmse: 0.70551 |  0:00:06s\n",
      "epoch 51 | loss: 0.52116 | val_0_rmse: 0.8053  |  0:00:06s\n",
      "epoch 52 | loss: 0.62853 | val_0_rmse: 0.75656 |  0:00:06s\n",
      "epoch 53 | loss: 0.51217 | val_0_rmse: 0.57942 |  0:00:06s\n",
      "epoch 54 | loss: 0.51595 | val_0_rmse: 0.5971  |  0:00:06s\n",
      "epoch 55 | loss: 0.38401 | val_0_rmse: 0.65712 |  0:00:07s\n",
      "epoch 56 | loss: 0.36744 | val_0_rmse: 0.60711 |  0:00:07s\n",
      "epoch 57 | loss: 0.39349 | val_0_rmse: 0.64798 |  0:00:07s\n",
      "epoch 58 | loss: 0.41238 | val_0_rmse: 0.54792 |  0:00:07s\n",
      "epoch 59 | loss: 0.39939 | val_0_rmse: 0.4526  |  0:00:07s\n",
      "epoch 60 | loss: 0.30118 | val_0_rmse: 0.52442 |  0:00:07s\n",
      "epoch 61 | loss: 0.27593 | val_0_rmse: 0.50967 |  0:00:07s\n",
      "epoch 62 | loss: 0.33199 | val_0_rmse: 0.46779 |  0:00:07s\n",
      "epoch 63 | loss: 0.29562 | val_0_rmse: 0.56238 |  0:00:08s\n",
      "epoch 64 | loss: 0.25744 | val_0_rmse: 0.48629 |  0:00:08s\n",
      "epoch 65 | loss: 0.2298  | val_0_rmse: 0.41439 |  0:00:08s\n",
      "epoch 66 | loss: 0.21199 | val_0_rmse: 0.43627 |  0:00:08s\n",
      "epoch 67 | loss: 0.21334 | val_0_rmse: 0.42798 |  0:00:08s\n",
      "epoch 68 | loss: 0.15373 | val_0_rmse: 0.31745 |  0:00:08s\n",
      "epoch 69 | loss: 0.22025 | val_0_rmse: 0.35152 |  0:00:08s\n",
      "epoch 70 | loss: 0.24178 | val_0_rmse: 0.48232 |  0:00:08s\n",
      "epoch 71 | loss: 0.22937 | val_0_rmse: 0.47937 |  0:00:08s\n",
      "epoch 72 | loss: 0.21878 | val_0_rmse: 0.35556 |  0:00:09s\n",
      "epoch 73 | loss: 0.22223 | val_0_rmse: 0.29095 |  0:00:09s\n",
      "epoch 74 | loss: 0.15357 | val_0_rmse: 0.38507 |  0:00:09s\n",
      "epoch 75 | loss: 0.16546 | val_0_rmse: 0.41294 |  0:00:09s\n",
      "epoch 76 | loss: 0.21143 | val_0_rmse: 0.32336 |  0:00:09s\n",
      "epoch 77 | loss: 0.15594 | val_0_rmse: 0.33812 |  0:00:09s\n",
      "epoch 78 | loss: 0.15485 | val_0_rmse: 0.39861 |  0:00:09s\n",
      "epoch 79 | loss: 0.19788 | val_0_rmse: 0.34935 |  0:00:09s\n",
      "epoch 80 | loss: 0.11384 | val_0_rmse: 0.28541 |  0:00:10s\n",
      "epoch 81 | loss: 0.20423 | val_0_rmse: 0.29908 |  0:00:10s\n",
      "epoch 82 | loss: 0.23887 | val_0_rmse: 0.35273 |  0:00:10s\n",
      "epoch 83 | loss: 0.13706 | val_0_rmse: 0.34007 |  0:00:10s\n",
      "epoch 84 | loss: 0.11117 | val_0_rmse: 0.27937 |  0:00:10s\n",
      "epoch 85 | loss: 0.18715 | val_0_rmse: 0.26066 |  0:00:10s\n",
      "epoch 86 | loss: 0.12357 | val_0_rmse: 0.30914 |  0:00:10s\n",
      "epoch 87 | loss: 0.11242 | val_0_rmse: 0.356   |  0:00:10s\n",
      "epoch 88 | loss: 0.12617 | val_0_rmse: 0.29964 |  0:00:10s\n",
      "epoch 89 | loss: 0.12961 | val_0_rmse: 0.24368 |  0:00:11s\n",
      "epoch 90 | loss: 0.09708 | val_0_rmse: 0.27717 |  0:00:11s\n",
      "epoch 91 | loss: 0.12508 | val_0_rmse: 0.3267  |  0:00:11s\n",
      "epoch 92 | loss: 0.09844 | val_0_rmse: 0.27769 |  0:00:11s\n",
      "epoch 93 | loss: 0.09297 | val_0_rmse: 0.2944  |  0:00:11s\n",
      "epoch 94 | loss: 0.10043 | val_0_rmse: 0.24842 |  0:00:11s\n",
      "epoch 95 | loss: 0.08891 | val_0_rmse: 0.26752 |  0:00:11s\n",
      "epoch 96 | loss: 0.07909 | val_0_rmse: 0.29739 |  0:00:11s\n",
      "epoch 97 | loss: 0.09145 | val_0_rmse: 0.25037 |  0:00:12s\n",
      "epoch 98 | loss: 0.09912 | val_0_rmse: 0.22393 |  0:00:12s\n",
      "epoch 99 | loss: 0.12463 | val_0_rmse: 0.25974 |  0:00:12s\n",
      "epoch 100| loss: 0.10534 | val_0_rmse: 0.34615 |  0:00:12s\n",
      "epoch 101| loss: 0.11549 | val_0_rmse: 0.33839 |  0:00:12s\n",
      "epoch 102| loss: 0.1123  | val_0_rmse: 0.24718 |  0:00:12s\n",
      "epoch 103| loss: 0.08205 | val_0_rmse: 0.20964 |  0:00:12s\n",
      "epoch 104| loss: 0.08974 | val_0_rmse: 0.24693 |  0:00:12s\n",
      "epoch 105| loss: 0.08004 | val_0_rmse: 0.33306 |  0:00:12s\n",
      "epoch 106| loss: 0.08864 | val_0_rmse: 0.31037 |  0:00:13s\n",
      "epoch 107| loss: 0.08979 | val_0_rmse: 0.22148 |  0:00:13s\n",
      "epoch 108| loss: 0.07389 | val_0_rmse: 0.21259 |  0:00:13s\n",
      "epoch 109| loss: 0.07341 | val_0_rmse: 0.25417 |  0:00:13s\n",
      "epoch 110| loss: 0.06461 | val_0_rmse: 0.24645 |  0:00:13s\n",
      "epoch 111| loss: 0.07186 | val_0_rmse: 0.21734 |  0:00:13s\n",
      "epoch 112| loss: 0.07177 | val_0_rmse: 0.21957 |  0:00:13s\n",
      "epoch 113| loss: 0.06237 | val_0_rmse: 0.2292  |  0:00:13s\n",
      "epoch 114| loss: 0.07263 | val_0_rmse: 0.22709 |  0:00:13s\n",
      "epoch 115| loss: 0.05694 | val_0_rmse: 0.21646 |  0:00:14s\n",
      "epoch 116| loss: 0.064   | val_0_rmse: 0.21381 |  0:00:14s\n",
      "epoch 117| loss: 0.05733 | val_0_rmse: 0.23667 |  0:00:14s\n",
      "epoch 118| loss: 0.06506 | val_0_rmse: 0.23802 |  0:00:14s\n",
      "epoch 119| loss: 0.08016 | val_0_rmse: 0.20528 |  0:00:14s\n",
      "epoch 120| loss: 0.05628 | val_0_rmse: 0.21887 |  0:00:14s\n",
      "epoch 121| loss: 0.06591 | val_0_rmse: 0.2086  |  0:00:14s\n",
      "epoch 122| loss: 0.05099 | val_0_rmse: 0.24215 |  0:00:14s\n",
      "epoch 123| loss: 0.08048 | val_0_rmse: 0.25326 |  0:00:15s\n",
      "epoch 124| loss: 0.07783 | val_0_rmse: 0.20846 |  0:00:15s\n",
      "epoch 125| loss: 0.04736 | val_0_rmse: 0.2235  |  0:00:15s\n",
      "epoch 126| loss: 0.07953 | val_0_rmse: 0.23519 |  0:00:15s\n",
      "epoch 127| loss: 0.08313 | val_0_rmse: 0.20197 |  0:00:15s\n",
      "epoch 128| loss: 0.04496 | val_0_rmse: 0.22159 |  0:00:15s\n",
      "epoch 129| loss: 0.06496 | val_0_rmse: 0.22699 |  0:00:15s\n",
      "epoch 130| loss: 0.06725 | val_0_rmse: 0.18813 |  0:00:15s\n",
      "epoch 131| loss: 0.04014 | val_0_rmse: 0.19124 |  0:00:16s\n",
      "epoch 132| loss: 0.06276 | val_0_rmse: 0.18589 |  0:00:16s\n",
      "epoch 133| loss: 0.04062 | val_0_rmse: 0.21621 |  0:00:16s\n",
      "epoch 134| loss: 0.05741 | val_0_rmse: 0.21792 |  0:00:16s\n",
      "epoch 135| loss: 0.05594 | val_0_rmse: 0.19908 |  0:00:16s\n",
      "epoch 136| loss: 0.04418 | val_0_rmse: 0.20078 |  0:00:16s\n",
      "epoch 137| loss: 0.04497 | val_0_rmse: 0.20651 |  0:00:16s\n",
      "epoch 138| loss: 0.04066 | val_0_rmse: 0.19904 |  0:00:16s\n",
      "epoch 139| loss: 0.04382 | val_0_rmse: 0.19995 |  0:00:16s\n",
      "epoch 140| loss: 0.04985 | val_0_rmse: 0.19898 |  0:00:17s\n",
      "epoch 141| loss: 0.04671 | val_0_rmse: 0.18854 |  0:00:17s\n",
      "epoch 142| loss: 0.04058 | val_0_rmse: 0.18701 |  0:00:17s\n",
      "epoch 143| loss: 0.03839 | val_0_rmse: 0.19774 |  0:00:17s\n",
      "epoch 144| loss: 0.05254 | val_0_rmse: 0.20196 |  0:00:17s\n",
      "epoch 145| loss: 0.04697 | val_0_rmse: 0.19051 |  0:00:17s\n",
      "epoch 146| loss: 0.03197 | val_0_rmse: 0.19433 |  0:00:17s\n",
      "epoch 147| loss: 0.05069 | val_0_rmse: 0.19664 |  0:00:17s\n",
      "epoch 148| loss: 0.04057 | val_0_rmse: 0.19167 |  0:00:17s\n",
      "epoch 149| loss: 0.03406 | val_0_rmse: 0.19691 |  0:00:17s\n",
      "epoch 150| loss: 0.04622 | val_0_rmse: 0.19769 |  0:00:18s\n",
      "epoch 151| loss: 0.04557 | val_0_rmse: 0.18669 |  0:00:18s\n",
      "epoch 152| loss: 0.03535 | val_0_rmse: 0.20161 |  0:00:18s\n",
      "epoch 153| loss: 0.04916 | val_0_rmse: 0.18544 |  0:00:18s\n",
      "epoch 154| loss: 0.05831 | val_0_rmse: 0.19598 |  0:00:18s\n",
      "epoch 155| loss: 0.04325 | val_0_rmse: 0.19402 |  0:00:18s\n",
      "epoch 156| loss: 0.03668 | val_0_rmse: 0.20914 |  0:00:18s\n",
      "epoch 157| loss: 0.05182 | val_0_rmse: 0.21686 |  0:00:18s\n",
      "epoch 158| loss: 0.0598  | val_0_rmse: 0.1873  |  0:00:19s\n",
      "epoch 159| loss: 0.03329 | val_0_rmse: 0.20437 |  0:00:19s\n",
      "epoch 160| loss: 0.05275 | val_0_rmse: 0.2017  |  0:00:19s\n",
      "epoch 161| loss: 0.04455 | val_0_rmse: 0.19075 |  0:00:19s\n",
      "epoch 162| loss: 0.03415 | val_0_rmse: 0.19244 |  0:00:19s\n",
      "epoch 163| loss: 0.0374  | val_0_rmse: 0.19322 |  0:00:19s\n",
      "epoch 164| loss: 0.03649 | val_0_rmse: 0.18995 |  0:00:19s\n",
      "epoch 165| loss: 0.03859 | val_0_rmse: 0.19911 |  0:00:19s\n",
      "epoch 166| loss: 0.04476 | val_0_rmse: 0.19568 |  0:00:20s\n",
      "epoch 167| loss: 0.03615 | val_0_rmse: 0.19618 |  0:00:20s\n",
      "epoch 168| loss: 0.04634 | val_0_rmse: 0.19569 |  0:00:20s\n",
      "epoch 169| loss: 0.05028 | val_0_rmse: 0.18676 |  0:00:20s\n",
      "epoch 170| loss: 0.03232 | val_0_rmse: 0.18593 |  0:00:20s\n",
      "epoch 171| loss: 0.03408 | val_0_rmse: 0.19228 |  0:00:20s\n",
      "epoch 172| loss: 0.04662 | val_0_rmse: 0.19113 |  0:00:20s\n",
      "epoch 173| loss: 0.03901 | val_0_rmse: 0.1875  |  0:00:20s\n",
      "epoch 174| loss: 0.03407 | val_0_rmse: 0.187   |  0:00:20s\n",
      "epoch 175| loss: 0.03006 | val_0_rmse: 0.18429 |  0:00:21s\n",
      "epoch 176| loss: 0.04144 | val_0_rmse: 0.18347 |  0:00:21s\n",
      "epoch 177| loss: 0.03595 | val_0_rmse: 0.18617 |  0:00:21s\n",
      "epoch 178| loss: 0.03381 | val_0_rmse: 0.18634 |  0:00:21s\n",
      "epoch 179| loss: 0.02912 | val_0_rmse: 0.18717 |  0:00:21s\n",
      "epoch 180| loss: 0.03554 | val_0_rmse: 0.18569 |  0:00:21s\n",
      "epoch 181| loss: 0.03866 | val_0_rmse: 0.18855 |  0:00:21s\n",
      "epoch 182| loss: 0.03354 | val_0_rmse: 0.19139 |  0:00:21s\n",
      "epoch 183| loss: 0.04703 | val_0_rmse: 0.1809  |  0:00:21s\n",
      "epoch 184| loss: 0.03625 | val_0_rmse: 0.18023 |  0:00:22s\n",
      "epoch 185| loss: 0.03345 | val_0_rmse: 0.1923  |  0:00:22s\n",
      "epoch 186| loss: 0.03527 | val_0_rmse: 0.19054 |  0:00:22s\n",
      "epoch 187| loss: 0.03231 | val_0_rmse: 0.18305 |  0:00:22s\n",
      "epoch 188| loss: 0.03916 | val_0_rmse: 0.18297 |  0:00:22s\n",
      "epoch 189| loss: 0.04275 | val_0_rmse: 0.18936 |  0:00:22s\n",
      "epoch 190| loss: 0.03284 | val_0_rmse: 0.19007 |  0:00:22s\n",
      "epoch 191| loss: 0.03326 | val_0_rmse: 0.18496 |  0:00:22s\n",
      "epoch 192| loss: 0.0349  | val_0_rmse: 0.1846  |  0:00:22s\n",
      "epoch 193| loss: 0.03797 | val_0_rmse: 0.19282 |  0:00:23s\n",
      "epoch 194| loss: 0.04031 | val_0_rmse: 0.19512 |  0:00:23s\n",
      "epoch 195| loss: 0.0394  | val_0_rmse: 0.18635 |  0:00:23s\n",
      "epoch 196| loss: 0.03101 | val_0_rmse: 0.18285 |  0:00:23s\n",
      "epoch 197| loss: 0.03396 | val_0_rmse: 0.19534 |  0:00:23s\n",
      "epoch 198| loss: 0.03342 | val_0_rmse: 0.19434 |  0:00:23s\n",
      "epoch 199| loss: 0.03213 | val_0_rmse: 0.18256 |  0:00:23s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 184 and best_val_0_rmse = 0.18023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:51:36,369] Trial 98 finished with value: 0.18023440282853045 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2164775837576647, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.011302435182992131, 'batch_size': 2048, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 098 | rmse_log=0.18023 | RMSE$=39,155 | MAE$=23,843 | MAPE=13.31% | n_d/n_a=48/24 steps=3 lr=0.01130 batch=2048 mask=entmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 140.01044| val_0_rmse: 11.52333|  0:00:00s\n",
      "epoch 1  | loss: 123.09627| val_0_rmse: 11.26653|  0:00:00s\n",
      "epoch 2  | loss: 109.54305| val_0_rmse: 10.98138|  0:00:00s\n",
      "epoch 3  | loss: 98.17663| val_0_rmse: 10.68883|  0:00:00s\n",
      "epoch 4  | loss: 88.75431| val_0_rmse: 10.38115|  0:00:00s\n",
      "epoch 5  | loss: 75.83798| val_0_rmse: 10.03261|  0:00:00s\n",
      "epoch 6  | loss: 66.25851| val_0_rmse: 9.64693 |  0:00:00s\n",
      "epoch 7  | loss: 58.58995| val_0_rmse: 9.25602 |  0:00:01s\n",
      "epoch 8  | loss: 52.70259| val_0_rmse: 8.86213 |  0:00:01s\n",
      "epoch 9  | loss: 46.05933| val_0_rmse: 8.46067 |  0:00:01s\n",
      "epoch 10 | loss: 38.7698 | val_0_rmse: 8.02012 |  0:00:01s\n",
      "epoch 11 | loss: 39.12524| val_0_rmse: 7.58619 |  0:00:01s\n",
      "epoch 12 | loss: 30.39873| val_0_rmse: 7.14456 |  0:00:01s\n",
      "epoch 13 | loss: 28.64372| val_0_rmse: 6.68901 |  0:00:01s\n",
      "epoch 14 | loss: 26.63823| val_0_rmse: 6.22289 |  0:00:01s\n",
      "epoch 15 | loss: 23.60553| val_0_rmse: 5.75027 |  0:00:02s\n",
      "epoch 16 | loss: 21.54768| val_0_rmse: 5.26762 |  0:00:02s\n",
      "epoch 17 | loss: 19.0323 | val_0_rmse: 4.80377 |  0:00:02s\n",
      "epoch 18 | loss: 17.42755| val_0_rmse: 4.3851  |  0:00:02s\n",
      "epoch 19 | loss: 17.54414| val_0_rmse: 3.9938  |  0:00:02s\n",
      "epoch 20 | loss: 16.19   | val_0_rmse: 3.6463  |  0:00:02s\n",
      "epoch 21 | loss: 16.16154| val_0_rmse: 3.34528 |  0:00:02s\n",
      "epoch 22 | loss: 13.68472| val_0_rmse: 3.14345 |  0:00:02s\n",
      "epoch 23 | loss: 12.69633| val_0_rmse: 2.89556 |  0:00:02s\n",
      "epoch 24 | loss: 10.12485| val_0_rmse: 2.73203 |  0:00:03s\n",
      "epoch 25 | loss: 9.75689 | val_0_rmse: 2.62829 |  0:00:03s\n",
      "epoch 26 | loss: 7.31438 | val_0_rmse: 2.56445 |  0:00:03s\n",
      "epoch 27 | loss: 5.48471 | val_0_rmse: 2.56591 |  0:00:03s\n",
      "epoch 28 | loss: 3.92754 | val_0_rmse: 2.5393  |  0:00:03s\n",
      "epoch 29 | loss: 5.28158 | val_0_rmse: 2.58153 |  0:00:03s\n",
      "epoch 30 | loss: 4.21552 | val_0_rmse: 2.5683  |  0:00:03s\n",
      "epoch 31 | loss: 4.25154 | val_0_rmse: 2.4312  |  0:00:04s\n",
      "epoch 32 | loss: 3.56572 | val_0_rmse: 2.16234 |  0:00:04s\n",
      "epoch 33 | loss: 4.4548  | val_0_rmse: 1.88223 |  0:00:04s\n",
      "epoch 34 | loss: 2.42797 | val_0_rmse: 1.49284 |  0:00:04s\n",
      "epoch 35 | loss: 2.2975  | val_0_rmse: 1.19735 |  0:00:04s\n",
      "epoch 36 | loss: 2.22697 | val_0_rmse: 1.04409 |  0:00:04s\n",
      "epoch 37 | loss: 1.98345 | val_0_rmse: 0.92029 |  0:00:04s\n",
      "epoch 38 | loss: 1.68676 | val_0_rmse: 1.04425 |  0:00:04s\n",
      "epoch 39 | loss: 1.26789 | val_0_rmse: 1.21005 |  0:00:04s\n",
      "epoch 40 | loss: 1.39635 | val_0_rmse: 1.35389 |  0:00:05s\n",
      "epoch 41 | loss: 1.60044 | val_0_rmse: 1.23216 |  0:00:05s\n",
      "epoch 42 | loss: 1.29365 | val_0_rmse: 0.95501 |  0:00:05s\n",
      "epoch 43 | loss: 1.01621 | val_0_rmse: 0.68941 |  0:00:05s\n",
      "epoch 44 | loss: 0.95668 | val_0_rmse: 0.55023 |  0:00:05s\n",
      "epoch 45 | loss: 0.96584 | val_0_rmse: 0.49409 |  0:00:05s\n",
      "epoch 46 | loss: 0.85401 | val_0_rmse: 0.59168 |  0:00:05s\n",
      "epoch 47 | loss: 0.72712 | val_0_rmse: 0.74577 |  0:00:05s\n",
      "epoch 48 | loss: 0.90587 | val_0_rmse: 0.80139 |  0:00:06s\n",
      "epoch 49 | loss: 0.83743 | val_0_rmse: 0.67238 |  0:00:06s\n",
      "epoch 50 | loss: 0.78057 | val_0_rmse: 0.50496 |  0:00:06s\n",
      "epoch 51 | loss: 0.64333 | val_0_rmse: 0.42477 |  0:00:06s\n",
      "epoch 52 | loss: 0.6944  | val_0_rmse: 0.47617 |  0:00:06s\n",
      "epoch 53 | loss: 0.57907 | val_0_rmse: 0.54711 |  0:00:06s\n",
      "epoch 54 | loss: 0.491   | val_0_rmse: 0.52978 |  0:00:06s\n",
      "epoch 55 | loss: 0.5836  | val_0_rmse: 0.53183 |  0:00:06s\n",
      "epoch 56 | loss: 0.46072 | val_0_rmse: 0.46351 |  0:00:06s\n",
      "epoch 57 | loss: 0.40483 | val_0_rmse: 0.42129 |  0:00:07s\n",
      "epoch 58 | loss: 0.38186 | val_0_rmse: 0.43251 |  0:00:07s\n",
      "epoch 59 | loss: 0.4239  | val_0_rmse: 0.48825 |  0:00:07s\n",
      "epoch 60 | loss: 0.35241 | val_0_rmse: 0.48903 |  0:00:07s\n",
      "epoch 61 | loss: 0.39076 | val_0_rmse: 0.4516  |  0:00:07s\n",
      "epoch 62 | loss: 0.36478 | val_0_rmse: 0.39871 |  0:00:07s\n",
      "epoch 63 | loss: 0.32748 | val_0_rmse: 0.45501 |  0:00:07s\n",
      "epoch 64 | loss: 0.26736 | val_0_rmse: 0.48332 |  0:00:08s\n",
      "epoch 65 | loss: 0.2721  | val_0_rmse: 0.37574 |  0:00:08s\n",
      "epoch 66 | loss: 0.31675 | val_0_rmse: 0.40083 |  0:00:08s\n",
      "epoch 67 | loss: 0.34441 | val_0_rmse: 0.47085 |  0:00:08s\n",
      "epoch 68 | loss: 0.31645 | val_0_rmse: 0.51766 |  0:00:08s\n",
      "epoch 69 | loss: 0.36699 | val_0_rmse: 0.46212 |  0:00:08s\n",
      "epoch 70 | loss: 0.35592 | val_0_rmse: 0.33082 |  0:00:08s\n",
      "epoch 71 | loss: 0.25206 | val_0_rmse: 0.33842 |  0:00:08s\n",
      "epoch 72 | loss: 0.25303 | val_0_rmse: 0.36541 |  0:00:08s\n",
      "epoch 73 | loss: 0.22236 | val_0_rmse: 0.30761 |  0:00:09s\n",
      "epoch 74 | loss: 0.2549  | val_0_rmse: 0.33145 |  0:00:09s\n",
      "epoch 75 | loss: 0.21888 | val_0_rmse: 0.36081 |  0:00:09s\n",
      "epoch 76 | loss: 0.26072 | val_0_rmse: 0.30884 |  0:00:09s\n",
      "epoch 77 | loss: 0.21626 | val_0_rmse: 0.31807 |  0:00:09s\n",
      "epoch 78 | loss: 0.15161 | val_0_rmse: 0.39572 |  0:00:09s\n",
      "epoch 79 | loss: 0.17137 | val_0_rmse: 0.35995 |  0:00:09s\n",
      "epoch 80 | loss: 0.25429 | val_0_rmse: 0.32222 |  0:00:09s\n",
      "epoch 81 | loss: 0.22994 | val_0_rmse: 0.36894 |  0:00:09s\n",
      "epoch 82 | loss: 0.158   | val_0_rmse: 0.49125 |  0:00:10s\n",
      "epoch 83 | loss: 0.20388 | val_0_rmse: 0.45857 |  0:00:10s\n",
      "epoch 84 | loss: 0.17049 | val_0_rmse: 0.29595 |  0:00:10s\n",
      "epoch 85 | loss: 0.17939 | val_0_rmse: 0.29687 |  0:00:10s\n",
      "epoch 86 | loss: 0.1812  | val_0_rmse: 0.39448 |  0:00:10s\n",
      "epoch 87 | loss: 0.23033 | val_0_rmse: 0.45213 |  0:00:10s\n",
      "epoch 88 | loss: 0.1674  | val_0_rmse: 0.37519 |  0:00:10s\n",
      "epoch 89 | loss: 0.16512 | val_0_rmse: 0.30738 |  0:00:10s\n",
      "epoch 90 | loss: 0.15414 | val_0_rmse: 0.37333 |  0:00:10s\n",
      "epoch 91 | loss: 0.13567 | val_0_rmse: 0.35519 |  0:00:11s\n",
      "epoch 92 | loss: 0.13359 | val_0_rmse: 0.2761  |  0:00:11s\n",
      "epoch 93 | loss: 0.15769 | val_0_rmse: 0.27991 |  0:00:11s\n",
      "epoch 94 | loss: 0.18002 | val_0_rmse: 0.32964 |  0:00:11s\n",
      "epoch 95 | loss: 0.1452  | val_0_rmse: 0.40222 |  0:00:11s\n",
      "epoch 96 | loss: 0.1499  | val_0_rmse: 0.40182 |  0:00:11s\n",
      "epoch 97 | loss: 0.21397 | val_0_rmse: 0.33878 |  0:00:11s\n",
      "epoch 98 | loss: 0.17421 | val_0_rmse: 0.25267 |  0:00:11s\n",
      "epoch 99 | loss: 0.14255 | val_0_rmse: 0.28614 |  0:00:12s\n",
      "epoch 100| loss: 0.20932 | val_0_rmse: 0.34293 |  0:00:12s\n",
      "epoch 101| loss: 0.19618 | val_0_rmse: 0.37015 |  0:00:12s\n",
      "epoch 102| loss: 0.17429 | val_0_rmse: 0.34242 |  0:00:12s\n",
      "epoch 103| loss: 0.16183 | val_0_rmse: 0.31397 |  0:00:12s\n",
      "epoch 104| loss: 0.13144 | val_0_rmse: 0.30075 |  0:00:12s\n",
      "epoch 105| loss: 0.14232 | val_0_rmse: 0.28557 |  0:00:12s\n",
      "epoch 106| loss: 0.20177 | val_0_rmse: 0.26336 |  0:00:12s\n",
      "epoch 107| loss: 0.10766 | val_0_rmse: 0.34156 |  0:00:12s\n",
      "epoch 108| loss: 0.12936 | val_0_rmse: 0.38944 |  0:00:13s\n",
      "epoch 109| loss: 0.17161 | val_0_rmse: 0.33449 |  0:00:13s\n",
      "epoch 110| loss: 0.12363 | val_0_rmse: 0.25712 |  0:00:13s\n",
      "epoch 111| loss: 0.12286 | val_0_rmse: 0.26852 |  0:00:13s\n",
      "epoch 112| loss: 0.12068 | val_0_rmse: 0.25842 |  0:00:13s\n",
      "epoch 113| loss: 0.08911 | val_0_rmse: 0.27932 |  0:00:13s\n",
      "epoch 114| loss: 0.10076 | val_0_rmse: 0.30866 |  0:00:13s\n",
      "epoch 115| loss: 0.10915 | val_0_rmse: 0.27583 |  0:00:13s\n",
      "epoch 116| loss: 0.11748 | val_0_rmse: 0.26395 |  0:00:13s\n",
      "epoch 117| loss: 0.11649 | val_0_rmse: 0.27828 |  0:00:14s\n",
      "epoch 118| loss: 0.1158  | val_0_rmse: 0.25388 |  0:00:14s\n",
      "epoch 119| loss: 0.0853  | val_0_rmse: 0.27837 |  0:00:14s\n",
      "epoch 120| loss: 0.11344 | val_0_rmse: 0.31122 |  0:00:14s\n",
      "epoch 121| loss: 0.16069 | val_0_rmse: 0.28305 |  0:00:14s\n",
      "epoch 122| loss: 0.10893 | val_0_rmse: 0.25235 |  0:00:14s\n",
      "epoch 123| loss: 0.07461 | val_0_rmse: 0.26466 |  0:00:14s\n",
      "epoch 124| loss: 0.08389 | val_0_rmse: 0.26699 |  0:00:14s\n",
      "epoch 125| loss: 0.07708 | val_0_rmse: 0.28899 |  0:00:14s\n",
      "epoch 126| loss: 0.09941 | val_0_rmse: 0.28303 |  0:00:15s\n",
      "epoch 127| loss: 0.08967 | val_0_rmse: 0.25319 |  0:00:15s\n",
      "epoch 128| loss: 0.10701 | val_0_rmse: 0.2714  |  0:00:15s\n",
      "epoch 129| loss: 0.07734 | val_0_rmse: 0.25648 |  0:00:15s\n",
      "epoch 130| loss: 0.0719  | val_0_rmse: 0.27067 |  0:00:15s\n",
      "epoch 131| loss: 0.08015 | val_0_rmse: 0.28889 |  0:00:15s\n",
      "epoch 132| loss: 0.08837 | val_0_rmse: 0.26404 |  0:00:15s\n",
      "epoch 133| loss: 0.07676 | val_0_rmse: 0.24846 |  0:00:15s\n",
      "epoch 134| loss: 0.06287 | val_0_rmse: 0.25837 |  0:00:16s\n",
      "epoch 135| loss: 0.10164 | val_0_rmse: 0.2584  |  0:00:16s\n",
      "epoch 136| loss: 0.06662 | val_0_rmse: 0.2589  |  0:00:16s\n",
      "epoch 137| loss: 0.05874 | val_0_rmse: 0.26279 |  0:00:16s\n",
      "epoch 138| loss: 0.06254 | val_0_rmse: 0.26239 |  0:00:16s\n",
      "epoch 139| loss: 0.07344 | val_0_rmse: 0.25212 |  0:00:16s\n",
      "epoch 140| loss: 0.07183 | val_0_rmse: 0.25602 |  0:00:16s\n",
      "epoch 141| loss: 0.07567 | val_0_rmse: 0.2492  |  0:00:16s\n",
      "epoch 142| loss: 0.0699  | val_0_rmse: 0.2622  |  0:00:16s\n",
      "epoch 143| loss: 0.07344 | val_0_rmse: 0.25904 |  0:00:17s\n",
      "epoch 144| loss: 0.06843 | val_0_rmse: 0.24599 |  0:00:17s\n",
      "epoch 145| loss: 0.06504 | val_0_rmse: 0.24434 |  0:00:17s\n",
      "epoch 146| loss: 0.10364 | val_0_rmse: 0.24604 |  0:00:17s\n",
      "epoch 147| loss: 0.07589 | val_0_rmse: 0.2485  |  0:00:17s\n",
      "epoch 148| loss: 0.05622 | val_0_rmse: 0.25159 |  0:00:17s\n",
      "epoch 149| loss: 0.07102 | val_0_rmse: 0.24791 |  0:00:17s\n",
      "epoch 150| loss: 0.06302 | val_0_rmse: 0.24157 |  0:00:17s\n",
      "epoch 151| loss: 0.05279 | val_0_rmse: 0.24086 |  0:00:17s\n",
      "epoch 152| loss: 0.05818 | val_0_rmse: 0.24069 |  0:00:18s\n",
      "epoch 153| loss: 0.05776 | val_0_rmse: 0.24235 |  0:00:18s\n",
      "epoch 154| loss: 0.05451 | val_0_rmse: 0.24562 |  0:00:18s\n",
      "epoch 155| loss: 0.0467  | val_0_rmse: 0.24455 |  0:00:18s\n",
      "epoch 156| loss: 0.04845 | val_0_rmse: 0.24255 |  0:00:18s\n",
      "epoch 157| loss: 0.0554  | val_0_rmse: 0.23814 |  0:00:18s\n",
      "epoch 158| loss: 0.05282 | val_0_rmse: 0.23861 |  0:00:18s\n",
      "epoch 159| loss: 0.05434 | val_0_rmse: 0.23996 |  0:00:18s\n",
      "epoch 160| loss: 0.05342 | val_0_rmse: 0.24417 |  0:00:19s\n",
      "epoch 161| loss: 0.05106 | val_0_rmse: 0.24884 |  0:00:19s\n",
      "epoch 162| loss: 0.05527 | val_0_rmse: 0.24208 |  0:00:19s\n",
      "epoch 163| loss: 0.05445 | val_0_rmse: 0.23592 |  0:00:19s\n",
      "epoch 164| loss: 0.04968 | val_0_rmse: 0.22957 |  0:00:19s\n",
      "epoch 165| loss: 0.05005 | val_0_rmse: 0.22721 |  0:00:19s\n",
      "epoch 166| loss: 0.0507  | val_0_rmse: 0.22399 |  0:00:19s\n",
      "epoch 167| loss: 0.04783 | val_0_rmse: 0.22621 |  0:00:19s\n",
      "epoch 168| loss: 0.04444 | val_0_rmse: 0.23068 |  0:00:20s\n",
      "epoch 169| loss: 0.04481 | val_0_rmse: 0.23206 |  0:00:20s\n",
      "epoch 170| loss: 0.04656 | val_0_rmse: 0.23077 |  0:00:20s\n",
      "epoch 171| loss: 0.04495 | val_0_rmse: 0.22782 |  0:00:20s\n",
      "epoch 172| loss: 0.04693 | val_0_rmse: 0.22386 |  0:00:20s\n",
      "epoch 173| loss: 0.03375 | val_0_rmse: 0.22511 |  0:00:20s\n",
      "epoch 174| loss: 0.04548 | val_0_rmse: 0.22551 |  0:00:20s\n",
      "epoch 175| loss: 0.04672 | val_0_rmse: 0.21978 |  0:00:20s\n",
      "epoch 176| loss: 0.03754 | val_0_rmse: 0.2184  |  0:00:21s\n",
      "epoch 177| loss: 0.04035 | val_0_rmse: 0.21563 |  0:00:21s\n",
      "epoch 178| loss: 0.03398 | val_0_rmse: 0.21824 |  0:00:21s\n",
      "epoch 179| loss: 0.03564 | val_0_rmse: 0.22112 |  0:00:21s\n",
      "epoch 180| loss: 0.04141 | val_0_rmse: 0.22423 |  0:00:21s\n",
      "epoch 181| loss: 0.03675 | val_0_rmse: 0.22109 |  0:00:21s\n",
      "epoch 182| loss: 0.03658 | val_0_rmse: 0.21683 |  0:00:21s\n",
      "epoch 183| loss: 0.03642 | val_0_rmse: 0.21497 |  0:00:21s\n",
      "epoch 184| loss: 0.03843 | val_0_rmse: 0.21514 |  0:00:21s\n",
      "epoch 185| loss: 0.0351  | val_0_rmse: 0.21605 |  0:00:22s\n",
      "epoch 186| loss: 0.03466 | val_0_rmse: 0.21567 |  0:00:22s\n",
      "epoch 187| loss: 0.0341  | val_0_rmse: 0.21359 |  0:00:22s\n",
      "epoch 188| loss: 0.03741 | val_0_rmse: 0.2197  |  0:00:22s\n",
      "epoch 189| loss: 0.04179 | val_0_rmse: 0.21724 |  0:00:22s\n",
      "epoch 190| loss: 0.03641 | val_0_rmse: 0.21655 |  0:00:22s\n",
      "epoch 191| loss: 0.03421 | val_0_rmse: 0.2203  |  0:00:22s\n",
      "epoch 192| loss: 0.03391 | val_0_rmse: 0.21539 |  0:00:22s\n",
      "epoch 193| loss: 0.03666 | val_0_rmse: 0.21512 |  0:00:22s\n",
      "epoch 194| loss: 0.03124 | val_0_rmse: 0.21659 |  0:00:23s\n",
      "epoch 195| loss: 0.03851 | val_0_rmse: 0.21096 |  0:00:23s\n",
      "epoch 196| loss: 0.03154 | val_0_rmse: 0.20983 |  0:00:23s\n",
      "epoch 197| loss: 0.0315  | val_0_rmse: 0.2096  |  0:00:23s\n",
      "epoch 198| loss: 0.05322 | val_0_rmse: 0.21473 |  0:00:23s\n",
      "epoch 199| loss: 0.03188 | val_0_rmse: 0.21115 |  0:00:23s\n",
      "Stop training because you reached max_epochs = 200 with best_epoch = 197 and best_val_0_rmse = 0.2096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "[I 2025-12-28 13:52:00,306] Trial 99 finished with value: 0.20959556667921062 and parameters: {'n_d': 48, 'n_a': 24, 'n_steps': 3, 'gamma': 1.2482312175087258, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.001, 'mask_type': 'entmax', 'lr': 0.010685443110938659, 'batch_size': 2048, 'virtual_batch_size': 128}. Best is trial 75 with value: 0.1600434375972372.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 099 | rmse_log=0.20960 | RMSE$=38,876 | MAE$=26,268 | MAPE=16.33% | n_d/n_a=48/24 steps=3 lr=0.01069 batch=2048 mask=entmax\n",
      "\n",
      " BEST RESULT\n",
      "Best rmse_log: 0.1600434375972372\n",
      "Best params: {'n_d': 24, 'n_a': 16, 'n_steps': 3, 'gamma': 1.867196997241307, 'n_independent': 2, 'n_shared': 3, 'lambda_sparse': 0.0001, 'mask_type': 'entmax', 'lr': 0.01777098127657113, 'batch_size': 512, 'virtual_batch_size': 64}\n",
      "\n",
      "Top 5 trials:\n",
      "   number  rmse_log  user_attrs_rmse_usd  user_attrs_mae_usd  user_attrs_mape\n",
      "0      75  0.160043         32632.965394        20026.878585        11.768393\n",
      "1      93  0.169799         34613.043894        23053.024173        13.006341\n",
      "2      96  0.174254         38834.707081        22019.774026        12.356991\n",
      "3      13  0.175026         35752.897625        21563.782588        12.861022\n",
      "4      57  0.175271         32794.449779        22151.293504        13.154037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 124.04973| val_0_rmse: 10.99165|  0:00:00s\n",
      "epoch 1  | loss: 89.06008| val_0_rmse: 9.9431  |  0:00:00s\n",
      "epoch 2  | loss: 59.40515| val_0_rmse: 8.74337 |  0:00:00s\n",
      "epoch 3  | loss: 38.15757| val_0_rmse: 7.40508 |  0:00:00s\n",
      "epoch 4  | loss: 23.74859| val_0_rmse: 5.91822 |  0:00:01s\n",
      "epoch 5  | loss: 14.49446| val_0_rmse: 4.49727 |  0:00:01s\n",
      "epoch 6  | loss: 11.89999| val_0_rmse: 3.49597 |  0:00:01s\n",
      "epoch 7  | loss: 9.40919 | val_0_rmse: 3.2735  |  0:00:01s\n",
      "epoch 8  | loss: 5.12708 | val_0_rmse: 3.25676 |  0:00:02s\n",
      "epoch 9  | loss: 3.27222 | val_0_rmse: 2.73516 |  0:00:02s\n",
      "epoch 10 | loss: 2.31068 | val_0_rmse: 1.89623 |  0:00:02s\n",
      "epoch 11 | loss: 1.48752 | val_0_rmse: 1.91827 |  0:00:02s\n",
      "epoch 12 | loss: 0.98775 | val_0_rmse: 1.75224 |  0:00:03s\n",
      "epoch 13 | loss: 0.87301 | val_0_rmse: 1.15747 |  0:00:03s\n",
      "epoch 14 | loss: 0.99645 | val_0_rmse: 1.19672 |  0:00:03s\n",
      "epoch 15 | loss: 0.72089 | val_0_rmse: 1.09848 |  0:00:03s\n",
      "epoch 16 | loss: 0.60768 | val_0_rmse: 0.87898 |  0:00:03s\n",
      "epoch 17 | loss: 0.55289 | val_0_rmse: 0.83893 |  0:00:04s\n",
      "epoch 18 | loss: 0.44769 | val_0_rmse: 0.91712 |  0:00:04s\n",
      "epoch 19 | loss: 0.41249 | val_0_rmse: 0.64369 |  0:00:04s\n",
      "epoch 20 | loss: 0.42459 | val_0_rmse: 0.79686 |  0:00:04s\n",
      "epoch 21 | loss: 0.37034 | val_0_rmse: 0.48618 |  0:00:05s\n",
      "epoch 22 | loss: 0.44724 | val_0_rmse: 0.81168 |  0:00:05s\n",
      "epoch 23 | loss: 0.41035 | val_0_rmse: 0.44587 |  0:00:05s\n",
      "epoch 24 | loss: 0.31928 | val_0_rmse: 1.08115 |  0:00:05s\n",
      "epoch 25 | loss: 0.38093 | val_0_rmse: 0.46043 |  0:00:05s\n",
      "epoch 26 | loss: 0.40426 | val_0_rmse: 0.7133  |  0:00:06s\n",
      "epoch 27 | loss: 0.32809 | val_0_rmse: 0.47409 |  0:00:06s\n",
      "epoch 28 | loss: 0.34675 | val_0_rmse: 0.4841  |  0:00:06s\n",
      "epoch 29 | loss: 0.26331 | val_0_rmse: 0.40301 |  0:00:06s\n",
      "epoch 30 | loss: 0.2826  | val_0_rmse: 0.54751 |  0:00:07s\n",
      "epoch 31 | loss: 0.20836 | val_0_rmse: 0.41264 |  0:00:07s\n",
      "epoch 32 | loss: 0.20897 | val_0_rmse: 0.60446 |  0:00:07s\n",
      "epoch 33 | loss: 0.24204 | val_0_rmse: 0.39281 |  0:00:07s\n",
      "epoch 34 | loss: 0.24509 | val_0_rmse: 0.50113 |  0:00:07s\n",
      "epoch 35 | loss: 0.18356 | val_0_rmse: 0.39132 |  0:00:08s\n",
      "epoch 36 | loss: 0.19041 | val_0_rmse: 0.52425 |  0:00:08s\n",
      "epoch 37 | loss: 0.17707 | val_0_rmse: 0.36114 |  0:00:08s\n",
      "epoch 38 | loss: 0.17863 | val_0_rmse: 0.51263 |  0:00:08s\n",
      "epoch 39 | loss: 0.14091 | val_0_rmse: 0.37194 |  0:00:09s\n",
      "epoch 40 | loss: 0.14292 | val_0_rmse: 0.4656  |  0:00:09s\n",
      "epoch 41 | loss: 0.14494 | val_0_rmse: 0.35195 |  0:00:09s\n",
      "epoch 42 | loss: 0.13493 | val_0_rmse: 0.54999 |  0:00:09s\n",
      "epoch 43 | loss: 0.18055 | val_0_rmse: 0.34379 |  0:00:09s\n",
      "epoch 44 | loss: 0.12357 | val_0_rmse: 0.39061 |  0:00:10s\n",
      "epoch 45 | loss: 0.13111 | val_0_rmse: 0.39766 |  0:00:10s\n",
      "epoch 46 | loss: 0.16074 | val_0_rmse: 0.41288 |  0:00:10s\n",
      "epoch 47 | loss: 0.11191 | val_0_rmse: 0.3226  |  0:00:10s\n",
      "epoch 48 | loss: 0.14168 | val_0_rmse: 0.36388 |  0:00:11s\n",
      "epoch 49 | loss: 0.12167 | val_0_rmse: 0.29938 |  0:00:11s\n",
      "epoch 50 | loss: 0.13097 | val_0_rmse: 0.41221 |  0:00:11s\n",
      "epoch 51 | loss: 0.12619 | val_0_rmse: 0.30826 |  0:00:11s\n",
      "epoch 52 | loss: 0.12613 | val_0_rmse: 0.37097 |  0:00:11s\n",
      "epoch 53 | loss: 0.10378 | val_0_rmse: 0.3073  |  0:00:12s\n",
      "epoch 54 | loss: 0.10905 | val_0_rmse: 0.38586 |  0:00:12s\n",
      "epoch 55 | loss: 0.13527 | val_0_rmse: 0.32512 |  0:00:12s\n",
      "epoch 56 | loss: 0.142   | val_0_rmse: 0.32251 |  0:00:12s\n",
      "epoch 57 | loss: 0.09975 | val_0_rmse: 0.28918 |  0:00:13s\n",
      "epoch 58 | loss: 0.08896 | val_0_rmse: 0.30547 |  0:00:13s\n",
      "epoch 59 | loss: 0.0816  | val_0_rmse: 0.34779 |  0:00:13s\n",
      "epoch 60 | loss: 0.09966 | val_0_rmse: 0.29923 |  0:00:13s\n",
      "epoch 61 | loss: 0.09957 | val_0_rmse: 0.33458 |  0:00:13s\n",
      "epoch 62 | loss: 0.10631 | val_0_rmse: 0.29603 |  0:00:14s\n",
      "epoch 63 | loss: 0.11164 | val_0_rmse: 0.31877 |  0:00:14s\n",
      "epoch 64 | loss: 0.0992  | val_0_rmse: 0.26801 |  0:00:14s\n",
      "epoch 65 | loss: 0.0904  | val_0_rmse: 0.2547  |  0:00:14s\n",
      "epoch 66 | loss: 0.08121 | val_0_rmse: 0.28477 |  0:00:15s\n",
      "epoch 67 | loss: 0.08898 | val_0_rmse: 0.26281 |  0:00:15s\n",
      "epoch 68 | loss: 0.07485 | val_0_rmse: 0.23902 |  0:00:15s\n",
      "epoch 69 | loss: 0.07417 | val_0_rmse: 0.24396 |  0:00:15s\n",
      "epoch 70 | loss: 0.07144 | val_0_rmse: 0.22611 |  0:00:15s\n",
      "epoch 71 | loss: 0.06113 | val_0_rmse: 0.25023 |  0:00:16s\n",
      "epoch 72 | loss: 0.06619 | val_0_rmse: 0.31383 |  0:00:16s\n",
      "epoch 73 | loss: 0.11103 | val_0_rmse: 0.24531 |  0:00:16s\n",
      "epoch 74 | loss: 0.07577 | val_0_rmse: 0.26926 |  0:00:16s\n",
      "epoch 75 | loss: 0.07233 | val_0_rmse: 0.24357 |  0:00:16s\n",
      "epoch 76 | loss: 0.06965 | val_0_rmse: 0.27296 |  0:00:17s\n",
      "epoch 77 | loss: 0.09358 | val_0_rmse: 0.22843 |  0:00:17s\n",
      "epoch 78 | loss: 0.05737 | val_0_rmse: 0.24378 |  0:00:17s\n",
      "epoch 79 | loss: 0.05363 | val_0_rmse: 0.22513 |  0:00:17s\n",
      "epoch 80 | loss: 0.0503  | val_0_rmse: 0.22794 |  0:00:18s\n",
      "epoch 81 | loss: 0.05369 | val_0_rmse: 0.22985 |  0:00:18s\n",
      "epoch 82 | loss: 0.06275 | val_0_rmse: 0.22439 |  0:00:18s\n",
      "epoch 83 | loss: 0.05071 | val_0_rmse: 0.24195 |  0:00:18s\n",
      "epoch 84 | loss: 0.05743 | val_0_rmse: 0.22406 |  0:00:18s\n",
      "epoch 85 | loss: 0.04777 | val_0_rmse: 0.22289 |  0:00:19s\n",
      "epoch 86 | loss: 0.05571 | val_0_rmse: 0.21825 |  0:00:19s\n",
      "epoch 87 | loss: 0.04588 | val_0_rmse: 0.23443 |  0:00:19s\n",
      "epoch 88 | loss: 0.04436 | val_0_rmse: 0.2314  |  0:00:19s\n",
      "epoch 89 | loss: 0.04445 | val_0_rmse: 0.23407 |  0:00:20s\n",
      "epoch 90 | loss: 0.04522 | val_0_rmse: 0.21302 |  0:00:20s\n",
      "epoch 91 | loss: 0.05564 | val_0_rmse: 0.23304 |  0:00:20s\n",
      "epoch 92 | loss: 0.04654 | val_0_rmse: 0.22725 |  0:00:20s\n",
      "epoch 93 | loss: 0.04954 | val_0_rmse: 0.22688 |  0:00:20s\n",
      "epoch 94 | loss: 0.04234 | val_0_rmse: 0.22413 |  0:00:21s\n",
      "epoch 95 | loss: 0.03592 | val_0_rmse: 0.20628 |  0:00:21s\n",
      "epoch 96 | loss: 0.04155 | val_0_rmse: 0.25136 |  0:00:21s\n",
      "epoch 97 | loss: 0.04765 | val_0_rmse: 0.22154 |  0:00:21s\n",
      "epoch 98 | loss: 0.04166 | val_0_rmse: 0.20725 |  0:00:22s\n",
      "epoch 99 | loss: 0.03869 | val_0_rmse: 0.22557 |  0:00:22s\n",
      "epoch 100| loss: 0.04156 | val_0_rmse: 0.21068 |  0:00:22s\n",
      "epoch 101| loss: 0.03877 | val_0_rmse: 0.20072 |  0:00:22s\n",
      "epoch 102| loss: 0.04802 | val_0_rmse: 0.20918 |  0:00:22s\n",
      "epoch 103| loss: 0.03904 | val_0_rmse: 0.1976  |  0:00:23s\n",
      "epoch 104| loss: 0.03501 | val_0_rmse: 0.21507 |  0:00:23s\n",
      "epoch 105| loss: 0.04194 | val_0_rmse: 0.19951 |  0:00:23s\n",
      "epoch 106| loss: 0.03489 | val_0_rmse: 0.20344 |  0:00:23s\n",
      "epoch 107| loss: 0.03566 | val_0_rmse: 0.20185 |  0:00:23s\n",
      "epoch 108| loss: 0.0361  | val_0_rmse: 0.19929 |  0:00:24s\n",
      "epoch 109| loss: 0.03412 | val_0_rmse: 0.19263 |  0:00:24s\n",
      "epoch 110| loss: 0.03467 | val_0_rmse: 0.19126 |  0:00:24s\n",
      "epoch 111| loss: 0.03398 | val_0_rmse: 0.20742 |  0:00:24s\n",
      "epoch 112| loss: 0.03595 | val_0_rmse: 0.19887 |  0:00:25s\n",
      "epoch 113| loss: 0.04379 | val_0_rmse: 0.19792 |  0:00:25s\n",
      "epoch 114| loss: 0.03521 | val_0_rmse: 0.20271 |  0:00:25s\n",
      "epoch 115| loss: 0.04069 | val_0_rmse: 0.25543 |  0:00:25s\n",
      "epoch 116| loss: 0.06009 | val_0_rmse: 0.21075 |  0:00:25s\n",
      "epoch 117| loss: 0.04626 | val_0_rmse: 0.25319 |  0:00:26s\n",
      "epoch 118| loss: 0.04826 | val_0_rmse: 0.2107  |  0:00:26s\n",
      "epoch 119| loss: 0.05946 | val_0_rmse: 0.23243 |  0:00:26s\n",
      "epoch 120| loss: 0.0518  | val_0_rmse: 0.2239  |  0:00:26s\n",
      "epoch 121| loss: 0.05159 | val_0_rmse: 0.22291 |  0:00:27s\n",
      "epoch 122| loss: 0.04134 | val_0_rmse: 0.22624 |  0:00:27s\n",
      "epoch 123| loss: 0.0522  | val_0_rmse: 0.21793 |  0:00:27s\n",
      "epoch 124| loss: 0.04816 | val_0_rmse: 0.21348 |  0:00:27s\n",
      "epoch 125| loss: 0.04589 | val_0_rmse: 0.22776 |  0:00:27s\n",
      "epoch 126| loss: 0.04312 | val_0_rmse: 0.2117  |  0:00:28s\n",
      "epoch 127| loss: 0.0409  | val_0_rmse: 0.22693 |  0:00:28s\n",
      "epoch 128| loss: 0.04631 | val_0_rmse: 0.21516 |  0:00:28s\n",
      "epoch 129| loss: 0.04916 | val_0_rmse: 0.22841 |  0:00:28s\n",
      "epoch 130| loss: 0.05038 | val_0_rmse: 0.20092 |  0:00:28s\n",
      "epoch 131| loss: 0.0437  | val_0_rmse: 0.24056 |  0:00:29s\n",
      "epoch 132| loss: 0.05195 | val_0_rmse: 0.18594 |  0:00:29s\n",
      "epoch 133| loss: 0.0334  | val_0_rmse: 0.20126 |  0:00:29s\n",
      "epoch 134| loss: 0.034   | val_0_rmse: 0.18291 |  0:00:29s\n",
      "epoch 135| loss: 0.03    | val_0_rmse: 0.19526 |  0:00:30s\n",
      "epoch 136| loss: 0.03489 | val_0_rmse: 0.17735 |  0:00:30s\n",
      "epoch 137| loss: 0.03281 | val_0_rmse: 0.17752 |  0:00:30s\n",
      "epoch 138| loss: 0.02858 | val_0_rmse: 0.186   |  0:00:30s\n",
      "epoch 139| loss: 0.03122 | val_0_rmse: 0.17315 |  0:00:30s\n",
      "epoch 140| loss: 0.02775 | val_0_rmse: 0.1777  |  0:00:31s\n",
      "epoch 141| loss: 0.03455 | val_0_rmse: 0.18014 |  0:00:31s\n",
      "epoch 142| loss: 0.03846 | val_0_rmse: 0.17251 |  0:00:31s\n",
      "epoch 143| loss: 0.03147 | val_0_rmse: 0.17737 |  0:00:31s\n",
      "epoch 144| loss: 0.02919 | val_0_rmse: 0.18539 |  0:00:32s\n",
      "epoch 145| loss: 0.02537 | val_0_rmse: 0.16698 |  0:00:32s\n",
      "epoch 146| loss: 0.0241  | val_0_rmse: 0.16825 |  0:00:32s\n",
      "epoch 147| loss: 0.02552 | val_0_rmse: 0.19099 |  0:00:32s\n",
      "epoch 148| loss: 0.02738 | val_0_rmse: 0.17075 |  0:00:32s\n",
      "epoch 149| loss: 0.02449 | val_0_rmse: 0.18804 |  0:00:33s\n",
      "epoch 150| loss: 0.0288  | val_0_rmse: 0.17177 |  0:00:33s\n",
      "epoch 151| loss: 0.02853 | val_0_rmse: 0.17384 |  0:00:33s\n",
      "epoch 152| loss: 0.02883 | val_0_rmse: 0.17793 |  0:00:33s\n",
      "epoch 153| loss: 0.02689 | val_0_rmse: 0.17907 |  0:00:34s\n",
      "epoch 154| loss: 0.02753 | val_0_rmse: 0.17293 |  0:00:34s\n",
      "epoch 155| loss: 0.0273  | val_0_rmse: 0.17627 |  0:00:34s\n",
      "epoch 156| loss: 0.02691 | val_0_rmse: 0.19885 |  0:00:34s\n",
      "epoch 157| loss: 0.0315  | val_0_rmse: 0.18374 |  0:00:34s\n",
      "epoch 158| loss: 0.03415 | val_0_rmse: 0.20615 |  0:00:35s\n",
      "epoch 159| loss: 0.03708 | val_0_rmse: 0.22638 |  0:00:35s\n",
      "epoch 160| loss: 0.04096 | val_0_rmse: 0.24609 |  0:00:35s\n",
      "epoch 161| loss: 0.05123 | val_0_rmse: 0.1815  |  0:00:35s\n",
      "epoch 162| loss: 0.03009 | val_0_rmse: 0.18926 |  0:00:35s\n",
      "epoch 163| loss: 0.02962 | val_0_rmse: 0.16873 |  0:00:36s\n",
      "epoch 164| loss: 0.02727 | val_0_rmse: 0.19425 |  0:00:36s\n",
      "epoch 165| loss: 0.03685 | val_0_rmse: 0.1827  |  0:00:36s\n",
      "epoch 166| loss: 0.02637 | val_0_rmse: 0.16541 |  0:00:36s\n",
      "epoch 167| loss: 0.02392 | val_0_rmse: 0.18323 |  0:00:37s\n",
      "epoch 168| loss: 0.03612 | val_0_rmse: 0.23568 |  0:00:37s\n",
      "epoch 169| loss: 0.07165 | val_0_rmse: 0.19132 |  0:00:37s\n",
      "epoch 170| loss: 0.04162 | val_0_rmse: 0.19379 |  0:00:37s\n",
      "epoch 171| loss: 0.03523 | val_0_rmse: 0.17885 |  0:00:38s\n",
      "epoch 172| loss: 0.03976 | val_0_rmse: 0.16845 |  0:00:38s\n",
      "epoch 173| loss: 0.02373 | val_0_rmse: 0.18194 |  0:00:38s\n",
      "epoch 174| loss: 0.02643 | val_0_rmse: 0.17327 |  0:00:38s\n",
      "epoch 175| loss: 0.02571 | val_0_rmse: 0.17235 |  0:00:38s\n",
      "epoch 176| loss: 0.02556 | val_0_rmse: 0.18487 |  0:00:39s\n",
      "epoch 177| loss: 0.03756 | val_0_rmse: 0.19819 |  0:00:39s\n",
      "epoch 178| loss: 0.02943 | val_0_rmse: 0.17279 |  0:00:39s\n",
      "epoch 179| loss: 0.02437 | val_0_rmse: 0.17435 |  0:00:39s\n",
      "epoch 180| loss: 0.02389 | val_0_rmse: 0.17212 |  0:00:39s\n",
      "epoch 181| loss: 0.02735 | val_0_rmse: 0.17395 |  0:00:40s\n",
      "epoch 182| loss: 0.02426 | val_0_rmse: 0.18218 |  0:00:40s\n",
      "epoch 183| loss: 0.02243 | val_0_rmse: 0.16248 |  0:00:40s\n",
      "epoch 184| loss: 0.0238  | val_0_rmse: 0.16987 |  0:00:40s\n",
      "epoch 185| loss: 0.02055 | val_0_rmse: 0.16004 |  0:00:40s\n",
      "epoch 186| loss: 0.01838 | val_0_rmse: 0.16518 |  0:00:41s\n",
      "epoch 187| loss: 0.0203  | val_0_rmse: 0.16789 |  0:00:41s\n",
      "epoch 188| loss: 0.02267 | val_0_rmse: 0.17588 |  0:00:41s\n",
      "epoch 189| loss: 0.02675 | val_0_rmse: 0.16433 |  0:00:41s\n",
      "epoch 190| loss: 0.01946 | val_0_rmse: 0.17515 |  0:00:42s\n",
      "epoch 191| loss: 0.02493 | val_0_rmse: 0.19964 |  0:00:42s\n",
      "epoch 192| loss: 0.03241 | val_0_rmse: 0.21683 |  0:00:42s\n",
      "epoch 193| loss: 0.03973 | val_0_rmse: 0.21004 |  0:00:42s\n",
      "epoch 194| loss: 0.05816 | val_0_rmse: 0.17397 |  0:00:43s\n",
      "epoch 195| loss: 0.03191 | val_0_rmse: 0.18369 |  0:00:43s\n",
      "epoch 196| loss: 0.02912 | val_0_rmse: 0.18904 |  0:00:43s\n",
      "epoch 197| loss: 0.02371 | val_0_rmse: 0.16034 |  0:00:43s\n",
      "epoch 198| loss: 0.02067 | val_0_rmse: 0.17245 |  0:00:43s\n",
      "epoch 199| loss: 0.0256  | val_0_rmse: 0.18082 |  0:00:44s\n",
      "epoch 200| loss: 0.02514 | val_0_rmse: 0.16512 |  0:00:44s\n",
      "epoch 201| loss: 0.02172 | val_0_rmse: 0.19059 |  0:00:44s\n",
      "epoch 202| loss: 0.03261 | val_0_rmse: 0.17934 |  0:00:44s\n",
      "epoch 203| loss: 0.02175 | val_0_rmse: 0.17166 |  0:00:44s\n",
      "epoch 204| loss: 0.02202 | val_0_rmse: 0.16305 |  0:00:45s\n",
      "epoch 205| loss: 0.0213  | val_0_rmse: 0.17407 |  0:00:45s\n",
      "epoch 206| loss: 0.02389 | val_0_rmse: 0.17063 |  0:00:45s\n",
      "epoch 207| loss: 0.01822 | val_0_rmse: 0.16522 |  0:00:45s\n",
      "epoch 208| loss: 0.01934 | val_0_rmse: 0.17031 |  0:00:46s\n",
      "epoch 209| loss: 0.02309 | val_0_rmse: 0.19383 |  0:00:46s\n",
      "epoch 210| loss: 0.02174 | val_0_rmse: 0.1672  |  0:00:46s\n",
      "epoch 211| loss: 0.02177 | val_0_rmse: 0.16548 |  0:00:46s\n",
      "epoch 212| loss: 0.02091 | val_0_rmse: 0.19032 |  0:00:47s\n",
      "epoch 213| loss: 0.02598 | val_0_rmse: 0.16896 |  0:00:47s\n",
      "epoch 214| loss: 0.02076 | val_0_rmse: 0.16051 |  0:00:47s\n",
      "epoch 215| loss: 0.01657 | val_0_rmse: 0.16009 |  0:00:47s\n",
      "\n",
      "Early stopping occurred at epoch 215 with best_epoch = 185 and best_val_0_rmse = 0.16004\n",
      "\n",
      " FINAL BEST MODEL PERFORMANCE\n",
      "RMSE log : 0.16004\n",
      "RMSE USD : $32,633\n",
      "MAE  USD : $20,027\n",
      "MAPE     : 11.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "def evaluate_metrics(model):\n",
    "    pred_log = model.predict(X_val_pp).reshape(-1)\n",
    "\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_val, pred_log))\n",
    "\n",
    "    y_val_usd = np.expm1(y_val)\n",
    "    pred_usd  = np.expm1(pred_log)\n",
    "\n",
    "    rmse_usd = np.sqrt(mean_squared_error(y_val_usd, pred_usd))\n",
    "    mae_usd  = mean_absolute_error(y_val_usd, pred_usd)\n",
    "    mape     = np.mean(np.abs((y_val_usd - pred_usd) / y_val_usd)) * 100\n",
    "\n",
    "    return rmse_log, rmse_usd, mae_usd, mape\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_categorical(\"n_d\", [16, 24, 32, 48, 64]),\n",
    "        \"n_a\": trial.suggest_categorical(\"n_a\", [16, 24, 32, 48, 64]),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 8),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.2, 2.0),\n",
    "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 3),\n",
    "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 3),\n",
    "        \"lambda_sparse\": trial.suggest_categorical(\"lambda_sparse\", [0.0, 1e-6, 1e-5, 1e-4, 1e-3]),\n",
    "        \"mask_type\": trial.suggest_categorical(\"mask_type\", [\"entmax\", \"sparsemax\"]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 3e-3, 2e-2, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [512, 1024, 2048]),\n",
    "        \"virtual_batch_size\": trial.suggest_categorical(\"virtual_batch_size\", [64, 128, 256]),\n",
    "    }\n",
    "\n",
    "    model = TabNetRegressor(\n",
    "        n_d=params[\"n_d\"],\n",
    "        n_a=params[\"n_a\"],\n",
    "        n_steps=params[\"n_steps\"],\n",
    "        gamma=params[\"gamma\"],\n",
    "        n_independent=params[\"n_independent\"],\n",
    "        n_shared=params[\"n_shared\"],\n",
    "        lambda_sparse=params[\"lambda_sparse\"],\n",
    "        mask_type=params[\"mask_type\"],\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=params[\"lr\"]),\n",
    "        device_name=DEVICE,\n",
    "    )\n",
    "\n",
    "    # IMPORTANT: removed verbose=0 for compatibility with your pytorch-tabnet version\n",
    "    model.fit(\n",
    "        X_train_pp, y_train_np,\n",
    "        eval_set=[(X_val_pp, y_val_np)],\n",
    "        eval_metric=[\"rmse\"],\n",
    "        max_epochs=200,\n",
    "        patience=25,\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        virtual_batch_size=params[\"virtual_batch_size\"],\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    rmse_log, rmse_usd, mae_usd, mape = evaluate_metrics(model)\n",
    "\n",
    "    trial.set_user_attr(\"rmse_usd\", float(rmse_usd))\n",
    "    trial.set_user_attr(\"mae_usd\", float(mae_usd))\n",
    "    trial.set_user_attr(\"mape\", float(mape))\n",
    "\n",
    "    print(\n",
    "        f\"Trial {trial.number:03d} | rmse_log={rmse_log:.5f} | \"\n",
    "        f\"RMSE$={rmse_usd:,.0f} | MAE$={mae_usd:,.0f} | MAPE={mape:.2f}% | \"\n",
    "        f\"n_d/n_a={params['n_d']}/{params['n_a']} steps={params['n_steps']} \"\n",
    "        f\"lr={params['lr']:.5f} batch={params['batch_size']} mask={params['mask_type']}\"\n",
    "    )\n",
    "\n",
    "    del model\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return rmse_log\n",
    "\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=10)\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=pruner)\n",
    "\n",
    "N_TRIALS = 100  # bump to 200/500 for \"crazy\"\n",
    "study.optimize(objective, n_trials=N_TRIALS, gc_after_trial=True, catch=(Exception,))\n",
    "\n",
    "print(\"\\n BEST RESULT\")\n",
    "print(\"Best rmse_log:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\", \"state\", \"user_attrs\"))\n",
    "df = df.rename(columns={\"value\": \"rmse_log\"}).sort_values(\"rmse_log\").reset_index(drop=True)\n",
    "df.to_csv(\"optuna_tabnet_trials.csv\", index=False)\n",
    "\n",
    "print(\"\\nTop 5 trials:\")\n",
    "print(df.head(5)[[\"number\", \"rmse_log\", \"user_attrs_rmse_usd\", \"user_attrs_mae_usd\", \"user_attrs_mape\"]])\n",
    "\n",
    "# Retrain best model (optional but useful)\n",
    "bp = study.best_params\n",
    "best_model = TabNetRegressor(\n",
    "    n_d=bp[\"n_d\"], n_a=bp[\"n_a\"],\n",
    "    n_steps=bp[\"n_steps\"], gamma=bp[\"gamma\"],\n",
    "    n_independent=bp[\"n_independent\"], n_shared=bp[\"n_shared\"],\n",
    "    lambda_sparse=bp[\"lambda_sparse\"], mask_type=bp[\"mask_type\"],\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=bp[\"lr\"]),\n",
    "    device_name=DEVICE\n",
    ")\n",
    "\n",
    "best_model.fit(\n",
    "    X_train_pp, y_train_np,\n",
    "    eval_set=[(X_val_pp, y_val_np)],\n",
    "    eval_metric=[\"rmse\"],\n",
    "    max_epochs=250,\n",
    "    patience=30,\n",
    "    batch_size=bp[\"batch_size\"],\n",
    "    virtual_batch_size=bp[\"virtual_batch_size\"],\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "rmse_log, rmse_usd, mae_usd, mape = evaluate_metrics(best_model)\n",
    "\n",
    "print(\"\\n FINAL BEST MODEL PERFORMANCE\")\n",
    "print(f\"RMSE log : {rmse_log:.5f}\")\n",
    "print(f\"RMSE USD : ${rmse_usd:,.0f}\")\n",
    "print(f\"MAE  USD : ${mae_usd:,.0f}\")\n",
    "print(f\"MAPE     : {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28963e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf98966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CUDA)",
   "language": "python",
   "name": "cuda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
